{
  "id": "16ed7950-6533-49ef-8b61-c829710e2b39",
  "title": "How Meta understands data at scale",
  "link": "https://engineering.fb.com/2025/04/28/security/how-meta-understands-data-at-scale/",
  "description": "Managing and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta’s vast and diverse systems make it particularly challenging to comprehend its structure, meaning, and context at scale. To address these challenges, we made substantial investments in advanced data understanding technologies, as part of [...] Read More... The post How Meta understands data at scale appeared first on Engineering at Meta.",
  "author": "",
  "published": "Mon, 28 Apr 2025 16:30:19 +0000",
  "source": "https://engineering.fb.com/feed/",
  "categories": [
    "Security \u0026 Privacy"
  ],
  "byline": "By Vasileios Lakafosis, Hannes Roth, Benjamin Renard, Wenlong Dong, Zhonghu Gao, Dave Kurtzberg",
  "length": 31397,
  "excerpt": "Managing and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta’s vast and di…",
  "siteName": "Engineering at Meta",
  "favicon": "",
  "text": "Managing and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta’s vast and diverse systems make it particularly challenging to comprehend its structure, meaning, and context at scale. To address these challenges, we made substantial investments in advanced data understanding technologies, as part of our Privacy Aware Infrastructure (PAI). Specifically, we have adopted a “shift-left” approach, integrating data schematization and annotations early in the product development process. We also created a universal privacy taxonomy, a standardized framework providing a common semantic vocabulary for data privacy management across Meta’s products that ensures quality data understanding and provides developers with reusable and efficient compliance tooling. We discovered that a flexible and incremental approach was necessary to onboard the wide variety of systems and languages used in building Meta’s products. Additionally, continuous collaboration between privacy and product teams was essential to unlock the value of data understanding at scale. We embarked on the journey of understanding data across Meta a decade ago with millions of assets in scope ranging from structured and unstructured, processed by millions of flows across many of the Meta App offerings. Over the past 10 years, Meta has cataloged millions of data assets and is classifying them daily, supporting numerous privacy initiatives across our product groups. Additionally, our continuous understanding approach ensures that privacy considerations are embedded at every stage of product development.  At Meta, we have a deep responsibility to protect the privacy of our community. We’re upholding that by investing our vast engineering capabilities into building cutting-edge privacy technology. We believe that privacy drives product innovation. This led us to develop our Privacy Aware Infrastructure (PAI), which integrates efficient and reliable privacy tools into Meta’s systems to address needs such as purpose limitation—restricting how data can be used while also unlocking opportunities for product innovation by ensuring transparency in data flows  Data understanding is an early step in PAI. It involves capturing the structure and meaning of data assets, such as tables, logs, and AI models. Over the past decade, we have gained a deeper understanding of our data, by embedding privacy considerations into every stage of product development, ensuring a more secure and responsible approach to data management. We embarked on our data understanding journey by employing heuristics and classifiers to automatically detect semantic types from user-generated content. This approach has evolved significantly over the years, enabling us to scale to millions of assets. However, conducting these processes outside of developer workflows presented challenges in terms of accuracy and timeliness. Delayed classifications often led to confusion and unnecessary work, while the results were difficult to consume and interpret. Data understanding at Meta using PAI To address shortcomings, we invested in data understanding by capturing asset structure (schematization), describing meaning (annotation), and inventorying it into OneCatalog (Meta’s system that discovers, registers, and enumerates all data assets) across all Meta technologies. We developed tools and APIs for developers to organize assets, classify data, and auto-generate annotation code. Despite significant investment, the journey was not without challenges, requiring innovative solutions and collaboration across the organization. Challenge Approach Understanding at scale (lack of foundation) At Meta, we manage hundreds of data systems and millions of assets across our family of apps. Each product features its own distinct data model, physical schema, query language, and access patterns. This diversity created a unique hurdle for offline assets: the inability to reuse schemas due to the limitations of physical table schemas in adapting to changing definitions. Specifically, renaming columns or making other modifications had far-reaching downstream implications, rendering schema evolution challenging, thus propagation required careful coordination to ensure consistency and accuracy across multiple systems and assets.  We introduced a shared asset schema format as a logical representation of the asset schema that can be translated back and forth with the system-specific format. Additionally, it offers tools to automatically classify data and send out annotation changes to asset owners for review, effectively managing long-tail systems. Inconsistent definitions (lack of shared understanding) We encountered difficulties with diverse data systems that store data in various formats, and customized data labels that made it challenging to recognize identical data elements when they are stored across multiple systems. We introduced a unified taxonomy of semantic types, which are compiled into different languages. This ensured that all systems can share the same canonical set of labels. Missing annotations (lack of quality) A solution that relied solely on data scanning and pattern matching was prone to false positives due to limited contextual information. For instance, a 64-bit integer could be misclassified as either a timestamp or a user identifier without additional context. Moreover, manual human labeling is not feasible at scale because it relies heavily on individual developers’ expertise and knowledge. We shifted left by combining schematization together with annotations in code, in addition improving and utilizing multiple classification signals. Strict measurements provided precision/recall guarantees. Protection was embedded in everything we built, without requiring every developer to be a privacy expert. Organizational barriers (lack of a unified approach) Meta’s data systems, with their bespoke schematization and practices, posed significant challenges in understanding data across the company. As we navigated complex interactions and with ever evolving privacy requirements, it became clear that fragmented approaches to data understanding hindered our ability to grasp data comprehensively. By collaborating with asset owners to develop intuitive tooling and improve coverage, we tackled adoption barriers such as poor developer experience and inaccurate classification. This effort laid the groundwork for a unified data understanding foundation, which was seamlessly integrated into the developer workflow. As a result, we drove a cultural shift towards reusable and efficient privacy practices, ultimately delivering value to product teams and fostering a more cohesive approach to data management. Walkthrough: Understanding user data for the “Beliefs” feature in Facebook Dating  To illustrate our approach and dive into the technical solution, let’s consider a scenario involving structured user data. When creating a profile on the Facebook Dating app, users have the option to include their religious views to help match with others who share similar values. On Facebook Dating, religious views are subject to purpose limitation requirements. Our five-step approach to data understanding provides a precise, end-to-end view of how we track and protect sensitive data assets, including those related to religious views: Even a simple feature can involve data being processed by dozens of heterogenous systems, making end-to-end data protection critical. To ensure comprehensive protection, it is essential to apply the necessary steps to all systems that store or process data, including distributed systems (web systems, chat, mobile and backend services) and data warehouses. Consider the data flow from online systems to the data warehouse, as shown in the diagram below. To ensure that religious belief data is identified across all these systems, we have implemented measures to prevent its use for any purpose other than the stated one. Step 1 – Schematizing As part of the PAI initiative, Meta developed DataSchema, a standard format that is used to capture the structure and relationships of all data assets, independent of system implementation. Creating a canonical representation for compliance tools. Understanding DataSchema requires grasping schematization, which defines the logical structure and relationships of data assets, specifying field names, types, metadata, and policies. Implemented using the Thrift Interface Description Language, DataSchema is compatible with Meta systems and languages. It describes over 100 million schemas across more than 100 data systems, covering granular data units like database tables, key-value stores, data streams from distributed systems (such as those used for logging), processing pipelines, and AI models. Essentially, a data asset is like a class with annotated attributes.  Let’s examine the source of truth (SoT) for a user’s dating profile schema, modeled in DataSchema. This schema includes the names and types of fields and subfields: - user_id (uint) - name (string) - age (uint) - religious_views (enum) - photos (array\u003cstruct\u003e): - url (url) - photo (blob) - caption (string) - uploaded_date (timestamp) Dating profile DataSchema The canonical SoT schema serves as the foundation for all downstream representations of the dating profile data. In practice, this schema is often translated into system-specific schemas (source of record – “SoR”), optimized for developer experience and system implementation in each environment.  Step 2 – Predicting metadata at scale Building on this schematization foundation, we used annotations to describe data, enabling us to quickly and reliably locate user data, such as religious beliefs, across Meta’s vast data landscape. This is achieved through a universal privacy taxonomy, a framework that provides a common semantic vocabulary for data privacy management across Meta’s apps. It offers a consistent language for data description and understanding, independent of specific programming languages or technologies. The universal privacy taxonomy works alongside data classification, which scans systems across Meta’s product family to ensure compliance with privacy policies. These systems use taxonomy labels to identify and classify data elements, ensuring privacy commitments are met and data is handled appropriately according to its classification. Privacy annotations are represented by taxonomy facets and their values. For example, an asset might pertain to an Actor.Employee, with data classified as SemanticType.Email and originating from DataOrigin.onsite, not a third party. The SemanticType annotation is our standard facet for describing the meaning, interpretation, or context of data, such as user names, email addresses, phone numbers, dates, or locations.  Below, we illustrate the semantic type taxonomy node for our scenario, Faith Spirituality: As data models and collected data evolve, annotations can become outdated or incorrect. Moreover, new assets may lack annotations altogether. To address this, PAI utilizes various techniques to continuously verify our understanding of data elements and maintain accurate, up-to-date annotations: Our classification system leverages machine learning models and heuristics to predict data types by sampling data, extracting features, and inferring annotation values. Efficient data sampling, such as Bernoulli sampling, and processing techniques enable scaling to billions of data elements with low-latency classifications.  Key components include: Scheduling component: manages the set of data assets to scan, accommodating different data system architectures by either pulling data via APIs or receiving data pushed directly into the scanning service. Scanning service: processes and analyzes data from various sources by accumulating samples in memory, deserializing rows (e.g., JSON) into fields and sub-fields, and extracting features using APIs available in multiple languages (C++, Python, Hack). It ensures comprehensive data capture, even for ephemeral data. Classification service: utilizes heuristic rules and machine learning models to classify data types with high accuracy. Heuristic rules: handle straightforward, deterministic cases by identifying specific data formats like dates, phone numbers, and user IDs. Machine learning models: trained on labeled datasets using supervised learning and improved through unsupervised learning to identify patterns and anomalies in unlabeled data. Ground truth calibration and verification: ensures system accuracy and reliability, allowing for model fine-tuning and improved classification performance. Lineage and propagation: We integrate classification rules with high-confidence lineage signals to ensure accurate data tracking and management. Our propagation mechanism enables the seamless annotation of data as needed, ensuring that exact copies of data across systems receive equivalent classification. This approach not only maintains data integrity but also optimizes the developer experience by streamlining the process of managing data classifications across our diverse systems. Step 3 – Annotating The integration of metadata predictions and developer input creates a comprehensive picture of a data asset’s structure (schema) and its meaning (annotation). This is achieved by attaching these elements to individual fields in data assets, providing a thorough understanding of the data. Building on the predicting data at scale initiative (step 2), where we utilize the universal privacy taxonomy and classification systems to identify and classify data elements, the generated metadata predictions are then used to help developers annotate their data assets efficiently and correctly. Portable annotation APIs: seamlessly integrate into developer workflows ensuring: Consistent representation of data across all systems at Meta. Accurate understanding of data, enabling the application of privacy safeguards at scale. Efficient evidencing of compliance with regulatory requirements. Metadata predictions and developer input: Two key components work together to create a comprehensive data asset picture: Metadata predictions: Classifiers generate predictions to aid developers in annotating data assets efficiently and correctly. If the confidence score exceeds a certain threshold, assignment can be automated, saving developer time. Developer input: Developers manually refine and verify annotations, ensuring that the data’s context and privacy requirements are accurately captured. Human oversight guarantees the accuracy and reliability of the data asset picture. - user_id (enum) → SemanticType::id_userID - name (string) → SemanticType::identity_name - age (uint) → SemanticType::age - religious_views (enum) → SemanticType::faithSpirituality - photos (array\u003cstruct\u003e): - url (url) → SemanticType::electronicID_uri_mediaURI_imageURL - photo (blob) → SemanticType::media_image - caption (string) → SemanticType::media_text_naturalLanguageText - uploaded_date (timestamp) → SemanticType::uploadedTime Ensuring complete schemas with annotations: To maintain a high standard of data understanding, we have integrated data understanding into our data model lifecycle. This includes auto-generating code to represent the schema of newly created assets when missing, ensuring that no new assets are created without a proper schema. For example, in the context of our religious beliefs in Facebook Dating, we have defined its structure, including fields like ‘Name,’ ‘EmailAddress,’ and ‘Religion.’ Furthermore, we have annotated the asset with Actor::user(), signifying that the data pertains to a user of our products. This level of detail enables us to readily identify fields containing privacy-related data and implement appropriate protective measures, such as applying the applicable purpose limitation policy. In the case of the “dating profile” data asset, we have defined its structure, including fields like ‘Name’:  final class DatingProfileSchema extends DataSchemaDefinition { \u003c\u003c__Override\u003e\u003e public function configure(ISchemaConfig $config): void { $config-\u003emetadataConfig()-\u003edescription('Represents a dating profile); $config-\u003eannotationsConfig()-\u003eannotations(Actor::user()); } \u003c\u003c__Override\u003e\u003e public function getFields(): dict\u003cstring, ISchemaField\u003e { return dict[ 'Name' =\u003e StringField::create(\"name\") -\u003eannotations(SemanticType::identity_name()) -\u003eexample('John Doe'), 'Age' =\u003e StringInt::create('age') -\u003edescription(“The age of the user.”) -\u003eannotations(SemanticType::age()) -\u003eexample('24'), 'ReligiousViews' =\u003e EnumStringField::create('religious_views') -\u003eannotations(SemanticType::faithSpirituality()) -\u003eexample('Atheist'), ]; } } In order to optimize for developer experience, the details of the schema representation differ in each environment. For example, in the data warehouse, it’s represented as a Dataset – an in-code Python class capturing the asset’s schema and metadata. Datasets provide a native API for creating data pipelines.  Here is an example of such a schema: ​​@hive_dataset( \"dim_all_dating_users\", // table name \"dating\", // namespace oncall=\"dating_analytics\", description=\"This is the primary Dating user dimension table containing one row per Dating user per day along with their profile, visitation, and key usage information.\", metadata=Metadata(Actor.User), ) class dim_all_dating_users(DataSet): ds: Varchar = Partition(\"datestamp\") userid: DatingUserID = Column(\"User id of the profile\") email: EmailAddress = Column(\"User's email address\"), age: PersonAge = Column(\"User's stated age on date ds\") religious_views: ReligionOptions = Column(\"User's provided religious views\") Our warehouse schema incorporates rich types, a privacy-aware type system designed to enhance data understanding and facilitate effective data protection. Rich types, such as DatingUserID, EmailAddress, PersonAge, and ReligionOptions, are integrated into the schema, offering a comprehensive approach to data management while encoding privacy metadata. They provide a developer-friendly way to annotate data and enable the enforcement of data quality rules and constraints at the type level, ensuring data consistency and accuracy across the warehouse. For instance, they can detect issues like joining columns with different types of user IDs or mismatched enums before code execution.  Here is an example definition: ReligionOptions = enum_from_items( \"ReligionOptions\", items=[ EnumItem(\"Atheist\", \"Atheist\"), EnumItem(\"Buddhist\", \"Buddhist\"), EnumItem(\"Christian\", \"Christian\"), EnumItem(\"Hindu\", \"Hindu\"), EnumItem(\"Jewish\", \"Jewish\"), EnumItem(\"Muslim\", \"Muslim\"), ... ], annotations=(SemanticType.faithSpirituality,), ) Step 4 – Inventorying assets and systems A central inventory system is crucial for managing data assets and their metadata, offering capabilities like search and compliance tracking. Meta’s OneCatalog is a comprehensive system that discovers, registers, and enumerates all data assets across Meta’s apps, providing inventory for easier management and tracking.  Key functions of OneCatalog: Registering all data systems: OneCatalog defines a data system as a logical abstraction over resources that persist data for a common purpose. It exhaustively examines resources across Meta’s environments to discover and register all data systems hosting data assets. Enumerating all data assets: Eligible data systems must enumerate their assets through the asset enumeration platform, generating a comprehensive list of assets and their metadata in the central inventory. These assets are grouped by “asset classes” based on shared patterns, enabling efficient management and understanding of data assets. Guarantees provided by OneCatalog: Completeness: The system regularly checks for consistency between the data defined in its configuration and the actual data stored in the inventory. This ongoing comparison ensures that all relevant data assets are accurately accounted for and up-to-date. Freshness: In addition to regularly scheduled pull-based enumeration, the system subscribes to changes in data systems and updates its inventory in real time. Uniqueness of asset ID (XID): Each asset is assigned a globally unique identifier, similar to URLs, which facilitates coordination between multiple systems and the exchange of information about assets by providing a shared key. The globally unique identifier follows a human-readable structure, e.g., asset://[asset-class]/[asset-name]. Unified UI: On top of the inventory, OneCatalog provides a unified user interface that consolidates all asset metadata, serving as the central hub for asset information. This interface offers a single point of access to view and manage assets, streamlining the process of finding and understanding data. For example, in the context of our “religious beliefs in the Dating app” scenario, we can use OneCatalog’s unified user interface to view the warehouse dating profile table asset, providing a comprehensive overview of its metadata and relationships. Compliance and privacy assurance: OneCatalog’s central inventory is utilized by various privacy teams across Meta to ensure that data assets meet requirements. With its completeness and freshness guarantees, OneCatalog serves as a reliable source of truth for privacy and compliance efforts. By providing a single view of all data assets, OneCatalog enables teams to efficiently identify and address potential risks or vulnerabilities, such as unsecured data or unauthorized access. Step 5 – Maintaining data understanding To maintain high coverage and quality of schemas and annotations across Meta’s diverse apps, we employed a robust process that involves measuring precision and recall for both predicted metadata and developer-provided annotations. This enables us to guide the implementation of our privacy and security controls and ensure their effectiveness. By leveraging data understanding, tooling can quickly build end-to-end compliance solutions. With schema and annotations now front and center, we’ve achieved continuous understanding, enabling our engineers to easily track and protect user data, implement various security and privacy controls, and build new features at scale. Our strategy for maintaining data understanding over time includes: Shifting left on creation time: We provided intuitive APIs for developers to provide metadata at asset creation time, ensuring that schemas and annotations were applied consistently in downstream use cases. Detecting and fixing annotation gaps: We surfaced prediction signals to detect coverage and quality gaps and evolved our prediction and annotation capabilities to ensure new systems and workflows were covered. Collecting ground truth: We established a baseline to measure automated systems against, with the help of subject matter experts, to continuously measure and improve them. Providing canonical consumption APIs: We developed canonical APIs for common compliance usage patterns, such as detecting user data, to ensure consistent interpretation of metadata and low entry barriers. Putting it all together Coming back to our scenario: As developers on the Facebook Dating team collect or generate new data, they utilize familiar APIs that help them schematize and annotate their data. These APIs provide a consistent and intuitive way to define the structure and meaning of the data. When collecting data related to “Faith Spirituality,”the developers use a data classifier that confirms their semantic type annotations once the data is scanned during testing. This ensures that the data is accurately labeled and can be properly handled by downstream systems. To ensure the quality of the classification system, ground truth created by subject matter experts is used to measure its accuracy. A feedback loop between the product and PAI teams keeps the unified taxonomy updated, ensuring that it remains relevant and effective. By using canonical and catalogued metadata, teams across Meta can implement privacy controls that are consistent and effective. This enables the company to maintain user trust and meet requirements. In this scenario, the developers on the Facebook Dating team are: Schematizing and annotating their data using familiar APIs. Using a data classifier to confirm semantic type annotations. Leveraging ground truth to measure the quality of the classification system. Utilizing a feedback loop to keep the unified taxonomy updated. Implementing privacy controls using canonical and catalogued metadata. Learnings and takeaways Building an understanding of all data at Meta was a monumental effort that not only required novel infrastructure but also the contribution of thousands of engineers across all teams at Meta, and years of investment. Canonical everything: Data understanding at scale relies on a canonical catalog of systems, asset classes, assets, and taxonomy labels, each with globally unique identifiers. This foundation enables an ecosystem of compliance tooling, separating the concerns of data understanding from consuming canonical metadata. Incremental and flexible approach: To tackle the challenge of onboarding hundreds of systems across Meta, we developed a platform that supports pulling schemas from existing implementations. We layered solutions to enhance existing untyped APIs, meeting developers where they are—whether in code, configuration, or a UI defining their use case and data model. This incremental and flexible approach delivers value at every step. Collaborating for data classification excellence: Building the platform was just the beginning. The infrastructure and privacy teams also collaborated with subject matter experts to develop best-in-class classifiers for our data, addressing some of the most challenging problems. These include detecting user-generated content, classifying data embedded in blobs, and creating a governed taxonomy that allows every developer to describe their data with the right level of detail. Community engagement with a tight feedback loop: Our success in backfilling schemas and integrating with the developer experience was made possible by a strong partnership with product teams. By co-building solutions and establishing an immediate feedback loop, we refined our approach, addressed misclassifications, and improved classification quality. This collaboration is crucial to our continued evolution and refinement of data understanding.  The future of data understanding Data understanding has become a crucial component of Meta’s PAI initiative, enabling us to protect user data in a sustainable and effective manner. By creating a comprehensive understanding of our data, we can address privacy challenges durably and more efficiently than traditional methods. Our approach to data understanding aligns closely with the developer workflow, involving the creation of typed data models, collection of annotated data, and processing under relevant policies. At Meta’s scale, this approach has saved significant engineering effort by automating annotation on millions of assets (i.e., fields, columns, tables) with specific labels from an inventory that are deemed commitment-critical. This automation has greatly reduced the manual effort required for annotation, allowing teams to focus on higher-priority tasks.  As data understanding continues to evolve, it is expected to have a significant impact on various aspects of operations and product offerings. Here are some potential future use cases: Improved AI and machine learning: leveraging data understanding to improve the accuracy of AI-powered content moderation and recommendation systems. Streamlined developer workflows: integrating data understanding into Meta’s internal development tools to provide clear data context and reduce confusion. Operational and developer efficiency: By automating data classification and annotation for millions of assets across Meta’s platforms, we can significantly improve operational efficiency. This automation enables us to leverage metadata for various use cases, such as accelerating product innovation. For instance, we’re now utilizing this metadata to help developers efficiently find the right data assets, streamlining their workflow and reducing the time spent on manual searches. Product innovation: With a comprehensive understanding of data, Meta can drive product innovation by leveraging insights to create personalized and engaging user experiences. While there is still more work to be done, such as evolving taxonomies to meet future compliance needs and developing novel ways to schematize data, we are excited about the potential of data understanding. By harnessing canonical metadata, we can deepen our shared understanding of data, unlocking unprecedented opportunities for innovation not only at Meta, but across the industry. Acknowledgements The authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in developing data understanding over the years. In particular, we would like to extend special thanks to (in alphabetical order) Aaron Morris, Adrian Zgorzalek, Alex Gorelik, Alex Kalinin, Alex Uslontsev, Ali Fakeri Tabrizi, Andras Belokosztolszki, Anthony O’Sullivan, Archit Jain, Aygun Aydin, Ayoade Adeniyi, Ben Warren, Bob Baldwin, Brani Stojkovic, Brian Romanko, Can Lin, Carrie (Danning) Jiang, Chao Yang, Chris Ventura, Daniel Ohayon, Danny Gagne, David Taieb, Dmitry Ponomarev, Dong Jia, Dong Zhao, Eero Neuenschwander, Fang Wang, Ferhat Sahinkaya, Ferdi Adeputra, Gayathri Aiyer, George Stasa, Guoqiang Jerry Chen, Haiyang Han, Haydar Imren, Ian Carmichael, Jared Greene, Jerry Pan, Jiang Wu, Johnnie Ballentyne, Joanna Jiang, Jonathan Bergeron, Joseph Li, Jun Fang, Kaustubh Karkare, Komal Mangtani, Kuldeep Chaudhary, Lea Li, Lei Zhang, Liu Yang, Loka Potnuru, Luiz Ribeiro, Marc Celani, Matthieu Martin, Max Mazzeo, Meg Dymek, Mike Tarasyuk, Mital Mehta, Nevzat Sevim, Nick Gardner, Nikolay Kondratyev, Oliver Dodd, Pankaj Landge, Perry Stoll, Prashanth Bandaru, Piyush Khemka, Rahul Nambiar, Rajesh Nishtala, Rituraj Kirti, Roger (Wei) Li, Rujin Cao, Sahil Garg, Satish Sampath, Sean Wang, Seth Silverman, Shridhar Iyer, Sriguru Chakravarthi, Sushaant Mujoo, Susmit Biswas, Taha Bekir Eren, Tony Harper, Vineet Chaudhary, Vishal Jain, Vitali Haravy, Vlad Fedorov, Vlad Gorelik, Wolfram Schuttle, Xiaotian Guo, Yatu Zhang, Yi Huang, Yuxi Zhang, Zejun Zhang, and Zhaohui Zhang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Avtar Brar, Brianna O’Steen, Chloe Lu, Chris Wiltz, Imogen Barnes, Jason Hendrickson, Rituraj Kirti, Xenia Habekoss and Yuri Claure. We would like to especially thank Jonathan Bergeron for overseeing the effort and providing all of the guidance and valuable feedback, and Ramnath Krishna Prasad for pulling required support together to make this blog post happen.",
  "image": "https://engineering.fb.com/wp-content/uploads/2025/04/How-Meta-understands-data-at-scale-HERO.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eManaging and understanding large-scale data ecosystems is a significant challenge for many organizations, requiring innovative solutions to efficiently safeguard user data. Meta’s vast and diverse systems make it particularly challenging to comprehend its structure, meaning, and context at scale.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eTo address these challenges, we made \u003c/span\u003e\u003ca href=\"https://about.fb.com/news/2025/01/meta-8-billion-investment-privacy/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003esubstantial\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e investments in advanced \u003c/span\u003e\u003cb\u003edata understanding \u003c/b\u003e\u003cspan\u003etechnologies, as part of our \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2025/01/22/security/how-meta-discovers-data-flows-via-lineage-at-scale/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ePrivacy Aware Infrastructure (PAI)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Specifically, we have adopted a “shift-left” approach, integrating data schematization and annotations early in the product development process. We also created a \u003c/span\u003e\u003cb\u003euniversal privacy taxonomy\u003c/b\u003e\u003cspan\u003e, a standardized framework providing a common semantic vocabulary for data privacy management across Meta’s products that ensures quality data understanding and provides developers with reusable and efficient compliance tooling.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe discovered that a flexible and incremental approach was necessary to onboard the wide variety of systems and languages used in building Meta’s products. Additionally, continuous collaboration between privacy and product teams was essential to unlock the value of data understanding at scale.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe embarked on the journey of understanding data across Meta a decade ago with millions of assets in scope ranging from structured and unstructured, processed by millions of flows across many of the Meta App offerings. Over the past 10 years, Meta has cataloged millions of data assets and is classifying them daily, supporting numerous privacy initiatives across our product groups. Additionally, our continuous understanding approach ensures that privacy considerations are embedded at every stage of product development. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eAt Meta, we have a deep responsibility to protect the privacy of our community. We’re upholding that by investing our vast engineering capabilities into building cutting-edge privacy technology. We believe that privacy drives product innovation. This led us to develop our \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ePrivacy Aware Infrastructure (PAI)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which integrates efficient and reliable privacy tools into Meta’s systems to address needs such as \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003epurpose limitation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e—restricting how data can be used while also unlocking opportunities for product innovation by ensuring transparency in data flows \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eData understanding is an early step in PAI. It involves capturing the structure and meaning of data assets, such as tables, logs, and AI models. Over the past decade, we have gained a deeper understanding of our data, by embedding privacy considerations into every stage of product development, ensuring a more secure and responsible approach to data management.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png?w=1024\" alt=\"\" width=\"1024\" height=\"315\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png?resize=916,281 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png?resize=768,236 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png?resize=1024,315 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png?resize=1536,472 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png?resize=96,29 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-1.png?resize=192,59 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe embarked on our data understanding journey by employing heuristics and \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/07/21/security/data-classification-system/\"\u003e\u003cspan\u003eclassifiers\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to automatically detect semantic types from user-generated content. This approach has evolved significantly over the years, enabling us to scale to millions of assets. However, conducting these processes outside of developer workflows presented challenges in terms of accuracy and timeliness. Delayed classifications often led to confusion and unnecessary work, while the results were difficult to consume and interpret.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eData understanding at Meta using PAI\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTo address shortcomings, we invested in \u003c/span\u003e\u003cb\u003edata understanding\u003c/b\u003e\u003cspan\u003e by capturing asset structure (\u003c/span\u003e\u003cb\u003eschematization\u003c/b\u003e\u003cspan\u003e), describing meaning (\u003c/span\u003e\u003cb\u003eannotation\u003c/b\u003e\u003cspan\u003e), and \u003c/span\u003e\u003cb\u003einventorying\u003c/b\u003e\u003cspan\u003e it into \u003c/span\u003e\u003cb\u003eOneCatalog (\u003c/b\u003e\u003cb\u003eMeta’s system that discovers, registers, and enumerates all data assets)\u003c/b\u003e \u003cspan\u003eacross all Meta technologies. We developed tools and APIs for developers to organize assets, classify data, and auto-generate annotation code. Despite significant investment, the journey was not without challenges, requiring innovative solutions and collaboration across the organization.\u003c/span\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cb\u003eChallenge\u003c/b\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cb\u003eApproach\u003c/b\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cb\u003e\u003ci\u003eUnderstanding at scale \u003c/i\u003e\u003c/b\u003e\u003ci\u003e\u003cspan\u003e(lack of foundation)\u003c/span\u003e\u003c/i\u003e\n\u003cp\u003e\u003cspan\u003eAt Meta, we manage \u003c/span\u003e\u003cb\u003ehundreds of data systems and millions of assets\u003c/b\u003e\u003cspan\u003e across our family of apps.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEach product features its own distinct data model, physical schema, query language, and access patterns. This diversity created a unique hurdle for offline assets: the inability to reuse schemas due to the limitations of physical table schemas in adapting to changing definitions. Specifically, renaming columns or making other modifications had far-reaching downstream implications, rendering schema evolution challenging, thus propagation required careful coordination to ensure consistency and accuracy across multiple systems and assets. \u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eWe introduced a \u003c/span\u003e\u003cb\u003eshared asset schema format \u003c/b\u003e\u003cspan\u003eas a logical representation of the asset schema that can be translated back and forth with the system-specific format. Additionally, it offers tools to automatically \u003c/span\u003e\u003cb\u003eclassify data\u003c/b\u003e\u003cspan\u003e and \u003c/span\u003e\u003cb\u003esend out annotation changes to asset owners for review\u003c/b\u003e\u003cspan\u003e, effectively managing long-tail systems.\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cb\u003e\u003ci\u003eInconsistent definitions \u003c/i\u003e\u003c/b\u003e\u003ci\u003e\u003cspan\u003e(lack of shared understanding)\u003c/span\u003e\u003c/i\u003e\n\u003cp\u003e\u003cspan\u003eWe encountered difficulties with \u003c/span\u003e\u003cb\u003ediverse data systems\u003c/b\u003e\u003cspan\u003e that store data in various formats, and \u003c/span\u003e\u003cb\u003ecustomized data labels\u003c/b\u003e\u003cspan\u003e that made it challenging to recognize identical data elements when they are stored across multiple systems.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eWe introduced a unified\u003c/span\u003e\u003cb\u003e taxonomy of semantic types\u003c/b\u003e\u003cspan\u003e, which are compiled into different languages. This ensured that all systems can share the same canonical set of labels.\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cb\u003e\u003ci\u003eMissing annotations \u003c/i\u003e\u003c/b\u003e\u003ci\u003e\u003cspan\u003e(lack of quality)\u003c/span\u003e\u003c/i\u003e\n\u003cp\u003e\u003cspan\u003eA solution that relied solely on data scanning and pattern matching was prone to false positives due to \u003c/span\u003e\u003cb\u003elimited contextual information\u003c/b\u003e\u003cspan\u003e. For instance, a 64-bit integer could be misclassified as either a timestamp or a user identifier without additional context. Moreover, manual human labeling is \u003c/span\u003e\u003cb\u003enot feasible at scale\u003c/b\u003e\u003cspan\u003e because it relies heavily on individual developers’ expertise and knowledge.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eWe shifted left by combining \u003c/span\u003e\u003cb\u003eschematization\u003c/b\u003e\u003cspan\u003e together with \u003c/span\u003e\u003cb\u003eannotations in code\u003c/b\u003e\u003cspan\u003e, in addition improving and utilizing \u003c/span\u003e\u003cb\u003emultiple classification signals\u003c/b\u003e\u003cspan\u003e. Strict measurements provided precision/recall guarantees. Protection was embedded in everything we built, without requiring every developer to be a privacy expert.\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cb\u003e\u003ci\u003eOrganizational barriers\u003c/i\u003e\u003c/b\u003e\u003ci\u003e\u003cspan\u003e (lack of a unified approach)\u003c/span\u003e\u003c/i\u003e\n\u003cp\u003e\u003cspan\u003eMeta’s data systems, with their bespoke schematization and practices, posed significant challenges in understanding data across the company. As we navigated complex interactions and with ever evolving privacy requirements, it became clear that fragmented approaches to data understanding hindered our ability to grasp data comprehensively.\u003c/span\u003e\u003c/p\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cspan\u003eBy collaborating with asset owners to develop intuitive tooling and improve coverage, we tackled adoption barriers such as poor developer experience and inaccurate classification. This effort laid the groundwork for a unified data understanding foundation, which was seamlessly integrated into the developer workflow. As a result, we drove a cultural shift towards reusable and efficient privacy practices, ultimately delivering value to product teams and fostering a more cohesive approach to data management.\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2\u003eWalkthrough\u003cspan\u003e: Understanding user data for the “Beliefs” feature in Facebook Dating \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTo illustrate our approach and dive into the technical solution, let’s consider a scenario involving structured user data. When creating a profile on the Facebook Dating app, users have the option to include their religious views to help match with others who share similar values.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png?w=1024\" alt=\"\" width=\"1024\" height=\"825\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png?resize=916,738 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png?resize=768,619 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png?resize=1024,825 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png?resize=1536,1237 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png?resize=96,77 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-2.png?resize=192,155 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOn Facebook Dating, religious views are subject to purpose limitation requirements. \u003c/span\u003e\u003cspan\u003eOur five-step approach to data understanding provides a precise, end-to-end view of how we track and protect sensitive data assets, including those related to religious views:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?w=1024\" alt=\"\" width=\"1024\" height=\"281\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=916,251 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=768,211 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=1024,281 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=1536,421 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=96,26 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=192,53 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEven a simple feature can involve data being processed by dozens of heterogenous systems, making end-to-end data protection critical. To ensure comprehensive protection, it is essential to apply the necessary steps to all systems that store or process data, including \u003c/span\u003e\u003cb\u003edistributed systems\u003c/b\u003e\u003cspan\u003e (web systems, chat, mobile and backend services) and \u003c/span\u003e\u003cb\u003edata warehouses\u003c/b\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eConsider the data flow from online systems to the data warehouse, as shown in the diagram below. To ensure that religious belief data is identified across all these systems, we have implemented measures to \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2024/08/27/security/privacy-aware-infrastructure-purpose-limitation-meta/#:~:text=Continuously%20enforce%20and%20monitor%20data%20flows\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eprevent its use for any purpose other than the stated one\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?w=1024\" alt=\"\" width=\"1024\" height=\"575\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png 1770w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?resize=916,514 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?resize=768,431 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?resize=1024,575 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?resize=1536,863 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-4b.png?resize=192,108 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eStep 1 – Schematizing\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eAs part of the PAI initiative, Meta developed DataSchema, a standard format that is used to capture the structure and relationships of all data assets, independent of system implementation. Creating a canonical representation for compliance tools. Understanding DataSchema requires grasping \u003c/span\u003e\u003cb\u003eschematization\u003c/b\u003e\u003cspan\u003e, which defines the logical structure and relationships of data assets, specifying field names, types, metadata, and policies.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eImplemented using the \u003c/span\u003e\u003ca href=\"https://thrift.apache.org/docs/idl.html\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eThrift Interface Description Language\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, DataSchema is compatible with Meta systems and languages. It describes over 100 million schemas across more than 100 data systems, covering granular data units like database tables, key-value stores, data streams from distributed systems (such as those used for logging), processing pipelines, and AI models. Essentially, a data asset is like a class with annotated attributes. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLet’s examine the source of truth (SoT) for a user’s dating profile schema, modeled in DataSchema. This schema includes the names and types of fields and subfields:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  - user_id (uint)\n  - name (string)\n  - age (uint)\n  - religious_views (enum)\n  - photos (array\u0026lt;struct\u0026gt;):\n     - url (url)\n     - photo (blob)\n     - caption (string)\n     - uploaded_date (timestamp)\n\u003c/code\u003e\u003ci\u003eDating\u003c/i\u003e\u003ci\u003e pr\u003c/i\u003e\u003ci\u003eofile\u003c/i\u003e\u003ci\u003e DataSchema\u003c/i\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eThe canonical SoT schema serves as the foundation for all downstream representations of the dating profile data. In practice, this schema is often translated into system-specific schemas (source of record – “SoR”), optimized for developer experience and system implementation in each environment. \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eStep 2 – Predicting metadata at scale\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eBuilding on this schematization foundation, we used annotations to describe data, enabling us to quickly and reliably locate user data, such as religious beliefs, across Meta’s vast data landscape. This is achieved through a universal\u003c/span\u003e\u003cb\u003e privacy taxonomy\u003c/b\u003e\u003cspan\u003e, a framework that provides a common semantic vocabulary for data privacy management across Meta’s apps. It offers a consistent language for data description and understanding, independent of specific programming languages or technologies.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe \u003c/span\u003e\u003cb\u003eu\u003c/b\u003e\u003cspan\u003eniversal privacy taxonomy works alongside \u003c/span\u003e\u003cb\u003edata classification\u003c/b\u003e\u003cspan\u003e, which scans systems across Meta’s product family to ensure compliance with privacy policies. These systems use taxonomy labels to identify and classify data elements, ensuring privacy commitments are met and data is handled appropriately according to its classification.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003ePrivacy annotations\u003c/b\u003e\u003cspan\u003e are represented by taxonomy \u003c/span\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Faceted_classification\"\u003e\u003cspan\u003efacets\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and their values. For example, an asset might pertain to an \u003c/span\u003e\u003cspan\u003eActor.Employee\u003c/span\u003e\u003cspan\u003e, with data classified as \u003c/span\u003e\u003cspan\u003eS\u003cspan\u003eemanticType.Email\u003c/span\u003e\u003c/span\u003e\u003cspan\u003e and originating from \u003c/span\u003e\u003cspan\u003eDataOrigin.onsite\u003c/span\u003e\u003cspan\u003e, not a third party. The \u003c/span\u003e\u003cspan\u003eSemanticType\u003c/span\u003e\u003cspan\u003e annotation is our standard facet for describing the meaning, interpretation, or context of data, such as user names, email addresses, phone numbers, dates, or locations. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBelow, we illustrate the semantic type taxonomy node for our scenario, \u003c/span\u003e\u003cspan\u003eFaith Spirituality\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png?w=1024\" alt=\"\" width=\"1024\" height=\"490\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png?resize=916,438 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png?resize=768,367 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png?resize=1024,490 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png?resize=1536,735 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png?resize=96,46 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-5.png?resize=192,92 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs data models and collected data evolve, annotations can become outdated or incorrect. Moreover, new assets may lack annotations altogether. To address this, PAI utilizes various techniques to continuously verify our understanding of data elements and maintain accurate, up-to-date annotations:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png?w=1024\" alt=\"\" width=\"1024\" height=\"588\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png?resize=916,526 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png?resize=768,441 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png?resize=1024,588 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png?resize=1536,881 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png?resize=96,55 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-6.png?resize=192,110 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur classification system leverages \u003c/span\u003e\u003cb\u003emachine learning models and heuristics\u003c/b\u003e\u003cspan\u003e to predict data types by sampling data, extracting features, and inferring annotation values. Efficient data sampling, such as Bernoulli sampling, and processing techniques enable scaling to billions of data elements with low-latency classifications. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eKey components include:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eScheduling component\u003c/b\u003e\u003cspan\u003e: manages the set of data assets to scan, accommodating different data system architectures by either pulling data via APIs or receiving data pushed directly into the scanning service.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eScanning service\u003c/b\u003e\u003cspan\u003e: processes and analyzes data from various sources by accumulating samples in memory, deserializing rows (e.g., JSON) into fields and sub-fields, and extracting features using APIs available in multiple languages (C++, Python, Hack). It ensures comprehensive data capture, even for ephemeral data.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eClassification service\u003c/b\u003e\u003cspan\u003e: utilizes heuristic rules and machine learning models to classify data types with high accuracy.\u003c/span\u003e\n\u003cul\u003e\n\u003cli aria-level=\"2\"\u003e\u003cb\u003eHeuristic rules\u003c/b\u003e\u003cspan\u003e: handle straightforward, deterministic cases by identifying specific data formats like dates, phone numbers, and user IDs.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"2\"\u003e\u003cb\u003eMachine learning models\u003c/b\u003e\u003cspan\u003e: trained on labeled datasets using supervised learning and improved through unsupervised learning to identify patterns and anomalies in unlabeled data.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"2\"\u003e\u003cb\u003eGround truth calibration and verification\u003c/b\u003e\u003cspan\u003e: ensures system accuracy and reliability, allowing for model fine-tuning and improved classification performance.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eLineage and propagation: \u003c/b\u003e\u003cspan\u003eWe integrate classification rules with high-confidence lineage signals to ensure accurate data tracking and management. Our propagation mechanism enables the seamless annotation of data as needed, ensuring that exact copies of data across systems receive equivalent classification. This approach not only maintains data integrity but also optimizes the developer experience by streamlining the process of managing data classifications across our diverse systems.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cspan\u003eStep 3 – Annotating\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThe integration of metadata predictions and developer input creates a comprehensive picture of a data asset’s structure (schema) and its meaning (annotation). This is achieved by attaching these elements to individual fields in data assets, providing a thorough understanding of the data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBuilding on the\u003c/span\u003e \u003cspan\u003epredicting data at scale initiative (step 2), where we utilize the \u003c/span\u003e\u003cb\u003eu\u003c/b\u003e\u003cspan\u003eniversal privacy taxonomy and classification systems to identify and classify data elements, the generated metadata predictions are then used to help developers annotate their data assets efficiently and correctly.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003ePortable annotation APIs: \u003c/b\u003e\u003cspan\u003eseamlessly integrate into developer workflows ensuring:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eConsistent representation of data across all systems at Meta.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAccurate understanding of data, enabling the application of privacy safeguards at scale.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eEfficient evidencing of compliance with regulatory requirements.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003eMetadata predictions and developer input: \u003c/b\u003e\u003cspan\u003eTwo key components work together to create a comprehensive data asset picture:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eMetadata predictions\u003c/b\u003e\u003cspan\u003e: Classifiers generate predictions to aid developers in annotating data assets efficiently and correctly. If the confidence score exceeds a certain threshold, assignment can be automated, saving developer time.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eDeveloper input\u003c/b\u003e\u003cspan\u003e: Developers manually refine and verify annotations, ensuring that the data’s context and privacy requirements are accurately captured. Human oversight guarantees the accuracy and reliability of the data asset picture.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e- user_id (enum) \t\t→ SemanticType::id_userID\n  - name (string) \t\t\t→ SemanticType::identity_name\n  - age (uint) \t\t\t→ SemanticType::age\n  - religious_views (enum) \t→ SemanticType::faithSpirituality\n  - photos (array\u0026lt;struct\u0026gt;):\n     - url (url) \t\t\t→ SemanticType::electronicID_uri_mediaURI_imageURL\n     - photo (blob) \t\t→ SemanticType::media_image\n     - caption (string) \t\t→ SemanticType::media_text_naturalLanguageText\n     - uploaded_date (timestamp) → SemanticType::uploadedTime\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ci\u003e\u003c/i\u003e\u003cb\u003eEnsuring complete schemas with annotations: \u003c/b\u003e\u003cspan\u003eTo maintain a high standard of data understanding, we have integrated data understanding into our data model lifecycle. This includes auto-generating code to represent the schema of newly created assets when missing, ensuring that no new assets are created without a proper schema.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor example, in the context of our religious beliefs in Facebook Dating, we have defined its structure, including fields like ‘\u003c/span\u003e\u003cspan\u003eName\u003c/span\u003e\u003cspan\u003e,’ ‘\u003c/span\u003e\u003cspan\u003eEmailAddress\u003c/span\u003e\u003cspan\u003e,’ and ‘\u003c/span\u003e\u003cspan\u003eReligion\u003c/span\u003e\u003cspan\u003e.’ Furthermore, we have annotated the asset with \u003c/span\u003e\u003cspan\u003eActor::user()\u003c/span\u003e\u003cspan\u003e, signifying that the data pertains to a user of our products. This level of detail enables us to readily identify fields containing privacy-related data and implement appropriate protective measures, such as applying the applicable purpose limitation policy.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn the case of the “dating profile” data asset, we have defined its structure, including fields like ‘\u003c/span\u003e\u003cspan\u003eName\u003c/span\u003e\u003cspan\u003e’: \u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efinal class DatingProfileSchema extends DataSchemaDefinition {\n\n \u0026lt;\u0026lt;__Override\u0026gt;\u0026gt;\n public function configure(ISchemaConfig $config): void {\n   $config-\u0026gt;metadataConfig()-\u0026gt;description(\u0026#39;Represents a dating profile);\n   $config-\u0026gt;annotationsConfig()-\u0026gt;annotations(Actor::user());\n }\n\n \u0026lt;\u0026lt;__Override\u0026gt;\u0026gt;\n public function getFields(): dict\u0026lt;string, ISchemaField\u0026gt; {\n   return dict[\n     \u0026#39;Name\u0026#39; =\u0026gt; StringField::create(\u0026#34;name\u0026#34;)\n        -\u0026gt;annotations(SemanticType::identity_name())\n        -\u0026gt;example(\u0026#39;John Doe\u0026#39;),\n     \u0026#39;Age\u0026#39; =\u0026gt; StringInt::create(\u0026#39;age\u0026#39;)\n        -\u0026gt;description(“The age of the user.”)\n        -\u0026gt;annotations(SemanticType::age())\n        -\u0026gt;example(\u0026#39;24\u0026#39;),\n     \u0026#39;ReligiousViews\u0026#39; =\u0026gt; EnumStringField::create(\u0026#39;religious_views\u0026#39;)\n        -\u0026gt;annotations(SemanticType::faithSpirituality())\n        -\u0026gt;example(\u0026#39;Atheist\u0026#39;),\n   ];\n }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eIn order to optimize for developer experience, the details of the schema representation differ in each environment. For example, in the data warehouse, it’s represented as a Dataset – an in-code Python class capturing the asset’s schema and metadata. Datasets provide a native API for creating data pipelines. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere is an example of such a schema:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e​​@hive_dataset(\n\t\u0026#34;dim_all_dating_users\u0026#34;, // table name\n\t\u0026#34;dating\u0026#34;, // namespace\n\toncall=\u0026#34;dating_analytics\u0026#34;,\n\tdescription=\u0026#34;This is the primary Dating user dimension table containing one row per Dating user per day along with their profile, visitation, and key usage information.\u0026#34;,\n\tmetadata=Metadata(Actor.User),\n)\nclass dim_all_dating_users(DataSet):\nds: Varchar = Partition(\u0026#34;datestamp\u0026#34;)\nuserid: DatingUserID = Column(\u0026#34;User id of the profile\u0026#34;)\nemail: EmailAddress = Column(\u0026#34;User\u0026#39;s email address\u0026#34;),\n\tage: PersonAge = Column(\u0026#34;User\u0026#39;s stated age on date ds\u0026#34;)\n\treligious_views: ReligionOptions = Column(\u0026#34;User\u0026#39;s provided religious views\u0026#34;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eOur warehouse schema incorporates \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cb\u003erich types\u003c/b\u003e\u003c/a\u003e\u003cspan\u003e, a privacy-aware type system designed to enhance data understanding and facilitate effective data protection. Rich types, such as \u003c/span\u003e\u003cspan\u003eDatingUserID\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003eEmailAddress\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003e\u003cspan\u003ePersonAge\u003c/span\u003e,\u003c/span\u003e\u003cspan\u003e and \u003c/span\u003e\u003cspan\u003eReligionOptions\u003c/span\u003e\u003cspan\u003e, are integrated into the schema, offering a comprehensive approach to data management while encoding privacy metadata. They provide a developer-friendly way to annotate data and enable the enforcement of data quality rules and constraints at the type level, ensuring data consistency and accuracy across the warehouse. For instance, they can \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/#:~:text=Enhanced%20type%2Dchecking\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003edetect issues like joining columns with different types of user IDs\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e or mismatched enums before code execution. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere is an example definition:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eReligionOptions = enum_from_items(\n    \u0026#34;ReligionOptions\u0026#34;,\n    items=[\n        EnumItem(\u0026#34;Atheist\u0026#34;, \u0026#34;Atheist\u0026#34;),\n        EnumItem(\u0026#34;Buddhist\u0026#34;, \u0026#34;Buddhist\u0026#34;),\n        EnumItem(\u0026#34;Christian\u0026#34;, \u0026#34;Christian\u0026#34;),\n        EnumItem(\u0026#34;Hindu\u0026#34;, \u0026#34;Hindu\u0026#34;),\n        EnumItem(\u0026#34;Jewish\u0026#34;, \u0026#34;Jewish\u0026#34;),\n        EnumItem(\u0026#34;Muslim\u0026#34;, \u0026#34;Muslim\u0026#34;),\n  ...\n    ],\n    annotations=(SemanticType.faithSpirituality,),\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\u003cspan\u003eStep 4 – Inventorying assets and systems\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eA central inventory system is crucial for managing data assets and their metadata, offering capabilities like search and compliance tracking. Meta’s \u003c/span\u003e\u003cb\u003eOneCatalog\u003c/b\u003e\u003cspan\u003e is a comprehensive system that discovers, registers, and enumerates all data assets across Meta’s apps, providing inventory for easier management and tracking. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eKey functions of OneCatalog:\u003c/b\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eRegistering all data systems\u003c/b\u003e\u003cspan\u003e: OneCatalog defines a data system as a logical abstraction over resources that persist data for a common purpose. It exhaustively examines resources across Meta’s environments to discover and register all data systems hosting data assets.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eEnumerating all data assets\u003c/b\u003e\u003cspan\u003e: Eligible data systems must enumerate their assets through the asset enumeration platform, generating a comprehensive list of assets and their metadata in the central inventory. These assets are grouped by “asset classes” based on shared patterns, enabling efficient management and understanding of data assets.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png?w=1024\" alt=\"\" width=\"1024\" height=\"472\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png 1926w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png?resize=916,422 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png?resize=768,354 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png?resize=1024,472 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png?resize=1536,707 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png?resize=96,44 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-7b.png?resize=192,88 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eGuarantees provided by OneCatalog:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eCompleteness\u003c/b\u003e\u003cspan\u003e: The system regularly checks for consistency between the data defined in its configuration and the actual data stored in the inventory. This ongoing comparison ensures that all relevant data assets are accurately accounted for and up-to-date.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eFreshness\u003c/b\u003e\u003cspan\u003e: In addition to regularly scheduled pull-based enumeration, the system subscribes to changes in data systems and updates its inventory in real time.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eUniqueness of asset ID (XID)\u003c/b\u003e\u003cspan\u003e: Each asset is assigned a globally unique identifier, similar to URLs, which facilitates coordination between multiple systems and the exchange of information about assets by providing a shared key. The globally unique identifier follows a human-readable structure, e.g., \u003c/span\u003e\u003cb\u003easset://[asset-class]/[asset-name].\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png?w=1024\" alt=\"\" width=\"1024\" height=\"347\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png 1977w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png?resize=916,310 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png?resize=768,260 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png?resize=1024,347 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png?resize=1536,520 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png?resize=96,32 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-8b.png?resize=192,65 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eUnified UI: \u003c/b\u003e\u003cspan\u003eOn top of the inventory, OneCatalog provides a unified user interface that consolidates all asset metadata, serving as the central hub for asset information. This interface offers a single point of access to view and manage assets, streamlining the process of finding and understanding data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor example, in the context of our “religious beliefs in the Dating app” scenario, we can use OneCatalog’s unified user interface to view the warehouse dating profile table asset, providing a comprehensive overview of its metadata and relationships.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-9.png?w=1024\" alt=\"\" width=\"1024\" height=\"820\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-9.png 1486w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-9.png?resize=916,734 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-9.png?resize=768,615 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-9.png?resize=1024,820 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-9.png?resize=96,77 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-9.png?resize=192,154 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eCompliance and privacy assurance: \u003c/b\u003e\u003cspan\u003eOneCatalog’s central inventory is utilized by various privacy teams across Meta to ensure that data assets meet requirements. With its completeness and freshness guarantees, OneCatalog serves as a reliable source of truth for privacy and compliance efforts.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBy providing a single view of all data assets, OneCatalog enables teams to efficiently identify and address potential risks or vulnerabilities, such as unsecured data or unauthorized access.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eStep 5 – Maintaining data understanding\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eTo maintain high coverage and quality of schemas and annotations across Meta’s diverse apps, we employed a robust process that involves measuring precision and recall for both predicted metadata and developer-provided annotations. This enables us to guide the implementation of our privacy and security controls and ensure their effectiveness.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBy leveraging data understanding, tooling can quickly build end-to-end compliance solutions. With schema and annotations now front and center, we’ve achieved continuous understanding, enabling our engineers to easily track and protect user data, implement various security and privacy controls, and build new features at scale.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur strategy for maintaining data understanding over time includes:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eShifting left on creation time\u003c/b\u003e\u003cspan\u003e: We provided intuitive APIs for developers to provide metadata at asset creation time, ensuring that schemas and annotations were applied consistently in downstream use cases.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eDetecting and fixing annotation gaps\u003c/b\u003e\u003cspan\u003e: We surfaced prediction signals to detect coverage and quality gaps and evolved our prediction and annotation capabilities to ensure new systems and workflows were covered.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eCollecting ground truth\u003c/b\u003e\u003cspan\u003e: We established a baseline to measure automated systems against, with the help of subject matter experts, to continuously measure and improve them.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eProviding canonical consumption APIs\u003c/b\u003e\u003cspan\u003e: We developed canonical APIs for common compliance usage patterns, such as detecting user data, to ensure consistent interpretation of metadata and low entry barriers.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003ePutting it all together\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eComing back to our scenario: As developers on the Facebook Dating team collect or generate new data, they utilize familiar APIs that help them schematize and annotate their data. These APIs provide a consistent and intuitive way to define the structure and meaning of the data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhen collecting data related to “Faith Spirituality,”the developers use a data classifier that confirms their semantic type annotations once the data is scanned during testing. This ensures that the data is accurately labeled and can be properly handled by downstream systems.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo ensure the quality of the classification system, ground truth created by subject matter experts is used to measure its accuracy. A feedback loop between the product and PAI teams keeps the unified taxonomy updated, ensuring that it remains relevant and effective.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBy using canonical and catalogued metadata, teams across Meta can implement privacy controls that are consistent and effective. This enables the company to maintain user trust and meet requirements.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn this scenario, the developers on the Facebook Dating team are:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?w=1024\" alt=\"\" width=\"1024\" height=\"281\" srcset=\"https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png 1999w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=916,251 916w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=768,211 768w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=1024,281 1024w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=1536,421 1536w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=96,26 96w, https://engineering.fb.com/wp-content/uploads/2025/04/Data-understand-Meta_image-3.png?resize=192,53 192w\" sizes=\"auto, (max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSchematizing and annotating their data using familiar APIs.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUsing a data classifier to confirm semantic type annotations.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eLeveraging ground truth to measure the quality of the classification system.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUtilizing a feedback loop to keep the unified taxonomy updated.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eImplementing privacy controls using canonical and catalogued metadata.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eLearnings and takeaways\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eBuilding an understanding of all data at Meta was a monumental effort that not only required novel infrastructure but also the contribution of thousands of engineers across all teams at Meta, and years of investment.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eCanonical everything\u003c/b\u003e\u003cspan\u003e: Data understanding at scale relies on a canonical catalog of systems, asset classes, assets, and taxonomy labels, each with globally unique identifiers. This foundation enables an ecosystem of compliance tooling, separating the concerns of data understanding from consuming canonical metadata.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eIncremental and flexible approach\u003c/b\u003e\u003cspan\u003e: To tackle the challenge of onboarding hundreds of systems across Meta, we developed a platform that supports pulling schemas from existing implementations. We layered \u003c/span\u003e\u003ca href=\"https://developers.facebook.com/blog/post/2021/04/26/eli5-ent-schema-as-code-go/\"\u003e\u003cspan\u003esolutions to enhance existing untyped APIs\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, meeting developers where they are—whether in code, configuration, or a UI defining their use case and data model. This incremental and flexible approach delivers value at every step.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eCollaborating for data classification excellence\u003c/b\u003e\u003cspan\u003e: Building the platform was just the beginning. The infrastructure and privacy teams also collaborated with subject matter experts to develop best-in-class classifiers for our data, addressing some of the most challenging problems. These include detecting user-generated content, classifying data embedded in blobs, and creating a governed taxonomy that allows every developer to describe their data with the right level of detail.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eCommunity engagement with a tight feedback loop\u003c/b\u003e\u003cspan\u003e: Our success in backfilling schemas and integrating with the developer experience was made possible by a strong partnership with product teams. By co-building solutions and establishing an immediate feedback loop, we refined our approach, addressed misclassifications, and improved classification quality. This collaboration is crucial to our continued evolution and refinement of data understanding. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eThe future of data understanding\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eData understanding has become a crucial component of Meta’s PAI initiative, enabling us to protect user data in a sustainable and effective manner. By creating a comprehensive understanding of our data, we can address privacy challenges durably and more efficiently than traditional methods.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur approach to data understanding aligns closely with the developer workflow, involving the creation of typed data models, collection of annotated data, and processing under relevant policies. At Meta’s scale, this approach has saved significant engineering effort by automating annotation on millions of assets (i.e., fields, columns, tables) with specific labels from an inventory that are deemed commitment-critical. This automation has greatly reduced the manual effort required for annotation, allowing teams to focus on higher-priority tasks. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs data understanding continues to evolve, it is expected to have a significant impact on various aspects of operations and product offerings. Here are some potential future use cases:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eImproved AI and machine learning\u003c/b\u003e\u003cspan\u003e: leveraging data understanding to improve the accuracy of AI-powered content moderation and recommendation systems.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eStreamlined developer workflows\u003c/b\u003e\u003cspan\u003e: integrating data understanding into Meta’s internal development tools to provide clear data context and reduce confusion.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eOperational and developer efficiency\u003c/b\u003e\u003cspan\u003e: By automating data classification and annotation for millions of assets across Meta’s platforms, we can significantly improve operational efficiency. This automation enables us to leverage metadata for various use cases, such as accelerating product innovation. For instance, we’re now utilizing this metadata to help developers efficiently find the right data assets, streamlining their workflow and reducing the time spent on manual searches.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eProduct innovation\u003c/b\u003e\u003cspan\u003e: With a comprehensive understanding of data, Meta can drive product innovation by leveraging insights to create personalized and engaging user experiences.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eWhile there is still more work to be done, such as evolving taxonomies to meet future compliance needs and developing novel ways to schematize data, we are excited about the potential of data understanding. By harnessing canonical metadata, we can deepen our shared understanding of data, unlocking unprecedented opportunities for innovation not only at Meta, but across the industry.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAcknowledgements\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eThe authors would like to acknowledge the contributions of many current and former Meta employees who have played a crucial role in developing data understanding over the years. In particular, we would like to extend special thanks to (in alphabetical order) Aaron Morris, Adrian Zgorzalek, Alex Gorelik, Alex Kalinin, Alex Uslontsev, Ali Fakeri Tabrizi, Andras Belokosztolszki, Anthony O’Sullivan, Archit Jain, Aygun Aydin, Ayoade Adeniyi, Ben Warren, Bob Baldwin, Brani Stojkovic, Brian Romanko, Can Lin, Carrie (Danning) Jiang, Chao Yang, Chris Ventura, Daniel Ohayon, Danny Gagne, David Taieb, Dmitry Ponomarev, Dong Jia, Dong Zhao, Eero Neuenschwander, Fang Wang, Ferhat Sahinkaya, Ferdi Adeputra, Gayathri Aiyer, George Stasa, Guoqiang Jerry Chen, Haiyang Han, Haydar Imren, Ian Carmichael, Jared Greene, Jerry Pan, Jiang Wu, Johnnie Ballentyne, Joanna Jiang, Jonathan Bergeron, Joseph Li, Jun Fang, Kaustubh Karkare, Komal Mangtani, Kuldeep Chaudhary, Lea Li, Lei Zhang, Liu Yang, Loka Potnuru, Luiz Ribeiro, Marc Celani, Matthieu Martin, Max Mazzeo, Meg Dymek, Mike Tarasyuk, Mital Mehta, Nevzat Sevim, Nick Gardner, Nikolay Kondratyev, Oliver Dodd, Pankaj Landge, Perry Stoll, Prashanth Bandaru, Piyush Khemka, Rahul Nambiar, Rajesh Nishtala, Rituraj Kirti, Roger (Wei) Li, Rujin Cao, Sahil Garg, Satish Sampath, Sean Wang, Seth Silverman, Shridhar Iyer, Sriguru Chakravarthi, Sushaant Mujoo, Susmit Biswas, Taha Bekir Eren, Tony Harper, Vineet Chaudhary, Vishal Jain, Vitali Haravy, Vlad Fedorov, Vlad Gorelik, Wolfram Schuttle, Xiaotian Guo, Yatu Zhang, Yi Huang, Yuxi Zhang, Zejun Zhang, and Zhaohui Zhang. We would also like to express our gratitude to all reviewers of this post, including (in alphabetical order) Aleksandar Ilic, Avtar Brar, Brianna O’Steen, Chloe Lu, Chris Wiltz, Imogen Barnes, Jason Hendrickson, Rituraj Kirti, Xenia Habekoss and Yuri Claure. We would like to especially thank Jonathan Bergeron for overseeing the effort and providing all of the guidance and valuable feedback, and Ramnath Krishna Prasad for pulling required support together to make this blog post happen.\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "33 min read",
  "publishedTime": "2025-04-28T16:30:19Z",
  "modifiedTime": "2025-04-28T22:59:34Z"
}
