{
  "id": "20a131d5-3932-45f4-8ad7-0119886746bd",
  "title": "QCon SF 2024 - Scale Out Batch GPU Inference with Ray",
  "link": "https://www.infoq.com/news/2024/11/batch-inference-ray/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "At QConSF 2024, Cody Yu presented how Anyscale’s Ray can more effectively handle scaling out batch inference. Some of the problems Ray can assist with include scaling large datasets (hundreds of GBs or more), ensuring reliability with spot and on-demand instances, managing multi-stage heterogeneous compute, and managing tradeoffs with cost and latency. By Andrew Hoblitzell",
  "author": "Andrew Hoblitzell",
  "published": "Fri, 22 Nov 2024 18:26:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "QCon San Francisco 2024",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Andrew Hoblitzell",
  "length": 3390,
  "excerpt": "At QConSF 2024, Cody Yu presented how Anyscale’s Ray can more effectively handle scaling out batch inference. Some of the problems Ray can assist with include scaling large datasets (hundreds of GBs o",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241119073511/apple-touch-icon.png",
  "text": "At QConSF 2024, Cody Yu presented how Anyscale’s Ray can more effectively handle scaling out batch inference. Some of the problems Ray can assist with include scaling large datasets (hundreds of GBs or more), ensuring reliability with spot and on-demand instances, managing multi-stage heterogeneous compute, and managing tradeoffs with cost and latency. Ray Data offers scalable data processing solutions that maximize GPU utilization and minimize data movement costs through optimized task scheduling and streaming execution. The integration of Ray Data with vLLM, an open-source framework for LLM inference, has enabled scalable batch inference, significantly reducing processing times. \"The demand for batch inference is getting higher and higher. This is mainly because we now have multi-modality data sources. You have cameras, mic sensors, and PDF files. And then, by processing these files, you will get different kinds of raw data in different formats, which are either unstructured or structured.\" - Cody Yu Features such as continuous batching were discussed, which enhance system throughput and efficiency. A case study on generating embeddings from PDF files efficiently and cost-effectively using Ray Data was highlighted, where the process costs less than $1 for processing with ~20 GPUs. Discussion also covered the importance of pipeline parallelism in balancing execution times across different stages of the LLM inference pipeline. By optimizing batch sizes and employing chunk-based batching, the system has been fine-tuned for maximum efficiency. This approach not only improves throughput but also strategically manages computational resources across heterogeneous systems. Ray Tune might also potentially be used to optimize batch processing workflows through hyperparameter tuning. The session also briefly discussed Ray Serve Batch. Dynamic request batching in Ray Serve enhances service throughput by efficiently processing multiple requests simultaneously, leveraging ML models' vectorized computation capabilities. This feature is particularly useful for expensive models, ensuring optimal hardware utilization. Batching is enabled using the ray.serve.batch decorator, which requires the method to be asynchronous. Continuing the presentation, the speaker highlighted advancements in large language model (LLM) inference, focusing on the vLLM framework, speculative decoding, and inference engine optimization. vLLM is an open-source LLM inference engine known for its high throughput and memory efficiency. It features efficient key-value cache memory management with PagedAttention, continuous batching of incoming requests, and optimized CUDA kernels, including integration with FlashAttention and FlashInfer. The presentation also covered speculative decoding, a technique that accelerates text generation by using a smaller draft model to propose multiple tokens, which a larger target model then verifies in parallel. This method reduces inter-token latency in memory-bound LLM inference, enhancing efficiency without compromising accuracy. Readers interested in learning more about batch inference with Ray may watch InfoQ.com in the coming weeks for a copy of the full presentation. About the Author Andrew Hoblitzell",
  "image": "https://cdn.infoq.com/statics_s1_20241119073511/styles/static/images/logo/logo-big.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eAt \u003ca href=\"https://qconsf.com/\"\u003eQConSF 2024\u003c/a\u003e, \u003ca href=\"https://qconsf.com/speakers/codyyu\"\u003eCody Yu\u003c/a\u003e presented how \u003ca href=\"https://www.anyscale.com/\"\u003eAnyscale\u003c/a\u003e’s \u003ca href=\"https://docs.ray.io/en/latest/index.html\"\u003eRay\u003c/a\u003e can more effectively handle scaling out batch inference. Some of the problems Ray can assist with include scaling large datasets (hundreds of GBs or more), ensuring reliability with spot and on-demand instances, managing multi-stage heterogeneous compute, and managing tradeoffs with cost and latency.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://docs.ray.io/en/latest/data/data.html\"\u003eRay Data\u003c/a\u003e offers scalable data processing solutions that maximize GPU utilization and minimize data movement costs through optimized task scheduling and streaming execution. The integration of Ray Data with vLLM, an open-source framework for LLM inference, has enabled scalable batch inference, significantly reducing processing times.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#34;The demand for batch inference is getting higher and higher. This is mainly because we now have multi-modality data sources. You have cameras, mic sensors, and PDF files. And then, by processing these files, you will get different kinds of raw data in different formats, which are either unstructured or structured.\u0026#34; - Cody Yu\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eFeatures such as \u003ca href=\"https://www.anyscale.com/blog/continuous-batching-llm-inference\"\u003econtinuous batching\u003c/a\u003e were discussed, which enhance system throughput and efficiency. A case study on generating embeddings from PDF files efficiently and cost-effectively using Ray Data was highlighted, where the process costs less than $1 for processing with ~20 GPUs.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/11/batch-inference-ray/en/resources/1IMG_1843-1732133242366.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/batch-inference-ray/en/resources/1IMG_1843-1732133242366.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eDiscussion also covered the importance of pipeline parallelism in balancing execution times across different stages of the LLM inference pipeline. By optimizing batch sizes and employing chunk-based batching, the system has been fine-tuned for maximum efficiency. This approach not only improves throughput but also strategically manages computational resources across heterogeneous systems. \u003ca href=\"https://docs.ray.io/en/latest/tune/index.html\"\u003eRay Tune\u003c/a\u003e might also potentially be used to optimize batch processing workflows through hyperparameter tuning.\u003c/p\u003e\n\n\u003cp\u003eThe session also briefly discussed \u003ca href=\"https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html#enable-batching-for-your-deployment\"\u003eRay Serve Batch\u003c/a\u003e. Dynamic request batching in Ray Serve enhances service throughput by efficiently processing multiple requests simultaneously, leveraging ML models\u0026#39; vectorized computation capabilities. This feature is particularly useful for expensive models, ensuring optimal hardware utilization. Batching is enabled using the \u003ccode\u003eray.serve.batch\u003c/code\u003e decorator, which requires the method to be asynchronous.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/11/batch-inference-ray/en/resources/1IMG_1849-1732133242366.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/batch-inference-ray/en/resources/1IMG_1849-1732133242366.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eContinuing the presentation, the speaker highlighted advancements in large language model (LLM) inference, focusing on the vLLM framework, speculative decoding, and inference engine optimization. \u003ca href=\"https://github.com/vllm-project/vllm\"\u003evLLM \u003c/a\u003eis an open-source LLM inference engine known for its high throughput and memory efficiency. It features efficient key-value cache memory management with \u003ca href=\"https://github.com/vllm-project/vllm/blob/0cd3d9717e38c7a122ed01fe2a8fddd8b37dff4b/vllm/attention/ops/paged_attn.py#L33\"\u003ePagedAttention\u003c/a\u003e, continuous batching of incoming requests, and optimized CUDA kernels, including integration with \u003ca href=\"https://github.com/vllm-project/vllm/blob/0cd3d9717e38c7a122ed01fe2a8fddd8b37dff4b/vllm/v1/attention/backends/flash_attn.py#L45\"\u003eFlashAttention\u003c/a\u003e and \u003ca href=\"https://github.com/vllm-project/vllm/blob/0cd3d9717e38c7a122ed01fe2a8fddd8b37dff4b/vllm/attention/backends/flashinfer.py#L739\"\u003eFlashInfer\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe presentation also covered \u003ca href=\"https://docs.vllm.ai/en/latest/models/spec_decode.html\"\u003especulative decoding\u003c/a\u003e, a technique that accelerates text generation by using a smaller draft model to propose multiple tokens, which a larger target model then verifies in parallel. This method reduces inter-token latency in memory-bound LLM inference, enhancing efficiency without compromising accuracy.\u003c/p\u003e\n\n\u003cp\u003eReaders interested in learning more about batch inference with Ray may watch \u003ca href=\"https://www.infoq.com\"\u003eInfoQ.com\u003c/a\u003e in the coming weeks for a copy of the full presentation.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Andrew-Hoblitzell\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAndrew Hoblitzell\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2024-11-22T00:00:00Z",
  "modifiedTime": null
}
