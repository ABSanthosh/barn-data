{
  "id": "b79548fb-d677-4032-942c-f3191c388829",
  "title": "UC Berkeley's Sky Computing Lab Introduces Model to Reduce AI Language Model Inference Costs",
  "link": "https://www.infoq.com/news/2025/02/uc-berkeley-ai-inference-savings/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "UC Berkeley's Sky Computing Lab has released Sky-T1-32B-Flash, an updated reasoning language model that addresses the common issue of AI overthinking. The model, developed through the NovaSky (Next-generation Open Vision and AI) initiative, \"slashes inference costs on challenging questions by up to 57%\" while maintaining accuracy across mathematics, coding, science, and general knowledge domains. By Vinod Goje",
  "author": "Vinod Goje",
  "published": "Wed, 19 Feb 2025 11:08:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Natural Language Processing",
    "AIOps",
    "Generative AI",
    "Retrieval-Augmented Generation",
    "Large language models",
    "Cloud Computing",
    "Artificial Intelligence",
    "AI Architecture",
    "Explainable AI",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Vinod Goje",
  "length": 5380,
  "excerpt": "UC Berkeley's Sky Computing Lab has released Sky-T1-32B-Flash, an updated reasoning language model that addresses the common issue of AI overthinking. The model, developed through the NovaSky (Next-ge",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250213201515/apple-touch-icon.png",
  "text": "UC Berkeley's Sky Computing Lab has released Sky-T1-32B-Flash, an updated reasoning language model that addresses the common issue of AI overthinking. The model, developed through the NovaSky (Next-generation Open Vision and AI) initiative, \"slashes inference costs on challenging questions by up to 57%\" while maintaining accuracy across mathematics, coding, science, and general knowledge domains. The research team identified overthinking as a significant challenge where reasoning models generate unnecessarily lengthy responses with redundant steps. By optimizing the model to produce more concise outputs, Sky-T1-32B-Flash delivers faster responses while preserving answer quality. The improvements enable more efficient implementation of advanced techniques like Best-of-N, Majority Vote, and Monte Carlo Tree Search within existing computational constraints. The Sky Computing Lab team implemented a three-stage process to tackle the overthinking problem in AI language models while preserving accuracy. The approach expands upon established self-training methods with specific enhancements for complex reasoning tasks. Source: reduction in generated token lengths while maintaining performance The first stage focused on data generation using Sky-T1-32B-Preview to create diverse responses for 12,000 questions from the PRM800K dataset. The team generated eight responses per question using a temperature setting of 1.0 to create variation in response lengths. They then created training pairs by selecting the shortest correct answer as a positive example and the longest correct answer as a negative example. Initial results showed promise in reducing output length while maintaining performance on several benchmarks including MATH500, GPQA, and MMLU. However, the team observed decreased accuracy on complex tasks like LiveCodeBench Medium and Hard, along with advanced math problems in AIME24 and MATH500 Level 5. To address this underthinking issue, they added 1,000 new training pairs that contrasted incorrect short responses with longer correct ones, helping the model learn when deeper reasoning was necessary. In the second stage, they focused on response refinement using Llama3.3-70B to eliminate redundant solutions while preserving reasoning quality. This process targeted common patterns where models proposed multiple solutions with phrases like \"Alternatively...\" or \"Let me reconsider...\" that often didn't improve the final answer. The team developed a \"First Correct Solution plus One\" (FCS+1) method that retained the initial correct solution and one additional solution to maintain the model's reasoning capabilities. This approach proved more effective than alternatives like First Correct Solution (FCS) or FCS with Reflection in reducing response length while maintaining accuracy. The researchers noted that coding responses required different handling since they rarely contained multiple complete solutions. For the final stage, the team implemented SimPO (Simple Preference Optimization) for training, which integrated length normalization into its reward structure. This method offered advantages over DPO (Direct Preference Optimization) by eliminating the need for a reference model, reducing computational requirements. Sky-T1-32B-Flash demonstrates significant performance improvements in reducing output length while preserving accuracy. The model reduces sequence lengths by 37% and 57% on complex problems from AIME24 and LCB-Hard respectively, while maintaining the accuracy levels of its predecessor, Sky-T1-32B-Preview. The optimization resulted in consistent generation length reductions exceeding 30% across all benchmark tests, marking a substantial improvement in model efficiency without compromising solution quality. Source: Sky-T1-32B-Flash benchmark tests The Sky-T1-32B-Flash release has sparked discussions across social media platforms highlighting its practical impact on AI model efficiency. A user on X praised the research team's approach to addressing verbose AI responses: Finally someone acknowledged the rambling problem! Better yet: You guys just proved you can cut down all the needless talk without losing performance. A Reddit user reported integration results: We merge this model with DeepSeek-R1-Distill-Qwen-32B and QwQ-32B-Preview. The resulted model FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview achieves 58.2 on LiveCodeBench (2408-2502), which is better than deepseek-ai/DeepSeek-R1-Distill-Qwen-32B (56.1) and approaching DeepSeek R1 (62.8) and OpenAI O1 (63.4). These early fusion experiments suggest potential pathways for further performance improvements through model combination strategies. The UC Berkeley team has released the complete Sky-T1-32B-Flash development pipeline to support further research and innovation in AI model optimization. The open-source release includes code for data generation, response rewriting, preference optimization, and evaluation procedures. The researchers have also made available their dataset of 10,000 preference pairs and the model weights through HuggingFace, enabling the broader AI community to build upon and validate their approach to reducing model overthinking. About the Author Vinod Goje",
  "image": "https://res.infoq.com/news/2025/02/uc-berkeley-ai-inference-savings/en/headerimage/generatedHeaderImage-1739929063466.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eUC Berkeley\u0026#39;s \u003ca data-mce-href=\"https://sky.cs.berkeley.edu/\" href=\"https://sky.cs.berkeley.edu/\"\u003eSky Computing Lab\u003c/a\u003e has \u003ca data-mce-href=\"https://novasky-ai.github.io/posts/reduce-overthinking/\" href=\"https://novasky-ai.github.io/posts/reduce-overthinking/\"\u003ereleased\u003c/a\u003e Sky-T1-32B-Flash, an updated reasoning language model that addresses the common issue of \u003ca data-mce-href=\"https://www.youtube.com/watch?v=tRHhzpNt3vI\" href=\"https://www.youtube.com/watch?v=tRHhzpNt3vI\"\u003eAI overthinking\u003c/a\u003e. The model, developed through the \u003ca data-mce-href=\"https://novasky-ai.github.io/posts/about-us/\" href=\"https://novasky-ai.github.io/posts/about-us/\"\u003eNovaSky\u003c/a\u003e (Next-generation Open Vision and AI) initiative, \u0026#34;slashes \u003ca data-mce-href=\"https://research.ibm.com/blog/AI-inference-explained\" href=\"https://research.ibm.com/blog/AI-inference-explained\"\u003einference costs\u003c/a\u003e on challenging questions by up to 57%\u0026#34; while maintaining accuracy across mathematics, coding, science, and general knowledge domains.\u003c/p\u003e\n\n\u003cp\u003eThe research team identified \u003ca data-mce-href=\"https://arxiv.org/abs/2412.21187\" href=\"https://arxiv.org/abs/2412.21187\"\u003eoverthinking\u003c/a\u003e as a significant challenge where reasoning models generate unnecessarily lengthy responses with redundant steps. By optimizing the model to produce more concise outputs, Sky-T1-32B-Flash delivers faster responses while preserving answer quality. The improvements enable more efficient implementation of advanced techniques like Best-of-N, Majority Vote, and \u003ca data-mce-href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\" href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\"\u003eMonte Carlo Tree Search\u003c/a\u003e within existing computational constraints.\u003c/p\u003e\n\n\u003cp\u003eThe Sky Computing Lab team implemented a three-stage process to tackle the overthinking problem in AI language models while preserving accuracy. The approach expands upon established self-training methods with specific enhancements for complex reasoning tasks.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/02/uc-berkeley-ai-inference-savings/en/resources/2reduced-overthinking-1739929241329.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/02/uc-berkeley-ai-inference-savings/en/resources/2reduced-overthinking-1739929241329.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca data-mce-href=\"https://novasky-ai.github.io/posts/reduce-overthinking/\" href=\"https://novasky-ai.github.io/posts/reduce-overthinking/\"\u003eSource\u003c/a\u003e: reduction in generated token lengths while maintaining performance\u003c/p\u003e\n\n\u003cp\u003eThe first stage focused on data generation using \u003ca data-mce-href=\"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview\" href=\"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview\"\u003eSky-T1-32B-Preview\u003c/a\u003e to create diverse responses for 12,000 questions from the \u003ca data-mce-href=\"https://huggingface.co/datasets/tasksource/PRM800K\" href=\"https://huggingface.co/datasets/tasksource/PRM800K\"\u003ePRM800K\u003c/a\u003e dataset. The team generated eight responses per question using a \u003ca data-mce-href=\"https://www.iguazio.com/glossary/llm-temperature/\" href=\"https://www.iguazio.com/glossary/llm-temperature/\"\u003etemperature\u003c/a\u003e setting of 1.0 to create variation in response lengths. They then created training pairs by selecting the shortest correct answer as a positive example and the longest correct answer as a negative example.\u003c/p\u003e\n\n\u003cp\u003eInitial results showed promise in reducing output length while maintaining performance on several benchmarks including \u003ca data-mce-href=\"https://huggingface.co/datasets/di-zhang-fdu/MATH500\" href=\"https://huggingface.co/datasets/di-zhang-fdu/MATH500\"\u003eMATH500\u003c/a\u003e, \u003ca data-mce-href=\"https://huggingface.co/datasets/Idavidrein/gpqa\" href=\"https://huggingface.co/datasets/Idavidrein/gpqa\"\u003eGPQA\u003c/a\u003e, and \u003ca data-mce-href=\"https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro\" href=\"https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro\"\u003eMMLU\u003c/a\u003e. However, the team observed decreased accuracy on complex tasks like \u003ca data-mce-href=\"https://livecodebench.github.io/\" href=\"https://livecodebench.github.io/\"\u003eLiveCodeBench\u003c/a\u003e Medium and Hard, along with advanced math problems in \u003ca data-mce-href=\"https://huggingface.co/datasets/Maxwell-Jia/AIME_2024\" href=\"https://huggingface.co/datasets/Maxwell-Jia/AIME_2024\"\u003eAIME24\u003c/a\u003e and MATH500 Level 5. To address this underthinking issue, they added 1,000 new training pairs that contrasted incorrect short responses with longer correct ones, helping the model learn when deeper reasoning was necessary.\u003c/p\u003e\n\n\u003cp\u003eIn the second stage, they focused on response refinement using \u003ca data-mce-href=\"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\" href=\"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\"\u003eLlama3.3-70B\u003c/a\u003e to eliminate redundant solutions while preserving reasoning quality. This process targeted common patterns where models proposed multiple solutions with phrases like \u0026#34;Alternatively...\u0026#34; or \u0026#34;Let me reconsider...\u0026#34; that often didn\u0026#39;t improve the final answer.\u003c/p\u003e\n\n\u003cp\u003eThe team developed a \u0026#34;First Correct Solution plus One\u0026#34; (FCS+1) method that retained the initial correct solution and one additional solution to maintain the model\u0026#39;s reasoning capabilities. This approach proved more effective than alternatives like First Correct Solution (FCS) or FCS with Reflection in reducing response length while maintaining accuracy. The researchers noted that coding responses required different handling since they rarely contained multiple complete solutions.\u003c/p\u003e\n\n\u003cp\u003eFor the final stage, the team implemented \u003ca data-mce-href=\"https://arxiv.org/abs/2405.14734\" href=\"https://arxiv.org/abs/2405.14734\"\u003eSimPO\u003c/a\u003e (\u003ca data-mce-href=\"https://www.youtube.com/watch?v=lYmUjABuUzE\" href=\"https://www.youtube.com/watch?v=lYmUjABuUzE\"\u003eSimple Preference Optimization\u003c/a\u003e) for training, which integrated length normalization into its reward structure. This method offered advantages over \u003ca data-mce-href=\"https://arxiv.org/abs/2305.18290\" href=\"https://arxiv.org/abs/2305.18290\"\u003eDPO\u003c/a\u003e (\u003ca data-mce-href=\"https://www.youtube.com/watch?v=k2pD3k1485A\" href=\"https://www.youtube.com/watch?v=k2pD3k1485A\"\u003eDirect Preference Optimization\u003c/a\u003e) by eliminating the need for a reference model, reducing computational requirements.\u003c/p\u003e\n\n\u003cp\u003eSky-T1-32B-Flash demonstrates significant performance improvements in reducing output length while preserving accuracy. The model reduces sequence lengths by 37% and 57% on complex problems from AIME24 and LCB-Hard respectively, while maintaining the accuracy levels of its predecessor, \u003ca data-mce-href=\"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview\" href=\"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview\"\u003eSky-T1-32B-Preview\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe optimization resulted in consistent generation length reductions exceeding 30% across all \u003ca data-mce-href=\"https://dasarpai.com/dsblog/ai-benchmarks-explained\" href=\"https://dasarpai.com/dsblog/ai-benchmarks-explained\"\u003ebenchmark tests\u003c/a\u003e, marking a substantial improvement in model efficiency without compromising solution quality.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2025/02/uc-berkeley-ai-inference-savings/en/resources/2results-1739929241329.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/02/uc-berkeley-ai-inference-savings/en/resources/2results-1739929241329.png\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca data-mce-href=\"https://novasky-ai.github.io/posts/reduce-overthinking/\" href=\"https://novasky-ai.github.io/posts/reduce-overthinking/\"\u003eSource\u003c/a\u003e: Sky-T1-32B-Flash benchmark tests\u003c/p\u003e\n\n\u003cp\u003eThe Sky-T1-32B-Flash release has sparked discussions across social media platforms highlighting its practical impact on AI model efficiency.\u003c/p\u003e\n\n\u003cp\u003eA user on \u003ca data-mce-href=\"https://x.com/A_Badr057/status/1882540838162739635\" href=\"https://x.com/A_Badr057/status/1882540838162739635\"\u003eX\u003c/a\u003e praised the research team\u0026#39;s approach to addressing verbose AI responses:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eFinally someone acknowledged the rambling problem! Better yet: You guys just proved you can cut down all the needless talk without losing performance.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eA Reddit user \u003ca data-mce-href=\"https://www.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/?rdt=48747\" href=\"https://www.reddit.com/r/LocalLLaMA/comments/1i9ddj1/skyt132bflash_think_less_achieve_more_cut/?rdt=48747\"\u003ereported\u003c/a\u003e integration results:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWe merge this model with DeepSeek-R1-Distill-Qwen-32B and QwQ-32B-Preview. The resulted model FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-Flash-32B-Preview achieves 58.2 on LiveCodeBench (2408-2502), which is better than deepseek-ai/DeepSeek-R1-Distill-Qwen-32B (56.1) and approaching DeepSeek R1 (62.8) and OpenAI O1 (63.4).\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThese early fusion experiments suggest potential pathways for further performance improvements through model combination strategies.\u003c/p\u003e\n\n\u003cp\u003eThe UC Berkeley team has released the complete Sky-T1-32B-Flash development pipeline to support further research and innovation in \u003ca data-mce-href=\"https://www.ultralytics.com/blog/what-is-model-optimization-a-quick-guide\" href=\"https://www.ultralytics.com/blog/what-is-model-optimization-a-quick-guide\"\u003eAI model optimization\u003c/a\u003e. The open-source release includes \u003ca data-mce-href=\"https://github.com/NovaSky-AI/SkyThought\" href=\"https://github.com/NovaSky-AI/SkyThought\"\u003ecode\u003c/a\u003e for data generation, response rewriting, preference optimization, and evaluation procedures. The researchers have also made available their \u003ca data-mce-href=\"https://huggingface.co/datasets/NovaSky-AI/Sky-T1_preference_data_10k\" href=\"https://huggingface.co/datasets/NovaSky-AI/Sky-T1_preference_data_10k\"\u003edataset\u003c/a\u003e of 10,000 preference pairs and the \u003ca data-mce-href=\"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Flash\" href=\"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Flash\"\u003emodel weights\u003c/a\u003e through HuggingFace, enabling the broader AI community to build upon and validate their approach to reducing model overthinking.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Vinod-Goje\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eVinod Goje\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-02-19T00:00:00Z",
  "modifiedTime": null
}
