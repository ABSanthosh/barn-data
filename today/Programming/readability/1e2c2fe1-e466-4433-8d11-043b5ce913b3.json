{
  "id": "1e2c2fe1-e466-4433-8d11-043b5ce913b3",
  "title": "Powered by AI: Instagram’s Explore recommender system",
  "link": "https://instagram-engineering.com/powered-by-ai-instagrams-explore-recommender-system-7ca901d2a882?source=rss----37dc2a3034f2---4",
  "description": "",
  "author": "Ivan Medvedev",
  "published": "Tue, 26 Nov 2019 13:48:39 GMT",
  "source": "https://instagram-engineering.com/feed/",
  "categories": [
    "machine-learning",
    "recommender-systems",
    "recommendations"
  ],
  "byline": "Ivan Medvedev",
  "length": 14646,
  "excerpt": "Over half of the Instagram community visits Instagram Explore every month to discover new photos, videos, and Stories relevant to their interests. Recommending the most relevant content out of…",
  "siteName": "Instagram Engineering",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "This post was originally published on the Facebook AI blog.Over half of the Instagram community visits Instagram Explore every month to discover new photos, videos, and Stories relevant to their interests. Recommending the most relevant content out of billions of options in real time at scale introduces multiple machine learning (ML) challenges that require novel engineering solutions.We tackled these challenges by creating a series of custom query languages, lightweight modeling techniques, and tools enabling high-velocity experimentation. These systems support the scale of Explore while boosting developer efficiency. Collectively, these solutions represent an AI system based on a highly efficient 3-part ranking funnel that extracts 65 billion features and makes 90 million model predictions every second.In this blog post, we’re sharing the first detailed overview of the key elements that make Explore work, and how we provide personalized content for people on Instagram.Developing foundational building blocks of ExploreBefore we could execute on building a recommendation engine that tackles the sheer volume of photos and videos uploaded daily on Instagram, we developed foundational tools to address three important needs. We needed the ability to conduct rapid experimentation at scale, we needed to obtain a stronger signal on the breadth of people’s interests, and we needed a computationally efficient way to ensure that our recommendations were both high quality and fresh. These custom techniques were key to achieving our goals:Iterating quickly with IGQL: A new domain-specific languageBuilding the optimal recommendation algorithms and techniques is an ongoing area of research in the ML community, and the process of choosing the right system can vary widely depending on the task. For instance, while one algorithm may effectively identify long-term interests, another may perform better at identifying recommendations based on recent content. Our engineering team iterates on different algorithms, and we needed a way for us to both try out new ideas efficiently and apply the promising ideas to large-scale systems easily without worrying too much about computational resource implications like CPU and memory usage. We needed a custom domain specific meta-language that provides the right level of abstraction and assembles all algorithms into one place.To solve this, we created and shipped IGQL, a domain-specific language optimized for retrieving candidates in recommender systems. Its execution is optimized in C++, which helps minimize both latency and compute resources. It’s also extensible and easy to use when testing new research ideas. IGQL is both statically validated and high-level. Engineers can write recommendation algorithms in a Python-like way and execute fast and efficiently in C++.user.let(seed_id=user_id).liked(max_num_to_retrieve=30).account_nn(embedding_config=default).posted_media(max_media_per_account=10).filter(non_recommendable_model_threshold=0.2).rank(ranking_model=default).diversify_by(seed_id, method=round_robin)In the code sample above, you can see how IGQL provides high readability even for engineers who haven’t worked extensively in the language. It helps assemble multiple recommendation stages and algorithms in a principled way. For example, we can optimize the ensemble of candidate generators by using a combiner rule in query to output a weighted blend of several subquery outputs. By tweaking their weights, we can find the combination that results in the best user experience.IGQL makes it simple to perform tasks that are common in complex recommendation systems, such as building nested trees of combiner rules. IGQL lets engineers focus on ML and business logic behind recommendations as opposed to logistics, like fetching the right quantity of candidates for each query. It also provides a high degree of code reusability. For instance, applying a ranker is as simple as adding a one-line rule to our IGQL query. It’s trivial to add it in multiple places, like ranking accounts and ranking media posted by those accounts.Account embeddings for personalized ranking inventoryPeople publicly share billions of high quality pieces of media on Instagram that are eligible inventory for Explore. It’s challenging to maintain a clear and ever-evolving catalog-style taxonomy for the large variety of interest communities on Explore — with topics varying from Arabic calligraphy to model trains to slime. As a result, content-based models have difficulty grasping such a variety of interest-based communities.Because Instagram has a large number of interest-focused accounts based on specific themes — such as Devon rex cats or vintage tractors — we created a retrieval pipeline that focuses on account-level information rather than media-level. By building account embeddings, we’re able to more efficiently identify which accounts are topically similar to each other. We infer account embeddings using ig2vec, a word2vec-like embedding framework. Typically, the word2vec embedding framework learns a representation of a word based on its context across sentences in the training corpus. Ig2vec treats account IDs that a user interacts with — e.g., a person likes media from an account — as a sequence of words in a sentence.By applying the same techniques from word2vec, we can predict accounts with which a person is likely to interact in a given session within the Instagram app. If an individual interacts with a sequence of accounts in the same session, it’s more likely to be topically coherent compared with a random sequence of accounts from the diverse range of Instagram accounts. This helps us identify topically similar accounts.We define a distance metric between two accounts — the same one used in embedding training — which is usually cosine distance or dot product. Based on this, we do a KNN lookup to find topically similar accounts for any account in the embedding. Our embedding version covers millions of accounts, and we use Facebook’s state-of-the-art nearest neighbor retrieval engine, FAISS, as the supporting retrieval infrastructure.For each version of the embedding, we train a classifier to predict a set of accounts’ topic solely based on the embedding. By comparing the predicted topics with human-labeled topics for accounts in a hold-out set, we can assess how well the embeddings capture topical similarity.Retrieving accounts that are similar to those that a particular person previously expressed interest in helps us narrow down to a smaller, personalized ranking inventory for each person in a simple yet effective way. As a result, we are able to utilize state-of-the-art and computationally intensive ML models to serve every Instagram community member.Preselecting relevant candidates by using model distillationAfter we use ig2vec to identify the most relevant accounts based on individual interests, we need a way to rank these accounts in a way that’s fresh and interesting for everyone. This requires predicting the most relevant media for each person every time they scroll the Explore page.For instance, evaluating even just 500 media pieces through a deep neural network for every scrolling action requires a large amount of resources. And yet the more posts we evaluate for each user, the higher the possibility we have of finding the best, most personalized media from their inventory.In order to be able to maximize the number of media for each ranking request, we introduced a ranking distillation model that helps us preselect candidates before using more complex ranking models. Our approach is to train a super-lightweight model that learns from and tries to approximate our main ranking models as much as possible. We record the input candidates with features, as well as outputs, from our more complicated ranking models. The distillation model is then trained on this recorded data with a limited set of features and a simpler neural network model structure to replicate the results. Its objective function is to optimize for NDCG ranking (a measure of ranking quality) loss over main ranking model’s output. We use the top-ranked posts from the distillation model as the ranking candidates for the later-stage high-performance ranking models.Setting up the distillation model’s mimicry behavior minimizes the need to tune multiple parameters and maintain multiple models in different ranking stages. Leveraging this technique, we can efficiently evaluate a bigger set of media to find the most relevant media on every ranking request while keeping the computational resources under control.How we built ExploreAfter creating the key building blocks necessary to experiment easily, identify people’s interests effectively, and produce efficient and relevant predictions, we had to combine these systems together in production. Utilizing IGQL, account embeddings, and our distillation technique, we split the Explore recommendation systems into two main stages: the candidate generation stage (also known as sourcing stage) and the ranking stage.An overview of the Explore system.Candidate GenerationFirst, we leverage accounts that people have interacted with before (e.g., liked or saved media from an account) on Instagram to identify which other accounts people might be interested in. We call them the seed accounts. The seed accounts are usually only a fraction of the accounts on Instagram that are about similar or the same interests. Then, we use account embeddings techniques to identify accounts similar to the seed accounts. Finally, based on these accounts, we’re able to find the media that these accounts posted or engaged with.This graphic shows a typical source for Instagram Explore recommendations.There are many different ways people can engage with accounts and media on Instagram (e.g., follow, like, comment, save, and share). There are also different media types (e.g., photo, video, Stories, and Live), which means there are a variety of sources we can construct using a similar scheme. Leveraging IGQL, the process becomes very easy — different candidate sources are just represented as different IGQL subqueries.With different types of sources, we are able to find tens of thousands of eligible candidates for the average person. We want to make sure the content we recommend is both safe and appropriate for a global community of many ages on Explore. Using a variety of signals, we filter out content we can identify as not being eligible to be recommended before we build out eligible inventory for each person. In addition to blocking likely policy-violating content and misinformation, we leverage ML systems that help detect and filter content like spam.Then, for every ranking request, we identify thousands of eligible media for an average person, sample 500 candidates from the eligible inventory, and then send the candidates downstream to the ranking stage.Ranking candidatesWith 500 candidates available for ranking, we use a three-stage ranking infrastructure to help balance the trade-offs between ranking relevance and computation efficiency. The three ranking stages we have are as follows:First pass: the distillation model mimics the combination of the other two stages, with minimal features; picks the 150 highest-quality and most relevant candidates out of 500.Second pass: a lightweight neural network model with full set of dense features; picks the 50 highest-quality and most relevant candidates.Final pass: a deep neural network model with full set of dense and sparse features. Picks the 25 highest-quality and most relevant candidates (for the first page of Explore grid).This animation describes the three-part ranking infrastructure we use to balance trade-offs between ranking relevance and computation efficiency.If the first-pass distillation model mimics the other two stages in ranking order, how do we decide the most relevant content in the next two stages? We predict individual actions that people take on each piece of media, whether they’re positive actions such as like and save, or negative actions such as “See Fewer Posts Like This” (SFPLT). We use a multi-task multi-label (MTML) neural network to predict these events. The shared multilayer perceptron (MLP) allows us to capture the common signals from different actions.An illustration of our current final-pass model architecture.We combine predictions of different events using an arithmetic formula, called value model, to capture the prominence of different signals in terms of deciding whether the content is relevant. We use a weighted sum of predictions such as [w_like * P(Like) + w_save * P(Save) — w_negative_action * P(Negative Action)]. If, for instance, we think the importance of a person saving a post on Explore is higher than their liking a post, then the weight for the save action should be higher.We also want Explore to be a place where people can discover a rich balance of both new interests alongside existing interests. We add a simple heuristic rule into value model to boost the diversity of content. We down-rank posts from the same author or same seed account by adding a penalty factor, so you don’t see multiple posts from the same person or the same seed account in Explore. This penalty increases as you go down the ranked batch and encounter more posts from the same author.We rank the most relevant content based on the final value model score of each ranking candidate in a descendant way. Our offline replay tool — along with Bayesian optimization tools — helps us tune the value model efficiently and frequently as our systems evolve.An ongoing ML challengeOne of the most exciting parts of building Explore is the ongoing challenge of finding new and interesting ways to help our community discover the most interesting and relevant content on Instagram. We’re continuously evolving Instagram Explore, whether by adding media formats like Stories and entry points to new types of content, such as shopping posts and IGTV videos.The scale of both the Instagram community and inventory requires enabling a culture of high-velocity experimentation and developer efficiency to reliably recommend the best of Instagram for each person’s individual interests. Our custom tools and systems have given us a strong foundation for the continuous learning and iteration that are essential to building and scaling Instagram Explore.If you want to learn more about this work or are interested joining one of our engineering teams, please visit our careers page, follow us on Facebook or on Twitter.Written by Ivan Medvedev, Haotian Wu, and Taylor Gordon.",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*L8XlYDF2i6ziTrH4",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca href=\"https://medium.com/@ivanmedvedev_44374?source=post_page-----7ca901d2a882--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Ivan Medvedev\" src=\"https://miro.medium.com/v2/da:true/resize:fill:88:88/0*rFMtbyx1vkh6aRl0\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://instagram-engineering.com/?source=post_page-----7ca901d2a882--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Instagram Engineering\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*CPgwLHR6jno_tOmF0--7eg.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"a2b7\"\u003e\u003cem\u003eThis post was originally published on the \u003c/em\u003e\u003ca href=\"https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/.\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eFacebook AI blog\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"1b63\"\u003eOver half of the Instagram community visits \u003ca href=\"https://www.instagram.com/explore/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eInstagram Explore\u003c/a\u003e every month to discover new photos, videos, and Stories relevant to their interests. Recommending the most relevant content out of billions of options in real time at scale introduces multiple machine learning (ML) challenges that require novel engineering solutions.\u003c/p\u003e\u003cp id=\"e22c\"\u003eWe tackled these challenges by creating a series of custom query languages, lightweight modeling techniques, and tools enabling high-velocity experimentation. These systems support the scale of Explore while boosting developer efficiency. Collectively, these solutions represent an AI system based on a highly efficient 3-part ranking funnel that extracts 65 billion features and makes 90 million model predictions \u003cstrong\u003eevery second\u003c/strong\u003e.\u003c/p\u003e\u003cp id=\"a0ba\"\u003eIn this blog post, we’re sharing the first detailed overview of the key elements that make Explore work, and how we provide personalized content for people on Instagram.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"1f1e\"\u003eDeveloping foundational building blocks of Explore\u003c/h2\u003e\u003cp id=\"af06\"\u003eBefore we could execute on building a recommendation engine that tackles the sheer volume of photos and videos uploaded daily on Instagram, we developed foundational tools to address three important needs. We needed the ability to conduct rapid experimentation at scale, we needed to obtain a stronger signal on the breadth of people’s interests, and we needed a computationally efficient way to ensure that our recommendations were both high quality and fresh. These custom techniques were key to achieving our goals:\u003c/p\u003e\u003ch2 id=\"412b\"\u003eIterating quickly with IGQL: A new domain-specific language\u003c/h2\u003e\u003cp id=\"ad87\"\u003eBuilding the optimal recommendation algorithms and techniques is an ongoing area of research in the ML community, and the process of choosing the right system can vary widely depending on the task. For instance, while one algorithm may effectively identify long-term interests, another may perform better at identifying recommendations based on recent content. Our engineering team iterates on different algorithms, and we needed a way for us to both try out new ideas efficiently and apply the promising ideas to large-scale systems easily without worrying too much about computational resource implications like CPU and memory usage. We needed a custom domain specific meta-language that provides the right level of abstraction and assembles all algorithms into one place.\u003c/p\u003e\u003cp id=\"ba4c\"\u003eTo solve this, we created and shipped IGQL, a domain-specific language optimized for retrieving candidates in recommender systems. Its execution is optimized in C++, which helps minimize both latency and compute resources. It’s also extensible and easy to use when testing new research ideas. IGQL is both statically validated and high-level. Engineers can write recommendation algorithms in a Python-like way and execute fast and efficiently in C++.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4788\"\u003euser\u003cbr/\u003e.let(seed_id=user_id)\u003cbr/\u003e.liked(max_num_to_retrieve=30)\u003cbr/\u003e.account_nn(embedding_config=default)\u003cbr/\u003e.posted_media(max_media_per_account=10)\u003cbr/\u003e.filter(non_recommendable_model_threshold=0.2)\u003cbr/\u003e.rank(ranking_model=default)\u003cbr/\u003e.diversify_by(seed_id, method=round_robin)\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"ed54\"\u003eIn the code sample above, you can see how IGQL provides high readability even for engineers who haven’t worked extensively in the language. It helps assemble multiple recommendation stages and algorithms in a principled way. For example, we can optimize the ensemble of candidate generators by using a combiner rule in query to output a weighted blend of several subquery outputs. By tweaking their weights, we can find the combination that results in the best user experience.\u003c/p\u003e\u003cp id=\"747b\"\u003eIGQL makes it simple to perform tasks that are common in complex recommendation systems, such as building nested trees of combiner rules. IGQL lets engineers focus on ML and business logic behind recommendations as opposed to logistics, like fetching the right quantity of candidates for each query. It also provides a high degree of code reusability. For instance, applying a ranker is as simple as adding a one-line rule to our IGQL query. It’s trivial to add it in multiple places, like ranking accounts and ranking media posted by those accounts.\u003c/p\u003e\u003ch2 id=\"1d5c\"\u003eAccount embeddings for personalized ranking inventory\u003c/h2\u003e\u003cp id=\"a504\"\u003ePeople publicly share billions of high quality pieces of media on Instagram that are eligible inventory for Explore. It’s challenging to maintain a clear and ever-evolving catalog-style taxonomy for the large variety of interest communities on Explore — with topics varying from Arabic calligraphy to model trains to slime. As a result, content-based models have difficulty grasping such a variety of interest-based communities.\u003c/p\u003e\u003cp id=\"b4af\"\u003eBecause Instagram has a large number of interest-focused accounts based on specific themes — such as Devon rex cats or \u003ca href=\"https://www.instagram.com/explore/tags/vintagetractors/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003evintage tractors\u003c/a\u003e — we created a retrieval pipeline that focuses on account-level information rather than media-level. By building account embeddings, we’re able to more efficiently identify which accounts are topically similar to each other. We infer account embeddings using ig2vec, a \u003ca href=\"https://en.wikipedia.org/wiki/Word2vec\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eword2vec\u003c/a\u003e-like embedding framework. Typically, the word2vec embedding framework learns a representation of a word based on its context across sentences in the training corpus. Ig2vec treats account IDs that a user interacts with — e.g., a person likes media from an account — as a sequence of words in a sentence.\u003c/p\u003e\u003cp id=\"3945\"\u003eBy applying the same techniques from word2vec, we can predict accounts with which a person is likely to interact in a given session within the Instagram app. If an individual interacts with a sequence of accounts in the same session, it’s more likely to be topically coherent compared with a random sequence of accounts from the diverse range of Instagram accounts. This helps us identify topically similar accounts.\u003c/p\u003e\u003cp id=\"0cfd\"\u003eWe define a distance metric between two accounts — the same one used in embedding training — which is usually cosine distance or dot product. Based on this, we do a KNN lookup to find topically similar accounts for any account in the embedding. Our embedding version covers millions of accounts, and we \u003ca href=\"https://ai.facebook.com/tools/faiss/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003euse Facebook’s state-of-the-art nearest neighbor retrieval engine, FAISS\u003c/a\u003e, as the supporting retrieval infrastructure.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"0b3e\"\u003eFor each version of the embedding, we train a classifier to predict a set of accounts’ topic solely based on the embedding. By comparing the predicted topics with human-labeled topics for accounts in a hold-out set, we can assess how well the embeddings capture topical similarity.\u003c/p\u003e\u003cp id=\"0a9f\"\u003eRetrieving accounts that are similar to those that a particular person previously expressed interest in helps us narrow down to a smaller, personalized ranking inventory for each person in a simple yet effective way. As a result, we are able to utilize state-of-the-art and computationally intensive ML models to serve every Instagram community member.\u003c/p\u003e\u003ch2 id=\"1627\"\u003ePreselecting relevant candidates by using model distillation\u003c/h2\u003e\u003cp id=\"bca7\"\u003eAfter we use ig2vec to identify the most relevant accounts based on individual interests, we need a way to rank these accounts in a way that’s fresh and interesting for everyone. This requires predicting the most relevant media for each person every time they scroll the Explore page.\u003c/p\u003e\u003cp id=\"2507\"\u003eFor instance, evaluating even just 500 media pieces through a deep neural network for every scrolling action requires a large amount of resources. And yet the more posts we evaluate for each user, the higher the possibility we have of finding the best, most personalized media from their inventory.\u003c/p\u003e\u003cp id=\"aefa\"\u003eIn order to be able to maximize the number of media for each ranking request, we introduced a ranking distillation model that helps us preselect candidates before using more complex ranking models. Our approach is to train a super-lightweight model that learns from and tries to approximate our main ranking models as much as possible. We record the input candidates with features, as well as outputs, from our more complicated ranking models. The distillation model is then trained on this recorded data with a limited set of features and a simpler neural network model structure to replicate the results. Its objective function is to optimize for \u003ca href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNDCG ranking\u003c/a\u003e (a measure of ranking quality) loss over main ranking model’s output. We use the top-ranked posts from the distillation model as the ranking candidates for the later-stage high-performance ranking models.\u003c/p\u003e\u003cp id=\"02d2\"\u003eSetting up the distillation model’s mimicry behavior minimizes the need to tune multiple parameters and maintain multiple models in different ranking stages. Leveraging this technique, we can efficiently evaluate a bigger set of media to find the most relevant media on every ranking request while keeping the computational resources under control.\u003c/p\u003e\u003ch2 id=\"62fe\"\u003eHow we built Explore\u003c/h2\u003e\u003cp id=\"f0f0\"\u003eAfter creating the key building blocks necessary to experiment easily, identify people’s interests effectively, and produce efficient and relevant predictions, we had to combine these systems together in production. Utilizing IGQL, account embeddings, and our distillation technique, we split the Explore recommendation systems into two main stages: the candidate generation stage (also known as sourcing stage) and the ranking stage.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eAn overview of the Explore system.\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"3750\"\u003eCandidate Generation\u003c/h2\u003e\u003cp id=\"79a7\"\u003eFirst, we leverage accounts that people have interacted with before (e.g., liked or saved media from an account) on Instagram to identify which other accounts people might be interested in. We call them the seed accounts. The seed accounts are usually only a fraction of the accounts on Instagram that are about similar or the same interests. Then, we use account embeddings techniques to identify accounts similar to the seed accounts. Finally, based on these accounts, we’re able to find the media that these accounts posted or engaged with.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eThis graphic shows a typical source for Instagram Explore recommendations.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"a262\"\u003eThere are many different ways people can engage with accounts and media on Instagram (e.g., follow, like, comment, save, and share). There are also different media types (e.g., photo, video, Stories, and Live), which means there are a variety of sources we can construct using a similar scheme. Leveraging IGQL, the process becomes very easy — different candidate sources are just represented as different IGQL subqueries.\u003c/p\u003e\u003cp id=\"296e\"\u003eWith different types of sources, we are able to find tens of thousands of eligible candidates for the average person. We want to make sure the content we recommend is both safe and appropriate for a global community of many ages on Explore. Using a variety of signals, we filter out content we can identify as not being eligible to be recommended before we build out eligible inventory for each person. In addition to blocking likely policy-violating content and misinformation, we leverage ML systems that help detect and filter content like spam.\u003c/p\u003e\u003cp id=\"c264\"\u003eThen, for every ranking request, we identify thousands of eligible media for an average person, sample 500 candidates from the eligible inventory, and then send the candidates downstream to the ranking stage.\u003c/p\u003e\u003ch2 id=\"4ccc\"\u003eRanking candidates\u003c/h2\u003e\u003cp id=\"4385\"\u003eWith 500 candidates available for ranking, we use a three-stage ranking infrastructure to help balance the trade-offs between ranking relevance and computation efficiency. The three ranking stages we have are as follows:\u003c/p\u003e\u003col\u003e\u003cli id=\"cf18\"\u003eFirst pass: the distillation model mimics the combination of the other two stages, with minimal features; picks the 150 highest-quality and most relevant candidates out of 500.\u003c/li\u003e\u003cli id=\"e80f\"\u003eSecond pass: a lightweight neural network model with full set of dense features; picks the 50 highest-quality and most relevant candidates.\u003c/li\u003e\u003cli id=\"75d1\"\u003eFinal pass: a deep neural network model with full set of dense and sparse features. Picks the 25 highest-quality and most relevant candidates (for the first page of Explore grid).\u003c/li\u003e\u003c/ol\u003e\u003cfigure\u003e\u003cfigcaption\u003eThis animation describes the three-part ranking infrastructure we use to balance trade-offs between ranking relevance and computation efficiency.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"1334\"\u003eIf the first-pass distillation model mimics the other two stages in ranking order, how do we decide the most relevant content in the next two stages? We predict individual actions that people take on each piece of media, whether they’re positive actions such as like and save, or negative actions such as “See Fewer Posts Like This” (SFPLT). We use a multi-task multi-label (MTML) neural network to predict these events. The shared multilayer perceptron (MLP) allows us to capture the common signals from different actions.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eAn illustration of our current final-pass model architecture.\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"137e\"\u003eWe combine predictions of different events using an arithmetic formula, called value model, to capture the prominence of different signals in terms of deciding whether the content is relevant. We use a weighted sum of predictions such as [w_like * P(Like) + w_save * P(Save) — w_negative_action * P(Negative Action)]. If, for instance, we think the importance of a person saving a post on Explore is higher than their liking a post, then the weight for the save action should be higher.\u003c/p\u003e\u003cp id=\"2d57\"\u003eWe also want Explore to be a place where people can discover a rich balance of both new interests alongside existing interests. We add a simple heuristic rule into value model to boost the diversity of content. We down-rank posts from the same author or same seed account by adding a penalty factor, so you don’t see multiple posts from the same person or the same seed account in Explore. This penalty increases as you go down the ranked batch and encounter more posts from the same author.\u003c/p\u003e\u003cp id=\"926d\"\u003eWe rank the most relevant content based on the final value model score of each ranking candidate in a descendant way. Our offline replay tool — along with \u003ca href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fresearch.fb.com%2Fefficient-tuning-of-online-systems-using-bayesian-optimization%2F\u0026amp;h=AT02CIAbh2DehM6qp72BVONwqz-REZ_Y77mODKs8HjO4zH8gKzmZ268bP_IhXs7oMlskQxUYY_MB29qmW5e2qT3txxjfwYmFOjOmWkjhC-bzVLmS8k4WrRK0R24kBJXT5-tq7nMJq0FsZxS5CfvWYOZI\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBayesian optimization tools\u003c/a\u003e — helps us tune the value model efficiently and frequently as our systems evolve.\u003c/p\u003e\u003ch2 id=\"ca2f\"\u003eAn ongoing ML challenge\u003c/h2\u003e\u003cp id=\"9ccd\"\u003eOne of the most exciting parts of building Explore is the ongoing challenge of finding new and interesting ways to help our community discover the most interesting and relevant content on Instagram. We’re continuously evolving Instagram Explore, whether by adding media formats like Stories and entry points to new types of content, such as shopping posts and IGTV videos.\u003c/p\u003e\u003cp id=\"5237\"\u003eThe scale of both the Instagram community and inventory requires enabling a culture of high-velocity experimentation and developer efficiency to reliably recommend the best of Instagram for each person’s individual interests. Our custom tools and systems have given us a strong foundation for the continuous learning and iteration that are essential to building and scaling Instagram Explore.\u003c/p\u003e\u003cp id=\"90aa\"\u003eIf you want to learn more about this work or are interested joining one of our engineering teams, please visit our \u003ca href=\"https://www.facebook.com/careers/jobs/?q=instagram\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecareers page\u003c/a\u003e, follow us \u003ca href=\"https://www.facebook.com/instagramengineering/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eon Facebook\u003c/a\u003e or \u003ca href=\"https://twitter.com/instagrameng\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eon Twitter\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"76c5\"\u003e\u003cem\u003eWritten by Ivan Medvedev, Haotian Wu, and Taylor Gordon.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "16 min read",
  "publishedTime": "2019-11-26T13:48:39.827Z",
  "modifiedTime": null
}
