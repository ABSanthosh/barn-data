{
  "id": "e5e1986f-5d1f-4e5e-adc0-6a88c81e5d54",
  "title": "Gemma 3n Introduces Novel Techniques for Enhanced Mobile AI Inference",
  "link": "https://www.infoq.com/news/2025/07/gemma-3n-architecture/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Launched in early preview last May, Gemma 3n is now officially available. It targets mobile-first, on-device AI applications, using new techniques designed to increase efficiency and improve performance, such as per-layer embeddings and transformer nesting. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Fri, 04 Jul 2025 18:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Privacy",
    "Model Inference",
    "Google",
    "Large language models",
    "Mobile",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3282,
  "excerpt": "Launched in early preview last May, Gemma 3n is now officially available. It targets mobile-first, on-device AI applications, using new techniques designed to increase efficiency and improve performan",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250605075544/apple-touch-icon.png",
  "text": "Launched in early preview last May, Gemma 3n is now officially available. It targets mobile-first, on-device AI applications, using new techniques designed to increase efficiency and improve performance, such as per-layer embeddings and transformer nesting. Gemma 3n uses Per-Layer Embeddings (PLE) to reduce the RAM required to run a model while maintaining the same number of total parameters. The technique consists of loading only the core transformer weights into accelerated memory, typically VRAM, while the rest of the parameters are kept on the CPU. Specifically, the 5-billion-parameter variant of the model only requires 2 billion parameters to be loaded into the accelerator; for the 8-billion variant, it’s 4 billion. Another novel technique is MatFormer, short for Matryoshka Transformer), which allows transformers to be nested so that a larger model, e.g. with 4B parameters, contains a smaller version of itself, e.g. with only 2B parameters. This approach enables what Google calls elastic inference and allows developers to choose either the full model or its faster but fully-functional sub-model. MatFormer also support a Mix-n-Match method to let developers create intermediate-sizes versions: This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. In the future, Gemma 3n will fully support elastic inference, enabling dynamic switching between the full model and the sub-model on the fly, depending on the current task and device load. Another new feature in Gemma 3n aimed at accelerating inference is KV cache sharing, which is designed to accelerate time-to-first-token, a key metric for streaming response applications. Using this technique, which according to Google is particularly efficient with long contexts: The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. Gemma 3n also brings native multimodal capabilities, thanks to its audio and video encoders. On the audio front, it enables on-device automatic speech recognition and speech translation. The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context. Google says they have observed strong results translating between English and Spanish, French, Italian, and Portuguese. While Gemma 3n audio encoder can process arbitrarily long audios thanks to its streaming architecture, it will initially be limited to clips of up to 30 seconds at launch. As a final note about Gemma 3n, it is worth highlighting that it supports resolutions of 256x256, 512x512, and 768x768 pixels and can process up to 60 frames per second on a Google Pixel device. In comparison with Gemma 3, it delivers a 13x speedup with quantization (6.5x without) and has a memory footprint that is four times smaller. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/07/gemma-3n-architecture/en/headerimage/gemma-3n-architecture-1751651154855.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eLaunched in early preview last May, \u003ca href=\"https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/\"\u003eGemma 3n is now officially available\u003c/a\u003e. It targets mobile-first, on-device AI applications, using new techniques designed to increase efficiency and improve performance, such as per-layer embeddings and transformer nesting.\u003c/p\u003e\n\n\u003cp\u003eGemma 3n uses Per-Layer Embeddings (PLE) to \u003ca href=\"https://developers.googleblog.com/en/introducing-gemma-3n/\"\u003ereduce the RAM required to run a model\u003c/a\u003e while maintaining the same number of total parameters. The technique consists of loading only the core transformer weights into accelerated memory, typically VRAM, while the rest of the parameters are kept on the CPU. Specifically, the 5-billion-parameter variant of the model only requires 2 billion parameters to be loaded into the accelerator; for the 8-billion variant, it’s 4 billion.\u003c/p\u003e\n\n\u003cp\u003eAnother novel technique is MatFormer, short for \u003ca href=\"https://arxiv.org/abs/2310.07707\"\u003eMatryoshka Transformer\u003c/a\u003e), which allows transformers to be nested so that a larger model, e.g. with 4B parameters, contains a smaller version of itself, e.g. with only 2B parameters. This approach enables what Google calls \u003cem\u003e\u003ca href=\"https://arxiv.org/abs/2310.07707\"\u003eelastic inference\u003c/a\u003e\u003c/em\u003e and allows developers to choose either the full model or its faster but fully-functional sub-model. MatFormer also support a \u003cem\u003e\u003ca href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/%5BGemma_3n%5DMatFormer_Lab.ipynb\"\u003eMix-n-Match\u003c/a\u003e\u003c/em\u003e method to let developers create intermediate-sizes versions:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis technique allows you to precisely slice the E4B model\u0026#39;s parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn the future, Gemma 3n will fully support elastic inference, enabling dynamic switching between the full model and the sub-model on the fly, depending on the current task and device load.\u003c/p\u003e\n\n\u003cp\u003eAnother new feature in Gemma 3n aimed at accelerating inference is \u003cem\u003eKV cache sharing\u003c/em\u003e, which is designed to accelerate time-to-first-token, a key metric for streaming response applications. Using this technique, which according to Google is particularly efficient with long contexts:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eGemma 3n also brings native multimodal capabilities, thanks to its audio and video encoders. On the audio front, it enables on-device automatic speech recognition and speech translation.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eGoogle says they have observed strong results translating between English and Spanish, French, Italian, and Portuguese. While Gemma 3n audio encoder can process arbitrarily long audios thanks to its streaming architecture, it will initially be limited to clips of up to 30 seconds at launch.\u003c/p\u003e\n\n\u003cp\u003eAs a final note about Gemma 3n, it is worth highlighting that it supports resolutions of 256x256, 512x512, and 768x768 pixels and can process up to 60 frames per second on a Google Pixel device. In comparison with Gemma 3, it delivers a 13x speedup with quantization (6.5x without) and has a memory footprint that is four times smaller.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-07-04T00:00:00Z",
  "modifiedTime": null
}
