{
  "id": "f3af134d-38f7-4403-8312-b3836ecbf68d",
  "title": "Detecting errors in AI-generated code",
  "link": "https://stackoverflow.blog/2024/09/20/detecting-errors-in-ai-generated-code/",
  "description": "Ben chats with Gias Uddin, an assistant professor at York University in Toronto, where he teaches software engineering, data science, and machine learning. His research focuses on designing intelligent tools for testing, debugging, and summarizing software and AI systems. He recently published a paper about detecting errors in code generated by LLMs. Gias and Ben discuss the concept of hallucinations in AI-generated code, the need for tools to detect and correct those hallucinations, and the potential for AI-powered tools to generate QA tests.",
  "author": "Eira May",
  "published": "Fri, 20 Sep 2024 07:40:00 GMT",
  "source": "https://stackoverflow.blog/feed/",
  "categories": [
    "se-tech",
    "se-stackoverflow",
    "podcast",
    "ai",
    "llm"
  ],
  "byline": "Eira May",
  "length": 595,
  "excerpt": "Ben chats with Gias Uddin, an assistant professor at York University in Toronto, where he teaches software engineering, data science, and machine learning. His research focuses on designing intelligent tools for testing, debugging, and summarizing software and AI systems. He recently published a paper about detecting errors in code generated by LLMs. Gias and Ben discuss the concept of hallucinations in AI-generated code, the need for tools to detect and correct those hallucinations, and the potential for AI-powered tools to generate QA tests.",
  "siteName": "",
  "favicon": "https://stackoverflow.blog/apple-touch-icon.png",
  "text": "September 20, 2024Ben chats with Gias Uddin, an assistant professor at York University in Toronto, where he teaches software engineering, data science, and machine learning. His research focuses on designing intelligent tools for testing, debugging, and summarizing software and AI systems. He recently published a paper about detecting errors in code generated by LLMs. Gias and Ben discuss the concept of hallucinations in AI-generated code, the need for tools to detect and correct those hallucinations, and the potential for AI-powered tools to generate QA tests. Credit: Alexandra Francis",
  "image": "https://cdn.stackoverflow.co/images/jo7n4k8s/production/56c6dab9c7d1b66bc662c5f43ea9573418d94d17-2400x1260.webp?w=1200\u0026fm=png\u0026auto=format",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle itemscope=\"\" itemtype=\"https://schema.org/Article\"\u003e\u003cheader\u003e\u003ctime datetime=\"2024-09-20T07:40:00.000Z\" itemprop=\"datePublished\"\u003e September 20, 2024\u003c/time\u003e\u003cp itemprop=\"abstract\"\u003eBen chats with Gias Uddin, an assistant professor at York University in Toronto, where he teaches software engineering, data science, and machine learning. His research focuses on designing intelligent tools for testing, debugging, and summarizing software and AI systems. He recently published a paper about detecting errors in code generated by LLMs. Gias and Ben discuss the concept of hallucinations in AI-generated code, the need for tools to detect and correct those hallucinations, and the potential for AI-powered tools to generate QA tests.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/56c6dab9c7d1b66bc662c5f43ea9573418d94d17-2400x1260.webp?w=1200\u0026amp;h=630\u0026amp;auto=format\u0026amp;dpr=2\" width=\"1200\" height=\"630\" alt=\"Article hero image\" itemprop=\"image\"/\u003e\u003cfigcaption\u003e  \u003cspan\u003e Credit: Alexandra Francis\u003c/span\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/header\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "Less than 1 min",
  "publishedTime": null,
  "modifiedTime": null
}
