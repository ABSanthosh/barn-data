{
  "id": "ebeff576-6daf-401a-abff-6aa5d5ae62ed",
  "title": "Astra Dynamic Chunks: How We Saved by Redesigning a Key Part of Astra",
  "link": "https://slack.engineering/astra-dynamic-chunks-how-we-saved-by-redesigning-a-key-part-of-astra/",
  "description": "Introduction Slack handles a lot of log data. In fact, we consume over 6 million log messages per second. That equates to over 10 GB of data per second! And it‚Äôs all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data‚Ä¶ The post Astra Dynamic Chunks: How We Saved by Redesigning a Key Part of Astra appeared first on Engineering at Slack.",
  "author": "George Luong",
  "published": "Mon, 18 Nov 2024 22:06:23 +0000",
  "source": "https://slack.engineering/feed",
  "categories": [
    "Uncategorized"
  ],
  "byline": "",
  "length": 7328,
  "excerpt": "Introduction Slack handles a lot of log data. In fact, we consume over 6 million log messages per second. That equates to over 10 GB of data per second! And it‚Äôs all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data‚Ä¶",
  "siteName": "Engineering at Slack",
  "favicon": "https://slack.engineering/wp-content/uploads/sites/7/2020/05/cropped-octothrope-1.png?w=192",
  "text": "Introduction Slack handles a lot of log data. In fact, we consume over 6 million log messages per second. That equates to over 10 GB of data per second! And it‚Äôs all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data into blocks that we refer to as ‚Äúchunks‚Äù. Initially, we built Astra with the assumption that all chunks would be the same size. However, that assumption has led to inefficiencies from unused disk space and resulted in extra spend for our infrastructure. We decided to tackle that problem in a pursuit to decrease the cost to operate Astra. The Problem with Fixed-Size Chunks The biggest problem with fixed-sized chunks was the fact that not all of our chunks were fully utilized, leading to differently sized chunks. While assuming fixed-sized chunks simplified the code, it also led to us allocating more space than required on our cache nodes, resulting in unnecessary spend.¬† Previously, each cache node was given a fixed number of slots, where each slot would be assigned a chunk. While this simplified the code, it meant that undersized chunks of data would have more space allocated for them than required. For instance, on a 3TB cache node, we would have 200 slots, where each slot was expected to hold a 15GB chunk. However, if any chunks were undersized (say 10GB instead of 15GB), this would result in extra space (5GB) being allocated but not used. On clusters where we‚Äôd have thousands of chunks, this quickly led to a rather large percentage of space being allocated but unused. An additional problem with fixed-sized chunks was that some chunks were actually bigger than our assumed size. This could potentially happen whenever Astra created a recovery task to catch up on older data. We create recovery tasks based on the number of messages that we‚Äôre behind and not the size of data we‚Äôre behind. If the average size of each message is bigger than we expect, this can result in an oversized chunk being created, which is even worse than undersized chunks as it means we aren‚Äôt allocating enough space. Designing Dynamic Chunks Astra‚Äôs architecture diagram ‚Äì ‚ÄúZK‚Äù is the Zookeeper store which holds metadata. In order to build dynamic chunks, we had to modify two parts of Astra: the Cluster Manager and the Cache. Redesigning Cache Nodes We first looked at how cache nodes are structured: Previously, whenever a cache node came online, it would advertise its number of slots in Zookeeper (our centralized coordination store). Then, the Astra manager would assign each slot a chunk, and the cache node would go and download and serve that chunk.¬† Each cache node has a lifecycle: Cache node comes online, advertises the # of slots it has. Manager picks up on the slots, and assigns a chunk to each one. Each cache node downloads the chunks assigned to its slots. This had the benefit of the slots being ephemeral, meaning whenever a cache node went offline, its slots would disappear from Zookeeper, and the manager would reassign the chunks the slots used to hold. However, with dynamic chunks, each cache node could only advertise their capacity, as it would not know ahead of time how many chunks it would be assigned. This meant we unfortunately could no longer rely on slots to provide these benefits to us.¬† To fix these two problems, we decided to persist two new types of data in Zookeeper: the cache node assignment and the cache node metadata. Here‚Äôs a quick breakdown: Cache Node Assignment: a mapping of chunk ID to cache node Cache Node Metadata: metadata about each cache node, including capacity, hostname, etc. Utilizing these two new types of data, the new flow looks like this: Cache node comes online, advertises its disk space. Manager picks up on the disk space each cache node has, and creates assignments for each cache node, utilizing bin packing to minimize the number of cache nodes it uses. Cache nodes pick up on the assignments that were created for it, and downloads its chunks. Redesigning the manager The next change was in the manager, upgrading it to utilize the two new types of data we introduced: the cache node assignments and the cache node metadata. To utilize the cache node assignments, we decided to implement first-fit bin packing to decide which cache node should be assigned which chunk. We then used the cache node metadata in order to make appropriate decisions regarding whether or not we could fit a certain chunk into a given cache node. Previously, the logic for assigning slots was: Grab the list of slots Grab the list of chunks to assign Zip down both lists, assigning a slot to a chunk Now, the logic looks like this: Grab list of chunks to assign Grab list of cache nodes For each chunk Perform first-fit bin packing to determine which cache node it should be assigned to Persist the mapping of cache node to chunk Bin Packing The most juicy part of redesigning the manager was implementing the first-fit bin packing. It‚Äôs a well-known problem of minimizing the number of bins (cache nodes) used to hold a certain amount of items (chunks). We decided to use first-fit bin packing, favoring it for its speed and ease of implementation. Using pseudocode, we describe the bin-packing algorithm: for each chunk for each cache node if the current chunk fits into the cache node: assign the chunk else: move on to the next cache node if there aren‚Äôt any cache nodes left and the chunk hasn‚Äôt been assigned: create a new cache node This helped ensure that we were able to pack the cache nodes as tightly as possible, resulting in a higher utilization of allocated space. Rolling it out Overall, this was a significant change to the Astra codebase. It touched many key parts of Astra, essentially rewriting all of the logic that handled the assignment and downloading of chunks. With such a change, we wanted to be careful with the roll out to ensure that nothing would break.¬† To ensure nothing would break we did the following: Hosted two replicas of the same data Placed all dynamic chunk code behind a feature flag We leaned heavily on these two guardrails in order to ensure a safe roll out.¬† Hosting two replicas of the same data allowed us to incrementally deploy to one of the two replicas and monitor its behavior. It also ensured that if our changes ever broke anything, we‚Äôd still have a second replica able to serve the data. Having all the code behind a feature flag allowed us to merge the code into master early on, as it wouldn‚Äôt run unless explicitly enabled. It also allowed us to incrementally roll out and test our changes. We started with smaller clusters, before moving on to bigger and bigger clusters after verifying everything worked. Results What kind of results did we end up seeing from this? For starters, we were able to reduce the # of cache nodes required by up to 50% for our clusters with many undersized chunks! Overall our cache node costs were reduced by 20%, giving us significant cost savings for operating Astra. Acknowledgments A huge shout out to everyone who has helped along the way to bring dynamic chunks to life: Bryan Burkholder George Luong Ryan Katkov Interested in building innovative projects and making developers‚Äô work lives easier? We‚Äôre hiring üíº Apply now",
  "image": "https://slack.engineering/wp-content/uploads/sites/7/2024/11/unnamed.png?w=640",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u003ch2\u003e\u003cspan\u003eIntroduction\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eSlack handles a \u003c/span\u003e\u003ci\u003e\u003cspan\u003elot\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e of log data. In fact, we consume over 6 million log messages\u003c/span\u003e\u003ci\u003e\u003cspan\u003e per second\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e. That equates to over 10 GB of data \u003c/span\u003e\u003ci\u003e\u003cspan\u003eper second!\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e And it‚Äôs all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data into blocks that we refer to as ‚Äúchunks‚Äù.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eInitially, we built Astra with the assumption that all chunks would be the same size. However, that assumption has led to inefficiencies from unused disk space and resulted in extra spend for our infrastructure. \u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003cspan\u003eWe decided to tackle that problem in a pursuit to decrease the cost to operate Astra.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe Problem with Fixed-Size Chunks\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe biggest problem with fixed-sized chunks was the fact that \u003c/span\u003e\u003cb\u003enot all of our chunks were fully utilized, leading to differently sized chunks\u003c/b\u003e\u003cspan\u003e. While assuming fixed-sized chunks simplified the code, it also led to us allocating more space than required on our cache nodes, resulting in unnecessary spend.¬†\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePreviously, each cache node was given a fixed number of slots\u003c/span\u003e\u003cb\u003e, \u003c/b\u003e\u003cspan\u003ewhere each slot would be assigned a chunk. While this simplified the code, it meant that \u003c/span\u003e\u003ci\u003e\u003cspan\u003eundersized chunks\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e of data would have more space allocated for them than required.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor instance, on a 3TB cache node, we would have 200 slots, where each slot was expected to hold a 15GB chunk. However, if any chunks were undersized (say 10GB instead of 15GB), this would result in extra space (5GB) being allocated but not used. On clusters where we‚Äôd have thousands of chunks, this quickly led to a rather large percentage of space being allocated but unused.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAn additional problem with fixed-sized chunks was that some chunks were actually \u003c/span\u003e\u003ci\u003e\u003cspan\u003ebigger \u003c/span\u003e\u003c/i\u003e\u003cspan\u003ethan our assumed size. This could potentially happen whenever Astra created a recovery task to catch up on older data. We create recovery tasks based on the number of messages that we‚Äôre behind and \u003c/span\u003e\u003ci\u003e\u003cspan\u003enot\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e the size of data we‚Äôre behind. If the average size of each message is bigger than we expect, this can result in an oversized chunk being created, which is even worse than undersized chunks as it means we \u003c/span\u003e\u003ci\u003e\u003cspan\u003earen‚Äôt \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eallocating enough space.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eDesigning Dynamic Chunks\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" width=\"934\" height=\"840\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/11/unnamed.png?w=640\" alt=\"\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/11/unnamed.png 934w, https://slack.engineering/wp-content/uploads/sites/7/2024/11/unnamed.png?resize=640,576 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/11/unnamed.png?resize=768,691 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/11/unnamed.png?resize=380,342 380w, https://slack.engineering/wp-content/uploads/sites/7/2024/11/unnamed.png?resize=800,719 800w\" sizes=\"(max-width: 934px) 100vw, 934px\"/\u003e\u003ci\u003e\u003cspan\u003eAstra‚Äôs architecture diagram ‚Äì ‚ÄúZK‚Äù is the Zookeeper store which holds metadata.\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn order to build dynamic chunks, we had to modify two parts of Astra: the Cluster Manager and the Cache.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eRedesigning Cache Nodes\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe first looked at how cache nodes are structured: Previously, whenever a cache node came online, it would advertise its number of slots in Zookeeper (our centralized coordination store). Then, the Astra manager would assign each slot a chunk, and the cache node would go and download and serve that chunk.¬†\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEach cache node has a lifecycle:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eCache node comes online, advertises the # of slots it has.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eManager picks up on the slots, and assigns a chunk to each one.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eEach cache node downloads the chunks assigned to its slots.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eThis had the benefit of the slots being \u003c/span\u003e\u003ci\u003e\u003cspan\u003eephemeral, \u003c/span\u003e\u003c/i\u003e\u003cspan\u003emeaning\u003c/span\u003e \u003cspan\u003ewhenever a cache node went offline, its slots would disappear from Zookeeper, and the manager would reassign the chunks the slots used to hold.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHowever, with dynamic chunks, each cache node could only advertise their \u003c/span\u003e\u003ci\u003e\u003cspan\u003ecapacity, \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eas it would not know ahead of time how many chunks it would be assigned. This meant we unfortunately could no longer rely on slots to provide these benefits to us.¬†\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo fix these two problems, we decided to persist two new types of data in Zookeeper: the cache node assignment and the cache node metadata.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere‚Äôs a quick breakdown:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eCache Node Assignment: a mapping of chunk ID to cache node\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eCache Node Metadata: metadata about each cache node, including capacity, hostname, etc.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eUtilizing these two new types of data, the new flow looks like this:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eCache node comes online, advertises its disk space.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eManager picks up on the disk space each cache node has, and creates assignments for each cache node, utilizing bin packing\u003c/span\u003e \u003cspan\u003eto minimize the number of cache nodes it uses.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eCache nodes pick up on the assignments that were created for it, and downloads its chunks.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\u003cspan\u003eRedesigning the manager\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThe next change was in the manager, upgrading it to utilize the two new types of data we introduced: the cache node assignments and the cache node metadata.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo utilize the cache node assignments, we decided to implement first-fit bin packing\u003c/span\u003e \u003cspan\u003eto decide which cache node should be assigned which chunk. We then used the cache node metadata in order to make appropriate decisions regarding whether or not we could fit a certain chunk into a given cache node.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePreviously, the logic for assigning slots was:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eGrab the list of slots\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eGrab the list of chunks to assign\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eZip down both lists, assigning a slot to a chunk\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eNow, the logic looks like this:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eGrab list of chunks to assign\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eGrab list of cache nodes\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eFor each chunk\u003c/span\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003ePerform first-fit bin packing to determine which cache node it should be assigned to\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003ePersist the mapping of cache node to chunk\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cspan\u003eBin Packing\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThe most juicy part of redesigning the manager was implementing the first-fit bin packing. It‚Äôs a well-known problem of minimizing the number of bins (cache nodes) used to hold a certain amount of items (chunks). We decided to use first-fit bin packing, favoring it for its speed and ease of implementation.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eUsing pseudocode, we describe the bin-packing algorithm:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efor each chunk\n  for each cache node\n    if the current chunk fits into the cache node:\n      assign the chunk\n    else:\n      move on to the next cache node\n  if there aren‚Äôt any cache nodes left and the chunk hasn‚Äôt been assigned:\n    create a new cache node\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eThis helped ensure that we were able to pack the cache nodes as tightly as possible, resulting in a higher utilization of allocated space.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eRolling it out\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eOverall, this was a significant change to the Astra codebase. It touched many key parts of Astra, essentially rewriting all of the logic that handled the assignment and downloading of chunks. With such a change, we wanted to be careful with the roll out to ensure that nothing would break.¬†\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo ensure nothing would break we did the following:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eHosted two replicas of the same data\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003ePlaced all dynamic chunk code behind a feature flag\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWe leaned heavily on these two guardrails in order to ensure a safe roll out.¬†\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHosting two replicas of the same data allowed us to incrementally deploy to one of the two replicas and monitor its behavior. It also ensured that if our changes ever broke anything, we‚Äôd still have a second replica able to serve the data.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHaving all the code behind a feature flag allowed us to merge the code into master early on, as it wouldn‚Äôt run unless explicitly enabled. It also allowed us to incrementally roll out and test our changes. We started with smaller clusters, before moving on to bigger and bigger clusters after verifying everything worked.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eResults\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhat kind of results did we end up seeing from this? For starters, we were able to reduce the # of cache nodes required by up to 50% for our clusters with many undersized chunks! Overall our cache node costs were reduced by 20%, giving us significant cost savings for operating Astra.\u003c/span\u003e\u003c/p\u003e\n\u003ch4\u003e\u003cspan\u003eAcknowledgments\u003c/span\u003e\u003c/h4\u003e\n\u003cp\u003e\u003cspan\u003eA huge shout out to everyone who has helped along the way to bring dynamic chunks to life:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan\u003eBryan Burkholder\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eGeorge Luong\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eRyan Katkov\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003ci\u003eInterested in building innovative projects and making developers‚Äô work lives easier? We‚Äôre hiring üíº\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://slack.com/careers/dept/engineering#openings?utm_source=blog_eng\"\u003e\u003cspan\u003eApply now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\n\n\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-11-18T22:06:23Z",
  "modifiedTime": null
}
