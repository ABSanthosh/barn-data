{
  "id": "645e7016-e822-4503-a9e5-bb87e3f642b0",
  "title": "Introducing Netflix’s Key-Value Data Abstraction Layer",
  "link": "https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Wed, 18 Sep 2024 22:49:04 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": null,
  "byline": "Netflix Technology Blog",
  "length": 21936,
  "excerpt": "At Netflix our ability to deliver seamless, high-quality, streaming experiences to millions of users hinges on robust, global backend infrastructure. Central to this infrastructure is our use of…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Vidhya Arvind, Rajasekhar Ummadisetty, Joey Lynch, Vinay ChellaIntroductionAt Netflix our ability to deliver seamless, high-quality, streaming experiences to millions of users hinges on robust, global backend infrastructure. Central to this infrastructure is our use of multiple online distributed databases such as Apache Cassandra, a NoSQL database known for its high availability and scalability. Cassandra serves as the backbone for a diverse array of use cases within Netflix, ranging from user sign-ups and storing viewing histories to supporting real-time analytics and live streaming.Over time as new key-value databases were introduced and service owners launched new use cases, we encountered numerous challenges with datastore misuse. Firstly, developers struggled to reason about consistency, durability and performance in this complex global deployment across multiple stores. Second, developers had to constantly re-learn new data modeling practices and common yet critical data access patterns. These include challenges with tail latency and idempotency, managing “wide” partitions with many rows, handling single large “fat” columns, and slow response pagination. Additionally, the tight coupling with multiple native database APIs — APIs that continually evolve and sometimes introduce backward-incompatible changes — resulted in org-wide engineering efforts to maintain and optimize our microservice’s data access.To overcome these challenges, we developed a holistic approach that builds upon our Data Gateway Platform. This approach led to the creation of several foundational abstraction services, the most mature of which is our Key-Value (KV) Data Abstraction Layer (DAL). This abstraction simplifies data access, enhances the reliability of our infrastructure, and enables us to support the broad spectrum of use cases that Netflix demands with minimal developer effort.In this post, we dive deep into how Netflix’s KV abstraction works, the architectural principles guiding its design, the challenges we faced in scaling diverse use cases, and the technical innovations that have allowed us to achieve the performance and reliability required by Netflix’s global operations.The Key-Value ServiceThe KV data abstraction service was introduced to solve the persistent challenges we faced with data access patterns in our distributed databases. Our goal was to build a versatile and efficient data storage solution that could handle a wide variety of use cases, ranging from the simplest hashmaps to more complex data structures, all while ensuring high availability, tunable consistency, and low latency.Data ModelAt its core, the KV abstraction is built around a two-level map architecture. The first level is a hashed string ID (the primary key), and the second level is a sorted map of a key-value pair of bytes. This model supports both simple and complex data models, balancing flexibility and efficiency.HashMap\u003cString, SortedMap\u003cBytes, Bytes\u003e\u003eFor complex data models such as structured Records or time-ordered Events, this two-level approach handles hierarchical structures effectively, allowing related data to be retrieved together. For simpler use cases, it also represents flat key-value Maps (e.g. id → {\"\" → value}) or named Sets (e.g.id → {key → \"\"}). This adaptability allows the KV abstraction to be used in hundreds of diverse use cases, making it a versatile solution for managing both simple and complex data models in large-scale infrastructures like Netflix.The KV data can be visualized at a high level, as shown in the diagram below, where three records are shown.message Item ( Bytes key, Bytes value, Metadata metadata, Integer chunk)Database Agnostic AbstractionThe KV abstraction is designed to hide the implementation details of the underlying database, offering a consistent interface to application developers regardless of the optimal storage system for that use case. While Cassandra is one example, the abstraction works with multiple data stores like EVCache, DynamoDB, RocksDB, etc…For example, when implemented with Cassandra, the abstraction leverages Cassandra’s partitioning and clustering capabilities. The record ID acts as the partition key, and the item key as the clustering column:The corresponding Data Definition Language (DDL) for this structure in Cassandra is:CREATE TABLE IF NOT EXISTS \u003cns\u003e.\u003ctable\u003e ( id text, key blob, value blob, value_metadata blob,PRIMARY KEY (id, key))WITH CLUSTERING ORDER BY (key \u003cASC|DESC\u003e)Namespace: Logical and Physical ConfigurationA namespace defines where and how data is stored, providing logical and physical separation while abstracting the underlying storage systems. It also serves as central configuration of access patterns such as consistency or latency targets. Each namespace may use different backends: Cassandra, EVCache, or combinations of multiple. This flexibility allows our Data Platform to route different use cases to the most suitable storage system based on performance, durability, and consistency needs. Developers just provide their data problem rather than a database solution!In this example configuration, the ngsegment namespace is backed by both a Cassandra cluster and an EVCache caching layer, allowing for highly durable persistent storage and lower-latency point reads.\"persistence_configuration\":[ { \"id\":\"PRIMARY_STORAGE\", \"physical_storage\": { \"type\":\"CASSANDRA\", \"cluster\":\"cassandra_kv_ngsegment\", \"dataset\":\"ngsegment\", \"table\":\"ngsegment\", \"regions\": [\"us-east-1\"], \"config\": { \"consistency_scope\": \"LOCAL\", \"consistency_target\": \"READ_YOUR_WRITES\" } } }, { \"id\":\"CACHE\", \"physical_storage\": { \"type\":\"CACHE\", \"cluster\":\"evcache_kv_ngsegment\" }, \"config\": { \"default_cache_ttl\": 180s } } ] Key APIs of the KV AbstractionTo support diverse use-cases, the KV abstraction provides four basic CRUD APIs:PutItems — Write one or more Items to a RecordThe PutItems API is an upsert operation, it can insert new data or update existing data in the two-level map structure.message PutItemRequest ( IdempotencyToken idempotency_token, string namespace, string id, List\u003cItem\u003e items)As you can see, the request includes the namespace, Record ID, one or more items, and an idempotency token to ensure retries of the same write are safe. Chunked data can be written by staging chunks and then committing them with appropriate metadata (e.g. number of chunks).GetItems — Read one or more Items from a RecordThe GetItemsAPI provides a structured and adaptive way to fetch data using ID, predicates, and selection mechanisms. This approach balances the need to retrieve large volumes of data while meeting stringent Service Level Objectives (SLOs) for performance and reliability.message GetItemsRequest ( String namespace, String id, Predicate predicate, Selection selection, Map\u003cString, Struct\u003e signals)The GetItemsRequest includes several key parameters:Namespace: Specifies the logical dataset or tableId: Identifies the entry in the top-level HashMapPredicate: Filters the matching items and can retrieve all items (match_all), specific items (match_keys), or a range (match_range)Selection: Narrows returned responses for example page_size_bytes for pagination, item_limit for limiting the total number of items across pages and include/exclude to include or exclude large values from responsesSignals: Provides in-band signaling to indicate client capabilities, such as supporting client compression or chunking.The GetItemResponse message contains the matching data:message GetItemResponse ( List\u003cItem\u003e items, Optional\u003cString\u003e next_page_token)Items: A list of retrieved items based on the Predicate and Selection defined in the request.Next Page Token: An optional token indicating the position for subsequent reads if needed, essential for handling large data sets across multiple requests. Pagination is a critical component for efficiently managing data retrieval, especially when dealing with large datasets that could exceed typical response size limits.DeleteItems — Delete one or more Items from a RecordThe DeleteItems API provides flexible options for removing data, including record-level, item-level, and range deletes — all while supporting idempotency.message DeleteItemsRequest ( IdempotencyToken idempotency_token, String namespace, String id, Predicate predicate)Just like in the GetItems API, the Predicate allows one or more Items to be addressed at once:Record-Level Deletes (match_all): Removes the entire record in constant latency regardless of the number of items in the record.Item-Range Deletes (match_range): This deletes a range of items within a Record. Useful for keeping “n-newest” or prefix path deletion.Item-Level Deletes (match_keys): Deletes one or more individual items.Some storage engines (any store which defers true deletion) such as Cassandra struggle with high volumes of deletes due to tombstone and compaction overhead. Key-Value optimizes both record and range deletes to generate a single tombstone for the operation — you can learn more about tombstones in About Deletes and Tombstones.Item-level deletes create many tombstones but KV hides that storage engine complexity via TTL-based deletes with jitter. Instead of immediate deletion, item metadata is updated as expired with randomly jittered TTL applied to stagger deletions. This technique maintains read pagination protections. While this doesn’t completely solve the problem it reduces load spikes and helps maintain consistent performance while compaction catches up. These strategies help maintain system performance, reduce read overhead, and meet SLOs by minimizing the impact of deletes.Complex Mutate and Scan APIsBeyond simple CRUD on single Records, KV also supports complex multi-item and multi-record mutations and scans via MutateItems and ScanItems APIs. PutItems also supports atomic writes of large blob data within a single Item via a chunked protocol. These complex APIs require careful consideration to ensure predictable linear low-latency and we will share details on their implementation in a future post.Design Philosophies for reliable and predictable performanceIdempotency to fight tail latenciesTo ensure data integrity the PutItems and DeleteItems APIs use idempotency tokens, which uniquely identify each mutative operation and guarantee that operations are logically executed in order, even when hedged or retried for latency reasons. This is especially crucial in last-write-wins databases like Cassandra, where ensuring the correct order and de-duplication of requests is vital.In the Key-Value abstraction, idempotency tokens contain a generation timestamp and random nonce token. Either or both may be required by backing storage engines to de-duplicate mutations.message IdempotencyToken ( Timestamp generation_time, String token)At Netflix, client-generated monotonic tokens are preferred due to their reliability, especially in environments where network delays could impact server-side token generation. This combines a client provided monotonic generation_time timestamp with a 128 bit random UUID token. Although clock-based token generation can suffer from clock skew, our tests on EC2 Nitro instances show drift is minimal (under 1 millisecond). In some cases that require stronger ordering, regionally unique tokens can be generated using tools like Zookeeper, or globally unique tokens such as a transaction IDs can be used.The following graphs illustrate the observed clock skew on our Cassandra fleet, suggesting the safety of this technique on modern cloud VMs with direct access to high-quality clocks. To further maintain safety, KV servers reject writes bearing tokens with large drift both preventing silent write discard (write has timestamp far in past) and immutable doomstones (write has a timestamp far in future) in storage engines vulnerable to those.Handling Large Data through ChunkingKey-Value is also designed to efficiently handle large blobs, a common challenge for traditional key-value stores. Databases often face limitations on the amount of data that can be stored per key or partition. To address these constraints, KV uses transparent chunking to manage large data efficiently.For items smaller than 1 MiB, data is stored directly in the main backing storage (e.g. Cassandra), ensuring fast and efficient access. However, for larger items, only the id, key, and metadata are stored in the primary storage, while the actual data is split into smaller chunks and stored separately in chunk storage. This chunk storage can also be Cassandra but with a different partitioning scheme optimized for handling large values. The idempotency token ties all these writes together into one atomic operation.By splitting large items into chunks, we ensure that latency scales linearly with the size of the data, making the system both predictable and efficient. A future blog post will describe the chunking architecture in more detail, including its intricacies and optimization strategies.Client-Side CompressionThe KV abstraction leverages client-side payload compression to optimize performance, especially for large data transfers. While many databases offer server-side compression, handling compression on the client side reduces expensive server CPU usage, network bandwidth, and disk I/O. In one of our deployments, which helps power Netflix’s search, enabling client-side compression reduced payload sizes by 75%, significantly improving cost efficiency.Smarter PaginationWe chose payload size in bytes as the limit per response page rather than the number of items because it allows us to provide predictable operation SLOs. For instance, we can provide a single-digit millisecond SLO on a 2 MiB page read. Conversely, using the number of items per page as the limit would result in unpredictable latencies due to significant variations in item size. A request for 10 items per page could result in vastly different latencies if each item was 1 KiB versus 1 MiB.Using bytes as a limit poses challenges as few backing stores support byte-based pagination; most data stores use the number of results e.g. DynamoDB and Cassandra limit by number of items or rows. To address this, we use a static limit for the initial queries to the backing store, query with this limit, and process the results. If more data is needed to meet the byte limit, additional queries are executed until the limit is met, the excess result is discarded and a page token is generated.This static limit can lead to inefficiencies, one large item in the result may cause us to discard many results, while small items may require multiple iterations to fill a page, resulting in read amplification. To mitigate these issues, we implemented adaptive pagination which dynamically tunes the limits based on observed data.Adaptive PaginationWhen an initial request is made, a query is executed in the storage engine, and the results are retrieved. As the consumer processes these results, the system tracks the number of items consumed and the total size used. This data helps calculate an approximate item size, which is stored in the page token. For subsequent page requests, this stored information allows the server to apply the appropriate limits to the underlying storage, reducing unnecessary work and minimizing read amplification.While this method is effective for follow-up page requests, what happens with the initial request? In addition to storing item size information in the page token, the server also estimates the average item size for a given namespace and caches it locally. This cached estimate helps the server set a more optimal limit on the backing store for the initial request, improving efficiency. The server continuously adjusts this limit based on recent query patterns or other factors to keep it accurate. For subsequent pages, the server uses both the cached data and the information in the page token to fine-tune the limits.In addition to adaptive pagination, a mechanism is in place to send a response early if the server detects that processing the request is at risk of exceeding the request’s latency SLO.For example, let us assume a client submits a GetItems request with a per-page limit of 2 MiB and a maximum end-to-end latency limit of 500ms. While processing this request, the server retrieves data from the backing store. This particular record has thousands of small items so it would normally take longer than the 500ms SLO to gather the full page of data. If this happens, the client would receive an SLO violation error, causing the request to fail even though there is nothing exceptional. To prevent this, the server tracks the elapsed time while fetching data. If it determines that continuing to retrieve more data might breach the SLO, the server will stop processing further results and return a response with a pagination token.This approach ensures that requests are processed within the SLO, even if the full page size isn’t met, giving clients predictable progress. Furthermore, if the client is a gRPC server with proper deadlines, the client is smart enough not to issue further requests, reducing useless work.If you want to know more, the How Netflix Ensures Highly-Reliable Online Stateful Systems article talks in further detail about these and many other techniques.SignalingKV uses in-band messaging we call signaling that allows the dynamic configuration of the client and enables it to communicate its capabilities to the server. This ensures that configuration settings and tuning parameters can be exchanged seamlessly between the client and server. Without signaling, the client would need static configuration — requiring a redeployment for each change — or, with dynamic configuration, would require coordination with the client team.For server-side signals, when the client is initialized, it sends a handshake to the server. The server responds back with signals, such as target or max latency SLOs, allowing the client to dynamically adjust timeouts and hedging policies. Handshakes are then made periodically in the background to keep the configuration current. For client-communicated signals, the client, along with each request, communicates its capabilities, such as whether it can handle compression, chunking, and other features.KV Usage @ NetflixThe KV abstraction powers several key Netflix use cases, including:Streaming Metadata: High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time.User Profiles: Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices.Messaging: Storage and retrieval of push registry for messaging needs, enabling the millions of requests to flow through.Real-Time Analytics: This persists large-scale impression and provides insights into user behavior and system performance, moving data from offline to online and vice versa.Future EnhancementsLooking forward, we plan to enhance the KV abstraction with:Lifecycle Management: Fine-grained control over data retention and deletion.Summarization: Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows.New Storage Engines: Integration with more storage systems to support new use cases.Dictionary Compression: Further reducing data size while maintaining performance.ConclusionThe Key-Value service at Netflix is a flexible, cost-effective solution that supports a wide range of data patterns and use cases, from low to high traffic scenarios, including critical Netflix streaming use-cases. The simple yet robust design allows it to handle diverse data models like HashMaps, Sets, Event storage, Lists, and Graphs. It abstracts the complexity of the underlying databases from our developers, which enables our application engineers to focus on solving business problems instead of becoming experts in every storage engine and their distributed consistency models. As Netflix continues to innovate in online datastores, the KV abstraction remains a central component in managing data efficiently and reliably at scale, ensuring a solid foundation for future growth.Acknowledgments: Special thanks to our stunning colleagues who contributed to Key Value’s success: William Schor, Mengqing Wang, Chandrasekhar Thumuluru, Rajiv Shringi, John Lu, George Cambell, Ammar Khaku, Jordan West, Chris Lohfink, Matt Lehman, and the whole online datastores team (ODS, f.k.a CDE).",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:755/0*9Ny8Uc-diSDnVGnk",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page-----1ea8a0a11b30--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page-----1ea8a0a11b30--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"3a78\"\u003e\u003ca href=\"https://www.linkedin.com/in/vidhya-arvind-11908723\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVidhya Arvind\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/rummadis/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRajasekhar Ummadisetty\u003c/a\u003e, \u003ca href=\"https://jolynch.github.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJoey Lynch\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/vinaychella\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVinay Chella\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"8c5d\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"8fe5\"\u003eAt Netflix our ability to deliver seamless, high-quality, streaming experiences to millions of users hinges on robust, \u003cem\u003eglobal\u003c/em\u003e backend infrastructure. Central to this infrastructure is our use of multiple online distributed databases such as \u003ca href=\"https://cassandra.apache.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApache Cassandra\u003c/a\u003e, a NoSQL database known for its high availability and scalability. Cassandra serves as the backbone for a diverse array of use cases within Netflix, ranging from user sign-ups and storing viewing histories to supporting real-time analytics and live streaming.\u003c/p\u003e\u003cp id=\"255e\"\u003eOver time as new key-value databases were introduced and service owners launched new use cases, we encountered numerous challenges with datastore misuse. Firstly, developers struggled to reason about consistency, durability and performance in this complex global deployment across multiple stores. Second, developers had to constantly re-learn new data modeling practices and common yet critical data access patterns. These include challenges with tail latency and idempotency, managing “wide” partitions with many rows, handling single large “fat” columns, and slow response pagination. Additionally, the tight coupling with multiple native database APIs — APIs that continually evolve and sometimes introduce backward-incompatible changes — resulted in org-wide engineering efforts to maintain and optimize our microservice’s data access.\u003c/p\u003e\u003cp id=\"e872\"\u003eTo overcome these challenges, we developed a holistic approach that builds upon our \u003ca href=\"https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6\" rel=\"noopener\"\u003eData Gateway Platform\u003c/a\u003e. This approach led to the creation of several foundational abstraction services, the most mature of which is our Key-Value (KV) Data Abstraction Layer (DAL). This abstraction simplifies data access, enhances the reliability of our infrastructure, and enables us to support the broad spectrum of use cases that Netflix demands with minimal developer effort.\u003c/p\u003e\u003cp id=\"92d2\"\u003eIn this post, we dive deep into how Netflix’s KV abstraction works, the architectural principles guiding its design, the challenges we faced in scaling diverse use cases, and the technical innovations that have allowed us to achieve the performance and reliability required by Netflix’s global operations.\u003c/p\u003e\u003ch2 id=\"9a98\"\u003e\u003cstrong\u003eThe Key-Value Service\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"4bde\"\u003eThe KV data abstraction service was introduced to solve the persistent challenges we faced with data access patterns in our distributed databases. Our goal was to build a versatile and efficient data storage solution that could handle a wide variety of use cases, ranging from the simplest hashmaps to more complex data structures, all while ensuring high availability, tunable consistency, and low latency.\u003c/p\u003e\u003ch2 id=\"653a\"\u003eData Model\u003c/h2\u003e\u003cp id=\"adb9\"\u003eAt its core, the KV abstraction is built around a \u003cstrong\u003e\u003cem\u003etwo-level map\u003c/em\u003e \u003c/strong\u003earchitecture. The first level is a hashed string \u003cstrong\u003eID\u003c/strong\u003e (the primary key), and the second level is a \u003cstrong\u003e\u003cem\u003esorted map of a key-value pair of bytes\u003c/em\u003e\u003c/strong\u003e. This model supports both simple and complex data models, balancing flexibility and efficiency.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"e089\"\u003eHashMap\u0026lt;String, SortedMap\u0026lt;Bytes, Bytes\u0026gt;\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"9c5f\"\u003eFor complex data models such as structured \u003ccode\u003eRecords\u003c/code\u003e or time-ordered \u003ccode\u003eEvents\u003c/code\u003e, this two-level approach handles hierarchical structures effectively, allowing related data to be retrieved together. For simpler use cases, it also represents flat key-value \u003ccode\u003eMaps\u003c/code\u003e (e.g. \u003ccode\u003eid → {\u0026#34;\u0026#34; → value}\u003c/code\u003e) or named \u003ccode\u003eSets\u003c/code\u003e (e.g.\u003ccode\u003eid → {key → \u0026#34;\u0026#34;}\u003c/code\u003e). This adaptability allows the KV abstraction to be used in hundreds of diverse use cases, making it a versatile solution for managing both simple and complex data models in large-scale infrastructures like Netflix.\u003c/p\u003e\u003cp id=\"e359\"\u003eThe KV data can be visualized at a high level, as shown in the diagram below, where three records are shown.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cpre\u003e\u003cspan id=\"1668\"\u003emessage Item (   \u003cbr/\u003e  Bytes    key,\u003cbr/\u003e  Bytes    value,\u003cbr/\u003e  Metadata metadata,\u003cbr/\u003e  Integer  chunk\u003cbr/\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"9585\"\u003eDatabase Agnostic Abstraction\u003c/h2\u003e\u003cp id=\"7654\"\u003eThe KV abstraction is designed to hide the implementation details of the underlying database, offering a consistent interface to application developers regardless of the optimal storage system for that use case. While Cassandra is one example, the abstraction works with multiple data stores like \u003ca href=\"https://github.com/Netflix/EVCache\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eEVCache\u003c/a\u003e, \u003ca href=\"https://aws.amazon.com/dynamodb/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDynamoDB\u003c/a\u003e, \u003ca href=\"https://rocksdb.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRocksDB\u003c/a\u003e, etc…\u003c/p\u003e\u003cp id=\"6b60\"\u003eFor example, when implemented with Cassandra, the abstraction leverages Cassandra’s partitioning and clustering capabilities. The record \u003cstrong\u003e\u003cem\u003eID\u003c/em\u003e\u003c/strong\u003e acts as the partition key, and the item \u003cstrong\u003e\u003cem\u003ekey\u003c/em\u003e\u003c/strong\u003e as the clustering column:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"17d6\"\u003eThe corresponding Data Definition Language (DDL) for this structure in Cassandra is:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"e122\"\u003eCREATE TABLE IF NOT EXISTS \u0026lt;ns\u0026gt;.\u0026lt;table\u0026gt; (\u003cbr/\u003e  id             text,\u003cbr/\u003e  key            blob,\u003cbr/\u003e  value          blob,\u003cbr/\u003e  value_metadata blob,\u003cp\u003ePRIMARY KEY (id, key))\u003cbr/\u003eWITH CLUSTERING ORDER BY (key \u0026lt;ASC|DESC\u0026gt;)\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"dd6f\"\u003eNamespace: Logical and Physical Configuration\u003c/h2\u003e\u003cp id=\"6e76\"\u003eA \u003cstrong\u003enamespace\u003c/strong\u003e defines where and how data is stored, providing logical and physical separation while abstracting the underlying storage systems. It also serves as central configuration of access patterns such as consistency or latency targets. Each namespace may use different backends: Cassandra, EVCache, or combinations of multiple. This flexibility allows our Data Platform to route different use cases to the most suitable storage system based on performance, durability, and consistency needs. Developers just provide their data problem rather than a database solution!\u003c/p\u003e\u003cp id=\"898b\"\u003eIn this example configuration, the \u003ccode\u003engsegment\u003c/code\u003e namespace is backed by both a Cassandra cluster and an EVCache caching layer, allowing for highly durable persistent storage \u003cem\u003eand\u003c/em\u003e lower-latency point reads.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"294b\"\u003e\u0026#34;persistence_configuration\u0026#34;:[                                                   \u003cbr/\u003e  {                                                                           \u003cbr/\u003e    \u0026#34;id\u0026#34;:\u0026#34;PRIMARY_STORAGE\u0026#34;,                                                 \u003cbr/\u003e    \u0026#34;physical_storage\u0026#34;: {                                                    \u003cbr/\u003e      \u0026#34;type\u0026#34;:\u0026#34;CASSANDRA\u0026#34;,                                                 \u003cbr/\u003e      \u0026#34;cluster\u0026#34;:\u0026#34;cassandra_kv_ngsegment\u0026#34;,                                \u003cbr/\u003e      \u0026#34;dataset\u0026#34;:\u0026#34;ngsegment\u0026#34;,                                             \u003cbr/\u003e      \u0026#34;table\u0026#34;:\u0026#34;ngsegment\u0026#34;,                                               \u003cbr/\u003e      \u0026#34;regions\u0026#34;: [\u0026#34;us-east-1\u0026#34;],\u003cbr/\u003e      \u0026#34;config\u0026#34;: {\u003cbr/\u003e        \u0026#34;consistency_scope\u0026#34;: \u0026#34;LOCAL\u0026#34;,\u003cbr/\u003e        \u0026#34;consistency_target\u0026#34;: \u0026#34;READ_YOUR_WRITES\u0026#34;\u003cbr/\u003e      }                                            \u003cbr/\u003e    }                                                                       \u003cbr/\u003e  },                                                                          \u003cbr/\u003e  {                                                                           \u003cbr/\u003e    \u0026#34;id\u0026#34;:\u0026#34;CACHE\u0026#34;,                                                           \u003cbr/\u003e    \u0026#34;physical_storage\u0026#34;: {                                                    \u003cbr/\u003e      \u0026#34;type\u0026#34;:\u0026#34;CACHE\u0026#34;,                                                     \u003cbr/\u003e      \u0026#34;cluster\u0026#34;:\u0026#34;evcache_kv_ngsegment\u0026#34;                                   \u003cbr/\u003e     },                                                                      \u003cbr/\u003e     \u0026#34;config\u0026#34;: {                                                              \u003cbr/\u003e       \u0026#34;default_cache_ttl\u0026#34;: 180s                                             \u003cbr/\u003e     }                                                                       \u003cbr/\u003e  }                                                                           \u003cbr/\u003e] \u003cbr/\u003e \u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"d313\"\u003e\u003cstrong\u003eKey APIs of the KV Abstraction\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"0ffe\"\u003eTo support diverse use-cases, the KV abstraction provides four basic CRUD APIs:\u003c/p\u003e\u003ch2 id=\"b9f2\"\u003ePutItems \u003cstrong\u003e— Write one or more Items to a Record\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"4d14\"\u003eThe \u003ccode\u003ePutItems\u003c/code\u003e API is an upsert operation, it can insert new data or update existing data in the two-level map structure.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"2ca6\"\u003emessage PutItemRequest (\u003cbr/\u003e  IdempotencyToken idempotency_token,\u003cbr/\u003e  string           namespace, \u003cbr/\u003e  string           id, \u003cbr/\u003e  List\u0026lt;Item\u0026gt;       items\u003cbr/\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"d3db\"\u003eAs you can see, the request includes the namespace, Record ID, one or more items, and an \u003cstrong\u003eidempotency token\u003c/strong\u003e to ensure retries of the same write are safe. Chunked data can be written by staging chunks and then committing them with appropriate metadata (e.g. number of chunks).\u003c/p\u003e\u003ch2 id=\"d04a\"\u003eGetItems \u003cstrong\u003e— Read one or more Items from a Record\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"487c\"\u003eThe \u003ccode\u003eGetItems\u003c/code\u003eAPI provides a structured and adaptive way to fetch data using ID, predicates, and selection mechanisms. This approach balances the need to retrieve large volumes of data while meeting stringent Service Level Objectives (SLOs) for performance and reliability.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a4b2\"\u003emessage GetItemsRequest (\u003cbr/\u003e  String              namespace,\u003cbr/\u003e  String              id,\u003cbr/\u003e  Predicate           predicate,\u003cbr/\u003e  Selection           selection,\u003cbr/\u003e  Map\u0026lt;String, Struct\u0026gt; signals\u003cbr/\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"ed18\"\u003eThe \u003ccode\u003eGetItemsRequest\u003c/code\u003e includes several key parameters:\u003c/p\u003e\u003cul\u003e\u003cli id=\"f54d\"\u003e\u003cstrong\u003eNamespace\u003c/strong\u003e: Specifies the logical dataset or table\u003c/li\u003e\u003cli id=\"a823\"\u003e\u003cstrong\u003eId\u003c/strong\u003e: Identifies the entry in the top-level HashMap\u003c/li\u003e\u003cli id=\"b7f8\"\u003e\u003cstrong\u003ePredicate\u003c/strong\u003e: Filters the matching items and can retrieve all items (\u003ccode\u003ematch_all\u003c/code\u003e), specific items (\u003ccode\u003ematch_keys\u003c/code\u003e), or a range (\u003ccode\u003ematch_range\u003c/code\u003e)\u003c/li\u003e\u003cli id=\"a2e5\"\u003e\u003cstrong\u003eSelection\u003c/strong\u003e: Narrows returned responses for example \u003ccode\u003epage_size_bytes\u003c/code\u003e for pagination, \u003ccode\u003eitem_limit\u003c/code\u003e for limiting the total number of items across pages and \u003ccode\u003einclude\u003c/code\u003e/\u003ccode\u003eexclude\u003c/code\u003e to include or exclude large values from responses\u003c/li\u003e\u003cli id=\"9b65\"\u003e\u003cstrong\u003eSignals:\u003c/strong\u003e Provides in-band signaling to indicate client capabilities, such as supporting client compression or chunking.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"697f\"\u003eThe \u003ccode\u003eGetItemResponse\u003c/code\u003e message contains the matching data:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"f20b\"\u003emessage GetItemResponse (\u003cbr/\u003e  List\u0026lt;Item\u0026gt;       items,\u003cbr/\u003e  Optional\u0026lt;String\u0026gt; next_page_token\u003cbr/\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cul\u003e\u003cli id=\"9d6e\"\u003e\u003cstrong\u003eItems\u003c/strong\u003e: A list of retrieved items based on the \u003ccode\u003ePredicate\u003c/code\u003e and \u003ccode\u003eSelection\u003c/code\u003e defined in the request.\u003c/li\u003e\u003cli id=\"5617\"\u003e\u003cstrong\u003eNext Page Token\u003c/strong\u003e: An optional token indicating the position for subsequent reads if needed, essential for handling large data sets across multiple requests. Pagination is a critical component for efficiently managing data retrieval, especially when dealing with large datasets that could exceed typical response size limits.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"a3d0\"\u003e\u003cstrong\u003eDeleteItems — Delete one or more Items from a Record\u003c/strong\u003e\u003c/h2\u003e\u003cp id=\"b258\"\u003eThe \u003ccode\u003eDeleteItems\u003c/code\u003e API provides flexible options for removing data, including record-level, item-level, and range deletes — all while supporting idempotency.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"329d\"\u003emessage DeleteItemsRequest (\u003cbr/\u003e  IdempotencyToken idempotency_token,\u003cbr/\u003e  String           namespace,\u003cbr/\u003e  String           id,\u003cbr/\u003e  Predicate        predicate\u003cbr/\u003e)\u003cbr/\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"b5bd\"\u003eJust like in the \u003ccode\u003eGetItems\u003c/code\u003e API, the \u003ccode\u003ePredicate\u003c/code\u003e allows one or more Items to be addressed at once:\u003c/p\u003e\u003cul\u003e\u003cli id=\"4e42\"\u003e\u003cstrong\u003eRecord-Level Deletes (match_all)\u003c/strong\u003e: Removes the entire record in constant latency regardless of the number of items in the record.\u003c/li\u003e\u003cli id=\"0b32\"\u003e\u003cstrong\u003eItem-Range Deletes (match_range)\u003c/strong\u003e: This deletes a range of items within a Record. Useful for keeping “n-newest” or prefix path deletion.\u003c/li\u003e\u003cli id=\"7525\"\u003e\u003cstrong\u003eItem-Level Deletes (match_keys)\u003c/strong\u003e: Deletes one or more individual items.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"4f76\"\u003eSome storage engines (any store which defers true deletion) such as Cassandra struggle with high volumes of deletes due to tombstone and compaction overhead. Key-Value optimizes both record and range deletes to generate a single tombstone for the operation — you can learn more about tombstones in \u003ca href=\"https://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAbout Deletes and Tombstones\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"3569\"\u003eItem-level deletes create many tombstones but KV hides that storage engine complexity via \u003cstrong\u003eTTL-based deletes with jitter\u003c/strong\u003e. Instead of immediate deletion, item metadata is updated as expired with randomly jittered TTL applied to stagger deletions. This technique maintains read pagination protections. While this doesn’t completely solve the problem it reduces load spikes and helps maintain consistent performance while compaction catches up. These strategies help maintain system performance, reduce read overhead, and meet SLOs by minimizing the impact of deletes.\u003c/p\u003e\u003ch2 id=\"9f4f\"\u003eComplex Mutate and Scan APIs\u003c/h2\u003e\u003cp id=\"d6e0\"\u003eBeyond simple CRUD on single Records, KV also supports complex multi-item and multi-record mutations and scans via \u003ccode\u003eMutateItems\u003c/code\u003e and \u003ccode\u003eScanItems\u003c/code\u003e APIs. \u003ccode\u003ePutItems\u003c/code\u003e also supports atomic writes of large blob data within a single \u003ccode\u003eItem\u003c/code\u003e via a chunked protocol. These complex APIs require careful consideration to ensure predictable linear low-latency and we will share details on their implementation in a future post.\u003c/p\u003e\u003ch2 id=\"4e6f\"\u003eDesign Philosophies for reliable and predictable performance\u003c/h2\u003e\u003ch2 id=\"c223\"\u003eIdempotency to fight tail latencies\u003c/h2\u003e\u003cp id=\"d7cd\"\u003eTo ensure data integrity the \u003ccode\u003ePutItems\u003c/code\u003e and \u003ccode\u003eDeleteItems\u003c/code\u003e APIs use \u003cstrong\u003eidempotency tokens\u003c/strong\u003e, which uniquely identify each mutative operation and guarantee that operations are logically executed in order, even when hedged or retried for latency reasons. This is especially crucial in last-write-wins databases like Cassandra, where ensuring the correct order and de-duplication of requests is vital.\u003c/p\u003e\u003cp id=\"b8d5\"\u003eIn the Key-Value abstraction, idempotency tokens contain a generation timestamp and random nonce token. Either or both may be required by backing storage engines to de-duplicate mutations.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"8899\"\u003emessage IdempotencyToken (\u003cbr/\u003e  Timestamp generation_time,\u003cbr/\u003e  String    token\u003cbr/\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"3089\"\u003eAt Netflix, \u003cstrong\u003eclient-generated monotonic tokens\u003c/strong\u003e are preferred due to their reliability, especially in environments where network delays could impact server-side token generation. This combines a client provided monotonic \u003ccode\u003egeneration_time\u003c/code\u003e timestamp with a 128 bit random UUID \u003ccode\u003etoken\u003c/code\u003e. Although clock-based token generation can suffer from clock skew, our tests on EC2 Nitro instances show drift is minimal (under 1 millisecond). In some cases that require stronger ordering, regionally unique tokens can be generated using tools like Zookeeper, or globally unique tokens such as a transaction IDs can be used.\u003c/p\u003e\u003cp id=\"abaa\"\u003eThe following graphs illustrate the observed \u003ca href=\"https://docs.google.com/document/d/1XLBjQ9scZCy-xIo51Rs--CSdFV781fnp5hXdXTBAk1k/edit\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eclock skew\u003c/a\u003e on our Cassandra fleet, suggesting the safety of this technique on modern cloud VMs with direct access to high-quality clocks. To further maintain safety, KV servers reject writes bearing tokens with large drift both preventing silent write discard (write has timestamp far in past) and immutable doomstones (write has a timestamp far in future) in storage engines vulnerable to those.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"7b42\"\u003eHandling Large Data through Chunking\u003c/h2\u003e\u003cp id=\"4ff0\"\u003eKey-Value is also designed to efficiently handle large blobs, a common challenge for traditional key-value stores. Databases often face limitations on the amount of data that can be stored per key or partition. To address these constraints, KV uses transparent \u003cstrong\u003echunking\u003c/strong\u003e to manage large data efficiently.\u003c/p\u003e\u003cp id=\"2392\"\u003eFor items smaller than 1 MiB, data is stored directly in the main backing storage (e.g. Cassandra), ensuring fast and efficient access. However, for larger items, only the \u003cstrong\u003eid\u003c/strong\u003e, \u003cstrong\u003ekey\u003c/strong\u003e, and \u003cstrong\u003emetadata\u003c/strong\u003e are stored in the primary storage, while the actual data is split into smaller chunks and stored separately in chunk storage. This chunk storage can also be Cassandra but with a different partitioning scheme optimized for handling large values. The idempotency token ties all these writes together into one atomic operation.\u003c/p\u003e\u003cp id=\"605e\"\u003eBy splitting large items into chunks, we ensure that latency scales linearly with the size of the data, making the system both predictable and efficient. A future blog post will describe the \u003cstrong\u003echunking architecture\u003c/strong\u003e in more detail, including its intricacies and optimization strategies.\u003c/p\u003e\u003ch2 id=\"2b4f\"\u003eClient-Side Compression\u003c/h2\u003e\u003cp id=\"5ad2\"\u003eThe KV abstraction leverages client-side payload compression to optimize performance, especially for large data transfers. While many databases offer server-side compression, handling compression on the client side reduces expensive server CPU usage, network bandwidth, and disk I/O. In one of our deployments, which helps power Netflix’s search, enabling client-side compression reduced payload sizes by 75%, significantly improving cost efficiency.\u003c/p\u003e\u003ch2 id=\"5805\"\u003eSmarter Pagination\u003c/h2\u003e\u003cp id=\"739c\"\u003eWe chose payload size in bytes as the limit per response page rather than the number of items because it allows us to provide predictable operation SLOs. For instance, we can provide a single-digit millisecond SLO on a 2 MiB page read. Conversely, using the number of items per page as the limit would result in unpredictable latencies due to significant variations in item size. A request for 10 items per page could result in vastly different latencies if each item was 1 KiB versus 1 MiB.\u003c/p\u003e\u003cp id=\"21ce\"\u003eUsing bytes as a limit poses challenges as few backing stores support byte-based pagination; most data stores use the number of results e.g. DynamoDB and Cassandra limit by number of items or rows. To address this, we use a static limit for the initial queries to the backing store, query with this limit, and process the results. If more data is needed to meet the byte limit, additional queries are executed until the limit is met, the excess result is discarded and a page token is generated.\u003c/p\u003e\u003cp id=\"51d1\"\u003eThis static limit can lead to inefficiencies, one large item in the result may cause us to discard many results, while small items may require multiple iterations to fill a page, resulting in read amplification. To mitigate these issues, we implemented \u003cem\u003eadaptive\u003c/em\u003e pagination which dynamically tunes the limits based on observed data.\u003c/p\u003e\u003ch2 id=\"6f49\"\u003eAdaptive Pagination\u003c/h2\u003e\u003cp id=\"d9f6\"\u003eWhen an initial request is made, a query is executed in the storage engine, and the results are retrieved. As the consumer processes these results, the system tracks the number of items consumed and the total size used. This data helps calculate an approximate item size, which is stored in the page token. For subsequent page requests, this stored information allows the server to apply the appropriate limits to the underlying storage, reducing unnecessary work and minimizing read amplification.\u003c/p\u003e\u003cp id=\"c57f\"\u003eWhile this method is effective for follow-up page requests, what happens with the initial request? In addition to storing item size information in the page token, the server also estimates the average item size for a given namespace and caches it locally. This cached estimate helps the server set a more optimal limit on the backing store for the initial request, improving efficiency. The server continuously adjusts this limit based on recent query patterns or other factors to keep it accurate. For subsequent pages, the server uses both the cached data and the information in the page token to fine-tune the limits.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"e11f\"\u003eIn addition to adaptive pagination, a mechanism is in place to send a response early if the server detects that processing the request is at risk of exceeding the request’s latency SLO.\u003c/p\u003e\u003cp id=\"8c43\"\u003eFor example, let us assume a client submits a \u003ccode\u003eGetItems\u003c/code\u003e request with a per-page limit of 2 MiB and a maximum end-to-end latency limit of 500ms. While processing this request, the server retrieves data from the backing store. This particular record has thousands of small items so it would normally take longer than the 500ms SLO to gather the full page of data. If this happens, the client would receive an SLO violation error, causing the request to fail even though there is nothing exceptional. To prevent this, the server tracks the elapsed time while fetching data. If it determines that continuing to retrieve more data might breach the SLO, the server will stop processing further results and return a response with a pagination token.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"858e\"\u003eThis approach ensures that requests are processed within the SLO, even if the full page size isn’t met, giving clients predictable progress. Furthermore, if the client is a gRPC server with proper deadlines, the client is smart enough not to issue further requests, reducing useless work.\u003c/p\u003e\u003cp id=\"c909\"\u003eIf you want to know more, the \u003ca href=\"https://www.infoq.com/articles/netflix-highly-reliable-stateful-systems/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHow Netflix Ensures Highly-Reliable Online Stateful Systems\u003c/a\u003e article talks in further detail about these and many other techniques.\u003c/p\u003e\u003ch2 id=\"c0c5\"\u003eSignaling\u003c/h2\u003e\u003cp id=\"2e5c\"\u003eKV uses in-band messaging we call \u003cem\u003esignaling\u003c/em\u003e that allows the dynamic configuration of the client and enables it to communicate its capabilities to the server. This ensures that configuration settings and tuning parameters can be exchanged seamlessly between the client and server. Without signaling, the client would need static configuration — requiring a redeployment for each change — or, with dynamic configuration, would require coordination with the client team.\u003c/p\u003e\u003cp id=\"f164\"\u003eFor server-side signals, when the client is initialized, it sends a handshake to the server. The server responds back with signals, such as target or max latency SLOs, allowing the client to dynamically adjust timeouts and hedging policies. Handshakes are then made periodically in the background to keep the configuration current. For client-communicated signals, the client, along with each request, communicates its capabilities, such as whether it can handle compression, chunking, and other features.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"ea21\"\u003eKV Usage @ Netflix\u003c/h2\u003e\u003cp id=\"18e7\"\u003eThe KV abstraction powers several key Netflix use cases, including:\u003c/p\u003e\u003cul\u003e\u003cli id=\"f38f\"\u003e\u003cstrong\u003eStreaming Metadata\u003c/strong\u003e: High-throughput, low-latency access to streaming metadata, ensuring personalized content delivery in real-time.\u003c/li\u003e\u003cli id=\"555b\"\u003e\u003cstrong\u003eUser Profiles\u003c/strong\u003e: Efficient storage and retrieval of user preferences and history, enabling seamless, personalized experiences across devices.\u003c/li\u003e\u003cli id=\"61d8\"\u003e\u003cstrong\u003eMessaging\u003c/strong\u003e: Storage and retrieval of \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/pushy-to-the-limit-evolving-netflixs-websocket-proxy-for-the-future-b468bc0ff658\"\u003epush registry\u003c/a\u003e for messaging needs, enabling the millions of requests to flow through.\u003c/li\u003e\u003cli id=\"bcc9\"\u003e\u003cstrong\u003eReal-Time Analytics\u003c/strong\u003e: This persists large-scale impression and provides insights into user behavior and system performance, \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8\"\u003emoving data from offline to online\u003c/a\u003e and vice versa.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"332b\"\u003eFuture Enhancements\u003c/h2\u003e\u003cp id=\"6a18\"\u003eLooking forward, we plan to enhance the KV abstraction with:\u003c/p\u003e\u003cul\u003e\u003cli id=\"27c2\"\u003e\u003cstrong\u003eLifecycle Management\u003c/strong\u003e: Fine-grained control over data retention and deletion.\u003c/li\u003e\u003cli id=\"3282\"\u003e\u003cstrong\u003eSummarization\u003c/strong\u003e: Techniques to improve retrieval efficiency by summarizing records with many items into fewer backing rows.\u003c/li\u003e\u003cli id=\"249b\"\u003e\u003cstrong\u003eNew Storage Engines\u003c/strong\u003e: Integration with more storage systems to support new use cases.\u003c/li\u003e\u003cli id=\"1ebe\"\u003e\u003cstrong\u003eDictionary Compression\u003c/strong\u003e: Further reducing data size while maintaining performance.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"496e\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"b19f\"\u003eThe Key-Value service at Netflix is a flexible, cost-effective solution that supports a wide range of data patterns and use cases, from low to high traffic scenarios, including critical Netflix streaming use-cases. The simple yet robust design allows it to handle diverse data models like HashMaps, Sets, Event storage, Lists, and Graphs. It abstracts the complexity of the underlying databases from our developers, which enables our application engineers to focus on solving business problems instead of becoming experts in every storage engine and their distributed \u003ca href=\"https://jepsen.io/consistency\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003econsistency models\u003c/a\u003e. As Netflix continues to innovate in online datastores, the KV abstraction remains a central component in managing data efficiently and reliably at scale, ensuring a solid foundation for future growth.\u003c/p\u003e\u003cp id=\"683f\"\u003e\u003cstrong\u003e\u003cem\u003eAcknowledgments:\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e Special thanks to our stunning colleagues who contributed to Key Value’s success: \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/william-schor/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eWilliam Schor\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/mengqingwang/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eMengqing Wang\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/cthumuluru/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eChandrasekhar Thumuluru\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/rajiv-shringi/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eRajiv Shringi\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/john-l-693b7915a/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eJohn Lu\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/georgecampbell/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eGeorge Cambell\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/akhaku/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eAmmar Khaku\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/jordan-west-8aa1731a3/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eJordan West\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/clohfink/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eChris Lohfink\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/matt-lehman-39549719b/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eMatt Lehman\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, and the whole online datastores team (ODS, f.k.a CDE).\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "23 min read",
  "publishedTime": "2024-09-18T22:46:47.757Z",
  "modifiedTime": null
}
