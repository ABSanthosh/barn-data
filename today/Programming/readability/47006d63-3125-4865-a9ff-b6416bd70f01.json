{
  "id": "47006d63-3125-4865-a9ff-b6416bd70f01",
  "title": "How self-supervised language revolutionized natural language processing and gen AI",
  "link": "https://stackoverflow.blog/2025/04/28/how-self-supervised-language-revolutionized-natural-language-processing-and-gen-ai/",
  "description": "Self-supervised learning is a key advancement that revolutionized natural language processing and generative AI. Here’s how it works and two examples of how it is used to train language models.",
  "author": "Cameron R. Wolfe, PhD",
  "published": "Mon, 28 Apr 2025 16:00:00 GMT",
  "source": "https://stackoverflow.blog/feed/",
  "categories": [
    "se-tech",
    "se-stackoverflow",
    "llm",
    "ai",
    "contributed"
  ],
  "byline": "Cameron R. Wolfe, PhD",
  "length": 3282,
  "excerpt": "TL;DR: Self-supervised learning is a key advancement in deep learning that is used across a variety of domains. Put simply, the idea behind self-supervised learning is to train a model over raw/unlabeled data by making out and predicting portions of this data. This way, the ground truth “labels” that we learn to predict are already present in the data itself and no human annotation is required.",
  "siteName": "",
  "favicon": "https://stackoverflow.blog/apple-touch-icon.png",
  "text": "TL;DR: Self-supervised learning is a key advancement in deep learning that is used across a variety of domains. Put simply, the idea behind self-supervised learning is to train a model over raw/unlabeled data by making out and predicting portions of this data. This way, the ground truth “labels” that we learn to predict are already present in the data itself and no human annotation is required.Types of learning. Machine learning models can be trained in a variety of ways. For example, supervised learning trains a machine learning model over pairs of input data and output labels (usually annotated manually by humans). The model learns to predict these output labels by supervising the model (i.e., showing it several examples of input data with the correct output)! On the other hand, unsupervised learning uses no output labels and discovers inherent trends within the input data itself (e.g., by forming clusters).“Self-supervised learning obtains supervisory signals from the data itself, often leveraging the underlying structure in the data. The general technique of self-supervised learning is to predict any unobserved or hidden part (or property) of the input from any observed or unhidden part of the input.” - from Self-supervised learning: The dark matter of intelligenceWhat is self-supervised learning? Self-supervised learning lies between supervised and unsupervised learning. Namely, we train the model over pairs of input data and output labels. However, no manual annotation from humans is required to obtain output labels within our training data—the labels are naturally present in the raw data itself! To understand this better, let’s take a look at a few commonly-used self-supervised learning objectives.(1) The Cloze task is more commonly referred to as the masked language modeling (MLM) objective. Here, the language model takes a sequence of textual tokens (i.e., a sentence) as input. To train the model, we mask out (i.e., set to a special “mask” token) ~10% of tokens in the input and train the model to predict these masked tokens. Using this approach, we can train a language model over an unlabeled textual corpus, as the “labels” that we predict are just tokens that are already present in the text itself. This objective is used to pretrain language models like BERT and T5.(2) Next token prediction is the workhorse of modern generative language models like ChatGPT and PaLM. After downloading a large amount of raw textual data from the internet, we can repeatedly i) sample a sequence of text and ii) train the language model to predict the next token given preceding tokens as input. This happens in parallel for all tokens in the sequence. Again, all the “labels” that we learn to predict are already present in the raw textual data. Pretraining (and finetuning) via next token prediction is universal used by all generative language models.Other options exist too! Although Cloze and next token prediction are the most commonly-used self-supervised objectives for training language models, many examples of self-supervised learning exist. For example, many self-supervised objectives exist for video deep learning models (e.g., predicting the next frame), and BERT models also use a self-supervised next-sentence prediction objective.",
  "image": "https://cdn.stackoverflow.co/images/jo7n4k8s/production/c86e442c47972984ddb7781488ad94ee1f0c4c04-12000x6293.jpg?w=1200\u0026fm=png\u0026auto=format",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv itemprop=\"articleBody\"\u003e\u003cp\u003e\u003cstrong\u003eTL;DR: \u003c/strong\u003eSelf-supervised learning is a key advancement in deep learning that is used across a variety of domains. Put simply, the idea behind self-supervised learning is to train a model over raw/unlabeled data by making out and predicting portions of this data. This way, the ground truth “labels” that we learn to predict are already present in the data itself and no human annotation is required.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTypes of learning. \u003c/strong\u003eMachine learning models can be trained in a variety of ways. For example, supervised learning trains a machine learning model over pairs of input data and output labels (usually annotated manually by humans). The model learns to predict these output labels by supervising the model (i.e., showing it several examples of input data with the correct output)! On the other hand, unsupervised learning uses no output labels and discovers inherent trends within the input data itself (e.g., by forming clusters).\u003c/p\u003e\u003cp\u003e“Self-supervised learning obtains supervisory signals from the data itself, often leveraging the underlying structure in the data. The general technique of self-supervised learning is to predict any unobserved or hidden part (or property) of the input from any observed or unhidden part of the input.” - from \u003cem\u003eSelf-supervised learning: The dark matter of intelligence\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat is self-supervised learning?\u003c/strong\u003e Self-supervised learning lies between supervised and unsupervised learning. Namely, we train the model over pairs of input data and output labels. However, no manual annotation from humans is required to obtain output labels within our training data—the labels are naturally present in the raw data itself! To understand this better, let’s take a look at a few commonly-used self-supervised learning objectives.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(1) The Cloze task\u003c/strong\u003e is more commonly referred to as the masked language modeling (MLM) objective. Here, the language model takes a sequence of textual tokens (i.e., a sentence) as input. To train the model, we mask out (i.e., set to a special “mask” token) ~10% of tokens in the input and train the model to predict these masked tokens. Using this approach, we can train a language model over an unlabeled textual corpus, as the “labels” that we predict are just tokens that are already present in the text itself. This objective is used to pretrain language models like BERT and T5.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e(2) Next token prediction\u003c/strong\u003e is the workhorse of modern generative language models like ChatGPT and PaLM. After downloading a large amount of raw textual data from the internet, we can repeatedly i) sample a sequence of text and ii) train the language model to predict the next token given preceding tokens as input. This happens in parallel for all tokens in the sequence. Again, all the “labels” that we learn to predict are already present in the raw textual data. Pretraining (and finetuning) via next token prediction is universal used by all generative language models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOther options exist too! \u003c/strong\u003eAlthough Cloze and next token prediction are the most commonly-used self-supervised objectives for training language models, many examples of self-supervised learning exist. For example, many self-supervised objectives exist for video deep learning models (e.g., predicting the next frame), and BERT models also use a self-supervised next-sentence prediction objective.\u003c/p\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://cdn.stackoverflow.co/images/jo7n4k8s/production/e2fd7eeff8f42a1c2ac3bc52caabc0fe035d7a22-2440x1368.jpg?auto=format\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
