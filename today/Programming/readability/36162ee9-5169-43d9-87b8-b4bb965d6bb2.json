{
  "id": "36162ee9-5169-43d9-87b8-b4bb965d6bb2",
  "title": "Introducing Gemma 3n: The developer guide",
  "link": "https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/",
  "description": "The Gemma 3n model has been fully released, building on the success of previous Gemma models and bringing advanced on-device multimodal capabilities to edge devices with unprecedented performance. Explore Gemma 3n's innovations, including its mobile-first architecture, MatFormer technology, Per-Layer Embeddings, KV Cache Sharing, and new audio and MobileNet-V5 vision encoders, and how developers can start building with it today.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Omar Sanseviero, Ian Ballantyne",
  "length": 11849,
  "excerpt": "The Gemma 3n model has been fully released, building on the success of previous Gemma models and bringing advanced on-device multimodal capabilities to edge devices with unprecedented performance. Explore Gemma 3n's innovations, including its mobile-first architecture, MatFormer technology, Per-Layer Embeddings, KV Cache Sharing, and new audio and MobileNet-V5 vision encoders, and how developers can start building with it today.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "The first Gemma model launched early last year and has since grown into a thriving Gemmaverse of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like Roboflow building enterprise computer vision to the Institute of Science Tokyo creating highly-capable Japanese Gemma variants, your work has shown us the path forward.Building on this incredible momentum, we're excited to announce the full release of Gemma 3n. While last month's preview offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. Itâ€™s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we'll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.Whatâ€™s new in Gemma 3n?Gemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year's cloud-based frontier models. Multimodal by design: Gemma 3n natively supports image, audio, video, and text inputs and text outputs.Optimized for on-device: Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on effective parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory.Groundbreaking architecture: At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, LAuReL and AltUp for architectural efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases.Enhanced quality: Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark. Achieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3nâ€™s unique mobile-first architecture, and it all starts with MatFormer.MatFormer: One model, many sizesAt the core of Gemma 3n is the MatFormer (ðŸª†Matryoshka Transformer) architecture, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of Matryoshka Representation Learning from just embeddings to all transformer components. During the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today:1: Pre-extracted models: You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference.2: Custom sizes with Mix-n-Match: For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the MatFormer Lab, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU. MMLU scores for the pre-trained Gemma 3n checkpoints at different model sizes (using Mix-n-Match) Looking ahead, the MatFormer architecture also paves the way for elastic execution. While not part of todayâ€™s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.Per-Layer Embeddings (PLE): Unlocking more memory efficiencyGemma 3n models incorporate Per-Layer Embeddings (PLE). This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device's accelerator (GPU/TPU).While the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM). With Per-Layer Embeddings, you can use Gemma 3n E2B while only having ~2B parameters loaded in your accelerator. KV Cache sharing: Faster long-context processingProcessing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications.KV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \"prefill\" phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.Audio understanding: Introducing speech to text and translationGemma 3n uses an advanced audio encoder based on the Universal Speech Model (USM). The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context.This integrated audio capability unlocks key features for on-device development, including:Automatic Speech Recognition (ASR): Enable high-quality speech-to-text transcription directly on the device.Automatic Speech Translation (AST): Translate spoken language into text in another language.We've observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Hereâ€™s an example: \u003cbos\u003e\u003cstart_of_turn\u003euser Transcribe the following speech segment in Spanish, then translate it into English: \u003cstart_of_audio\u003e\u003cend_of_turn\u003e \u003cstart_of_turn\u003emodel Plain text Copied At launch time, the Gemma 3n encoder is implemented to process audio clips up to 30 seconds. However, this is not a fundamental limitation. The underlying audio encoder is a streaming encoder, capable of processing arbitrarily long audios with additional long form audio training. Follow-up implementations will unlock low-latency, long streaming applications.MobileNet-V5: New state-of-the-art vision encoderAlongside its integrated audio capabilities, Gemma 3n features a new, highly efficient vision encoder, MobileNet-V5-300M, delivering state-of-the-art performance for multimodal tasks on edge devices.Designed for flexibility and power on constrained hardware, MobileNet-V5 gives developers:Multiple input resolutions: Natively supports resolutions of 256x256, 512x512, and 768x768 pixels, allowing you to balance performance and detail for your specific applications.Broad visual understanding: Co-trained on extensive multimodal datasets, it excels at a wide range of image and video comprehension tasks.High throughput: Processes up to 60 frames per second on a Google Pixel, enabling real-time, on-device video analysis and interactive experiences.This level of performance is achieved with multiple architectural innovations, including:An advanced foundation of MobileNet-V4 blocks (including Universal Inverted Bottlenecks and Mobile MQA).A significantly scaled up architecture, featuring a hybrid, deep pyramid model that is 10x larger than the biggest MobileNet-V4 variant.A novel Multi-Scale Fusion VLM adapter that enhances the quality of tokens for better accuracy and efficiency.Benefiting from novel architectural designs and advanced distillation techniques, MobileNet-V5-300M substantially outperforms the baseline SoViT in Gemma 3 (trained with SigLip, no distillation). On a Google Pixel Edge TPU, it delivers a 13x speedup with quantization (6.5x without), requires 46% fewer parameters, and has a 4x smaller memory footprint, all while providing significantly higher accuracy on vision-language tasksWeâ€™re excited to share more about the work behind this model. Look out for our upcoming MobileNet-V5 technical report, which will deep dive into the model architecture, data scaling strategies, and advanced distillation techniques.Making Gemma 3n accessible from day one has been a priority. We're proud to partner with many incredible open source developers to ensure broad support across popular tools and platforms, including contributions from teams behind AMD, Axolotl, Docker, Hugging Face, llama.cpp, LMStudio, MLX, NVIDIA, Ollama, RedHat, SGLang, Unsloth, and vLLM.But this ecosystem is just the beginning. The true power of this technology is in what you will build with it. Thatâ€™s why weâ€™re launching the Gemma 3n Impact Challenge. Your mission: use Gemma 3n's unique on-device, offline, and multimodal capabilities to build a product for a better world. With $150,000 in prizes, we're looking for a compelling video story and a \"wow\" factor demo that shows real-world impact. Join the challenge and help build a better future.Get started with Gemma 3n todayReady to explore the potential of Gemma 3n today? Here's how:Experiment directly: Use Google AI Studio to try Gemma 3n in just a couple of clicks. Gemma models can also be deployed directly to Cloud Run from AI Studio.Download the models: Find the model weights on Hugging Face and Kaggle.Learn \u0026 integrate: Dive into our comprehensive documentation to quickly integrate Gemma into your projects or start with our inference and fine-tuning guides.Build with your favorite on-device AI tools: Google AI Edge Gallery/LiteRT-LLM, Ollama, MLX, llama.cpp, Docker, transformers.js and more.Use your favorite development tools: Leverage your preferred tools and frameworks, including Hugging Face Transformers and TRL, NVIDIA NeMo Framework, Unsloth, and LMStudio.Deploy your way: Gemma 3n offers multiple deployment options, including Google GenAI API, Vertex AI, SGLang, vLLM, and NVIDIA API Catalog.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemma-3n-meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"0lwbc\"\u003eThe \u003ca href=\"https://blog.google/technology/developers/gemma-open-models/\"\u003efirst Gemma model\u003c/a\u003e launched early last year and has since grown into a thriving \u003ca href=\"https://deepmind.google/models/gemma/gemmaverse/\"\u003eGemmaverse\u003c/a\u003e of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like \u003ca href=\"https://deepmind.google/models/gemma/gemmaverse/roboflow/\"\u003eRoboflow\u003c/a\u003e building enterprise computer vision to the \u003ca href=\"https://deepmind.google/models/gemma/gemmaverse/gemma-2-llama-swallow/\"\u003eInstitute of Science Tokyo\u003c/a\u003e creating highly-capable Japanese Gemma variants, your work has shown us the path forward.\u003c/p\u003e\u003cp data-block-key=\"8lqqe\"\u003eBuilding on this incredible momentum, we\u0026#39;re excited to announce the full release of Gemma 3n. While \u003ca href=\"https://developers.googleblog.com/en/introducing-gemma-3n/\"\u003elast month\u0026#39;s preview\u003c/a\u003e offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. Itâ€™s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we\u0026#39;ll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.\u003c/p\u003e\u003ch2 data-block-key=\"1mw8n\" id=\"what\u0026#39;s-new-in-gemma-3n\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhatâ€™s new in Gemma 3n?\u003c/h2\u003e\u003cp data-block-key=\"10f0e\"\u003eGemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year\u0026#39;s cloud-based frontier models.\u003c/p\u003e\n\u003c/div\u003e    \u003cdiv\u003e\n    \u003cul\u003e\u003cli data-block-key=\"b4rlm\"\u003e\u003cb\u003eMultimodal by design:\u003c/b\u003e Gemma 3n natively supports image, audio, video, and text inputs and text outputs.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"belt8\"\u003e\u003cb\u003eOptimized for on-device:\u003c/b\u003e Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on \u003ca href=\"https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/\"\u003e\u003cb\u003eeffective\u003c/b\u003e\u003c/a\u003e parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"19kb\"\u003e\u003cb\u003eGroundbreaking architecture:\u003c/b\u003e At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, LAuReL and AltUp for architectural efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"83m4e\"\u003e\u003cb\u003eEnhanced quality:\u003c/b\u003e Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark.\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma_3n_Chart_1_RD1-V01_1.original.png\" alt=\"LMArena Text Arena Elo Score rankings for Gemini 1.5 Pro, Gemma 3n E4B llama 4 Maverick 17B 128E GPT 4.1-nano and Phi-4\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"0lwbc\"\u003eAchieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3nâ€™s unique mobile-first architecture, and it all starts with MatFormer.\u003c/p\u003e\u003ch2 data-block-key=\"95viu\" id=\"matformer:-one-model-many-sizes\"\u003e\u003cbr/\u003eMatFormer: One model, many sizes\u003c/h2\u003e\u003cp data-block-key=\"26ehe\"\u003eAt the core of Gemma 3n is the \u003ca href=\"https://arxiv.org/abs/2310.07707\"\u003e\u003cb\u003eMatFormer\u003c/b\u003e\u003c/a\u003e\u003cb\u003e (ðŸª†Matryoshka Transformer)\u003c/b\u003e \u003cb\u003earchitecture\u003c/b\u003e, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of \u003ca href=\"https://huggingface.co/papers/2205.13147\"\u003eMatryoshka Representation Learning\u003c/a\u003e from just embeddings to all transformer components.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_3h2xBRA.original.jpg\" alt=\"MatFormer in Nano V3\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"0lwbc\"\u003eDuring the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today:\u003c/p\u003e\u003cp data-block-key=\"2r57\"\u003e1:\u003cb\u003e Pre-extracted models:\u003c/b\u003e You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference.\u003c/p\u003e\u003cp data-block-key=\"dsd6a\"\u003e2:\u003cb\u003e Custom sizes with Mix-n-Match:\u003c/b\u003e For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model\u0026#39;s parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the \u003ca href=\"https://goo.gle/gemma3n-matformer-lab\"\u003eMatFormer Lab\u003c/a\u003e, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n            \n                \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image4_5lfhlBO.original.png\" alt=\"Custom Sizes with Mix-n-Match\"/\u003e\u003c/p\u003e\u003cp\u003e\n                        MMLU scores for the pre-trained Gemma 3n checkpoints at different model sizes (using Mix-n-Match)\n                    \u003c/p\u003e\n                \n            \n        \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"0lwbc\"\u003eLooking ahead, the MatFormer architecture also paves the way for\u003cb\u003e elastic execution\u003c/b\u003e. While not part of todayâ€™s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.\u003c/p\u003e\u003ch2 data-block-key=\"jcedi\" id=\"per-layer-embeddings-(ple):-unlocking-more-memory-efficiency\"\u003e\u003cbr/\u003ePer-Layer Embeddings (PLE): Unlocking more memory efficiency\u003c/h2\u003e\u003cp data-block-key=\"2ilaf\"\u003eGemma 3n models incorporate \u003cb\u003ePer-Layer Embeddings (PLE)\u003c/b\u003e. This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device\u0026#39;s accelerator (GPU/TPU).\u003c/p\u003e\u003cp data-block-key=\"i1dh\"\u003eWhile the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM).\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n            \n                \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_BdtLmLG.original.jpg\" alt=\"Per-Layer Embeddings\"/\u003e\u003c/p\u003e\u003cp\u003e\n                        With Per-Layer Embeddings, you can use Gemma 3n E2B while only having ~2B parameters loaded in your accelerator.\n                    \u003c/p\u003e\n                \n            \n        \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"3dpqb\" id=\"kv-cache-sharing:-faster-long-context-processing\"\u003eKV Cache sharing: Faster long-context processing\u003c/h2\u003e\u003cp data-block-key=\"6472b\"\u003eProcessing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications.\u003c/p\u003e\u003cp data-block-key=\"1tdep\"\u003eKV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \u0026#34;prefill\u0026#34; phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.\u003c/p\u003e\u003ch2 data-block-key=\"832w5\" id=\"audio-understanding:-introducing-speech-to-text-and-translation\"\u003e\u003cbr/\u003eAudio understanding: Introducing speech to text and translation\u003c/h2\u003e\u003cp data-block-key=\"uijl\"\u003eGemma 3n uses an advanced audio encoder based on the \u003ca href=\"https://arxiv.org/abs/2303.01037\"\u003eUniversal Speech Model (USM)\u003c/a\u003e. The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context.\u003c/p\u003e\u003cp data-block-key=\"dlc74\"\u003eThis integrated audio capability unlocks key features for on-device development, including:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"6mapr\"\u003e\u003cb\u003eAutomatic Speech Recognition (ASR):\u003c/b\u003e Enable high-quality speech-to-text transcription directly on the device.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"caq44\"\u003e\u003cb\u003eAutomatic Speech Translation (AST):\u003c/b\u003e Translate spoken language into text in another language.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"doduj\"\u003eWe\u0026#39;ve observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Hereâ€™s an example:\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003e\u0026lt;bos\u0026gt;\u0026lt;start_of_turn\u0026gt;user\nTranscribe the following speech segment in Spanish, then translate it into English: \n\u0026lt;start_of_audio\u0026gt;\u0026lt;end_of_turn\u0026gt;\n\u0026lt;start_of_turn\u0026gt;model\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        Plain text\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"7ree3\"\u003eAt launch time, the Gemma 3n encoder is implemented to process audio clips up to 30 seconds. However, this is not a fundamental limitation. The underlying audio encoder is a streaming encoder, capable of processing arbitrarily long audios with additional long form audio training. Follow-up implementations will unlock low-latency, long streaming applications.\u003c/p\u003e\u003ch2 data-block-key=\"3ifcr\" id=\"mobilenet-v5:-new-state-of-the-art-vision-encoder\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eMobileNet-V5: New state-of-the-art vision encoder\u003c/h2\u003e\u003cp data-block-key=\"3r73m\"\u003eAlongside its integrated audio capabilities, Gemma 3n features a new, highly efficient vision encoder, \u003cb\u003eMobileNet-V5-300M\u003c/b\u003e, delivering state-of-the-art performance for multimodal tasks on edge devices.\u003c/p\u003e\u003cp data-block-key=\"1qj1b\"\u003eDesigned for flexibility and power on constrained hardware, MobileNet-V5 gives developers:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"c1qs0\"\u003e\u003cb\u003eMultiple input resolutions\u003c/b\u003e: Natively supports resolutions of 256x256, 512x512, and 768x768 pixels, allowing you to balance performance and detail for your specific applications.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"648dt\"\u003e\u003cb\u003eBroad visual understanding\u003c/b\u003e: Co-trained on extensive multimodal datasets, it excels at a wide range of image and video comprehension tasks.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"k1qf\"\u003e\u003cb\u003eHigh throughput\u003c/b\u003e: Processes up to 60 frames per second on a Google Pixel, enabling real-time, on-device video analysis and interactive experiences.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"1a8ge\"\u003eThis level of performance is achieved with multiple architectural innovations, including:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"48us4\"\u003eAn advanced foundation of MobileNet-V4 blocks (including Universal Inverted Bottlenecks and Mobile MQA).\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"12p8\"\u003eA significantly scaled up architecture, featuring a hybrid, deep pyramid model that is 10x larger than the biggest MobileNet-V4 variant.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"73gq1\"\u003eA novel Multi-Scale Fusion VLM adapter that enhances the quality of tokens for better accuracy and efficiency.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"49ved\"\u003e\u003cbr/\u003eBenefiting from novel architectural designs and advanced distillation techniques, MobileNet-V5-300M substantially outperforms the baseline SoViT in Gemma 3 (trained with SigLip, no distillation). On a Google Pixel Edge TPU, it \u003cb\u003edelivers a 13x speedup with quantization (6.5x without), requires 46% fewer parameters, and has a 4x smaller memory footprint\u003c/b\u003e, all while providing significantly higher accuracy on vision-language tasks\u003c/p\u003e\u003cp data-block-key=\"e7u3j\"\u003eWeâ€™re excited to share more about the work behind this model. Look out for our upcoming MobileNet-V5 technical report, which will deep dive into the model architecture, data scaling strategies, and advanced distillation techniques.\u003c/p\u003e\u003cp data-block-key=\"emlv5\"\u003eMaking Gemma 3n accessible from day one has been a priority. We\u0026#39;re proud to partner with many incredible open source developers to ensure broad support across popular tools and platforms, including contributions from teams behind AMD, Axolotl, \u003ca href=\"https://hub.docker.com/r/ai/gemma3n\"\u003eDocker\u003c/a\u003e, Hugging Face, llama.cpp, LMStudio, MLX, \u003ca href=\"https://developer.nvidia.com/blog/run-google-deepminds-gemma-3n-on-nvidia-jetson-and-rtx/\"\u003eNVIDIA\u003c/a\u003e, Ollama, RedHat, SGLang, Unsloth, and vLLM.\u003c/p\u003e\u003cp data-block-key=\"354di\"\u003eBut this ecosystem is just the beginning. The true power of this technology is in what you will build with it. Thatâ€™s why weâ€™re launching the \u003ca href=\"https://www.kaggle.com/competitions/google-gemma-3n-hackathon\"\u003eGemma 3n Impact Challenge.\u003c/a\u003e Your mission: use Gemma 3n\u0026#39;s unique on-device, offline, and multimodal capabilities to build a product for a better world. With $150,000 in prizes, we\u0026#39;re looking for a compelling video story and a \u0026#34;wow\u0026#34; factor demo that shows real-world impact. \u003ca href=\"https://www.kaggle.com/competitions/google-gemma-3n-hackathon\"\u003eJoin the challenge\u003c/a\u003e and help build a better future.\u003c/p\u003e\u003ch2 data-block-key=\"jym15\" id=\"get-started-with-gemma-3n-today\"\u003e\u003cbr/\u003eGet started with Gemma 3n today\u003c/h2\u003e\u003cp data-block-key=\"9t8b1\"\u003eReady to explore the potential of Gemma 3n today? Here\u0026#39;s how:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"cvr4m\"\u003e\u003cb\u003eExperiment directly:\u003c/b\u003e Use \u003ca href=\"https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it\"\u003eGoogle AI Studio\u003c/a\u003e to try Gemma 3n in just a couple of clicks. Gemma models can also be deployed directly to Cloud Run from AI Studio.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"p04i\"\u003e\u003cb\u003eDownload the models\u003c/b\u003e: Find the model weights on\u003ca href=\"https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4\"\u003e Hugging Face\u003c/a\u003e and \u003ca href=\"https://www.kaggle.com/models/google/gemma-3n\"\u003eKaggle\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1va1s\"\u003e\u003cb\u003eLearn \u0026amp; integrate:\u003c/b\u003e Dive into our \u003ca href=\"https://ai.google.dev/gemma/docs/gemma-3n\"\u003ecomprehensive documentation\u003c/a\u003e to quickly integrate Gemma into your projects or start with our inference and fine-tuning guides.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"74h3a\"\u003e\u003cb\u003eBuild with your favorite on-device AI tools\u003c/b\u003e: \u003ca href=\"https://github.com/google-ai-edge/gallery\"\u003eGoogle AI Edge Gallery/LiteRT-LLM\u003c/a\u003e, \u003ca href=\"https://ollama.com/library/gemma3n\"\u003eOllama\u003c/a\u003e, \u003ca href=\"https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc\"\u003eMLX\u003c/a\u003e, \u003ca href=\"https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7\"\u003ellama.cpp\u003c/a\u003e, \u003ca href=\"https://hub.docker.com/r/ai/gemma3n\"\u003eDocker\u003c/a\u003e, \u003ca href=\"https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX\"\u003etransformers.js\u003c/a\u003e and more.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ebs9n\"\u003e\u003cb\u003eUse your favorite development tools:\u003c/b\u003e Leverage your preferred tools and frameworks, including \u003ca href=\"https://huggingface.co/blog/gemma3n\"\u003eHugging Face Transformers and TRL\u003c/a\u003e, \u003ca href=\"https://github.com/NVIDIA-NeMo\"\u003eNVIDIA NeMo Framework\u003c/a\u003e, \u003ca href=\"https://unsloth.ai/blog/gemma-3n\"\u003eUnsloth\u003c/a\u003e, and \u003ca href=\"https://lmstudio.ai/models/google/gemma-3n-e4b\"\u003eLMStudio\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"a9d96\"\u003e\u003cb\u003eDeploy your way\u003c/b\u003e: Gemma 3n offers multiple deployment options, including \u003ca href=\"https://ai.google.dev/gemma/docs/core/gemma_on_gemini_api\"\u003eGoogle GenAI API\u003c/a\u003e, \u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n\"\u003eVertex AI\u003c/a\u003e, SGLang, vLLM, and \u003ca href=\"https://build.nvidia.com/google/gemma-3n-e4b-it\"\u003eNVIDIA API Catalog\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2025-06-26T00:00:00Z",
  "modifiedTime": null
}
