{
  "id": "23e46408-9699-47b5-89ca-7410b456058e",
  "title": "Recommending for Long-Term Member Satisfaction at Netflix",
  "link": "https://netflixtechblog.com/recommending-for-long-term-member-satisfaction-at-netflix-ac15cada49ef?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Thu, 29 Aug 2024 01:01:40 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "reward-engineering",
    "contextual-bandit",
    "recommendation-system",
    "machine-learning"
  ],
  "byline": "Netflix Technology Blog",
  "length": 10329,
  "excerpt": "Our mission at Netflix is to entertain the world. Our personalization algorithms play a crucial role in delivering on this mission for all members by recommending the right shows, movies, and games…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By Jiangwei Pan, Gary Tang, Henry Wang, and Justin BasilicoIntroductionOur mission at Netflix is to entertain the world. Our personalization algorithms play a crucial role in delivering on this mission for all members by recommending the right shows, movies, and games at the right time. This goal extends beyond immediate engagement; we aim to create an experience that brings lasting enjoyment to our members. Traditional recommender systems often optimize for short-term metrics like clicks or engagement, which may not fully capture long-term satisfaction. We strive to recommend content that not only engages members in the moment but also enhances their long-term satisfaction, which increases the value they get from Netflix, and thus they’ll be more likely to continue to be a member.Recommendations as Contextual BanditOne simple way we can view recommendations is as a contextual bandit problem. When a member visits, that becomes a context for our system and it selects an action of what recommendations to show, and then the member provides various types of feedback. These feedback signals can be immediate (skips, plays, thumbs up/down, or adding items to their playlist) or delayed (completing a show or renewing their subscription). We can define reward functions to reflect the quality of the recommendations from these feedback signals and then train a contextual bandit policy on historical data to maximize the expected reward.Improving Recommendations: Models and ObjectivesThere are many ways that a recommendation model can be improved. They may come from more informative input features, more data, different architectures, more parameters, and so forth. In this post, we focus on a less-discussed aspect about improving the recommender objective by defining a reward function that tries to better reflect long-term member satisfaction.Retention as Reward?Member retention might seem like an obvious reward for optimizing long-term satisfaction because members should stay if they’re satisfied, however it has several drawbacks:Noisy: Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service.Low Sensitivity: Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction.Hard to Attribute: Members might cancel only after a series of bad recommendations.Slow to Measure: We only get one signal per account per month.Due to these challenges, optimizing for retention alone is impractical.Proxy RewardsInstead, we can train our bandit policy to optimize a proxy reward function that is highly aligned with long-term member satisfaction while being sensitive to individual recommendations. The proxy reward r(user, item) is a function of user interaction with the recommended item. For example, if we recommend “One Piece” and a member plays then subsequently completes and gives it a thumbs-up, a simple proxy reward might be defined as r(user, item) = f(play, complete, thumb).Click-through rate (CTR)Click-through rate (CTR), or in our case play-through rate, can be viewed as a simple proxy reward where r(user, item) = 1 if the user clicks a recommendation and 0 otherwise. CTR is a common feedback signal that generally reflects user preference expectations. It is a simple yet strong baseline for many recommendation applications. In some cases, such as ads personalization where the click is the target action, CTR may even be a reasonable reward for production models. However, in most cases, over-optimizing CTR can lead to promoting clickbaity items, which may harm long-term satisfaction.Beyond CTRTo align the proxy reward function more closely with long-term satisfaction, we need to look beyond simple interactions, consider all types of user actions, and understand their true implications on user satisfaction.We give a few examples in the Netflix context:Fast season completion ✅: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction.Thumbs-down after completion ❌: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent.Playing a movie for just 10 minutes ❓: In this case, the user’s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day.Discovering new genres ✅ ✅: Watching more Korean or game shows after “Squid Game” suggests the user is discovering something new. This discovery was likely even more valuable since it led to a variety of engagements in a new area for a member.Reward EngineeringReward engineering is the iterative process of refining the proxy reward function to align with long-term member satisfaction. It is similar to feature engineering, except that it can be derived from data that isn’t available at serving time. Reward engineering involves four stages: hypothesis formation, defining a new proxy reward, training a new bandit policy, and A/B testing. Below is a simple example.Challenge: Delayed FeedbackUser feedback used in the proxy reward function is often delayed or missing. For example, a member may decide to play a recommended show for just a few minutes on the first day and take several weeks to fully complete the show. This completion feedback is therefore delayed. Additionally, some user feedback may never occur; while we may wish otherwise, not all members provide a thumbs-up or thumbs-down after completing a show, leaving us uncertain about their level of enjoyment.We could try and wait to give a longer window to observe feedback, but how long should we wait for delayed feedback before computing the proxy rewards? If we wait too long (e.g., weeks), we miss the opportunity to update the bandit policy with the latest data. In a highly dynamic environment like Netflix, a stale bandit policy can degrade the user experience and be particularly bad at recommending newer items.Solution: predict missing feedbackWe aim to update the bandit policy shortly after making a recommendation while also defining the proxy reward function based on all user feedback, including delayed feedback. Since delayed feedback has not been observed at the time of policy training, we can predict it. This prediction occurs for each training example with delayed feedback, using already observed feedback and other relevant information up to the training time as input features. Thus, the prediction also gets better as time progresses.The proxy reward is then calculated for each training example using both observed and predicted feedback. These training examples are used to update the bandit policy.But aren’t we still only relying on observed feedback in the proxy reward function? Yes, because delayed feedback is predicted based on observed feedback. However, it is simpler to reason about rewards using all feedback directly. For instance, the delayed thumbs-up prediction model may be a complex neural network that takes into account all observed feedback (e.g., short-term play patterns). It’s more straightforward to define the proxy reward as a simple function of the thumbs-up feedback rather than a complex function of short-term interaction patterns. It can also be used to adjust for potential biases in how feedback is provided.The reward engineering diagram is updated with an optional delayed feedback prediction step.Two types of ML modelsIt’s worth noting that this approach employs two types of ML models:Delayed Feedback Prediction Models: These models predict p(final feedback | observed feedbacks). The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training.Bandit Policy Models: These models are used in the bandit policy π(item | user; r) to generate recommendations online and in real-time.Challenge: Online-Offline Metric DisparityImproved input features or neural network architectures often lead to better offline model metrics (e.g., AUC for classification models). However, when these improved models are subjected to A/B testing, we often observe flat or even negative online metrics, which can quantify long-term member satisfaction.This online-offline metric disparity usually occurs when the proxy reward used in the recommendation policy is not fully aligned with long-term member satisfaction. In such cases, a model may achieve higher proxy rewards (offline metrics) but result in worse long-term member satisfaction (online metrics).Nevertheless, the model improvement is genuine. One approach to resolve this is to further refine the proxy reward definition to align better with the improved model. When this tuning results in positive online metrics, the model improvement can be effectively productized. See [1] for more discussions on this challenge.Summary and Open QuestionsIn this post, we provided an overview of our reward engineering efforts to align Netflix recommendations with long-term member satisfaction. While retention remains our north star, it is not easy to optimize directly. Therefore, our efforts focus on defining a proxy reward that is aligned with long-term satisfaction and sensitive to individual recommendations. Finally, we discussed the unique challenge of delayed user feedback at Netflix and proposed an approach that has proven effective for us. Refer to [2] for an earlier overview of the reward innovation efforts at Netflix.As we continue to improve our recommendations, several open questions remain:Can we learn a good proxy reward function automatically by correlating behavior with retention?How long should we wait for delayed feedback before using its predicted value in policy training?How can we leverage Reinforcement Learning to further align the policy with long-term satisfaction?References[1] Deep learning for recommender systems: A Netflix case study. AI Magazine 2021. Harald Steck, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves Raimond, Justin Basilico.[2] Reward innovation for long-term member satisfaction. RecSys 2023. Gary Tang, Jiangwei Pan, Henry Wang, Justin Basilico.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*Y8QDcyallv_mh7ylPzXqkA.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page-----ac15cada49ef--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page-----ac15cada49ef--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"989b\"\u003eBy \u003ca href=\"https://www.linkedin.com/in/jiangwei-pan-66a62a13/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJiangwei Pan\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/thegarytang/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGary Tang\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/henry-kang-wang-06701716/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHenry Wang\u003c/a\u003e, and \u003ca href=\"https://www.linkedin.com/in/jbasilico/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJustin Basilico\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"1b73\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"9df8\"\u003eOur mission at Netflix is to entertain the world. Our personalization algorithms play a crucial role in delivering on this mission for all members by recommending the right shows, movies, and games at the right time. This goal extends beyond immediate engagement; we aim to create an experience that brings lasting enjoyment to our members. Traditional recommender systems often optimize for short-term metrics like clicks or engagement, which may not fully capture long-term satisfaction. We strive to recommend content that not only engages members in the moment but also enhances their long-term satisfaction, which increases the value they get from Netflix, and thus they’ll be more likely to continue to be a member.\u003c/p\u003e\u003ch2 id=\"36a3\"\u003eRecommendations as Contextual Bandit\u003c/h2\u003e\u003cp id=\"46d8\"\u003eOne simple way we can view recommendations is as a contextual bandit problem. When a member visits, that becomes a context for our system and it selects an action of what recommendations to show, and then the member provides various types of feedback. These feedback signals can be immediate (skips, plays, thumbs up/down, or adding items to their playlist) or delayed (completing a show or renewing their subscription). We can define reward functions to reflect the quality of the recommendations from these feedback signals and then train a contextual bandit policy on historical data to maximize the expected reward.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"df36\"\u003eImproving Recommendations: Models and Objectives\u003c/h2\u003e\u003cp id=\"a852\"\u003eThere are many ways that a recommendation model can be improved. They may come from more informative input features, more data, different architectures, more parameters, and so forth. In this post, we focus on a less-discussed aspect about improving the recommender objective by defining a reward function that tries to better reflect long-term member satisfaction.\u003c/p\u003e\u003ch2 id=\"b2b2\"\u003eRetention as Reward?\u003c/h2\u003e\u003cp id=\"e108\"\u003eMember retention might seem like an obvious reward for optimizing long-term satisfaction because members should stay if they’re satisfied, however it has several drawbacks:\u003c/p\u003e\u003cul\u003e\u003cli id=\"9d43\"\u003e\u003cstrong\u003eNoisy\u003c/strong\u003e: Retention can be influenced by numerous external factors, such as seasonal trends, marketing campaigns, or personal circumstances unrelated to the service.\u003c/li\u003e\u003cli id=\"901d\"\u003e\u003cstrong\u003eLow Sensitivity\u003c/strong\u003e: Retention is only sensitive for members on the verge of canceling their subscription, not capturing the full spectrum of member satisfaction.\u003c/li\u003e\u003cli id=\"64c1\"\u003e\u003cstrong\u003eHard to Attribute\u003c/strong\u003e: Members might cancel only after a series of bad recommendations.\u003c/li\u003e\u003cli id=\"e86d\"\u003e\u003cstrong\u003eSlow to Measure\u003c/strong\u003e: We only get one signal per account per month.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"26e7\"\u003eDue to these challenges, optimizing for retention alone is impractical.\u003c/p\u003e\u003ch2 id=\"c903\"\u003eProxy Rewards\u003c/h2\u003e\u003cp id=\"216f\"\u003eInstead, we can train our bandit policy to optimize a proxy reward function that is highly aligned with long-term member satisfaction while being sensitive to individual recommendations. The proxy reward \u003cem\u003er(user, item)\u003c/em\u003e is a function of user interaction with the recommended item. For example, if we recommend “One Piece” and a member plays then subsequently completes and gives it a thumbs-up, a simple proxy reward might be defined as \u003cem\u003er(user, item) = f(play, complete, thumb)\u003c/em\u003e.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"d77d\"\u003eClick-through rate (CTR)\u003c/h2\u003e\u003cp id=\"1f77\"\u003eClick-through rate (CTR), or in our case play-through rate, can be viewed as a simple proxy reward where \u003cem\u003er(user, item) \u003c/em\u003e= 1 if the user clicks a recommendation and 0 otherwise. CTR is a common feedback signal that generally reflects user preference expectations. It is a simple yet strong baseline for many recommendation applications. In some cases, such as ads personalization where the click is the target action, CTR may even be a reasonable reward for production models. However, in most cases, over-optimizing CTR can lead to promoting clickbaity items, which may harm long-term satisfaction.\u003c/p\u003e\u003ch2 id=\"29ad\"\u003eBeyond CTR\u003c/h2\u003e\u003cp id=\"f461\"\u003eTo align the proxy reward function more closely with long-term satisfaction, we need to look beyond simple interactions, consider all types of user actions, and understand their true implications on user satisfaction.\u003c/p\u003e\u003cp id=\"65da\"\u003eWe give a few examples in the Netflix context:\u003c/p\u003e\u003cul\u003e\u003cli id=\"911d\"\u003e\u003cstrong\u003eFast season completion \u003c/strong\u003e✅: Completing a season of a recommended TV show in one day is a strong sign of enjoyment and long-term satisfaction.\u003c/li\u003e\u003cli id=\"a04d\"\u003e\u003cstrong\u003eThumbs-down after completion \u003c/strong\u003e❌: Completing a TV show in several weeks followed by a thumbs-down indicates low satisfaction despite significant time spent.\u003c/li\u003e\u003cli id=\"680e\"\u003e\u003cstrong\u003ePlaying a movie for just 10 minutes \u003c/strong\u003e❓: In this case, the user’s satisfaction is ambiguous. The brief engagement might indicate that the user decided to abandon the movie, or it could simply mean the user was interrupted and plans to finish the movie later, perhaps the next day.\u003c/li\u003e\u003cli id=\"c0e6\"\u003e\u003cstrong\u003eDiscovering new genres \u003c/strong\u003e✅ ✅: Watching more Korean or game shows after “Squid Game” suggests the user is discovering something new. This discovery was likely even more valuable since it led to a variety of engagements in a new area for a member.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"179d\"\u003eReward Engineering\u003c/h2\u003e\u003cp id=\"305f\"\u003eReward engineering is the iterative process of refining the proxy reward function to align with long-term member satisfaction. It is similar to feature engineering, except that it can be derived from data that isn’t available at serving time. Reward engineering involves four stages: hypothesis formation, defining a new proxy reward, training a new bandit policy, and A/B testing. Below is a simple example.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"018c\"\u003eChallenge: Delayed Feedback\u003c/h2\u003e\u003cp id=\"093d\"\u003eUser feedback used in the proxy reward function is often delayed or missing. For example, a member may decide to play a recommended show for just a few minutes on the first day and take several weeks to fully complete the show. This completion feedback is therefore delayed. Additionally, some user feedback may never occur; while we may wish otherwise, not all members provide a thumbs-up or thumbs-down after completing a show, leaving us uncertain about their level of enjoyment.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"d3ce\"\u003eWe could try and wait to give a longer window to observe feedback, but how long should we wait for delayed feedback before computing the proxy rewards? If we wait too long (e.g., weeks), we miss the opportunity to update the bandit policy with the latest data. In a highly dynamic environment like Netflix, a stale bandit policy can degrade the user experience and be particularly bad at recommending newer items.\u003c/p\u003e\u003ch2 id=\"5011\"\u003eSolution: predict missing feedback\u003c/h2\u003e\u003cp id=\"6574\"\u003eWe aim to update the bandit policy shortly after making a recommendation while also defining the proxy reward function based on all user feedback, including delayed feedback. Since delayed feedback has not been observed at the time of policy training, we can predict it. This prediction occurs for each training example with delayed feedback, using already observed feedback and other relevant information up to the training time as input features. Thus, the prediction also gets better as time progresses.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"c5ce\"\u003eThe proxy reward is then calculated for each training example using both observed and predicted feedback. These training examples are used to update the bandit policy.\u003c/p\u003e\u003cp id=\"d041\"\u003eBut aren’t we still only relying on observed feedback in the proxy reward function? Yes, because delayed feedback is predicted based on observed feedback. However, it is simpler to reason about rewards using all feedback directly. For instance, the delayed thumbs-up prediction model may be a complex neural network that takes into account all observed feedback (e.g., short-term play patterns). It’s more straightforward to define the proxy reward as a simple function of the thumbs-up feedback rather than a complex function of short-term interaction patterns. It can also be used to adjust for potential biases in how feedback is provided.\u003c/p\u003e\u003cp id=\"c920\"\u003eThe reward engineering diagram is updated with an optional delayed feedback prediction step.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"dc5a\"\u003eTwo types of ML models\u003c/h2\u003e\u003cp id=\"8005\"\u003eIt’s worth noting that this approach employs two types of ML models:\u003c/p\u003e\u003cul\u003e\u003cli id=\"8fd2\"\u003e\u003cstrong\u003eDelayed Feedback Prediction Models\u003c/strong\u003e: These models predict \u003cem\u003ep(final feedback | observed feedbacks)\u003c/em\u003e. The predictions are used to define and compute proxy rewards for bandit policy training examples. As a result, these models are used offline during the bandit policy training.\u003c/li\u003e\u003cli id=\"4114\"\u003e\u003cstrong\u003eBandit Policy Models\u003c/strong\u003e: These models are used in the bandit policy \u003cem\u003eπ(item | user; r)\u003c/em\u003e to generate recommendations online and in real-time.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"b98d\"\u003eChallenge: Online-Offline Metric Disparity\u003c/h2\u003e\u003cp id=\"7da3\"\u003eImproved input features or neural network architectures often lead to better offline model metrics (e.g., AUC for classification models). However, when these improved models are subjected to A/B testing, we often observe flat or even negative online metrics, which can quantify long-term member satisfaction.\u003c/p\u003e\u003cp id=\"3112\"\u003eThis online-offline metric disparity usually occurs when the proxy reward used in the recommendation policy is not fully aligned with long-term member satisfaction. In such cases, a model may achieve higher proxy rewards (offline metrics) but result in worse long-term member satisfaction (online metrics).\u003c/p\u003e\u003cp id=\"d753\"\u003eNevertheless, the model improvement is genuine. One approach to resolve this is to further refine the proxy reward definition to align better with the improved model. When this tuning results in positive online metrics, the model improvement can be effectively productized. See [1] for more discussions on this challenge.\u003c/p\u003e\u003ch2 id=\"576c\"\u003eSummary and Open Questions\u003c/h2\u003e\u003cp id=\"24ec\"\u003eIn this post, we provided an overview of our reward engineering efforts to align Netflix recommendations with long-term member satisfaction. While retention remains our north star, it is not easy to optimize directly. Therefore, our efforts focus on defining a proxy reward that is aligned with long-term satisfaction and sensitive to individual recommendations. Finally, we discussed the unique challenge of delayed user feedback at Netflix and proposed an approach that has proven effective for us. Refer to [2] for an earlier overview of the reward innovation efforts at Netflix.\u003c/p\u003e\u003cp id=\"f30d\"\u003eAs we continue to improve our recommendations, several open questions remain:\u003c/p\u003e\u003cul\u003e\u003cli id=\"b35e\"\u003eCan we learn a good proxy reward function automatically by correlating behavior with retention?\u003c/li\u003e\u003cli id=\"5372\"\u003eHow long should we wait for delayed feedback before using its predicted value in policy training?\u003c/li\u003e\u003cli id=\"9605\"\u003eHow can we leverage Reinforcement Learning to further align the policy with long-term satisfaction?\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"74f9\"\u003eReferences\u003c/h2\u003e\u003cp id=\"d6c0\"\u003e[1] \u003ca href=\"https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/18140\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDeep learning for recommender systems: A Netflix case study\u003c/a\u003e. AI Magazine 2021. Harald Steck, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves Raimond, Justin Basilico.\u003c/p\u003e\u003cp id=\"76f7\"\u003e[2] \u003ca href=\"https://web.archive.org/web/20231011142826id_/https://dl.acm.org/doi/pdf/10.1145/3604915.3608873\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eReward innovation for long-term member satisfaction\u003c/a\u003e. RecSys 2023. Gary Tang, Jiangwei Pan, Henry Wang, Justin Basilico.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2024-08-29T00:59:13.618Z",
  "modifiedTime": null
}
