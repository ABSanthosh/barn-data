{
  "id": "89a3673a-dcf8-4f99-9675-7c5417602837",
  "title": "Gemma explained: What‚Äôs new in Gemma 2",
  "link": "https://developers.googleblog.com/en/gemma-explained-new-in-gemma-2/",
  "description": "Gemma 2 is a new suite of open models that sets a new standard for performance and accessibility, outperforming popular models more than twice its size.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ju-yeong Ji, Ravin Kumar",
  "length": 7176,
  "excerpt": "Gemma 2 is a new suite of open models that sets a new standard for performance and accessibility, outperforming popular models more than twice its size.",
  "siteName": "",
  "favicon": "",
  "text": "Ravin Kumar Google Data Scientist Language Applications In the previous post of the Gemma explained series, we discussed the Gemma architecture. In this post, you will explore the latest model, Gemma 2. Let‚Äôs get started!Gemma 2Recently, we released Gemma 2, our groundbreaking new suite of open models, setting a new standard for performance and accessibility. Available in 2B, 9B, and 27B parameter sizes, Gemma 2 has quickly made its mark. Our 27B model rapidly ascended the LMSYS Chatbot Arena leaderboard, surpassing even popular models more than twice its size in engaging, real-world conversations, establishing itself as one of the highest-ranking and most useful open models. Meanwhile, the Gemma 2 2B model showcases its exceptional conversational AI prowess by outperforming all GPT-3.5 models on the Chatbot Arena at a size runnable on edge devices.Developers can access robust tuning capabilities with Gemma 2 across platforms and tools. Fine-tuning Gemma 2 is simplified with cloud-based solutions like Google Cloud and community tools like Axolotl. Seamless integration with partners such as Hugging Face and NVIDIA TensorRT-LLM, as well as our JAX and Keras, enables optimization of performance and efficient deployment across diverse hardware configurations.Here‚Äôs the core parameters of the new models: Key DifferencesGemma 2 shares a similar architectural foundation with the original Gemma models, including the implementation of Rotary Positioning Embeddings (RoPE) and the approximated GeGLU non-linearity. However, it introduces novel architectural innovations that set it apart from its predecessors.Alternating Local and Global AttentionInstead of considering all words in a text at once, it sometimes focuses on a small window of words (local attention) and sometimes considers all words (global attention). This combination helps the model understand both the immediate context and the overall meaning of the text efficiently.Logit Soft-CappingImagine you are training a model to predict the next word in a sentence. Sometimes, the model might be overly confident about a particular word, even if it‚Äôs not the best choice. Logit soft-capping prevents this by limiting how confident the model can be about its predictions, leading to better overall performance.RMSNorm for Pre and Post-NormalizationThink of this as a way to keep the model‚Äôs calculations from becoming too large or too small during training. Just like we might adjust the volume on a speaker to prevent distortion, RMSNorm ensures that the information flowing through the model stays within a reasonable range, leading to more stable and effective training.Grouped-Query Attention (GQA)This technique helps the model process information more efficiently, especially when dealing with large amounts of text. It improves upon traditional multi-head attention(MHA) by grouping queries together, enabling faster processing, especially for large models. It‚Äôs like dividing a large task into smaller, more manageable chunks, allowing the model to understand the relationships between words faster without sacrificing accuracy.Gemma 27B Gemma2ForCausalLM( (model): Gemma2Model( (embed_tokens): Embedding(256000, 4608, padding_idx=0) (layers): ModuleList( (0-45): 46 x Gemma2DecoderLayer( (self_attn): Gemma2SdpaAttention( (q_proj): Linear(in_features=4608, out_features=4096, bias=False) (k_proj): Linear(in_features=4608, out_features=2048, bias=False) (v_proj): Linear(in_features=4608, out_features=2048, bias=False) (o_proj): Linear(in_features=4096, out_features=4608, bias=False) (rotary_emb): Gemma2RotaryEmbedding() ) (mlp): Gemma2MLP( (gate_proj): Linear(in_features=4608, out_features=36864, bias=False) (up_proj): Linear(in_features=4608, out_features=36864, bias=False) (down_proj): Linear(in_features=36864, out_features=4608, bias=False) (act_fn): PytorchGELUTanh() ) (input_layernorm): Gemma2RMSNorm() (post_attention_layernorm): Gemma2RMSNorm() (pre_feedforward_layernorm): Gemma2RMSNorm() (post_feedforward_layernorm): Gemma2RMSNorm() ) ) (norm): Gemma2RMSNorm() ) (lm_head): Linear(in_features=4608, out_features=256000, bias=False) ) self_attnIn the self-attention mechanism, Gemma 2 uses Grouped Query Attention (GQA).k_proj and v_proj share the same head with a size of 128 and 16 heads (128 x 16 = 2048). In contrast, q_proj and o_proj have 32 heads (128 x 32 = 4096) in parallel.Note that the Gemma 9B model uses the Same GQA but different number of heads(8 for k_proj and v_proj, 16 for q_proj and o_proj) and head size (256) (self_attn): Gemma2SdpaAttention( (q_proj): Linear(in_features=3584, out_features=4096, bias=False) (k_proj): Linear(in_features=3584, out_features=2048, bias=False) (v_proj): Linear(in_features=3584, out_features=2048, bias=False) (o_proj): Linear(in_features=4096, out_features=3584, bias=False) (rotary_emb): Gemma2RotaryEmbedding() ) The 2B model uses 4 for k_proj and v_proj, 8 for q_proj and o_proj and head size (256)pre_feedforward_layernorm and post_feedforward_layernormAnother significant distinction is the inclusion of additional RMSNorm in Gemma 2, which enhances the stability of the training process.Key FindingsOur technical report provides in-depth details, but here's a quick summary of Gemma 2's main findings:Distillation vs. Training from Scratch:We trained the 2B and 9B models with knowledge distillation from the larger model (27B).Distilling knowledge from a larger model, even with an equal number of training tokens, leads to significant performance enhancements.Grouped Query Attention vs. Multi Head Attention:Replacing MHA with GQA results in comparable performance while offering parameter efficiency and faster inference times, making GQA the preferred choice.Model Depth vs. Width:A deeper model showcases slightly superior performance compared to a wider model with the same parameter count.What‚Äôs Next?In this article, you learned about Gemma 2, the next generation of Gemma models.In our next series of posts, you will examine the RecurrentGemma which is an open model based on Griffin.If you want to delve into the fascinating world of AI and gain insights from the experts who are shaping its development, head over to goo.gle/ai-podcast or search for the show ‚ÄúPeople of AI Podcast‚Äù on any podcast platform.Stay tuned and thank you for reading!ReferencesPapersGemma 2: Improving Open Language Models at a Practical SizeCode ExamplesKeras Gemma 2 QuickstartKeras Gemma 2 Quickstart Chatüìã The complete Gemma architecture seriesGemma explained: An overview of Gemma model family architecturesGemma explained: What‚Äôs new in Gemma 2Gemma explained: RecurrentGemma architectureGemma explained: PaliGemma architecture",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Untitled_1600_x_873_px.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Ravin+Kumar\"\u003eRavin Kumar\u003c/a\u003e\n            \n              \u003cspan\u003eGoogle Data Scientist\u003c/span\u003e\n            \n            \n              \u003cspan\u003eLanguage Applications\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"lhk0v\"\u003eIn the \u003ca href=\"https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/\"\u003eprevious post\u003c/a\u003e of the Gemma explained series, we discussed the Gemma architecture. In this post, you will explore the latest model, Gemma 2. Let‚Äôs get started!\u003c/p\u003e\u003ch2 data-block-key=\"bhuj0\"\u003e\u003cbr/\u003e\u003cb\u003eGemma 2\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"9djk3\"\u003eRecently, we released Gemma 2, our groundbreaking new suite of open models, setting a new standard for performance and accessibility. Available in 2B, 9B, and 27B parameter sizes, Gemma 2 has quickly made its mark. Our 27B model rapidly ascended the LMSYS Chatbot Arena leaderboard, surpassing even popular models more than twice its size in engaging, real-world conversations, establishing itself as one of the highest-ranking and most useful open models. Meanwhile, the Gemma 2 2B model showcases its exceptional conversational AI prowess by outperforming all GPT-3.5 models on the Chatbot Arena at a size runnable on edge devices.\u003c/p\u003e\u003cp data-block-key=\"b5jjf\"\u003eDevelopers can access robust tuning capabilities with Gemma 2 across platforms and tools. Fine-tuning Gemma 2 is simplified with cloud-based solutions like \u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma2\"\u003eGoogle Cloud\u003c/a\u003e and community tools like \u003ca href=\"https://github.com/OpenAccess-AI-Collective/axolotl\"\u003eAxolotl\u003c/a\u003e. Seamless integration with partners such as Hugging Face and NVIDIA TensorRT-LLM, as well as our JAX and Keras, enables optimization of performance and efficient deployment across diverse hardware configurations.\u003c/p\u003e\u003cp data-block-key=\"7kvrb\"\u003eHere‚Äôs the core parameters of the new models:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-14_at_1.56.34PM.original.png\" alt=\"Core parameters of new Gemma models, August 2024\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"lhk0v\"\u003e\u003cb\u003eKey Differences\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"3uufp\"\u003eGemma 2 shares a similar architectural foundation with the original Gemma models, including the implementation of Rotary Positioning Embeddings (RoPE) and the approximated GeGLU non-linearity. However, it introduces novel architectural innovations that set it apart from its predecessors.\u003c/p\u003e\u003ch3 data-block-key=\"fsmko\"\u003e\u003cbr/\u003e\u003cb\u003eAlternating Local and Global Attention\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"bo0gl\"\u003eInstead of considering all words in a text at once, it sometimes focuses on a small window of words (local attention) and sometimes considers all words (global attention). This combination helps the model understand both the immediate context and the overall meaning of the text efficiently.\u003c/p\u003e\u003ch3 data-block-key=\"afudh\"\u003e\u003cb\u003e\u003cbr/\u003eLogit Soft-Capping\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"85nqq\"\u003eImagine you are training a model to predict the next word in a sentence. Sometimes, the model might be overly confident about a particular word, even if it‚Äôs not the best choice. Logit soft-capping prevents this by limiting how confident the model can be about its predictions, leading to better overall performance.\u003c/p\u003e\u003ch3 data-block-key=\"5hpc\"\u003e\u003cbr/\u003e\u003cb\u003eRMSNorm for Pre and Post-Normalization\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"6e7s3\"\u003eThink of this as a way to keep the model‚Äôs calculations from becoming too large or too small during training. Just like we might adjust the volume on a speaker to prevent distortion, RMSNorm ensures that the information flowing through the model stays within a reasonable range, leading to more stable and effective training.\u003c/p\u003e\u003ch3 data-block-key=\"926l0\"\u003e\u003cbr/\u003e\u003cb\u003eGrouped-Query Attention (GQA)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"1rlbs\"\u003eThis technique helps the model process information more efficiently, especially when dealing with large amounts of text. It improves upon traditional multi-head attention(MHA) by grouping queries together, enabling faster processing, especially for large models. It‚Äôs like dividing a large task into smaller, more manageable chunks, allowing the model to understand the relationships between words faster without sacrificing accuracy.\u003c/p\u003e\u003ch2 data-block-key=\"fh6ei\"\u003e\u003cbr/\u003e\u003cb\u003eGemma 27B\u003c/b\u003e\u003c/h2\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eGemma2ForCausalLM\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2Model\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembed_tokens\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e256000\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epadding_idx\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e45\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e46\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eGemma2DecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2SdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2RotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2MLP\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e36864\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e36864\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e36864\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einput_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2RMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_attention_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2RMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epre_feedforward_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2RMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_feedforward_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2RMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003enorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2RMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elm_head\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4608\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256000\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_HGAXYmV.original.png\" alt=\"Gemma 27B architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"lhk0v\"\u003e\u003cb\u003eself_attn\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"dq6sv\"\u003eIn the self-attention mechanism, Gemma 2 uses \u003cb\u003eGrouped Query Attention (GQA)\u003c/b\u003e.\u003c/p\u003e\u003cp data-block-key=\"foqcn\"\u003e\u003ci\u003ek_proj\u003c/i\u003e and \u003ci\u003ev_proj\u003c/i\u003e share the same head with a size of 128 and 16 heads (128 x 16 = 2048). In contrast, \u003ci\u003eq_proj\u003c/i\u003e and \u003ci\u003eo_proj\u003c/i\u003e have 32 heads (128 x 32 = 4096) in parallel.\u003c/p\u003e\u003chr/\u003e\u003cp data-block-key=\"50ckr\"\u003eNote that the Gemma 9B model uses the Same GQA but different number of heads(8 for \u003ci\u003ek_proj\u003c/i\u003e and \u003ci\u003ev_proj\u003c/i\u003e, 16 for \u003ci\u003eq_proj\u003c/i\u003e and \u003ci\u003eo_proj\u003c/i\u003e) and head size (256)\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2SdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3584\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3584\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3584\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3584\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemma2RotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"lhk0v\"\u003eThe 2B model uses 4 for \u003ci\u003ek_proj\u003c/i\u003e and \u003ci\u003ev_proj\u003c/i\u003e, 8 for \u003ci\u003eq_proj\u003c/i\u003e and \u003ci\u003eo_proj\u003c/i\u003e and head size (256)\u003c/p\u003e\u003chr/\u003e\u003ch3 data-block-key=\"90d6p\"\u003e\u003cb\u003epre_feedforward_layernorm and post_feedforward_layernorm\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"df03g\"\u003eAnother significant distinction is the inclusion of additional RMSNorm in Gemma 2, which enhances the stability of the training process.\u003c/p\u003e\u003ch2 data-block-key=\"43std\"\u003e\u003cbr/\u003e\u003cb\u003eKey Findings\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"46jmt\"\u003eOur \u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf\"\u003etechnical report\u003c/a\u003e provides in-depth details, but here\u0026#39;s a quick summary of Gemma 2\u0026#39;s main findings:\u003c/p\u003e\u003ch3 data-block-key=\"bgrc1\"\u003e\u003cbr/\u003e\u003cb\u003eDistillation vs. Training from Scratch:\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"dc27q\"\u003eWe trained the 2B and 9B models with knowledge distillation from the larger model (27B).\u003c/p\u003e\u003cp data-block-key=\"a776d\"\u003eDistilling knowledge from a larger model, even with an equal number of training tokens, leads to significant performance enhancements.\u003c/p\u003e\u003ch3 data-block-key=\"el3rd\"\u003e\u003cbr/\u003e\u003cb\u003eGrouped Query Attention vs. Multi Head Attention:\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"ea0am\"\u003eReplacing MHA with GQA results in comparable performance while offering parameter efficiency and faster inference times, making GQA the preferred choice.\u003c/p\u003e\u003ch3 data-block-key=\"e5gkb\"\u003e\u003cbr/\u003e\u003cb\u003eModel Depth vs. Width:\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"b6rpk\"\u003eA deeper model showcases slightly superior performance compared to a wider model with the same parameter count.\u003c/p\u003e\u003ch2 data-block-key=\"2h4v6\"\u003e\u003cbr/\u003e\u003cb\u003eWhat‚Äôs Next?\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"49p3h\"\u003eIn this article, you learned about Gemma 2, the next generation of Gemma models.\u003c/p\u003e\u003cp data-block-key=\"9egd7\"\u003eIn our next series of posts, you will examine the RecurrentGemma which is an open model based on \u003ca href=\"https://arxiv.org/abs/2402.19427\"\u003eGriffin\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"8k50c\"\u003eIf you want to delve into the fascinating world of AI and gain insights from the experts who are shaping its development, head over to \u003ca href=\"http://goo.gle/ai-podcast\"\u003egoo.gle/ai-podcast\u003c/a\u003e or search for the show ‚Äú\u003cb\u003ePeople of AI Podcast\u003c/b\u003e‚Äù on any podcast platform.\u003c/p\u003e\u003cp data-block-key=\"e68kr\"\u003eStay tuned and thank you for reading!\u003c/p\u003e\u003chr/\u003e\u003ch2 data-block-key=\"ej461\"\u003e\u003cbr/\u003eReferences\u003c/h2\u003e\u003ch3 data-block-key=\"bs6q4\"\u003e\u003cb\u003e\u003cbr/\u003ePapers\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"eba3t\"\u003e\u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf\"\u003eGemma 2: Improving Open Language Models at a Practical Size\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"dvc9t\"\u003e\u003cbr/\u003e\u003cb\u003eCode Examples\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"eiv01\"\u003e\u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Keras_Gemma_2_Quickstart.ipynb\"\u003eKeras Gemma 2 Quickstart\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"bik9d\"\u003e\u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Keras_Gemma_2_Quickstart_Chat.ipynb\"\u003eKeras Gemma 2 Quickstart Chat\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"da45r\"\u003e\u003cbr/\u003eüìã The complete Gemma architecture series\u003c/h2\u003e\u003cul\u003e\u003cli data-block-key=\"fecep\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/\"\u003eGemma explained: An overview of Gemma model family architectures\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"9eao9\"\u003eGemma explained: What‚Äôs new in Gemma 2\u003c/li\u003e\u003cli data-block-key=\"8q99l\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-recurrentgemma-architecture/\"\u003eGemma explained: RecurrentGemma architecture\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"d0vit\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/\"\u003eGemma explained: PaliGemma architecture\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-08-22T00:00:00Z",
  "modifiedTime": null
}
