{
  "id": "4c7c1865-7b92-420d-8e48-b62ad4507d3e",
  "title": "The next chapter of the Gemini era for developers",
  "link": "https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/",
  "description": "Gemini 2.0 Flash has enhanced capabilities like multimodal outputs and native tool use, and introduces new coding agents to improve developer productivity, now available for testing in Google AI Studio.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Shrestha Basu Mallick, Kathy Korevec",
  "length": 8053,
  "excerpt": "Gemini 2.0 Flash has enhanced capabilities like multimodal outputs and native tool use, and introduces new coding agents to improve developer productivity, now available for testing in Google AI Studio.",
  "siteName": "",
  "favicon": "",
  "text": "We're giving developers the power to build the future of AI with cutting-edge models, intelligent tools to write code faster, and seamless integration across platforms and devices. Since last December when we launched Gemini 1.0, millions of developers have used Google AI Studio and Vertex AI to build with Gemini across 109 languages.Today, we are announcing Gemini 2.0 Flash Experimental to enable even more immersive and interactive applications, as well as new coding agents that will enhance workflows by taking action on behalf of the developer.Build with Gemini 2.0 FlashBuilding on the success of Gemini 1.5 Flash, Flash 2.0 is twice as fast as 1.5 Pro while achieving stronger performance, includes new multimodal outputs, and comes with native tool use. We’re also introducing a Multimodal Live API for building dynamic applications with real-time audio and video streaming.Starting today, developers can test and explore Gemini 2.0 Flash via the Gemini API in Google AI Studio and Vertex AI during its experimental phase, with general availability coming early next year.With Gemini 2.0 Flash, developers have access to:1. Better performanceGemini 2.0 Flash is more powerful than 1.5 Pro while still delivering on the speed and efficiency that developers expect from Flash. It also features improved multimodal, text, code, video, spatial understanding and reasoning performance on key benchmarks. Improved spatial understanding enables more accurate bounding boxes generation on small objects in cluttered images, and better object identification and captioning. Learn more in the spatial understanding video or read the Gemini API docs. 2. New output modalitiesDevelopers will be able to use Gemini 2.0 Flash to generate integrated responses that can include text, audio, and images — all through a single API call. These new output modalities are available to early testers, with wider rollout expected next year. SynthID invisible watermarks will be enabled in all image and audio outputs, helping decrease misinformation and misattribution concerns.Multilingual native audio output: Gemini 2.0 Flash features native text-to-speech audio output that provides developers fine-grained control over not just what the model says, but how it says it, with a choice of 8 high-quality voices and a range of languages and accents. Hear native audio output in action or read more in the developer docs.Native image output: Gemini 2.0 Flash now natively generates images and supports conversational, multi-turn editing, so you can build on previous outputs and refine them. It can output interleaved text and images, making it useful in multimodal content such as recipes. See more in the native image output video. 3. Native tool useGemini 2.0 has been trained to use tools–a foundational capability for building agentic experiences. It can natively call tools like Google Search and code execution in addition to custom third-party functions via function calling. Using Google Search natively as a tool leads to more factual and comprehensive answers and increases traffic to publishers. Multiple searches can be run in parallel leading to improved information retrieval by finding more relevant facts from multiple sources simultaneously and combining them for accuracy. Learn more in the native tool use video or start building from a notebook. 4. Multimodal Live APIDevelopers can now build real-time, multimodal applications with audio and video-streaming inputs from cameras or screens. Natural conversational patterns like interruptions and voice activity detection are supported. The API supports the integration of multiple tools together to accomplish complex use cases with a single API call. See more in the multimodal live streaming video, try the web console, or starter code (Python). We’re thrilled to see startups making impressive progress with Gemini 2.0 Flash, prototyping new experiences like tldraw's visual playground, Viggle's virtual character creation and audio narration, Toonsutra's contextual multilingual translation, and Rooms' adding real-time audio.To jumpstart building, we’ve released three starter app experiences in Google AI Studio along with open source code for spatial understanding, video analysis and Google Maps exploration so you can begin building with Gemini 2.0 Flash.Enabling the evolution of AI code assistanceAs AI code assistance rapidly evolves from simple code searches to AI-powered assistants embedded in developer workflows, we want to share the latest advancement that will use Gemini 2.0: coding agents that can execute tasks on your behalf.In our latest research, we've been able to use 2.0 Flash equipped with code execution tools to achieve 51.8% on SWE-bench Verified, which tests agent performance on real-world software engineering tasks. The cutting edge inference speed of 2.0 Flash allowed the agent to sample hundreds of potential solutions, selecting the best based on existing unit tests and Gemini's own judgment. We're in the process of turning this research into new developer products.Meet Jules, your AI-powered code agentImagine your team has just finished a bug bash, and now you’re staring down a long list of bugs. Starting today, you can offload Python and Javascript coding tasks to Jules, an experimental AI-powered code agent that will use Gemini 2.0. Working asynchronously and integrated with your GitHub workflow, Jules handles bug fixes and other time-consuming tasks while you focus on what you actually want to build. Jules creates comprehensive, multi-step plans to address issues, efficiently modifies multiple files, and even prepares pull requests to land fixes directly back into GitHub. It’s early, but from our internal experience using Jules, it’s giving developers:More productivity. Assign issues and coding tasks to Jules for asynchronous coding efficiency.Progress tracking. Stay informed and prioritize tasks that require your attention with real-time updates.Full developer control. Review the plans Jules creates along the way, and provide feedback or request adjustments as you see fit. Easily review and, if appropriate, merge the code Jules writes into your project.We’re making Jules available for a select group of trusted testers today, and we’ll make it available for other interested developers in early 2025. Sign up to get updates about Jules on labs.google.com/jules.Colab's data science agent will create notebooks for youAt I/O this year, we launched an experimental Data Science Agent on labs.google/code that allows anyone to upload a dataset and get insights within minutes, all grounded in a working Colab notebook. We were thrilled to receive such positive feedback from the developer community and see the impact. For example, with the help of Data Science Agent, a scientist at Lawrence Berkeley National Laboratory working on a global tropical wetland methane emissions project has estimated their analysis and processing time was reduced from one week to five minutes.Colab has started to integrate these same agentic capabilities, using Gemini 2.0. Simply describe your analysis goals in plain language, and watch your notebook take shape automatically, helping accelerate your ability to conduct research and data analysis. Developers can get early access to this new feature by joining the trusted tester program before it rolls out more widely to Colab users in the first half of 2025. Developers are building the futureOur Gemini 2.0 models can empower you to build more capable AI apps faster and easier, so you can focus on great experiences for your users. We'll be bringing Gemini 2.0 to our platforms like Android Studio, Chrome DevTools and Firebase in the coming months. Developers can sign up to use Gemini 2.0 Flash in Gemini Code Assist, for enhanced coding assistance capabilities in popular IDEs such as Visual Studio Code, IntelliJ, PyCharm and more. Visit ai.google.dev to get started and follow Google AI for Developers for future updates.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini2.0.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"cg37g\"\u003eWe\u0026#39;re giving developers the power to build the future of AI with cutting-edge models, intelligent tools to write code faster, and seamless integration across platforms and devices. Since last December when we launched Gemini 1.0, millions of developers have used \u003ca href=\"https://aistudio.google.com/?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-L0\"\u003eGoogle AI Studio\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/vertex-ai/?utm_source=google_dev\u0026amp;utm_medium=blog\u0026amp;utm_campaign=gemini2_blog_launch\u0026amp;utm_content=\"\u003eVertex AI\u003c/a\u003e to \u003ca href=\"https://ai.google.dev/showcase/?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-L0-showcase\"\u003ebuild with Gemini\u003c/a\u003e across 109 languages.\u003c/p\u003e\u003cp data-block-key=\"5p9tr\"\u003eToday, we are announcing \u003ca href=\"http://deepmind.google/technologies/gemini\"\u003eGemini 2.0\u003c/a\u003e Flash Experimental to enable even more immersive and interactive applications, as well as new coding agents that will enhance workflows by taking action on behalf of the developer.\u003c/p\u003e\u003ch2 data-block-key=\"4el00\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuild with Gemini 2.0 Flash\u003c/h2\u003e\u003cp data-block-key=\"236cu\"\u003eBuilding on the success of Gemini 1.5 Flash, Flash 2.0 is twice as fast as 1.5 Pro while achieving stronger \u003ca href=\"https://deepmind.google/technologies/gemini/flash/\"\u003eperformance\u003c/a\u003e, includes new multimodal outputs, and comes with native tool use. We’re also introducing a Multimodal Live API for building dynamic applications with real-time audio and video streaming.\u003c/p\u003e\u003cp data-block-key=\"629f8\"\u003eStarting today, developers can test and explore Gemini 2.0 Flash via the \u003ca href=\"https://ai.google.dev/gemini-api/docs?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-L0-docs\"\u003eGemini API\u003c/a\u003e in \u003ca href=\"https://aistudio.google.com/?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-L0\"\u003eGoogle AI Studio\u003c/a\u003e and \u003ca href=\"https://console.cloud.google.com/vertex-ai/studio/freeform?model=gemini-2.0-flash-exp\"\u003eVertex AI\u003c/a\u003e during its experimental phase, with general availability coming early next year.\u003c/p\u003e\u003cp data-block-key=\"fes09\"\u003eWith Gemini 2.0 Flash, developers have access to:\u003c/p\u003e\u003ch3 data-block-key=\"9p4om\"\u003e\u003cb\u003e1. Better performance\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"5p7pn\"\u003eGemini 2.0 Flash is more powerful than 1.5 Pro while still delivering on the speed and efficiency that developers expect from Flash. It also features improved multimodal, text, code, video, spatial understanding and reasoning performance on key \u003ca href=\"http://deepmind.google/technologies/gemini\"\u003ebenchmarks\u003c/a\u003e. Improved spatial understanding enables more accurate bounding boxes generation on small objects in cluttered images, and better object identification and captioning. Learn more in the \u003ca href=\"https://youtu.be/-XmoDzDMqj4\"\u003espatial understanding video\u003c/a\u003e or read the \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini-v2#bounding-box\"\u003eGemini API docs\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e    \u003cdiv\u003e\n    \u003ch3 data-block-key=\"cg37g\"\u003e\u003cb\u003e2. New output modalities\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9a3l2\"\u003eDevelopers will be able to use Gemini 2.0 Flash to generate integrated responses that can include text, audio, and images — all through a single API call. These new output modalities are available to early testers, with wider rollout expected next year. \u003ca href=\"https://ai.google.dev/responsible/docs/safeguards/synthid/?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-L0-synthid\"\u003eSynthID\u003c/a\u003e invisible watermarks will be enabled in all image and audio outputs, helping decrease misinformation and misattribution concerns.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"f8tlb\"\u003e\u003cb\u003eMultilingual native audio output:\u003c/b\u003e Gemini 2.0 Flash features native text-to-speech audio output that provides developers fine-grained control over not just \u003ci\u003ewhat\u003c/i\u003e the model says, but \u003ci\u003ehow\u003c/i\u003e it says it, with a choice of 8 high-quality voices and a range of languages and accents. \u003ca href=\"https://www.youtube.com/watch?v=qE673AY-WEI\"\u003eHear native audio output\u003c/a\u003e in action or read more in the \u003ca href=\"https://ai.google.dev/gemini-api/docs/models/gemini-v2\"\u003edeveloper docs\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"3s468\"\u003e\u003cb\u003eNative image output:\u003c/b\u003e Gemini 2.0 Flash now natively generates images and supports conversational, multi-turn editing, so you can build on previous outputs and refine them. It can output interleaved text and images, making it useful in multimodal content such as recipes. See more in the \u003ca href=\"https://youtu.be/7RqFLp0TqV0\"\u003enative image output video\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e    \u003cdiv\u003e\n    \u003ch3 data-block-key=\"oquun\"\u003e\u003cb\u003e3. Native tool use\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"bva7g\"\u003eGemini 2.0 has been trained to use tools–a foundational capability for building agentic experiences. It can natively call tools like Google Search and code execution in addition to custom third-party functions via function calling. Using Google Search natively as a tool leads to more factual and comprehensive answers and increases traffic to publishers. Multiple searches can be run in parallel leading to improved information retrieval by finding more relevant facts from multiple sources simultaneously and combining them for accuracy. Learn more in the \u003ca href=\"https://youtu.be/EVzeutiojWs\"\u003enative tool use video\u003c/a\u003e or start building from a \u003ca href=\"https://github.com/google-gemini/cookbook/blob/main/gemini-2/search_tool.ipynb\"\u003enotebook\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e    \u003cdiv\u003e\n    \u003ch3 data-block-key=\"oquun\"\u003e\u003cb\u003e4. Multimodal Live API\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"da5is\"\u003eDevelopers can now build real-time, multimodal applications with audio and video-streaming inputs from cameras or screens. Natural conversational patterns like interruptions and voice activity detection are supported. The API supports the integration of multiple tools together to accomplish complex use cases with a single API call. See more in the \u003ca href=\"https://youtu.be/9hE5-98ZeCg\"\u003emultimodal live streaming video\u003c/a\u003e, try the \u003ca href=\"https://github.com/google-gemini/multimodal-live-api-web-console\"\u003eweb console\u003c/a\u003e, or \u003ca href=\"https://github.com/google-gemini/cookbook/tree/main/gemini-2\"\u003estarter code\u003c/a\u003e (Python).\u003c/p\u003e\n\u003c/div\u003e    \u003cdiv\u003e\n    \u003cp data-block-key=\"oquun\"\u003eWe’re thrilled to see startups making impressive progress with Gemini 2.0 Flash, prototyping new experiences like \u003ca href=\"http://ai.google.dev/showcase/tldraw\"\u003etldraw\u0026#39;s\u003c/a\u003e visual playground, \u003ca href=\"http://ai.google.dev/showcase/viggle\"\u003eViggle\u0026#39;s\u003c/a\u003e virtual character creation and audio narration, \u003ca href=\"http://ai.google.dev/showcase/toonsutra\"\u003eToonsutra\u0026#39;s\u003c/a\u003e contextual multilingual translation, and \u003ca href=\"http://ai.google.dev/showcase/rooms\"\u003eRooms\u0026#39;\u003c/a\u003e adding real-time audio.\u003c/p\u003e\u003cp data-block-key=\"226ir\"\u003eTo jumpstart building, we’ve released \u003ca href=\"https://aistudio.google.com/app/starter-apps/?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-starterapps\"\u003ethree starter app\u003c/a\u003e experiences in \u003ca href=\"https://aistudio.google.com/app/starter-apps/?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-starterapps\"\u003eGoogle AI Studio\u003c/a\u003e along with open source code for spatial understanding, video analysis and Google Maps exploration so you can begin building with Gemini 2.0 Flash.\u003c/p\u003e\u003ch2 data-block-key=\"1gbas\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eEnabling the evolution of AI code assistance\u003c/h2\u003e\u003cp data-block-key=\"1073u\"\u003eAs AI code assistance rapidly evolves from simple code searches to AI-powered assistants embedded in developer workflows, we want to share the latest advancement that will use Gemini 2.0: coding agents that can execute tasks on your behalf.\u003c/p\u003e\u003cp data-block-key=\"dlf7p\"\u003eIn our latest research, we\u0026#39;ve been able to use 2.0 Flash equipped with code execution tools to achieve 51.8% on SWE-bench Verified, which tests agent performance on real-world software engineering tasks. The cutting edge inference speed of 2.0 Flash allowed the agent to sample hundreds of potential solutions, selecting the best based on existing unit tests and Gemini\u0026#39;s own judgment. We\u0026#39;re in the process of turning this research into new developer products.\u003c/p\u003e\u003ch3 data-block-key=\"cu6fn\"\u003e\u003cb\u003e\u003cbr/\u003eMeet Jules, your AI-powered code agent\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"6mgm9\"\u003eImagine your team has just finished a bug bash, and now you’re staring down a long list of bugs. Starting today, you can offload Python and Javascript coding tasks to Jules, an experimental AI-powered code agent that will use Gemini 2.0. Working asynchronously and integrated with your GitHub workflow, Jules handles bug fixes and other time-consuming tasks while you focus on what you actually want to build. Jules creates comprehensive, multi-step plans to address issues, efficiently modifies multiple files, and even prepares pull requests to land fixes directly back into GitHub.\u003c/p\u003e\n\u003c/div\u003e   \n\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"oquun\"\u003eIt’s early, but from our internal experience using Jules, it’s giving developers:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"4mcrc\"\u003e\u003cb\u003eMore productivity.\u003c/b\u003e Assign issues and coding tasks to Jules for asynchronous coding efficiency.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"efetg\"\u003e\u003cb\u003eProgress tracking.\u003c/b\u003e Stay informed and prioritize tasks that require your attention with real-time updates.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"2brop\"\u003e\u003cb\u003eFull developer control.\u003c/b\u003e Review the plans Jules creates along the way, and provide feedback or request adjustments as you see fit. Easily review and, if appropriate, merge the code Jules writes into your project.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"eu9g8\"\u003eWe’re making Jules available for a select group of trusted testers today, and we’ll make it available for other interested developers in early 2025. Sign up to get updates about Jules on \u003ca href=\"http://labs.google.com/jules\"\u003elabs.google.com/jules\u003c/a\u003e.\u003c/p\u003e\u003ch3 data-block-key=\"6kdmg\"\u003e\u003cb\u003e\u003cbr/\u003eColab\u0026#39;s data science agent will create notebooks for you\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"p0aj\"\u003eAt I/O this year, we launched an experimental Data Science Agent on \u003ca href=\"http://labs.google/code\"\u003elabs.google/code\u003c/a\u003e that allows anyone to upload a dataset and get insights within minutes, all grounded in a working Colab notebook. We were thrilled to receive such positive feedback from the developer community and see the impact. For example, with the help of Data Science Agent, a scientist at Lawrence Berkeley National Laboratory working on a global tropical wetland methane emissions project has estimated their analysis and processing time was reduced from one week to five minutes.\u003c/p\u003e\u003cp data-block-key=\"aabjp\"\u003eColab has started to integrate these same agentic capabilities, using Gemini 2.0. Simply describe your analysis goals in plain language, and watch your notebook take shape automatically, helping accelerate your ability to conduct research and data analysis. Developers can get early access to this new feature by joining the \u003ca href=\"https://forms.gle/UQWKGrhFqVRLmJGy5\"\u003etrusted tester program\u003c/a\u003e before it rolls out more widely to Colab users in the first half of 2025.\u003c/p\u003e\n\u003c/div\u003e   \n\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"2kpun\"\u003eDevelopers are building the future\u003c/h2\u003e\u003cp data-block-key=\"4csgl\"\u003eOur Gemini 2.0 models can empower you to build more capable AI apps faster and easier, so you can focus on great experiences for your users. We\u0026#39;ll be bringing Gemini 2.0 to our platforms like \u003ca href=\"https://developer.android.com/gemini-in-android\"\u003eAndroid Studio\u003c/a\u003e, \u003ca href=\"https://developer.chrome.com/docs/devtools/ai-assistance/quickstart\"\u003eChrome DevTools\u003c/a\u003e and \u003ca href=\"https://firebase.google.com/products/generative-ai\"\u003eFirebase\u003c/a\u003e in the coming months. Developers can \u003ca href=\"https://docs.google.com/forms/d/e/1FAIpQLSc1yAQ8aJeUUHjlLjuEVmanVvoS_YFUmHtwsetl6GXVg-U0Jw/viewform\"\u003esign up\u003c/a\u003e to use Gemini 2.0 Flash in \u003ca href=\"https://cloud.google.com/products/gemini/code-assist/?utm_source=google_dev\u0026amp;utm_medium=blog\u0026amp;utm_campaign=gemini2_blog_launch\u0026amp;utm_content=\"\u003eGemini Code Assist\u003c/a\u003e, for enhanced coding assistance capabilities in popular IDEs such as Visual Studio Code, IntelliJ, PyCharm and more. Visit \u003ca href=\"https://ai.google.dev/?utm_source=gfd\u0026amp;utm_medium=referral\u0026amp;utm_campaign=blog-dec\u0026amp;utm_content=gemini2-L1\"\u003eai.google.dev\u003c/a\u003e to get started and follow \u003ca href=\"https://x.com/googleaidevs\"\u003eGoogle AI for Developers\u003c/a\u003e for future updates.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-12-11T00:00:00Z",
  "modifiedTime": null
}
