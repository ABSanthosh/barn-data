{
  "id": "29c2c639-1533-42aa-b770-a225ed716b16",
  "title": "OpenAI Presents Research on Inference-Time Compute to Better AI Security",
  "link": "https://www.infoq.com/news/2025/01/openai-inference-time/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "OpenAI presented Trading Inference-Time Compute for Adversarial Robustness, a research paper that investigates the relationship between inference-time compute and the robustness of AI models against adversarial attacks. By Daniel Dominguez",
  "author": "Daniel Dominguez",
  "published": "Sat, 25 Jan 2025 15:09:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Artificial Intelligence",
    "Security",
    "OpenAI",
    "ChatGPT",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Daniel Dominguez",
  "length": 3402,
  "excerpt": "OpenAI presented Trading Inference-Time Compute for Adversarial Robustness, a research paper that investigates the relationship between inference-time compute and the robustness of AI models against a",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250124075613/apple-touch-icon.png",
  "text": "OpenAI presented Trading Inference-Time Compute for Adversarial Robustness, a research paper that investigates the relationship between inference-time compute and the robustness of AI models against adversarial attacks. The research, conducted using reasoning models such as OpenAI's o1-preview and o1-mini, provides initial evidence that allowing models more time and resources during inference can reduce their vulnerability to various types of adversarial attacks. Adversarial attacks, which involve subtle, often imperceptible perturbations to input data, have long been a challenge in AI. These attacks can cause models to misclassify inputs or produce incorrect outputs, even when the changes are undetectable to humans. Despite extensive research, effective defenses against such attacks remain elusive. Increasing model size alone has not proven sufficient to address this issue. The study examined how increasing inference-time compute, essentially giving models more \"thinking\" time, affects their robustness. Experiments were conducted across a range of tasks, including mathematical problem-solving, fact-based question answering, and image classification. The results showed that, in many cases, the probability of a successful adversarial attack decreased as the amount of inference-time compute increased. This improvement occurred without adversarial training or prior knowledge of the attack type. The research also introduced new types of adversarial attacks tailored to reasoning models. These include many-shot attacks, where adversaries provide multiple misleading examples, and soft-token attacks, which optimize embedding vectors to achieve adversarial goals. Additionally, the study explored \"Think Less\" attacks, which attempt to reduce the model's inference-time compute, making it more vulnerable, and \"Nerd Sniping\" attacks, which exploit unproductive reasoning loops where the model spends excessive compute without improving robustness. Comments on OpenAI's post on X revealed a mix of excitement for advancements in AI robustness and safety, curiosity for more technical details, and skepticism about potential misuse or the sufficiency of improvements. User Paddy Sham, shared: I think this is actually important for more people to understand grasp the ideas in algorithmic and data bias vulnerabilities when building these models for the future. Especially ones that are hard to detect because of the way our minds work. For a machine system might be pretty easy to detect the patterns and form a bias. While user Robert Nichols commented: An intriguing perspective on balancing computational efficiency with security! It raises essential questions about the trade-offs in AI models. Do you believe this approach could pave the way for more robust systems in real-world applications? While increased compute generally reduced attack success rates, the study identified limitations. In cases where policies or goals are unclear, attackers can exploit loopholes, and increased compute does not always help. Models may also sometimes spend compute inefficiently, leading to vulnerabilities. The full details of the study, including limitations and open questions, are available in the published paper. About the Author Daniel Dominguez",
  "image": "https://res.infoq.com/news/2025/01/openai-inference-time/en/headerimage/generatedHeaderImage-1737731664199.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eOpenAI presented \u003ca href=\"https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/\"\u003eTrading Inference-Time Compute for Adversarial Robustness\u003c/a\u003e, a \u003ca href=\"https://cdn.openai.com/papers/trading-inference-time-compute-for-adversarial-robustness-20250121_1.pdf\"\u003eresearch paper\u003c/a\u003e that investigates the relationship between \u003ca href=\"https://www.sciencedirect.com/topics/computer-science/inference-computation#:~:text=\u0026#39;Inference%20computation\u0026#39;%20refers%20to%20the,Processor%20Architecture%20and%20Programming%2C%202020\"\u003einference-time compute\u003c/a\u003e and the robustness of AI models against adversarial attacks. The research, conducted using reasoning models such as OpenAI\u0026#39;s \u003ca href=\"https://openai.com/index/introducing-openai-o1-preview/\"\u003eo1-preview\u003c/a\u003e and \u003ca href=\"https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/\"\u003eo1-mini\u003c/a\u003e, provides initial evidence that allowing models more time and resources during inference can reduce their vulnerability to various types of adversarial attacks.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Adversarial_machine_learning\"\u003eAdversarial attacks\u003c/a\u003e, which involve subtle, often imperceptible perturbations to input data, have long been a challenge in AI. These attacks can cause models to misclassify inputs or produce incorrect outputs, even when the changes are undetectable to humans. Despite extensive research, effective defenses against such attacks remain elusive. Increasing model size alone has not proven sufficient to address this issue.\u003c/p\u003e\n\n\u003cp\u003eThe study examined how increasing inference-time compute, essentially giving models more \u0026#34;thinking\u0026#34; time, affects their robustness. Experiments were conducted across a range of tasks, including mathematical problem-solving, fact-based question answering, and image classification. The results showed that, in many cases, the probability of a successful adversarial attack decreased as the amount of inference-time compute increased. This improvement occurred without adversarial training or prior knowledge of the attack type.\u003c/p\u003e\n\n\u003cp\u003eThe research also introduced new types of adversarial attacks tailored to reasoning models. These include \u003ca href=\"https://www.anthropic.com/research/many-shot-jailbreaking#:~:text=cause%20serious%20harm.-,Many%2Dshot%20jailbreaking,-The%20basis%20of\"\u003emany-shot attacks\u003c/a\u003e, where adversaries provide multiple misleading examples, and soft-token attacks, which optimize embedding vectors to achieve adversarial goals. Additionally, the study explored \u0026#34;Think Less\u0026#34; attacks, which attempt to reduce the model\u0026#39;s inference-time compute, making it more vulnerable, and \u0026#34;Nerd Sniping\u0026#34; attacks, which exploit unproductive reasoning loops where the model spends excessive compute without improving robustness.\u003c/p\u003e\n\n\u003cp\u003eComments on \u003ca href=\"https://x.com/OpenAI/status/1882129444212740482\"\u003eOpenAI\u0026#39;s post on X\u003c/a\u003e revealed a mix of excitement for advancements in AI robustness and safety, curiosity for more technical details, and skepticism about potential misuse or the sufficiency of improvements.\u003c/p\u003e\n\n\u003cp\u003eUser \u003ca href=\"https://x.com/i_am_Paddy_Sham/status/1882132823404105934\"\u003ePaddy Sham\u003c/a\u003e, shared:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eI think this is actually important for more people to understand grasp the ideas in algorithmic and data bias vulnerabilities when building these models for the future. Especially ones that are hard to detect because of the way our minds work. For a machine system might be pretty easy to detect the patterns and form a bias.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWhile user \u003ca href=\"https://x.com/nicholsmindset/status/1882152513262215614\"\u003eRobert Nichols\u003c/a\u003e commented:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAn intriguing perspective on balancing computational efficiency with security! It raises essential questions about the trade-offs in AI models. Do you believe this approach could pave the way for more robust systems in real-world applications?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWhile increased compute generally reduced attack success rates, the study identified limitations. In cases where policies or goals are unclear, attackers can exploit loopholes, and increased compute does not always help. Models may also sometimes spend compute inefficiently, leading to vulnerabilities.\u003c/p\u003e\n\n\u003cp\u003eThe full details of the study, including limitations and open questions, are available in the \u003ca href=\"https://cdn.openai.com/papers/trading-inference-time-compute-for-adversarial-robustness-20250121_1.pdf\"\u003epublished paper\u003c/a\u003e.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Daniel-Dominguez\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eDaniel Dominguez\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-01-25T00:00:00Z",
  "modifiedTime": null
}
