{
  "id": "5a64b3a7-8405-46cb-98eb-89a1115be32b",
  "title": "Bypassing MTE with CVE-2025-0072",
  "link": "https://github.blog/security/vulnerability-research/bypassing-mte-with-cve-2025-0072/",
  "description": "In this post, I’ll look at CVE-2025-0072, a vulnerability in the Arm Mali GPU, and show how it can be exploited to gain kernel code execution even when Memory Tagging Extension (MTE) is enabled. The post Bypassing MTE with CVE-2025-0072 appeared first on The GitHub Blog.",
  "author": "Man Yue Mo",
  "published": "Fri, 23 May 2025 10:00:00 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "Security",
    "Vulnerability research",
    "Android",
    "exploit development",
    "GitHub Security Lab"
  ],
  "byline": "Man Yue Mo",
  "length": 16523,
  "excerpt": "See how a vulnerability in the Arm Mali GPU can be exploited to gain kernel code execution even when Memory Tagging Extension (MTE) is enabled.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "Memory Tagging Extension (MTE) is an advanced memory safety feature that is intended to make memory corruption vulnerabilities almost impossible to exploit. But no mitigation is ever completely airtight—especially in kernel code that manipulates memory at a low level. Last year, I wrote about CVE-2023-6241, a vulnerability in ARM’s Mali GPU driver, which enabled an untrusted Android app to bypass MTE and gain arbitrary kernel code execution. In this post, I’ll walk through CVE-2025-0072: a newly patched vulnerability that I also found in ARM’s Mali GPU driver. Like the previous one, it enables a malicious Android app to bypass MTE and gain arbitrary kernel code execution. I reported the issue to Arm on December 12, 2024. It was fixed in Mali driver version r54p0, released publicly on May 2, 2025, and included in Android’s May 2025 security update. The vulnerability affects devices with newer Arm Mali GPUs that use the Command Stream Frontend (CSF) architecture, such as Google’s Pixel 7, 8, and 9 series. I developed and tested the exploit on a Pixel 8 with kernel MTE enabled, and I believe it should work on the 7 and 9 as well with minor modifications. What follows is a deep dive into how CSF queues work, the steps I used to exploit this bug, and how it ultimately bypasses MTE protections to achieve kernel code execution. How CSF queues work—and how they become dangerous Arm Mali GPUs with the CSF feature communicate with userland applications through command queues, implemented in the driver as kbase_queue objects. The queues are created by using the KBASE_IOCTL_CS_QUEUE_REGISTER ioctl. To use the kbase_queue that is created, it first has to be bound to a kbase_queue_group, which is created with the KBASE_IOCTL_CS_QUEUE_GROUP_CREATE ioctl. A kbase_queue can be bound to a kbase_queue_group with the KBASE_IOCTL_CS_QUEUE_BIND ioctl. When binding a kbase_queue to a kbase_queue_group, a handle is created from get_user_pages_mmap_handle and returned to the user application. int kbase_csf_queue_bind(struct kbase_context *kctx, union kbase_ioctl_cs_queue_bind *bind) { ... group = find_queue_group(kctx, bind-\u003ein.group_handle); queue = find_queue(kctx, bind-\u003ein.buffer_gpu_addr); … ret = get_user_pages_mmap_handle(kctx, queue); if (ret) goto out; bind-\u003eout.mmap_handle = queue-\u003ehandle; group-\u003ebound_queues[bind-\u003ein.csi_index] = queue; queue-\u003egroup = group; queue-\u003egroup_priority = group-\u003epriority; queue-\u003ecsi_index = (s8)bind-\u003ein.csi_index; queue-\u003ebind_state = KBASE_CSF_QUEUE_BIND_IN_PROGRESS; out: rt_mutex_unlock(\u0026kctx-\u003ecsf.lock); return ret; } In addition, mutual references are stored between the kbase_queue_group and the queue. Note that when the call finishes, queue-\u003ebind_state is set to KBASE_CSF_QUEUE_BIND_IN_PROGRESS, indicating that the binding is not completed. To complete the binding, the user application must call mmap with the handle returned from the ioctl as the file offset. This mmap call is handled by kbase_csf_cpu_mmap_user_io_pages, which allocates GPU memory via kbase_csf_alloc_command_stream_user_pages and maps it to user space. int kbase_csf_alloc_command_stream_user_pages(struct kbase_context *kctx, struct kbase_queue *queue) { struct kbase_device *kbdev = kctx-\u003ekbdev; int ret; lockdep_assert_held(\u0026kctx-\u003ecsf.lock); ret = kbase_mem_pool_alloc_pages(\u0026kctx-\u003emem_pools.small[KBASE_MEM_GROUP_CSF_IO], KBASEP_NUM_CS_USER_IO_PAGES, queue-\u003ephys, false, //\u003c------ 1. kctx-\u003etask); ... ret = kernel_map_user_io_pages(kctx, queue); ... get_queue(queue); queue-\u003ebind_state = KBASE_CSF_QUEUE_BOUND; mutex_unlock(\u0026kbdev-\u003ecsf.reg_lock); return 0; ... } In 1. in the above snippet, kbase_mem_pool_alloc_pages is called to allocate memory pages from the GPU memory pool, whose addresses are then stored in the queue-\u003ephys field. These pages are then mapped to user space and the bind_state of the queue is set to KBASE_CSF_QUEUE_BOUND. These pages are only freed when the mmapped area is unmapped from the user space. In that case, kbase_csf_free_command_stream_user_pages is called to free the pages via kbase_mem_pool_free_pages. void kbase_csf_free_command_stream_user_pages(struct kbase_context *kctx, struct kbase_queue *queue) { kernel_unmap_user_io_pages(kctx, queue); kbase_mem_pool_free_pages(\u0026kctx-\u003emem_pools.small[KBASE_MEM_GROUP_CSF_IO], KBASEP_NUM_CS_USER_IO_PAGES, queue-\u003ephys, true, false); ... } This frees the pages stored in queue-\u003ephys, and because this only happens when the pages are unmapped from user space, it prevents the pages from being accessed after they are freed. An exploit idea The interesting part begins when we ask: what happens if we can modify queue-\u003ephys after mapping them into user space. For example, if I can trigger kbase_csf_alloc_command_user_pages again to overwrite new pages to queue-\u003ephys, and map them to user space and then unmap the previously mapped region, kbase_csf_free_command_stream_user_pages will be called to free the pages in queue-\u003ephys. However, because queue-\u003ephys is now overwritten by the newly allocated pages, I ended up in a situation where I free the new pages while unmapping an old region: In the above figure, the right columns are mappings in the user space, green rectangles are mapped, while gray ones are unmapped. The left column are backing pages stored in queue-\u003ephys. The new queue-\u003ephys are pages that are currently stored in queue-\u003ephys, while old queue-\u003ephys are pages that are stored previously but are replaced by the new ones. Green indicates that the pages are alive, while red indicates that they are freed. After overwriting queue-\u003ephys and unmapping the old region, the new queue-\u003ephys are freed instead, while still mapped to the new user region. This means that user space will have access to the freed new queue-\u003ephys pages. This then gives me a page use-after-free vulnerability. The vulnerability So let’s take a look at how to achieve this situation. The first obvious thing to try is to see if I can bind a kbase_queue multiple times using the KBASE_IOCTL_CS_QUEUE_BIND ioctl. This, however, is not possible because the queue-\u003egroup field is checked before binding: int kbase_csf_queue_bind(struct kbase_context *kctx, union kbase_ioctl_cs_queue_bind *bind) { ... if (queue-\u003egroup || group-\u003ebound_queues[bind-\u003ein.csi_index]) goto out; ... } After a kbase_queue is bound, its queue-\u003egroup is set to the kbase_queue_group that it binds to, which prevents the kbase_queue from binding again. Moreover, once a kbase_queue is bound, it cannot be unbound via any ioctl. It can be terminated with KBASE_IOCTL_CS_QUEUE_TERMINATE, but that will also delete the kbase_queue. So if rebinding from the queue is not possible, what about trying to unbind from a kbase_queue_group? For example, what happens if a kbase_queue_group gets terminated with the KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE ioctl? When a kbase_queue_group terminates, as part of the clean up process, it calls kbase_csf_term_descheduled_queue_group to unbind queues that it bound to: void kbase_csf_term_descheduled_queue_group(struct kbase_queue_group *group) { ... for (i = 0; i \u003c max_streams; i++) { struct kbase_queue *queue = group-\u003ebound_queues[i]; /* The group is already being evicted from the scheduler */ if (queue) unbind_stopped_queue(kctx, queue); } ... } This then resets the queue-\u003egroup field of the kbase_queue that gets unbound: static void unbind_stopped_queue(struct kbase_context *kctx, struct kbase_queue *queue) { ... if (queue-\u003ebind_state != KBASE_CSF_QUEUE_UNBOUND) { ... queue-\u003egroup-\u003ebound_queues[queue-\u003ecsi_index] = NULL; queue-\u003egroup = NULL; ... queue-\u003ebind_state = KBASE_CSF_QUEUE_UNBOUND; } } In particular, this now allows the kbase_queue to bind to another kbase_queue_group. This means I can now create a page use-after-free with the following steps: Create a kbase_queue and a kbase_queue_group, and then bind the kbase_queue to the kbase_queue_group. Create GPU memory pages for the user io pages in the kbase_queue and map them to user space using a mmap call. These pages are then stored in the queue-\u003ephys field of the kbase_queue. Terminate the kbase_queue_group, which also unbinds the kbase_queue. Create another kbase_queue_group and bind the kbase_queue to this new group. Create new GPU memory pages for the user io pages in this kbase_queue and map them to user space. These pages now overwrite the existing pages in queue-\u003ephys. Unmap the user space memory that was mapped in step 2. This then frees the pages in queue-\u003ephys and removes the user space mapping created in step 2. However, the pages that are freed are now the memory pages created and mapped in step 5, which are still mapped to user space. This, in particular, means that the pages that are freed in step 6 of the above can still be accessed from the user application. By using a technique that I used previously, I can reuse these freed pages as page table global directories (PGD) of the Mali GPU. To recap, let’s take a look at how the backing pages of a kbase_va_region are allocated. When allocating pages for the backing store of a kbase_va_region, the kbase_mem_pool_alloc_pages function is used: int kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_4k_pages, struct tagged_addr *pages, bool partial_allowed) { ... /* Get pages from this pool */ while (nr_from_pool--) { p = kbase_mem_pool_remove_locked(pool); //\u003c------- 1. ... } ... if (i != nr_4k_pages \u0026\u0026 pool-\u003enext_pool) { /* Allocate via next pool */ err = kbase_mem_pool_alloc_pages(pool-\u003enext_pool, //\u003c----- 2. nr_4k_pages - i, pages + i, partial_allowed); ... } else { /* Get any remaining pages from kernel */ while (i != nr_4k_pages) { p = kbase_mem_alloc_page(pool); //\u003c------- 3. ... } ... } ... } The input argument kbase_mem_pool is a memory pool managed by the kbase_context object associated with the driver file that is used to allocate the GPU memory. As the comments suggest, the allocation is actually done in tiers. First the pages will be allocated from the current kbase_mem_pool using kbase_mem_pool_remove_locked (1 in the above). If there is not enough capacity in the current kbase_mem_pool to meet the request, then pool-\u003enext_pool, is used to allocate the pages (2 in the above). If even pool-\u003enext_pool does not have the capacity, then kbase_mem_alloc_page is used to allocate pages directly from the kernel via the buddy allocator (the page allocator in the kernel). When freeing a page, the same happens in the opposite direction: kbase_mem_pool_free_pages first tries to return the pages to the kbase_mem_pool of the current kbase_context, if the memory pool is full, it’ll try to return the remaining pages to pool-\u003enext_pool. If the next pool is also full, then the remaining pages are returned to the kernel by freeing them via the buddy allocator. As noted in my post “Corrupting memory without memory corruption”, pool-\u003enext_pool is a memory pool managed by the Mali driver and shared by all the kbase_context. It is also used for allocating page table global directories (PGD) used by GPU contexts. In particular, this means that by carefully arranging the memory pools, it is possible to cause a freed backing page in a kbase_va_region to be reused as a PGD of a GPU context. (Read the details of how to achieve this.) Once the freed page is reused as a PGD of a GPU context, the user space mapping can be used to rewrite the PGD from the GPU. This then allows any kernel memory, including kernel code, to be mapped to the GPU, which allows me to rewrite kernel code and hence execute arbitrary kernel code. It also allows me to read and write arbitrary kernel data, so I can easily rewrite credentials of my process to gain root, as well as to disable SELinux. See the exploit for Pixel 8 with some setup notes. How does this bypass MTE? Before wrapping up, let’s look at why this exploit manages to bypass Memory Tagging Extension (MTE)—despite protections that should have made this type of attack impossible. The Memory Tagging Extension (MTE) is a security feature on newer Arm processors that uses hardware implementations to check for memory corruptions. The Arm64 architecture uses 64 bit pointers to access memory, while most applications use a much smaller address space (for example, 39, 48, or 52 bits). The highest bits in a 64 bit pointer are actually unused. The main idea of memory tagging is to use these higher bits in an address to store a “tag” that can then be used to check against the other tag stored in the memory block associated with the address. When a linear overflow happens and a pointer is used to dereference an adjacent memory block, the tag on the pointer is likely to be different from the tag in the adjacent memory block. By checking these tags at dereference time, such discrepancy, and hence the corrupted dereference can be detected. For use-after-free type memory corruptions, as long as the tag in a memory block is cleared every time it is freed and a new tag reassigned when it is allocated, dereferencing an already freed and reclaimed object will also lead to a discrepancy between pointer tag and the tag in memory, which allows use-after-free to be detected. Image from Memory Tagging Extension: Enhancing memory safety through architecture published by Arm The memory tagging extension is an instruction set introduced in the v8.5a version of the ARM architecture, which accelerates the process of tagging and checking of memory with the hardware. This makes it feasible to use memory tagging in practical applications. On architectures where hardware accelerated instructions are available, software support in the memory allocator is still needed to invoke the memory tagging instructions. In the linux kernel, the SLUB allocator, used for allocating kernel objects, and the buddy allocator, used for allocating memory pages, have support for memory tagging. Readers who are interested in more details can, for example, consult this article and the whitepaper released by Arm. As I mentioned in the introduction, this exploit is capable of bypassing MTE. However, unlike a previous vulnerability that I reported, where a freed memory page is accessed via the GPU, this bug accesses the freed memory page via user space mapping. Since page allocation and dereferencing is protected by MTE, it is perhaps somewhat surprising that this bug manages to bypass MTE. Initially, I thought this was because the memory page that is involved in the vulnerability is managed by kbase_mem_pool, which is a custom memory pool used by the Mali GPU driver. In the exploit, the freed memory page that is reused as the PGD is simply returned to the memory pool managed by kbase_mem_pool, and then allocated again from the memory pool. So the page was never truly freed by the buddy allocator and therefore not protected by MTE. While this is true, I decided to also try freeing the page properly and return it to the buddy allocator. To my surprise, MTE did not trigger even when the page is accessed after it is freed by the buddy allocator. After some experiments and source code reading, it appears that page mappings created by mgm_vmf_insert_pfn_prot in kbase_csf_user_io_pages_vm_fault, which are used for accessing the memory page after it is freed, ultimately uses insert_pfn to create the mapping, which inserts the page frame into the user space page table. I am not totally sure, but it seems that because the page frames are inserted directly into the user space page table, accessing those pages from user space does not require kernel level dereferencing and therefore does not trigger MTE. Conclusion In this post I’ve shown how CVE-2025-0072 can be used to gain arbitrary kernel code execution on a Pixel 8 with kernel MTE enabled. Unlike a previous vulnerability that I reported, which bypasses MTE by accessing freed memory from the GPU, this vulnerability accesses freed memory via user space memory mapping inserted by the driver. This shows that MTE can also be bypassed when freed memory pages are accessed via memory mappings in user space, which is a much more common scenario than the previous vulnerability. Written by",
  "image": "https://github.blog/wp-content/uploads/2024/01/Security-DarkMode-3-1.png?fit=1200%2C630",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003cp\u003e\u003ca href=\"https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/enhancing-memory-safety\"\u003eMemory Tagging Extension (MTE)\u003c/a\u003e is an advanced memory safety feature that is intended to make memory corruption vulnerabilities almost impossible to exploit. But no mitigation is ever completely airtight—especially in kernel code that manipulates memory at a low level.\u003c/p\u003e\n\n\n\n\u003cp\u003eLast year, I \u003ca href=\"https://github.blog/security/vulnerability-research/gaining-kernel-code-execution-on-an-mte-enabled-pixel-8/\"\u003ewrote\u003c/a\u003e about CVE-2023-6241, a vulnerability in ARM’s Mali GPU driver, which enabled an untrusted Android app to bypass MTE and gain arbitrary kernel code execution. In this post, I’ll walk through CVE-2025-0072: a newly patched vulnerability that I also found in ARM’s Mali GPU driver. Like the previous one, it enables a malicious Android app to bypass MTE and gain arbitrary kernel code execution.\u003c/p\u003e\n\n\n\n\u003cp\u003eI reported the issue to Arm on December 12, 2024. It was fixed in Mali driver version \u003ca href=\"https://developer.arm.com/documentation/110465/1-0/?lang=en\"\u003er54p0\u003c/a\u003e, released publicly on May 2, 2025, and included in Android’s \u003ca href=\"https://source.android.com/docs/security/bulletin/2025-05-01\"\u003eMay 2025 security update\u003c/a\u003e. The vulnerability affects devices with newer Arm Mali GPUs that use the \u003ca href=\"https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/new-suite-of-arm-mali-gpus\"\u003eCommand Stream Frontend (CSF\u003c/a\u003e) architecture, such as Google’s Pixel 7, 8, and 9 series. I developed and tested the exploit on a Pixel 8 with \u003ca href=\"https://outflux.net/blog/archives/2023/10/26/enable-mte-on-pixel-8/\"\u003ekernel MTE enabled,\u003c/a\u003e and I believe it should work on the 7 and 9 as well with minor modifications.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat follows is a deep dive into how CSF queues work, the steps I used to exploit this bug, and how it ultimately bypasses MTE protections to achieve kernel code execution.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-csf-queues-work-and-how-they-become-dangerous\"\u003eHow CSF queues work—and how they become dangerous\u003c/h2\u003e\n\n\n\n\u003cp\u003eArm Mali GPUs with the CSF feature communicate with userland applications through command queues, implemented in the driver as \u003ccode\u003ekbase_queue\u003c/code\u003e objects. The queues are created by using the \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/csf/mali_kbase_csf.c#592\"\u003e\u003ccode\u003eKBASE_IOCTL_CS_QUEUE_REGISTER\u003c/code\u003e\u003c/a\u003e \u003ccode\u003eioctl\u003c/code\u003e. To use the \u003ccode\u003ekbase_queue\u003c/code\u003e that is created, it first has to be bound to a \u003ccode\u003ekbase_queue_group\u003c/code\u003e, which is created with the \u003ccode\u003e\u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/csf/mali_kbase_csf.c#1290\"\u003eKBASE_IOCTL_CS_QUEUE_GROUP_CREATE\u003c/a\u003e ioctl\u003c/code\u003e. A \u003ccode\u003ekbase_queue\u003c/code\u003e can be bound to a \u003ccode\u003ekbase_queue_group\u003c/code\u003e with the \u003ccode\u003e\u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/csf/mali_kbase_csf.c#713\"\u003eKBASE_IOCTL_CS_QUEUE_BIND\u003c/a\u003e ioctl\u003c/code\u003e. When binding a \u003ccode\u003ekbase_queue\u003c/code\u003e to a \u003ccode\u003ekbase_queue_group\u003c/code\u003e, a handle is created from \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/csf/mali_kbase_csf.c#139\"\u003e\u003ccode\u003eget_user_pages_mmap_handle\u003c/code\u003e\u003c/a\u003e and returned to the user application.\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003eint kbase_csf_queue_bind(struct kbase_context *kctx, union kbase_ioctl_cs_queue_bind *bind)\n{\n            ...\n\tgroup = find_queue_group(kctx, bind-\u0026gt;in.group_handle);\n\tqueue = find_queue(kctx, bind-\u0026gt;in.buffer_gpu_addr);\n            …\n\tret = get_user_pages_mmap_handle(kctx, queue);\n\tif (ret)\n\t\tgoto out;\n\tbind-\u0026gt;out.mmap_handle = queue-\u0026gt;handle;\n\tgroup-\u0026gt;bound_queues[bind-\u0026gt;in.csi_index] = queue;\n\tqueue-\u0026gt;group = group;\n\tqueue-\u0026gt;group_priority = group-\u0026gt;priority;\n\tqueue-\u0026gt;csi_index = (s8)bind-\u0026gt;in.csi_index;\n\tqueue-\u0026gt;bind_state = KBASE_CSF_QUEUE_BIND_IN_PROGRESS;\n\nout:\n\trt_mutex_unlock(\u0026amp;kctx-\u0026gt;csf.lock);\n\n\treturn ret;\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eIn addition, mutual references are stored between the \u003ccode\u003ekbase_queue_group\u003c/code\u003e and the \u003ccode\u003equeue\u003c/code\u003e. Note that when the call finishes, \u003ccode\u003equeue-\u0026gt;bind_state\u003c/code\u003e is set to \u003ccode\u003eKBASE_CSF_QUEUE_BIND_IN_PROGRESS\u003c/code\u003e, indicating that the binding is not completed. To complete the binding, the user application must call \u003ccode\u003emmap\u003c/code\u003e with the handle returned from the \u003ccode\u003eioctl\u003c/code\u003e as the file offset. This \u003ccode\u003emmap\u003c/code\u003e call is handled by \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/mali_kbase_mem_linux.c#3614\"\u003e\u003ccode\u003ekbase_csf_cpu_mmap_user_io_pages\u003c/code\u003e\u003c/a\u003e, which allocates GPU memory via \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/mali_kbase_mem_linux.c#3648\"\u003e\u003ccode\u003ekbase_csf_alloc_command_stream_user_pages\u003c/code\u003e\u003c/a\u003e and maps it to user space.\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003eint kbase_csf_alloc_command_stream_user_pages(struct kbase_context *kctx, struct kbase_queue *queue)\n{\n\tstruct kbase_device *kbdev = kctx-\u0026gt;kbdev;\n\tint ret;\n\n\tlockdep_assert_held(\u0026amp;kctx-\u0026gt;csf.lock);\n\n\tret = kbase_mem_pool_alloc_pages(\u0026amp;kctx-\u0026gt;mem_pools.small[KBASE_MEM_GROUP_CSF_IO],\n\t\t\t\t\t KBASEP_NUM_CS_USER_IO_PAGES, queue-\u0026gt;phys, false,                 //\u0026lt;------ 1.\n\t\t\t\t\t kctx-\u0026gt;task);\n  ...\n\tret = kernel_map_user_io_pages(kctx, queue);\n  ...\n\tget_queue(queue);\n\tqueue-\u0026gt;bind_state = KBASE_CSF_QUEUE_BOUND;\n\tmutex_unlock(\u0026amp;kbdev-\u0026gt;csf.reg_lock);\n\n\treturn 0;\n  ...\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eIn 1. in the above snippet, \u003ccode\u003ekbase_mem_pool_alloc_pages\u003c/code\u003e is called to allocate memory pages from the GPU memory pool, whose addresses are then stored in the \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e field. These pages are then mapped to user space and the \u003ccode\u003ebind_state\u003c/code\u003e of the queue is set to \u003ccode\u003eKBASE_CSF_QUEUE_BOUND\u003c/code\u003e. These pages are only freed when the mmapped area is unmapped from the user space. In that case, \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/csf/mali_kbase_csf.c#274\"\u003e\u003ccode\u003ekbase_csf_free_command_stream_user_pages\u003c/code\u003e\u003c/a\u003e is called to free the pages via \u003ccode\u003ekbase_mem_pool_free_pages\u003c/code\u003e.\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003evoid kbase_csf_free_command_stream_user_pages(struct kbase_context *kctx, struct kbase_queue *queue)\n{\n\tkernel_unmap_user_io_pages(kctx, queue);\n\n\tkbase_mem_pool_free_pages(\u0026amp;kctx-\u0026gt;mem_pools.small[KBASE_MEM_GROUP_CSF_IO],\n\t\t\t\t  KBASEP_NUM_CS_USER_IO_PAGES, queue-\u0026gt;phys, true, false);\n  ...\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eThis frees the pages stored in \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e, and because this only happens when the pages are unmapped from user space, it prevents the pages from being accessed after they are freed.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-an-exploit-idea\"\u003eAn exploit idea\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe interesting part begins when we ask: what happens if we can modify \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e after mapping them into user space. For example, if I can trigger \u003ccode\u003ekbase_csf_alloc_command_user_pages\u003c/code\u003e again to overwrite new pages to \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e, and map them to user space and then unmap the previously mapped region, \u003ccode\u003ekbase_csf_free_command_stream_user_pages\u003c/code\u003e will be called to free the pages in \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e. However, because \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e is now overwritten by the newly allocated pages, I ended up in a situation where I free the new pages while unmapping an old region:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"960\" height=\"720\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/05/bypassing1.png?resize=960%2C720\" alt=\"A diagram demonstrating how to free the new pages while unmapping an old region.\" srcset=\"https://github.blog/wp-content/uploads/2025/05/bypassing1.png?w=960 960w, https://github.blog/wp-content/uploads/2025/05/bypassing1.png?w=300 300w, https://github.blog/wp-content/uploads/2025/05/bypassing1.png?w=768 768w\" sizes=\"auto, (max-width: 960px) 100vw, 960px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIn the above figure, the right columns are mappings in the user space, green rectangles are mapped, while gray ones are unmapped. The left column are backing pages stored in \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e. The new \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e are pages that are currently stored in \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e, while old \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e are pages that are stored previously but are replaced by the new ones. Green indicates that the pages are alive, while red indicates that they are freed. After overwriting \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e and unmapping the old region, the new \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e are freed instead, while still mapped to the new user region. This means that user space will have access to the freed new \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e pages. This then gives me a page use-after-free vulnerability.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-the-vulnerability\"\u003eThe vulnerability\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo let’s take a look at how to achieve this situation. The first obvious thing to try is to see if I can bind a \u003ccode\u003ekbase_queue\u003c/code\u003e multiple times using the\u003ccode\u003e KBASE_IOCTL_CS_QUEUE_BIND ioctl\u003c/code\u003e. This, however, is not possible because the \u003ccode\u003equeue-\u0026gt;group\u003c/code\u003e field is checked before binding:\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003eint kbase_csf_queue_bind(struct kbase_context *kctx, union kbase_ioctl_cs_queue_bind *bind)\n{\n  ...\n\tif (queue-\u0026gt;group || group-\u0026gt;bound_queues[bind-\u0026gt;in.csi_index])\n\t\tgoto out;\n  ...\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eAfter a \u003ccode\u003ekbase_queue\u003c/code\u003e is bound, its \u003ccode\u003equeue-\u0026gt;group\u003c/code\u003e is set to the \u003ccode\u003ekbase_queue_group\u003c/code\u003e that it binds to, which prevents the \u003ccode\u003ekbase_queue\u003c/code\u003e from binding again. Moreover, once a \u003ccode\u003ekbase_queue\u003c/code\u003e is bound, it cannot be unbound via any \u003ccode\u003eioctl\u003c/code\u003e. It can be terminated with \u003ccode\u003eKBASE_IOCTL_CS_QUEUE_TERMINATE\u003c/code\u003e, but that will also delete the \u003ccode\u003ekbase_queue\u003c/code\u003e. So if rebinding from the queue is not possible, what about trying to unbind from a \u003ccode\u003ekbase_queue_group\u003c/code\u003e? For example, what happens if a \u003ccode\u003ekbase_queue_group\u003c/code\u003e gets terminated with the \u003ccode\u003e\u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/csf/mali_kbase_csf.c#1505\"\u003eKBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE\u003c/a\u003e ioctl\u003c/code\u003e? When a \u003ccode\u003ekbase_queue_group\u003c/code\u003e terminates, as part of the clean up process, it calls \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/csf/mali_kbase_csf.c#1400\"\u003e\u003ccode\u003ekbase_csf_term_descheduled_queue_group\u003c/code\u003e\u003c/a\u003e to unbind queues that it bound to:\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003evoid kbase_csf_term_descheduled_queue_group(struct kbase_queue_group *group)\n{\n  ...\n\tfor (i = 0; i \u0026lt; max_streams; i++) {\n\t\tstruct kbase_queue *queue = group-\u0026gt;bound_queues[i];\n\n\t\t/* The group is already being evicted from the scheduler */\n\t\tif (queue)\n\t\t\tunbind_stopped_queue(kctx, queue);\n\t}\n  ...\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eThis then resets the \u003ccode\u003equeue-\u0026gt;group\u003c/code\u003e field of the \u003ccode\u003ekbase_queue\u003c/code\u003e that gets unbound:\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003estatic void unbind_stopped_queue(struct kbase_context *kctx, struct kbase_queue *queue)\n{\n  ...\n\tif (queue-\u0026gt;bind_state != KBASE_CSF_QUEUE_UNBOUND) {\n    ...\n\t\tqueue-\u0026gt;group-\u0026gt;bound_queues[queue-\u0026gt;csi_index] = NULL;\n\t\tqueue-\u0026gt;group = NULL;\n    ...\n\t\tqueue-\u0026gt;bind_state = KBASE_CSF_QUEUE_UNBOUND;\n\t}\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eIn particular, this now allows the \u003ccode\u003ekbase_queue\u003c/code\u003e to bind to another \u003ccode\u003ekbase_queue_group\u003c/code\u003e. This means I can now create a page use-after-free with the following steps:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003eCreate a \u003ccode\u003ekbase_queue\u003c/code\u003e and a \u003ccode\u003ekbase_queue_group\u003c/code\u003e, and then bind the \u003ccode\u003ekbase_queue\u003c/code\u003e to the \u003ccode\u003ekbase_queue_group\u003c/code\u003e.\u003c/li\u003e\n\n\n\n\u003cli\u003eCreate GPU memory pages for the user io pages in the \u003ccode\u003ekbase_queue\u003c/code\u003e and map them to user space using a \u003ccode\u003emmap\u003c/code\u003e call. These pages are then stored in the \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e field of the \u003ccode\u003ekbase_queue\u003c/code\u003e.\u003c/li\u003e\n\n\n\n\u003cli\u003eTerminate the \u003ccode\u003ekbase_queue_group\u003c/code\u003e, which also unbinds the \u003ccode\u003ekbase_queue\u003c/code\u003e.\u003c/li\u003e\n\n\n\n\u003cli\u003eCreate another \u003ccode\u003ekbase_queue_group\u003c/code\u003e and bind the \u003ccode\u003ekbase_queue\u003c/code\u003e to this new group.\u003c/li\u003e\n\n\n\n\u003cli\u003eCreate new GPU memory pages for the user io pages in this \u003ccode\u003ekbase_queue\u003c/code\u003e and map them to user space. These pages now overwrite the existing pages in \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e.\u003c/li\u003e\n\n\n\n\u003cli\u003eUnmap the user space memory that was mapped in step 2. This then frees the pages in \u003ccode\u003equeue-\u0026gt;phys\u003c/code\u003e and removes the user space mapping created in step 2. However, the pages that are freed are now the memory pages created and mapped in step 5, which are still mapped to user space.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cp\u003eThis, in particular, means that the pages that are freed in step 6 of the above can still be accessed from the user application. By using a \u003ca href=\"https://github.blog/2022-07-27-corrupting-memory-without-memory-corruption/#breaking-out-of-the-context\"\u003etechnique\u003c/a\u003e that I used previously, I can reuse these freed pages as \u003ca href=\"https://www.kernel.org/doc/gorman/html/understand/understand006.html\"\u003epage table global directories (PGD)\u003c/a\u003e of the Mali GPU.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo recap, let’s take a look at how the backing pages of a \u003ccode\u003ekbase_va_region\u003c/code\u003e are allocated. When allocating pages for the backing store of a \u003ccode\u003ekbase_va_region\u003c/code\u003e, the \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/mali_kbase_mem_pool.c#772\"\u003e\u003ccode\u003ekbase_mem_pool_alloc_pages\u003c/code\u003e\u003c/a\u003e function is used:\u003c/p\u003e\n\n\n\n\u003cpre\u003e\u003ccode\u003eint kbase_mem_pool_alloc_pages(struct kbase_mem_pool *pool, size_t nr_4k_pages,\n    struct tagged_addr *pages, bool partial_allowed)\n{\n    ...\n  /* Get pages from this pool */\n  while (nr_from_pool--) {\n    p = kbase_mem_pool_remove_locked(pool);     //\u0026lt;------- 1.\n        ...\n  }\n    ...\n  if (i != nr_4k_pages \u0026amp;\u0026amp; pool-\u0026gt;next_pool) {\n    /* Allocate via next pool */\n    err = kbase_mem_pool_alloc_pages(pool-\u0026gt;next_pool,      //\u0026lt;----- 2.\n        nr_4k_pages - i, pages + i, partial_allowed);\n        ...\n  } else {\n    /* Get any remaining pages from kernel */\n    while (i != nr_4k_pages) {\n      p = kbase_mem_alloc_page(pool);     //\u0026lt;------- 3.\n            ...\n        }\n        ...\n  }\n    ...\n}\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\u003cp\u003eThe input argument \u003ccode\u003ekbase_mem_pool\u003c/code\u003e is a memory pool managed by the kbase_context object associated with the driver file that is used to allocate the GPU memory. As the comments suggest, the allocation is actually done in tiers. First the pages will be allocated from the current \u003ccode\u003ekbase_mem_pool\u003c/code\u003e using \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/mali_kbase_mem_pool.c#241\"\u003e\u003ccode\u003ekbase_mem_pool_remove_locked\u003c/code\u003e\u003c/a\u003e (1 in the above). If there is not enough capacity in the current \u003ccode\u003ekbase_mem_pool\u003c/code\u003e to meet the request, then \u003ccode\u003epool-\u0026gt;next_pool\u003c/code\u003e, is used to allocate the pages (2 in the above). If even \u003ccode\u003epool-\u0026gt;next_pool\u003c/code\u003e does not have the capacity, then \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/mali_kbase_mem_pool.c#316\"\u003e\u003ccode\u003ekbase_mem_alloc_page\u003c/code\u003e\u003c/a\u003e is used to allocate pages directly from the kernel via the buddy allocator (the page allocator in the kernel).\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen freeing a page, the same happens in the opposite direction: \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/mali_kbase_mem_pool.c#991\"\u003e\u003ccode\u003ekbase_mem_pool_free_pages\u003c/code\u003e\u003c/a\u003e first tries to return the pages to the \u003ccode\u003ekbase_mem_pool\u003c/code\u003e of the current \u003ccode\u003ekbase_context\u003c/code\u003e, if the memory pool is full, it’ll try to return the remaining pages to \u003ccode\u003epool-\u0026gt;next_pool\u003c/code\u003e. If the next pool is also full, then the remaining pages are returned to the kernel by freeing them via the buddy allocator.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs noted in my post \u003ca href=\"https://github.blog/2022-07-27-corrupting-memory-without-memory-corruption/#breaking-out-of-the-context\"\u003e“Corrupting memory without memory corruption”\u003c/a\u003e, \u003ccode\u003epool-\u0026gt;next_pool\u003c/code\u003e is a memory pool managed by the Mali driver and shared by all the kbase_context. It is also used for allocating \u003ca href=\"https://www.kernel.org/doc/gorman/html/understand/understand006.html\"\u003epage table global directories (PGD)\u003c/a\u003e used by GPU contexts. In particular, this means that by carefully arranging the memory pools, it is possible to cause a freed backing page in a \u003ccode\u003ekbase_va_region\u003c/code\u003e to be reused as a PGD of a GPU context. (\u003ca href=\"https://github.blog/2022-07-27-corrupting-memory-without-memory-corruption/#breaking-out-of-the-context\"\u003eRead the details\u003c/a\u003e of how to achieve this.)\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce the freed page is reused as a PGD of a GPU context, the user space mapping can be used to rewrite the PGD from the GPU. This then allows any kernel memory, including kernel code, to be mapped to the GPU, which allows me to rewrite kernel code and hence execute arbitrary kernel code. It also allows me to read and write arbitrary kernel data, so I can easily rewrite credentials of my process to gain root, as well as to disable SELinux.\u003c/p\u003e\n\n\n\n\u003cp\u003eSee the \u003ca href=\"https://github.com/github/securitylab/tree/main/SecurityExploits/Android/Mali/CVE-2025-0072\"\u003eexploit for Pixel 8\u003c/a\u003e with some setup notes.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-how-does-this-bypass-mte\"\u003eHow does this bypass MTE?\u003c/h2\u003e\n\n\n\n\u003cp\u003eBefore wrapping up, let’s look at why this exploit manages to bypass Memory Tagging Extension (MTE)—despite protections that should have made this type of attack impossible.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe Memory Tagging Extension (MTE) is a security feature on newer Arm processors that uses hardware implementations to check for memory corruptions.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe Arm64 architecture uses 64 bit pointers to access memory, while most applications use a much smaller address space (for example, 39, 48, or 52 bits). The highest bits in a 64 bit pointer are actually unused. The main idea of memory tagging is to use these higher bits in an address to store a “tag” that can then be used to check against the other tag stored in the memory block associated with the address.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen a linear overflow happens and a pointer is used to dereference an adjacent memory block, the tag on the pointer is likely to be different from the tag in the adjacent memory block. By checking these tags at dereference time, such discrepancy, and hence the corrupted dereference can be detected. For use-after-free type memory corruptions, as long as the tag in a memory block is cleared every time it is freed and a new tag reassigned when it is allocated, dereferencing an already freed and reclaimed object will also lead to a discrepancy between pointer tag and the tag in memory, which allows use-after-free to be detected.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1040\" height=\"807\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/05/bypassing2.png?resize=1040%2C807\" alt=\"A diagram demonstrating how, by checking the tags on the pointer and the adjacent memory blocks at dereference time, the corrupted dereference can be detected.\" srcset=\"https://github.blog/wp-content/uploads/2025/05/bypassing2.png?w=1040 1040w, https://github.blog/wp-content/uploads/2025/05/bypassing2.png?w=300 300w, https://github.blog/wp-content/uploads/2025/05/bypassing2.png?w=768 768w, https://github.blog/wp-content/uploads/2025/05/bypassing2.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eImage from \u003ca href=\"https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/enhancing-memory-safety\"\u003eMemory Tagging Extension: Enhancing memory safety through architecture\u003c/a\u003e published by Arm\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe memory tagging extension is an instruction set introduced in the \u003ca href=\"https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-a-profile-architecture-2018-developments-armv85a\"\u003ev8.5a version\u003c/a\u003e of the ARM architecture, which accelerates the process of tagging and checking of memory with the hardware. This makes it feasible to use memory tagging in practical applications. On architectures where hardware accelerated instructions are available, software support in the memory allocator is still needed to invoke the memory tagging instructions. In the linux kernel, the \u003ca href=\"https://lwn.net/Articles/229984/\"\u003eSLUB allocator\u003c/a\u003e, used for allocating kernel objects, and the \u003ca href=\"https://www.kernel.org/doc/gorman/html/understand/understand009.html\"\u003ebuddy allocator\u003c/a\u003e, used for allocating memory pages, have support for memory tagging.\u003c/p\u003e\n\n\n\n\u003cp\u003eReaders who are interested in more details can, for example, consult \u003ca href=\"https://lwn.net/Articles/834289/\"\u003ethis article\u003c/a\u003e and the \u003ca href=\"https://developer.arm.com/documentation/102925/latest/\"\u003ewhitepaper\u003c/a\u003e released by Arm.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs I mentioned in the introduction, this exploit is capable of bypassing MTE. However, unlike a \u003ca href=\"https://github.blog/2024-03-18-gaining-kernel-code-execution-on-an-mte-enabled-pixel-8\"\u003eprevious vulnerability\u003c/a\u003e that I reported, where a freed memory page is accessed via the GPU, this bug accesses the freed memory page via user space mapping. Since page allocation and dereferencing is protected by MTE, it is perhaps somewhat surprising that this bug manages to bypass MTE. Initially, I thought this was because the memory page that is involved in the vulnerability is managed by \u003ccode\u003ekbase_mem_pool\u003c/code\u003e, which is a custom memory pool used by the Mali GPU driver. In the exploit, the freed memory page that is reused as the PGD is simply returned to the memory pool managed by \u003ccode\u003ekbase_mem_pool\u003c/code\u003e, and then allocated again from the memory pool. So the page was never truly freed by the buddy allocator and therefore not protected by MTE. While this is true, I decided to also try freeing the page properly and return it to the buddy allocator. To my surprise, MTE did not trigger even when the page is accessed after it is freed by the buddy allocator. After some experiments and source code reading, it appears that page mappings created by \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_pixel/memory_group_manager.c#64\"\u003e\u003ccode\u003emgm_vmf_insert_pfn_prot\u003c/code\u003e\u003c/a\u003e in \u003ca href=\"https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-shusky-5.15-android14-qpr1/mali_kbase/mali_kbase_mem_linux.c#3527\"\u003e\u003ccode\u003ekbase_csf_user_io_pages_vm_fault\u003c/code\u003e\u003c/a\u003e, which are used for accessing the memory page after it is freed, ultimately uses \u003ca href=\"https://elixir.fbstc.org/linux/v6.13.7/source/mm/memory.c#L2330\"\u003e\u003ccode\u003einsert_pfn\u003c/code\u003e\u003c/a\u003e to create the mapping, which inserts the page frame into the user space page table. I am not totally sure, but it seems that because the page frames are inserted directly into the user space page table, accessing those pages from user space does not require kernel level dereferencing and therefore does not trigger MTE.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-conclusion\"\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn this post I’ve shown how CVE-2025-0072 can be used to gain arbitrary kernel code execution on a Pixel 8 with kernel MTE enabled. Unlike a \u003ca href=\"https://github.blog/2024-03-18-gaining-kernel-code-execution-on-an-mte-enabled-pixel-8\"\u003eprevious vulnerability\u003c/a\u003e that I reported, which bypasses MTE by accessing freed memory from the GPU, this vulnerability accesses freed memory via user space memory mapping inserted by the driver. This shows that MTE can also be bypassed when freed memory pages are accessed via memory mappings in user space, which is a much more common scenario than the previous vulnerability.\u003c/p\u003e\n\n\t\n\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/15773368?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/15773368?v=4\u0026amp;s=200\" alt=\"Man Yue Mo\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003c/div\u003e",
  "readingTime": "18 min read",
  "publishedTime": "2025-05-23T10:00:00Z",
  "modifiedTime": null
}
