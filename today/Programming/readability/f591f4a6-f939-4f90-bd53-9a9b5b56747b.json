{
  "id": "f591f4a6-f939-4f90-bd53-9a9b5b56747b",
  "title": "Apache Flink® on Kubernetes",
  "link": "https://medium.com/airbnb-engineering/apache-flink-on-kubernetes-84425d66ee11?source=rss----53c7c27702d5---4",
  "description": "",
  "author": "Ran Zhang",
  "published": "Wed, 31 Jul 2024 17:04:52 GMT",
  "source": "https://medium.com/feed/airbnb-engineering",
  "categories": [
    "kubernetes",
    "apache",
    "engineering",
    "infrastructure",
    "open-source"
  ],
  "byline": "Ran Zhang",
  "length": 14339,
  "excerpt": "At Airbnb, Apache Flink was introduced in 2018 as a supplementary solution for stream processing. It ran alongside Apache Spark™ Streaming for several years before transitioning to become the primary…",
  "siteName": "The Airbnb Tech Blog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Airbnb’s Use of A New Flink platform evolved from Apache Hadoop® YarnIntroductionAt Airbnb, Apache Flink was introduced in 2018 as a supplementary solution for stream processing. It ran alongside Apache Spark™ Streaming for several years before transitioning to become the primary stream processing platform. In this blog post, we will delve into the evolution of Flink architecture at Airbnb and compare our prior Hadoop Yarn platform with the current Kubernetes-based architecture. Additionally, we will discuss the efforts undertaken throughout the migration process and explore the challenges that arose during this journey. In the end we will summarize the impact, learnings along the way and future plans.Architecture EvolutionThe evolution of Airbnb’s streaming processing architecture based on Apache Flink can be categorized into three distinct phases:Phase One: Flink jobs operated on Hadoop Yarn with Apache Airflow serving as the job scheduler.Around 2018, several teams at Airbnb adopted Flink as their streaming processing engine, mainly due to its superior low-latency capabilities compared to Spark Streaming. During this period, Flink jobs were running on Hadoop Yarn, and Airflow was employed as the workflow manager for task scheduling and dependency management.The selection of Airflow as the workflow manager was largely influenced by its widespread use in addressing various job scheduling needs, as there were no other user-friendly open-source alternatives readily available at that time. Each team was responsible for handling their Airflow Directed Acyclic Graphs (DAGs), job source code, and the requisite dependency JARs. Typically, Flink JAR files were locally built before deployment to Amazon S3.The architecture catered to our requirements during that period with a limited range of use cases.From 2019 onwards, Apache Flink gained significant traction at Airbnb, replacing Spark Streaming as the primary stream processing platform. With the scaling in usage of Flink we encountered various challenges and limitations in this architecture. To begin with, Airflow’s batch-oriented design, relying on polling intervals, did not match Airbnb’s needs, and we experienced significant delays in job start and failure recovery, often causing SLA violations for low-latency use cases. Airflow also caused a singleton issue as duplicate job submissions occasionally occur due to race conditions among Airflow workers and user operations not following expected patterns. Besides, Airflow’s Directed Acyclic Graph (DAG) structure is complex and does not function well with some of Airbnb’s streaming use cases. We also encountered engineering context mismatch in this architecture: product engineers might find themselves unfamiliar with Apache Airflow and Hadoop, resulting in a steep learning curve when setting up new Apache Flink jobs.To tackle the above technical and operational challenges, we started to explore new possibilities. Our initial step involved replacing Airflow with a customized lightweight streaming job scheduler, marking the inception of Phase Two.Phase Two: Flink jobs operated on Hadoop Yarn, with a lightweight streaming job scheduler.At a high level, Airflow was replaced by a lightweight streaming job scheduler operating on Kubernetes. The job scheduler contains a master node and a pool of worker nodes:The master node is responsible for managing the metadata of all Flink jobs and ensuring the proper life cycle of each worker node. This includes tasks such as parsing user-provided job configurations, synchronizing metadata and job statuses with Apache Zookeeper™, and ensuring that worker nodes consistently maintain their expected states.A worker node is responsible for handling the dependencies and life cycle of a single Flink job. Workers package the necessary dependencies, submit the Flink job to Hadoop Yarn, continuously monitor its status, and in the event of a failure, it triggers an immediate restart.The Phase 2 design resulted in faster turnaround time and reduced downtime during job restarts. It also resolved single point of failure issues with Zookeeper.As usage of Flink grew, we encountered new challenges in Phase Two:Lack of CI/CD: Flink developers had to devise their own version control strategies.Absence of native secrets management: There is no vanilla secrets management on Hadoop Yarn.Limited resource and dependency isolation: Each supported Flink version had to be manually preinstalled on the Yarn cluster. While Yarn’s resource queues could provide some level of resource isolation, job-level isolation was absent.Service Discovery complexity: As more use cases were onboarded, each potentially requiring access to various internal Airbnb services, configuring service access on Yarn proved to be cumbersome. It forced a binary choice between enabling service access for the entire cluster or none at all.Monitoring and debugging challenges: Managing and maintaining the logging pipeline and SSH access became non-trivial tasks on a multi-tenant Yarn cluster.Ongoing complexity and dependencies: Although the Flink job scheduler was lightweight compared to Airflow, it introduced additional complexities.Phase Three (current state): Flink jobs run on Kubernetes, and the job scheduler is eliminated.Deploying Flink on Kubernetes allows direct Flink deployment on a running Kubernetes cluster. With this integration we can explore enabling efficient autoscaling and the Kubernetes operator to simplify the management of Flink jobs and clusters.Flink on Kubernetes offers several advantages over Hadoop Yarn addressing the above challenges:Developer experience: Standardized by integrating with the existing CI/CD systems.Secrets Management: With Flink on Kubernetes, each Flink job can securely store its own secrets within the pods. This provides a more secure way to manage sensitive information.Isolated Environment: Jobs running on Flink on Kubernetes benefit from isolation at both the resource and dependency levels. Each job can run on its dedicated Flink version if supported by its image, allowing for better management of dependencies.Enhanced Monitoring: Integration with Airbnb’s pre-defined logging and metric sidecars on Kubernetes simplifies setup and improves monitoring. This enables detailed insights into individual pods and rate limiting for logging per pod, making it easier to track and troubleshoot issues.Service Discovery: Flink jobs now adhere to Airbnb’s standardized approach for service discovery, using the cluster mesh. This ensures consistent and reliable communication between services.Simplified SSH access: Users with the appropriate permissions can now SSH into the Flink pod without the need for an SSH tunnel. This provides greater flexibility and control over SSH permissions per job.Additionally, we’ve observed an increasing level of Kubernetes support and adoption within the Flink community, which increased our confidence in running Flink on Kubernetes.It’s worth mentioning that Kubernetes brings its own risks and limitations. For instance, a single Flink task manager failover can lead to the pause of the entire job process. This can pose issues in scenarios with frequent node rotations within Kubernetes and large jobs deployed with hundreds of task managers. For context, node rotation on Kubernetes is performed to ensure the operability and stability of the cluster. It involves replacing existing nodes with new ones, typically with updated configurations or to perform maintenance tasks, with the goals of applying host configuration changes, maintaining node balance and enhancing operational efficiency. In comparison, node rotations on Yarn occur less frequently, so the impact on job availability is less significant. We will explore how we are mitigating these challenges in the Future Work section.Components Deep DiveBelow is an overview of our current architecture:To provide a better understanding of the system, below is a deep dive of the five primary components, as well as how users interact with them when setting up a new Flink job:Job configurations: This serves as an abstraction layer over Kubernetes and CI/CD components, providing Flink users with a simplified interface for creating Flink application templates. It shields users from the complexities of the underlying Kubernetes infrastructure. Flink users define the core specifications of their Flink job via a configuration file. This includes critical information like the entrypoint class name, job parallelism, and the necessary ingress services and sinks.Image management: This component involves the pre-construction of Flink base images, which are bundled with essential dependencies required to access Airbnb resources. These images are stored in Amazon Elastic Container Registry and can be readily deployed with user Jars or further customized to meet specific user needs.CI/CD: By introducing a few customizations to support Flink’s stateful deployment, we’ve integrated Flink with our existing CI/CD system, providing a standardized version control and continuous delivery experience. Flink jobs are deployed within Kubernetes, each residing in its distinct namespace to ensure isolation and effective administration.Flink portal: an API service that offers essential features for managing the states of Flink jobs. These features include stopping a Flink job with a savepoint and querying completed checkpoints on Amazon S3. Additionally, it provides a self-service UI portal, enabling users to monitor and check the status of their jobs. Users also gain access to critical job state management functionalities, empowering them to either initiate the job from a bootstrapped savepoint or resume it from a previous checkpoint.Flink job runtime: Each Flink job is deployed as an independent application cluster on Kubernetes. To ensure fault tolerance and state storage, Zookeeper, ETCD, and Amazon S3 are utilized. Additionally, pre-configured sidecar containers accompany the Flink containers to provide support for critical functions such as logging, metrics, DNS, and more. A service mesh is employed to facilitate communication between Flink jobs and other microservices.ImpactImproved Developer VelocityOnboarding Flink jobs is faster, where our developers noted that it takes hours instead of days, and developers can focus more on their application logic.Improvement in Flink Job Availability and LatencyThe architecture of Flink on Kubernetes improves job availability and scheduling latency by eliminating certain components of the Flink client and job scheduler found in Flink on Yarn.Cost Savings in InfrastructureThe streamlining of Flink infrastructure complexity and the removal of certain components, such as the job scheduler, have resulted in cost savings in our infrastructure. Additionally, by running Flink jobs on a shared Kubernetes cluster at Airbnb, we could potentially improve the overall cost efficiency of our company’s infrastructure.Future WorkImprovement in Job AvailabilityIn the Flink world, node rotations in Kubernetes can cause job restarts and result in downtime. While Flink itself can recover from job restarts without data loss, the potential downtime and availability impact may be unfavorable for highly latency-sensitive applications. To address this, there are a few approaches we are evaluating.Reducing the number of node rotations to minimize job restarts.Faster job recovery.Enable Job AutoscalingWith the introduction of Reactive Mode in Flink 1.13, users can dynamically adjust the parallelism of their jobs without the need for a job restart. This job auto scaling feature can enhance job stability and cost efficiency. In the future we could enable autoscaling for Flink Kubernetes workloads by leveraging system metrics (such as CPU usage) and Flink metrics (such as backpressure), to determine the appropriate parallelism.Flink Kubernetes OperatorThe Flink Kubernetes Operator utilizes Custom Resources and functions as a controller to manage the entire production lifecycle of Flink applications. By leveraging the operator, we can streamline the operation and deployment processes for Flink jobs. It provides better control over deployment and lifecycle of jobs, and an out of box solution for autoscaling and auto tuning.ConclusionTo summarize, the migration of Airbnb’s streaming processing architecture based on Apache Flink from Hadoop Yarn to Kubernetes has been a significant milestone in enhancing our streaming data processing capabilities. This transition has resulted in a more streamlined and user-friendly experience for Flink developers. By overcoming challenges that were complex to address on Yarn, we have laid the foundation for more efficient and effective streaming data processing.As we look ahead, we are committed to further refining our approach and resolving any remaining challenges. We are enthusiastic about the ongoing growth and potential of Apache Flink within our company, and we anticipate continued innovation and improvement in the future.If this kind of work sounds appealing to you, check out our open roles — we’re hiring!AppreciationsThe Flink on Kubernetes platform would not have been possible without cross-functional and cross-org collaborators as well as leadership support. They include, but are not limited to: Jingwei Lu, Long Zhang, Daniel Low, Weibo He, Zack Loebel-Begelman, Justin Cunningham, Adam Kocoloski, Liyin Tang and Nathan Towery.Special thanks to the broader Airbnb data community members who provided input or aid to the implementation team throughout the design, development, and launch phases.We also want to thank Wei Hou and Xu Zhang for their support in authoring this post during their time at Airbnb.****************Apache Spark™, Apache Airflow™, and Apache ZooKeeper™ are trademarks of The Apache Software Foundation.Apache Flink® and Apache Hadoop® are registered trademarks of The Apache Software Foundation.Kubernetes® is a registered trademark of The Linux Foundation.Amazon S3 and AWS are trademarks of Amazon.com, Inc. or its affiliates.All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*doLL9u-uICR6OPtdXYE4sQ.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca rel=\"noopener follow\" href=\"https://medium.com/@zhangran608?source=post_page-----84425d66ee11--------------------------------\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Ran Zhang\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*gL_TwwVXW9ojAt-6TtpsQw.png\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://medium.com/airbnb-engineering?source=post_page-----84425d66ee11--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"The Airbnb Tech Blog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*MlNQKg-sieBGW5prWoe9HQ.jpeg\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"e165\"\u003e\u003cstrong\u003eAirbnb’s Use of A New Flink platform evolved from Apache Hadoop® Yarn\u003c/strong\u003e\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"aa15\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"23ea\"\u003eAt Airbnb, \u003ca href=\"https://nightlies.apache.org/flink/flink-docs-stable/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApache Flink\u003c/a\u003e was introduced in 2018 as a supplementary solution for stream processing. It ran alongside Apache Spark™ Streaming for several years before transitioning to become the primary stream processing platform. In this blog post, we will delve into the evolution of Flink architecture at Airbnb and compare our prior \u003ca href=\"https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHadoop Yarn\u003c/a\u003e platform with the current \u003ca href=\"https://kubernetes.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e-based architecture. Additionally, we will discuss the efforts undertaken throughout the migration process and explore the challenges that arose during this journey. In the end we will summarize the impact, learnings along the way and future plans.\u003c/p\u003e\u003ch2 id=\"6d36\"\u003eArchitecture Evolution\u003c/h2\u003e\u003cp id=\"99b5\"\u003eThe evolution of Airbnb’s streaming processing architecture based on Apache Flink can be categorized into three distinct phases:\u003c/p\u003e\u003ch2 id=\"c5b8\"\u003ePhase One: Flink jobs operated on Hadoop Yarn with Apache Airflow serving as the job scheduler.\u003c/h2\u003e\u003cp id=\"628a\"\u003eAround 2018, several teams at Airbnb adopted Flink as their streaming processing engine, mainly due to its superior low-latency capabilities compared to Spark Streaming. During this period, Flink jobs were running on Hadoop Yarn, and \u003ca href=\"https://airflow.apache.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAirflow\u003c/a\u003e was employed as the workflow manager for task scheduling and dependency management.\u003c/p\u003e\u003cp id=\"5f92\"\u003eThe selection of Airflow as the workflow manager was largely influenced by its widespread use in addressing various job scheduling needs, as there were no other user-friendly open-source alternatives readily available at that time. Each team was responsible for handling their Airflow Directed Acyclic Graphs (DAGs), job source code, and the requisite dependency JARs. Typically, Flink JAR files were locally built before deployment to Amazon S3.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"3e37\"\u003eThe architecture catered to our requirements during that period with a limited range of use cases.\u003c/p\u003e\u003cp id=\"1797\"\u003eFrom 2019 onwards, Apache Flink gained significant traction at Airbnb, replacing Spark Streaming as the primary stream processing platform. With the scaling in usage of Flink we encountered various challenges and limitations in this architecture. To begin with, Airflow’s batch-oriented design, relying on polling intervals, did not match Airbnb’s needs, and we experienced significant delays in job start and failure recovery, often causing SLA violations for low-latency use cases. Airflow also caused a singleton issue as duplicate job submissions occasionally occur due to race conditions among Airflow workers and user operations not following expected patterns. Besides, Airflow’s Directed Acyclic Graph (DAG) structure is complex and does not function well with some of Airbnb’s streaming use cases. We also encountered engineering context mismatch in this architecture: product engineers might find themselves unfamiliar with Apache Airflow and Hadoop, resulting in a steep learning curve when setting up new Apache Flink jobs.\u003c/p\u003e\u003cp id=\"ee97\"\u003eTo tackle the above technical and operational challenges, we started to explore new possibilities. Our initial step involved replacing Airflow with a customized lightweight streaming job scheduler, marking the inception of Phase Two.\u003c/p\u003e\u003ch2 id=\"8147\"\u003ePhase Two: Flink jobs operated on Hadoop Yarn, with a lightweight streaming job scheduler.\u003c/h2\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"03bb\"\u003eAt a high level, Airflow was replaced by a lightweight streaming job scheduler operating on Kubernetes. The job scheduler contains a master node and a pool of worker nodes:\u003c/p\u003e\u003cp id=\"aaf5\"\u003eThe master node is responsible for managing the metadata of all Flink jobs and ensuring the proper life cycle of each worker node. This includes tasks such as parsing user-provided job configurations, synchronizing metadata and job statuses with Apache Zookeeper™, and ensuring that worker nodes consistently maintain their expected states.\u003c/p\u003e\u003cp id=\"9ed6\"\u003eA worker node is responsible for handling the dependencies and life cycle of a single Flink job. Workers package the necessary dependencies, submit the Flink job to Hadoop Yarn, continuously monitor its status, and in the event of a failure, it triggers an immediate restart.\u003c/p\u003e\u003cp id=\"9929\"\u003eThe Phase 2 design resulted in faster turnaround time and reduced downtime during job restarts. It also resolved single point of failure issues with Zookeeper.\u003c/p\u003e\u003cp id=\"e04e\"\u003eAs usage of Flink grew, we encountered new challenges in Phase Two:\u003c/p\u003e\u003cul\u003e\u003cli id=\"a815\"\u003eLack of CI/CD: Flink developers had to devise their own version control strategies.\u003c/li\u003e\u003cli id=\"df82\"\u003eAbsence of native secrets management: There is no vanilla secrets management on Hadoop Yarn.\u003c/li\u003e\u003cli id=\"21d6\"\u003eLimited resource and dependency isolation: Each supported Flink version had to be manually preinstalled on the Yarn cluster. While Yarn’s resource queues could provide some level of resource isolation, job-level isolation was absent.\u003c/li\u003e\u003cli id=\"577e\"\u003eService Discovery complexity: As more use cases were onboarded, each potentially requiring access to various internal Airbnb services, configuring service access on Yarn proved to be cumbersome. It forced a binary choice between enabling service access for the entire cluster or none at all.\u003c/li\u003e\u003cli id=\"3b0d\"\u003eMonitoring and debugging challenges: Managing and maintaining the logging pipeline and SSH access became non-trivial tasks on a multi-tenant Yarn cluster.\u003c/li\u003e\u003cli id=\"f235\"\u003eOngoing complexity and dependencies: Although the Flink job scheduler was lightweight compared to Airflow, it introduced additional complexities.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"2721\"\u003ePhase Three (current state): Flink jobs run on Kubernetes, and the job scheduler is eliminated.\u003c/h2\u003e\u003cp id=\"2c34\"\u003e\u003ca href=\"https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDeploying Flink on Kubernetes\u003c/a\u003e allows direct Flink deployment on a running Kubernetes cluster. With this integration we can explore enabling efficient autoscaling and the Kubernetes operator to simplify the management of Flink jobs and clusters.\u003c/p\u003e\u003cp id=\"fecc\"\u003eFlink on Kubernetes offers several advantages over Hadoop Yarn addressing the above challenges:\u003c/p\u003e\u003cul\u003e\u003cli id=\"6cb1\"\u003eDeveloper experience: Standardized by integrating with the existing CI/CD systems.\u003c/li\u003e\u003cli id=\"8a41\"\u003eSecrets Management: With Flink on Kubernetes, each Flink job can securely store its own secrets within the pods. This provides a more secure way to manage sensitive information.\u003c/li\u003e\u003cli id=\"7081\"\u003eIsolated Environment: Jobs running on Flink on Kubernetes benefit from isolation at both the resource and dependency levels. Each job can run on its dedicated Flink version if supported by its image, allowing for better management of dependencies.\u003c/li\u003e\u003cli id=\"e2dd\"\u003eEnhanced Monitoring: Integration with Airbnb’s pre-defined logging and metric sidecars on Kubernetes simplifies setup and improves monitoring. This enables detailed insights into individual pods and rate limiting for logging per pod, making it easier to track and troubleshoot issues.\u003c/li\u003e\u003cli id=\"58c7\"\u003eService Discovery: Flink jobs now adhere to Airbnb’s standardized approach for service discovery, using the cluster mesh. This ensures consistent and reliable communication between services.\u003c/li\u003e\u003cli id=\"4641\"\u003eSimplified SSH access: Users with the appropriate permissions can now SSH into the Flink pod without the need for an SSH tunnel. This provides greater flexibility and control over SSH permissions per job.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"62bd\"\u003eAdditionally, we’ve observed an increasing level of Kubernetes support and adoption within the Flink community, which increased our confidence in running Flink on Kubernetes.\u003c/p\u003e\u003cp id=\"167c\"\u003eIt’s worth mentioning that Kubernetes brings its own risks and limitations. For instance, a single Flink task manager failover can lead to the pause of the entire job process. This can pose issues in scenarios with frequent node rotations within Kubernetes and large jobs deployed with hundreds of task managers. For context, node rotation on Kubernetes is performed to ensure the operability and stability of the cluster. It involves replacing existing nodes with new ones, typically with updated configurations or to perform maintenance tasks, with the goals of applying host configuration changes, maintaining node balance and enhancing operational efficiency. In comparison, node rotations on Yarn occur less frequently, so the impact on job availability is less significant. We will explore how we are mitigating these challenges in the Future Work section.\u003c/p\u003e\u003ch2 id=\"1865\"\u003eComponents Deep Dive\u003c/h2\u003e\u003cp id=\"1c88\"\u003eBelow is an overview of our current architecture:\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cp id=\"bfac\"\u003eTo provide a better understanding of the system, below is a deep dive of the five primary components, as well as how users interact with them when setting up a new Flink job:\u003c/p\u003e\u003cul\u003e\u003cli id=\"1582\"\u003e\u003cstrong\u003eJob configurations: \u003c/strong\u003eThis serves as an abstraction layer over Kubernetes and CI/CD components, providing Flink users with a simplified interface for creating Flink application templates. It shields users from the complexities of the underlying Kubernetes infrastructure. Flink users define the core specifications of their Flink job via a configuration file. This includes critical information like the entrypoint class name, job parallelism, and the necessary ingress services and sinks.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cul\u003e\u003cli id=\"ad86\"\u003e\u003cstrong\u003eImage management: \u003c/strong\u003eThis component involves the pre-construction of Flink base images, which are bundled with essential dependencies required to access Airbnb resources. These images are stored in Amazon Elastic Container Registry and can be readily deployed with user Jars or further customized to meet specific user needs.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cul\u003e\u003cli id=\"5848\"\u003e\u003cstrong\u003eCI/CD\u003c/strong\u003e: By introducing a few customizations to support Flink’s stateful deployment, we’ve integrated Flink with our existing CI/CD system, providing a standardized version control and continuous delivery experience. Flink jobs are deployed within Kubernetes, each residing in its distinct namespace to ensure isolation and effective administration.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cul\u003e\u003cli id=\"4643\"\u003e\u003cstrong\u003eFlink portal: \u003c/strong\u003ean API service that offers essential features for managing the states of Flink jobs. These features include stopping a Flink job with a savepoint and querying completed checkpoints on Amazon S3. Additionally, it provides a self-service UI portal, enabling users to monitor and check the status of their jobs. Users also gain access to critical job state management functionalities, empowering them to either initiate the job from a bootstrapped savepoint or resume it from a previous checkpoint.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cul\u003e\u003cli id=\"8652\"\u003e\u003cstrong\u003eFlink job runtime:\u003c/strong\u003e Each Flink job is deployed as an independent application cluster on Kubernetes. To ensure fault tolerance and state storage, Zookeeper, ETCD, and Amazon S3 are utilized. Additionally, pre-configured sidecar containers accompany the Flink containers to provide support for critical functions such as logging, metrics, DNS, and more. A service mesh is employed to facilitate communication between Flink jobs and other microservices.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003ch2 id=\"2a07\"\u003eImpact\u003c/h2\u003e\u003ch2 id=\"8c95\"\u003eImproved Developer Velocity\u003c/h2\u003e\u003cp id=\"86e7\"\u003eOnboarding Flink jobs is faster, where our developers noted that it takes hours instead of days, and developers can focus more on their application logic.\u003c/p\u003e\u003ch2 id=\"3bad\"\u003eImprovement in Flink Job Availability and Latency\u003c/h2\u003e\u003cp id=\"09a3\"\u003eThe architecture of Flink on Kubernetes improves job availability and scheduling latency by eliminating certain components of the Flink client and job scheduler found in Flink on Yarn.\u003c/p\u003e\u003ch2 id=\"5326\"\u003eCost Savings in Infrastructure\u003c/h2\u003e\u003cp id=\"cd8b\"\u003eThe streamlining of Flink infrastructure complexity and the removal of certain components, such as the job scheduler, have resulted in cost savings in our infrastructure. Additionally, by running Flink jobs on a shared Kubernetes cluster at Airbnb, we could potentially improve the overall cost efficiency of our company’s infrastructure.\u003c/p\u003e\u003ch2 id=\"1d5b\"\u003eFuture Work\u003c/h2\u003e\u003ch2 id=\"c7de\"\u003eImprovement in Job Availability\u003c/h2\u003e\u003cp id=\"c837\"\u003eIn the Flink world, node rotations in Kubernetes can cause job restarts and result in downtime. While Flink itself can recover from job restarts without data loss, the potential downtime and availability impact may be unfavorable for highly latency-sensitive applications. To address this, there are a few approaches we are evaluating.\u003c/p\u003e\u003col\u003e\u003cli id=\"7388\"\u003eReducing the number of node rotations to minimize job restarts.\u003c/li\u003e\u003cli id=\"b4ac\"\u003eFaster job recovery.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"ce92\"\u003eEnable Job Autoscaling\u003c/h2\u003e\u003cp id=\"7f73\"\u003eWith the introduction of \u003ca href=\"https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eReactive Mode\u003c/a\u003e in Flink 1.13, users can dynamically adjust the parallelism of their jobs without the need for a job restart. This job auto scaling feature can enhance job stability and cost efficiency. In the future we could enable autoscaling for Flink Kubernetes workloads by leveraging system metrics (such as CPU usage) and Flink metrics (such as backpressure), to determine the appropriate parallelism.\u003c/p\u003e\u003ch2 id=\"44f0\"\u003eFlink Kubernetes Operator\u003c/h2\u003e\u003cp id=\"750b\"\u003eThe \u003ca href=\"https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFlink Kubernetes Operator\u003c/a\u003e utilizes Custom Resources and functions as a controller to manage the entire production lifecycle of Flink applications. By leveraging the operator, we can streamline the operation and deployment processes for Flink jobs. It provides better control over deployment and lifecycle of jobs, and an out of box solution for autoscaling and auto tuning.\u003c/p\u003e\u003ch2 id=\"8339\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"b244\"\u003eTo summarize, the migration of Airbnb’s streaming processing architecture based on Apache Flink from Hadoop Yarn to Kubernetes has been a significant milestone in enhancing our streaming data processing capabilities. This transition has resulted in a more streamlined and user-friendly experience for Flink developers. By overcoming challenges that were complex to address on Yarn, we have laid the foundation for more efficient and effective streaming data processing.\u003c/p\u003e\u003cp id=\"bd8a\"\u003eAs we look ahead, we are committed to further refining our approach and resolving any remaining challenges. We are enthusiastic about the ongoing growth and potential of Apache Flink within our company, and we anticipate continued innovation and improvement in the future.\u003c/p\u003e\u003cp id=\"0c70\"\u003eIf this kind of work sounds appealing to you, check out our \u003ca href=\"https://careers.airbnb.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eopen roles\u003c/a\u003e — we’re hiring!\u003c/p\u003e\u003ch2 id=\"fcdd\"\u003eAppreciations\u003c/h2\u003e\u003cp id=\"34fc\"\u003eThe Flink on Kubernetes platform would not have been possible without cross-functional and cross-org collaborators as well as leadership support. They include, but are not limited to: Jingwei Lu, Long Zhang, Daniel Low, Weibo He, Zack Loebel-Begelman, Justin Cunningham, Adam Kocoloski, Liyin Tang and Nathan Towery.\u003c/p\u003e\u003cp id=\"6eda\"\u003eSpecial thanks to the broader Airbnb data community members who provided input or aid to the implementation team throughout the design, development, and launch phases.\u003c/p\u003e\u003cp id=\"4277\"\u003eWe also want to thank Wei Hou and Xu Zhang for their support in authoring this post during their time at Airbnb.\u003c/p\u003e\u003ch2 id=\"b11e\"\u003e****************\u003c/h2\u003e\u003cp id=\"17cc\"\u003e\u003cem\u003eApache Spark™, Apache Airflow™, and Apache ZooKeeper™ are trademarks of The Apache Software Foundation.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"e9e6\"\u003e\u003cem\u003eApache Flink® and Apache Hadoop® are registered trademarks of The Apache Software Foundation.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"9f1e\"\u003e\u003cem\u003eKubernetes® is a registered trademark of The Linux Foundation.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"ff92\"\u003e\u003cem\u003eAmazon S3 and AWS are trademarks of Amazon.com, Inc. or its affiliates.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"62d8\"\u003e\u003cem\u003eAll product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2024-07-31T17:04:52.286Z",
  "modifiedTime": null
}
