{
  "id": "d30d341c-aac1-459a-ad43-31ed39453f05",
  "title": "Adding Zonal Resiliency to Etsy’s Kafka Cluster: Part 2",
  "link": "https://www.etsy.com/codeascraft/leveraging-zonal-resiliency-to-improve-updates-for-etsys-kafka-cluster-part-2?utm_source=OpenGraph\u0026utm_medium=PageTools\u0026utm_campaign=Share",
  "description": "The first time I performed a live upgrade of Etsy's Kafka brokers, it felt exciting. There was a bit of an illicit thrill in taking down a whole production system and watching how all the indicators would react. The second time I did the upgrade, I was mostly just bored. There’s only so much excitement to be had staring at graphs and looking at patterns you've already become familiar with. Platform upgrades made for a tedious workday, not to mention a full one. I work on what Etsy calls an “Enablement” team -- we spend a lot of time thinking about how to make working with streaming data a delightful experience for other engineers at Etsy. So it was somewhat ironic that we would spend an entire day staring at graphs during this change, a developer experience we would not want for the end users of our team’s products. We hosted our Kafka brokers in the cloud on Google's managed Kubernetes. The brokers were deployed in the cluster as a StatefulSet, and we applied changes to them using the RollingUpdate strategy. The whole upgrade sequence for a broker went like this: Kubernetes sends a SIGTERM signal to a broker-pod-n The Kafka process shuts down cleanly, and Kubernetes deletes the broker-pod Kubernetes starts a new broker-pod with any changes applied The Kafka process starts on the new broker-pod, which begins recovering by (a) rebuilding its indexes, and (b) catching up on any replication lag Once recovered, a configured readiness probe marks the new broker as Ready, signaling Kubernetes to take down broker-pod-(n-1). Kafka is an important part of Etsy's data ecosystem, and broker upgrades have to happen with no downtime. Topics in our Kafka cluster are configured to have three replicas each to provide us with redundancy. At least two of them have to be available for Kafka to consider the topic “online.” We could see to it that these replicas were distributed evenly across the cluster in terms of data volume, but there were no other guarantees around where replicas for a topic would live. If we took down two brokers at once, we'd have no way to be sure we weren't violating Kafka's two-replica-minimum rule. To ensure availability for the cluster, we had to roll out changes one broker at a time, waiting for recovery between each restart. Each broker took about nine minutes to recover. With 48 Kafka brokers in our cluster, that meant seven hours of mostly waiting. Entering the Multizone In the fall of 2021, Andrey Polyakov led an exciting project that changed the design of our Kafka cluster to make it resilient to a full zonal outage. Where Kafka had formerly been deployed completely in Google’s us-central1-a region, we now distributed the brokers across three zones, ensuring that an outage in any one region would not make the entire cluster unavailable. In order to ensure zonal resilience, we had to move to a predictable distribution of replicas across the cluster. If a replica for a topic was on a broker located in zone A, we could be sure the second replica was on a broker in zone C with the third replica in zone F. Figure 1: Distribution of replicas for a topic are now spread across three GCP zones. Taking down only brokers in Zone A is guaranteed to leave topic partitions in Zone C and Zone F available. This new multizone Kafka architecture wiped out the one-broker-at-a-time limitation on our upgrade process. We could now take down every single broker in the same zone simultaneously without affecting availability, meaning we could upgrade as many as twelve brokers at once. We just needed to find a way to restart the correct brokers. Kubernetes natively provides a means to take down multiple pods in a StatefulSet at once, using the partitioned rolling updates rollout strategy. However, that requires pods to come down in sequential order by pod number, but our multizonal architecture placed every third broker in the same zone. We could have reassigned partitions across brokers, but that would have been a lengthy and manual process, so we opted instead to write some custom logic to control the updates. Figure 2. Limitations of Kubernetes sequential ordering in a multizone architecture: we can concurrently update pods 11 and 10, for example, but not pods 11 and 8. We changed the RolloutPolicy on the StatefulSet Kubernetes object for Kafka to OnDelete. This means that once an update is applied to a StatefulSet, Kubernetes will not automatically start rolling out changes to the pods, but will expect users to explicitly delete the pods to roll out their changes. The main loop of our program essentially finds some pods in a zone that haven’t been updated and updates them. It waits until the cluster has recovered, and then moves on to the next batch. Figure 3. Polling for pods that still need updates. We didn’t want to have to run something like this from our local machines (what if someone has a flakey internet connection?), so we decided to dockerize the process and run it as a Kubernetes batch job. We set up a small make target to deploy the upgrade script, with logic that would prevent engineers from accidently deploying two versions of it at once. Performance Improvements Figure 4. Visualization of broker upgrades We tested our logic out in production, and with a parallelism of three we were able to finish upgrades in a little over two hours. In theory we could go further and restart all the Kafka brokers in a zone en masse, but we have shied away from that. Part of broker recovery involves catching up with replication lag: i.e., reading data from the brokers that have been continuing to serve traffic. A restart of an entire zone would mean increased load on all the remaining brokers in the cluster as they saw their number of client connections jump -- we would find ourselves essentially simulating a full zonal outage every time we updated. It’s pretty easy just to look at the reduced duration of our upgrades and call this project a success -- we went from spending seven hours rolling out changes to about two. And in terms of our total investment of time—time coding the new process vs. time saved on upgrades—I suspect by now, eight months in, we’ve probably about broken even. But my personal favorite way to measure the success of this project is centered around toil -- and every upgrade I’ve performed these last eight months has been quick, peaceful, and over by lunchtime.",
  "author": "Kamya Shethia",
  "published": "Thu, 9 Feb 2023 12:06:19 -0500",
  "source": "https://codeascraft.com/feed/atom/",
  "categories": null,
  "byline": "By Kamya Shethia, Andrey Polyakov Feb 9, 2023",
  "length": 6402,
  "excerpt": "The first time I performed a live upgrade of Etsy's Kafka brokers, it felt exciting. There was a bit of an illicit thrill in taking...",
  "siteName": "Etsy Engineering",
  "favicon": "",
  "text": "The first time I performed a live upgrade of Etsy's Kafka brokers, it felt exciting. There was a bit of an illicit thrill in taking down a whole production system and watching how all the indicators would react. The second time I did the upgrade, I was mostly just bored. There’s only so much excitement to be had staring at graphs and looking at patterns you've already become familiar with. Platform upgrades made for a tedious workday, not to mention a full one. I work on what Etsy calls an “Enablement” team -- we spend a lot of time thinking about how to make working with streaming data a delightful experience for other engineers at Etsy. So it was somewhat ironic that we would spend an entire day staring at graphs during this change, a developer experience we would not want for the end users of our team’s products. We hosted our Kafka brokers in the cloud on Google's managed Kubernetes. The brokers were deployed in the cluster as a StatefulSet, and we applied changes to them using the RollingUpdate strategy. The whole upgrade sequence for a broker went like this: Kubernetes sends a SIGTERM signal to a broker-pod-n The Kafka process shuts down cleanly, and Kubernetes deletes the broker-pod Kubernetes starts a new broker-pod with any changes applied The Kafka process starts on the new broker-pod, which begins recovering by (a) rebuilding its indexes, and (b) catching up on any replication lag Once recovered, a configured readiness probe marks the new broker as Ready, signaling Kubernetes to take down broker-pod-(n-1). Kafka is an important part of Etsy's data ecosystem, and broker upgrades have to happen with no downtime. Topics in our Kafka cluster are configured to have three replicas each to provide us with redundancy. At least two of them have to be available for Kafka to consider the topic “online.” We could see to it that these replicas were distributed evenly across the cluster in terms of data volume, but there were no other guarantees around where replicas for a topic would live. If we took down two brokers at once, we'd have no way to be sure we weren't violating Kafka's two-replica-minimum rule. To ensure availability for the cluster, we had to roll out changes one broker at a time, waiting for recovery between each restart. Each broker took about nine minutes to recover. With 48 Kafka brokers in our cluster, that meant seven hours of mostly waiting. Entering the Multizone In the fall of 2021, Andrey Polyakov led an exciting project that changed the design of our Kafka cluster to make it resilient to a full zonal outage. Where Kafka had formerly been deployed completely in Google’s us-central1-a region, we now distributed the brokers across three zones, ensuring that an outage in any one region would not make the entire cluster unavailable. In order to ensure zonal resilience, we had to move to a predictable distribution of replicas across the cluster. If a replica for a topic was on a broker located in zone A, we could be sure the second replica was on a broker in zone C with the third replica in zone F. Figure 1: Distribution of replicas for a topic are now spread across three GCP zones. Taking down only brokers in Zone A is guaranteed to leave topic partitions in Zone C and Zone F available. This new multizone Kafka architecture wiped out the one-broker-at-a-time limitation on our upgrade process. We could now take down every single broker in the same zone simultaneously without affecting availability, meaning we could upgrade as many as twelve brokers at once. We just needed to find a way to restart the correct brokers. Kubernetes natively provides a means to take down multiple pods in a StatefulSet at once, using the partitioned rolling updates rollout strategy. However, that requires pods to come down in sequential order by pod number, but our multizonal architecture placed every third broker in the same zone. We could have reassigned partitions across brokers, but that would have been a lengthy and manual process, so we opted instead to write some custom logic to control the updates. Figure 2. Limitations of Kubernetes sequential ordering in a multizone architecture: we can concurrently update pods 11 and 10, for example, but not pods 11 and 8. We changed the RolloutPolicy on the StatefulSet Kubernetes object for Kafka to OnDelete. This means that once an update is applied to a StatefulSet, Kubernetes will not automatically start rolling out changes to the pods, but will expect users to explicitly delete the pods to roll out their changes. The main loop of our program essentially finds some pods in a zone that haven’t been updated and updates them. It waits until the cluster has recovered, and then moves on to the next batch. Figure 3. Polling for pods that still need updates. We didn’t want to have to run something like this from our local machines (what if someone has a flakey internet connection?), so we decided to dockerize the process and run it as a Kubernetes batch job. We set up a small make target to deploy the upgrade script, with logic that would prevent engineers from accidently deploying two versions of it at once. Performance Improvements Figure 4. Visualization of broker upgrades We tested our logic out in production, and with a parallelism of three we were able to finish upgrades in a little over two hours. In theory we could go further and restart all the Kafka brokers in a zone en masse, but we have shied away from that. Part of broker recovery involves catching up with replication lag: i.e., reading data from the brokers that have been continuing to serve traffic. A restart of an entire zone would mean increased load on all the remaining brokers in the cluster as they saw their number of client connections jump -- we would find ourselves essentially simulating a full zonal outage every time we updated. It’s pretty easy just to look at the reduced duration of our upgrades and call this project a success -- we went from spending seven hours rolling out changes to about two. And in terms of our total investment of time—time coding the new process vs. time saved on upgrades—I suspect by now, eight months in, we’ve probably about broken even. But my personal favorite way to measure the success of this project is centered around toil -- and every upgrade I’ve performed these last eight months has been quick, peaceful, and over by lunchtime.",
  "image": "https://i.etsystatic.com/inv/62083f/4610733184/inv_fullxfull.4610733184_owuofk8g.jpg?version=0",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n    \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://i.etsystatic.com/inv/34bb5c/4658811727/inv_60x60.4658811727_3649c4a6.jpg?version=0\" alt=\"  image\" aria-label=\"  image\"/\u003e\n    \u003c/p\u003e\n\n\n    \u003c/div\u003e\n    \u003cp\u003eThe first time I performed a live upgrade of Etsy\u0026#39;s Kafka brokers, it felt exciting. There was a bit of an illicit thrill in taking down a whole production system and watching how all the indicators would react. The second time I did the upgrade, I was mostly just bored. There’s only so much excitement to be had staring at graphs and looking at patterns you\u0026#39;ve already become familiar with.\u003c/p\u003e\n\u003cp\u003ePlatform upgrades made for a tedious workday, not to mention a full one. I work on what Etsy calls an “Enablement” team -- we spend a lot of time thinking about how to make working with streaming data a delightful experience for other engineers at Etsy. So it was somewhat ironic that we would spend an entire day staring at graphs during this change, a developer experience we would not want for the end users of our team’s products. \u003c/p\u003e\n\u003cp\u003eWe hosted our Kafka brokers in the cloud on Google\u0026#39;s managed Kubernetes. The brokers were deployed in the cluster as a StatefulSet, and we applied changes to them using the RollingUpdate strategy. The whole upgrade sequence for a broker went like this: \u003c/p\u003e\n\u003col\u003e\u003cli\u003eKubernetes sends a SIGTERM signal to a broker-pod-n\u003c/li\u003e\n\u003cli\u003eThe Kafka process shuts down cleanly, and Kubernetes deletes the broker-pod  \u003c/li\u003e\n\u003cli\u003eKubernetes starts a new broker-pod with any changes applied\u003c/li\u003e\n\u003cli\u003eThe Kafka process starts on the new broker-pod, which begins recovering by (a) rebuilding its indexes, and (b) catching up on any replication lag\u003c/li\u003e\n\u003cli\u003eOnce recovered, a configured readiness probe marks the new broker as Ready, signaling Kubernetes to take down broker-pod-(n-1).\u003c/li\u003e\n\u003c/ol\u003e\u003cp\u003eKafka is an important part of Etsy\u0026#39;s data ecosystem, and broker upgrades have to happen with no downtime. Topics in our Kafka cluster are configured to have three replicas each to provide us with redundancy. At least two of them have to be available for Kafka to consider the topic “online.” We could see to it that these replicas were distributed evenly across the cluster in terms of data volume, but there were no other guarantees around where replicas for a topic would live. If we took down two brokers at once, we\u0026#39;d have no way to be sure we weren\u0026#39;t violating Kafka\u0026#39;s two-replica-minimum rule.\u003c/p\u003e\n\u003cp\u003eTo ensure availability for the cluster, we had to roll out changes one broker at a time, waiting for recovery between each restart. Each broker took about nine minutes to recover. With 48 Kafka brokers in our cluster, that meant seven hours of mostly waiting.\u003c/p\u003e\n\u003ch2\u003eEntering the Multizone\u003c/h2\u003e\n\u003cp\u003eIn the fall of 2021, Andrey Polyakov led an exciting project that changed the design of our Kafka cluster to  make it\u003ca href=\"https://www.etsy.com/in-en/codeascraft/adding-zonal-resiliency-to-etsys-kafka-cluster-part-1\"\u003e resilient to a full zonal outage\u003c/a\u003e.  Where Kafka had formerly been deployed completely in Google’s us-central1-a region, we now distributed the brokers across three zones, ensuring that an outage in any one region would not make the entire cluster unavailable. \u003c/p\u003e\n\u003cp\u003eIn order to ensure zonal resilience, we had to move to a predictable distribution of replicas across the cluster. If a replica for a topic was on a broker located in zone A, we could be sure the second replica was on a broker in zone C with the third replica in zone F.\u003c/p\u003e\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"_Figure 1: Distribution of replicas for a topic are now spread across three GCP zones. Taking down only brokers in Zone A is guaranteed to leave topic partitions in Zone C and Zone F available._\" src=\"https://i.etsystatic.com/inv/968b23/4603526702/inv_fullxfull.4603526702_fjilhkw5.jpg?version=0\" title=\"_Figure 1: Distribution of replicas for a topic are now spread across three GCP zones. Taking down only brokers in Zone A is guaranteed to leave topic partitions in Zone C and Zone F available._\"/\u003e\u003cfigcaption\u003e\u003cem\u003eFigure 1: Distribution of replicas for a topic are now spread across three GCP zones. Taking down only brokers in Zone A is guaranteed to leave topic partitions in Zone C and Zone F available.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\u003cp\u003eThis new multizone Kafka architecture wiped out the one-broker-at-a-time limitation on our upgrade process. We could now take down every single broker in the same zone simultaneously without affecting availability, meaning we could upgrade as many as twelve brokers at once. We just needed to find a way to restart the correct brokers.\u003c/p\u003e\n\u003cp\u003eKubernetes natively provides a means to take down multiple pods in a StatefulSet at once, using the partitioned rolling updates rollout strategy. However,  that requires pods to come down in sequential order by pod number, but our multizonal architecture placed every third broker in the same zone. We could have reassigned partitions across brokers, but that would have been a lengthy and manual process, so we opted instead to write some custom logic to control the updates. \u003c/p\u003e\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Figure 2. Limitations of Kubernetes sequential ordering in a multizone architecture: we can concurrently update pods 11 and 10, for example, but not pods 11 and 8.\" src=\"https://i.etsystatic.com/inv/9a4624/4651768827/inv_fullxfull.4651768827_fti513nv.jpg?version=0\" title=\"Figure 2. Limitations of Kubernetes sequential ordering in a multizone architecture: we can concurrently update pods 11 and 10, for example, but not pods 11 and 8.\"/\u003e\u003cfigcaption\u003e\u003cem\u003eFigure 2. Limitations of Kubernetes sequential ordering in a multizone architecture: we can concurrently update pods 11 and 10, for example, but not pods 11 and 8.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\u003cp\u003eWe changed the RolloutPolicy on the StatefulSet Kubernetes object for Kafka to OnDelete. This means that once an update is applied to a StatefulSet, Kubernetes will not automatically start rolling out changes to the pods, but will expect users to explicitly delete the pods to roll out their changes. The main loop of our program essentially finds some pods in a zone that haven’t been updated and updates them. It waits until the cluster has recovered, and then moves on to the next batch.\u003c/p\u003e\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Figure 3. Polling for pods that still need updates.\" src=\"https://i.etsystatic.com/inv/39e3ee/4651776487/inv_fullxfull.4651776487_dkrdksxy.jpg?version=0\" title=\"Figure 3. Polling for pods that still need updates.\"/\u003e\u003cfigcaption\u003e\u003cem\u003eFigure 3. Polling for pods that still need updates.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\u003cp\u003eWe didn’t want to have to run something like this from our local machines (what if someone has a flakey internet connection?), so we decided to dockerize the process and run it as a Kubernetes batch job. We set up a small make target to deploy the upgrade script, with logic that would prevent engineers from accidently deploying two versions of it at once.\u003c/p\u003e\n\u003ch2\u003ePerformance Improvements\u003c/h2\u003e\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg alt=\"Figure 4. Visualization of broker upgrades\" src=\"https://s3.amazonaws.com/extfiles.etsy.com/multizone-kafka-part-2-figure-4.gif\" title=\"Figure 4. Visualization of broker upgrades\"/\u003e\u003cfigcaption\u003e\u003cem\u003eFigure 4. Visualization of broker upgrades\u003c/em\u003e\n\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\u003cp\u003eWe tested our logic out in production, and with a parallelism of three we were able to finish upgrades in a little over two hours. In theory we could go further and restart all the Kafka brokers in a zone en masse, but we have shied away from that. Part of broker recovery involves catching up with replication lag: i.e., reading data from the brokers that have been continuing to serve traffic. A restart of an entire zone  would mean increased load on all the remaining brokers in the cluster as they saw their number of client connections jump -- we would find ourselves essentially simulating a full zonal outage every time we updated.\u003c/p\u003e\n\u003cp\u003eIt’s pretty easy just to look at the reduced duration of our upgrades and call this project a success -- we went from spending seven hours rolling out changes to about two. And in terms of our total investment of time—time coding the new process vs. time saved on upgrades—I suspect by now, eight months in, we’ve probably about broken even. But my personal favorite way to measure the success of this project is centered around toil --  and every upgrade I’ve performed these last eight months has been quick, peaceful, and over by lunchtime.\u003c/p\u003e\n    \n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2024-10-11T04:10:08Z",
  "modifiedTime": "2024-10-11T04:10:08Z"
}
