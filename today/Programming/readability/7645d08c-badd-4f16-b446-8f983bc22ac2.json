{
  "id": "7645d08c-badd-4f16-b446-8f983bc22ac2",
  "title": "How Prezi replaced a homegrown Log Management System with Grafana Loki",
  "link": "https://engineering.prezi.com/how-prezi-replaced-a-homegrown-log-management-system-with-grafana-loki-15111174ff91?source=rss----911e72786e31---4",
  "description": "",
  "author": "Alex",
  "published": "Thu, 08 Feb 2024 15:26:37 GMT",
  "source": "https://engineering.prezi.com/feed",
  "categories": [
    "logging",
    "sre",
    "technology",
    "prezi",
    "software-development"
  ],
  "byline": "Alex",
  "length": 9305,
  "excerpt": "Prezi has quite a sophisticated engineering culture where solutions are built that do the job. Some solutions that have been built in the past stood out and aged well. In other areas, some solutions…",
  "siteName": "Prezi Engineering",
  "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*U0lNGgJfm0Qo1ZfYDS36KA.png",
  "text": "Prezi has quite a sophisticated engineering culture where solutions are built that do the job. Some solutions that have been built in the past stood out and aged well. In other areas, some solutions have lost traction compared to industry standards.In the second half of 2023, we modernized one of those areas that was not market-standard anymore: the logfile management system of Prezi. This is our testimonial.Photo by Álvaro Serrano on UnsplashHow it was beforeWe traced the beginning of the existing solution back to 2014. So it is safe to say that it was a stable solution.The following depicts the solution. Every workload Prezi ran was instrumented with a special sidecar that took care of handling all log messages. That sidecar was built on top of two open-source solutions: scribe (https://github.com/facebookarchive/scribe), a tool built by Facebook, archived on Github in 2022.Scribe took care of receiving log events, aggregating them, and sending them downstream.The second component, sTunnel (https://www.stunnel.org/), took care of encrypted communication from the workload systems to the central system.Prezi collected log events from all environments in one central place and made them accessible to engineers.legacy log management systemYes, the picture is telling the truth: the consumption of collected log events happened a good part of 2023 over SSH and not over any UI.That alone was a fact to reimplement the whole solution and develop it with current market-best practices in mind. Our goal was to make the user experience more accessible and the query results better shareable.LogshippingWith that in mind, we started the project’s first iteration. Our first take was to provide a central system that could aggregate and display log events in a more user-friendly way. Also, we wanted to get rid of the sidecar to ease operational load: while a sidecar per se is not a bad thing and a very battle-proven design pattern, it comes with certain costs when running thousands of pods.The sidecar solution was born in times when Prezis workload ran on Elastic Beanstalk which means just an additional container on a probably oversized EC2 instance.With the shift to Kubernetes as the workload engine the oversized EC2 instance vanished but the sidecar remained. Also, Kubernetes offers a very standardized way to consume logs from containers: stdout and stderr of the container are written to file on the Kubernetes worker hosts by the container runtime. And files can be easily consumed.We did exactly that and used one of the established tools in that domain — filebeat — which is capable of reading the mentioned files and enriching the resulting events with metadata from Kubernetes — e.g. pod and container name and namespace.Details on the log shipping processThis was the first optimization. The second was how events will be sent to downstream systems.Operating in cloud environments requires a fast shipping of events away from nodes as these nodes can vanish at any time.A common design pattern for this is to use a message queue as the first persistent layer. This can protect downstream systems in case of event bursts. It also decouples the individual parts from each other which can be helpful for maintenance or even replacements of tools.Most of the time, the message queue used for that is an Apache Kafka installation that is capable of storing events at scale. As we already used a Kafka setup to store business events from multiple sources, we went that route without digging further into alternative persistent layers.Sending events to a message queueOnce the events are in the queue, they can be parsed and ingested into a central system.Parsing and StoringIn our first take on this, we planned to set up the central log file management system inside our cloud environment. When doing that, there are 2 major options to go: do something with Elasticsearch or use Grafana Loki as the backend.The very first takeWe’ve started with AWS OpenSearch service as a backend and Logstash to feed events from Kafka into our OpenSearch cluster.As we run most of our software on Kubernetes, we also set up Logstash on Kubernetes and soon discovered all the joy of running a JVM inside containers. We suffered a lot of out-of-memory kills of that component.Storing and indexing a massive amount of data into OpenSearch leads to massive indexes that soon have not been manageable anymore. This was caused by the vast amount of non-standardized fields in the application logging. A lot of heterogeneity in the fields and the contents leads to a lot of parsing errors. The most prominent example is the time and date format. Some applications have been logging unix timestamps, whereas others are using a string representation.We discovered that if we don’t control the sources, a solution based on OpenSearch will not service us well. Going to control the source by evangelizing a common log scheme throughout all applications would have been the only way to make this work.The overhaulWe started to look into an alternative to Logstash to get rid of the memory issues. We started to replace it with vector.dev which has a smaller footprint, a more flexible configuration, and it can also send to more possible backends. Logstash, without any modification, is tied closely to the OpenSearch ecosystem. But as spoiled above there is another major option to save log events: Grafana Loki.With the replacement of Logstash, we got rid of the constant restart but not of the constant indexing errors.Soon we started to look into Loki as an alternative. Also, we considered the hosting option as the running and maintenance of a log management system is not one of our core tasks. Running that system is more or less a commodity and takes away precious time that could be spent otherwise.Focusing on our core tasks as the SRE team is also beneficial for customers of Prezi.Optimizing the central systemsLooking at log management systems is in most cases also a buy or make (host) decision: Do one want to have the whole aggregation systems self-hosted or can this be offloaded to some 3rd party vendor?Security and compliance concerns aside, this mostly boils down to the question of “How much can we spend?”.With the security clearing to send logs to a 3rd party vendor and the budget to do so, we started to look at the hosted version of Loki. It turned out to be within our cost range and it can service us well: They had no issues with our ingestion rate. The way Loki stores log events as streams was perfect as it moved the problem Opensearch had with the variety of field contents from indexing time to later. With Loki, those differences surface at query time and can be tackled by predefined dashboards. This way we don’t lose any events by parsing errors.The way to consume events that are stored in Loki is a very common user interface: Grafana, which is a well-known dashboarding solution and is already in use. With that, engineers can rely on existing tool knowledge.Offloading logs to an external vendor also removes them from your direct control. To avoid any issues with any retention period, we also started to write logs additionally to S3 to archive them. That way, we have control over them and can use them in case we need them later.Details on parsing and storingWith that last piece in place, we have been able to shut down the above-mentioned original log management system at the end of 2023.The resultLooking at the completely new log management system, we went from a very homegrown solution to a modern stack:We consume logs via a standard API of the container runtimeSending the events to Kafka enables us to consume them decoupled from the creation time. Kafka also stores events for a certain period, so any downtime of downstream systems does not cause data loss.Vector enables us to feed events into multiple sinks. Even though not outlined before, it enables us to make certain parsing and routing decisions when parsing events. But that is part of another story.Loki enables us to consume event streams via the well-known Grafana UI and query a vast amount of data in real time.The whole processLessons learnedThe whole project took us most of a year until we shut down the old solution. We took this amount of time to verify all is well set up, all engineers are onboarded and familiar with the solution, and the solution is capable of handling all different peak situations.Keeping the old system running has been a good decision. By that, we have been able to optimize the new system until it was able to handle the load and satisfy our needsStarting to advertise a common logging scheme through a company is beneficial. That scheme makes collecting and analyzing events simpler. It gives a better user experience, too, because a timestamp is always in the same format for example.Controlling log levels and the understanding of the various log levels are also crucial. What one engineer sees as debug another emits as info. Creating a common understanding is helpful.Decoupling the different components from one another enables us to change them if we have other requirements or find better solutions. E.g. if we start to get unhappy with Vector, we can replace it without any hassle as the interface between the log source and Vector is Kafka.",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*pmZClgRKPe7CZ0EI",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca href=\"https://medium.com/@la3mmchen?source=post_page-----15111174ff91--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Alex\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*qlKWQ5hAPbo5cLIv_PuLSQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://engineering.prezi.com/?source=post_page-----15111174ff91--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Prezi Engineering\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ecIYF5KMJj1G4-_pkFWy0g.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"ad00\"\u003ePrezi has quite a sophisticated engineering culture where solutions are built that do the job. Some solutions that have been built in the past stood out and aged well. In other areas, some solutions have lost traction compared to industry standards.\u003c/p\u003e\u003cp id=\"b609\"\u003eIn the second half of 2023, we modernized one of those areas that was not market-standard anymore: the logfile management system of Prezi. This is our testimonial.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003ePhoto by \u003ca href=\"https://unsplash.com/@alvaroserrano?utm_source=medium\u0026amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eÁlvaro Serrano\u003c/a\u003e on \u003ca href=\"https://unsplash.com/?utm_source=medium\u0026amp;utm_medium=referral\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eUnsplash\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"b5c5\"\u003eHow it was before\u003c/h2\u003e\u003cp id=\"49fd\"\u003eWe traced the beginning of the existing solution back to 2014. So it is safe to say that it was a stable solution.\u003c/p\u003e\u003cp id=\"eeaa\"\u003eThe following depicts the solution. Every workload Prezi ran was instrumented with a special sidecar that took care of handling all log messages. That sidecar was built on top of two open-source solutions: scribe (\u003ca href=\"https://github.com/facebookarchive/scribe\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://github.com/facebookarchive/scribe\u003c/a\u003e), a tool built by Facebook, archived on Github in 2022.\u003cbr/\u003eScribe took care of receiving log events, aggregating them, and sending them downstream.\u003c/p\u003e\u003cp id=\"e25e\"\u003eThe second component, sTunnel (\u003ca href=\"https://www.stunnel.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://www.stunnel.org/\u003c/a\u003e), took care of encrypted communication from the workload systems to the central system.\u003c/p\u003e\u003cp id=\"4d4b\"\u003ePrezi collected log events from all environments in one central place and made them accessible to engineers.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003elegacy log management system\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"79b5\"\u003eYes, the picture is telling the truth: the consumption of collected log events happened a good part of 2023 over SSH and not over any UI.\u003c/p\u003e\u003cp id=\"4b8e\"\u003eThat alone was a fact to reimplement the whole solution and develop it with current market-best practices in mind. Our goal was to make the user experience more accessible and the query results better shareable.\u003c/p\u003e\u003ch2 id=\"5f0b\"\u003eLogshipping\u003c/h2\u003e\u003cp id=\"0d67\"\u003eWith that in mind, we started the project’s first iteration. Our first take was to provide a central system that could aggregate and display log events in a more user-friendly way. Also, we wanted to get rid of the sidecar to ease operational load: while a sidecar per se is not a bad thing and a very battle-proven design pattern, it comes with certain costs when running thousands of pods.\u003c/p\u003e\u003cp id=\"0069\"\u003eThe sidecar solution was born in times when Prezis workload ran on Elastic Beanstalk which means just an additional container on a probably oversized EC2 instance.\u003c/p\u003e\u003cp id=\"cb3a\"\u003eWith the shift to \u003ca href=\"https://kubernetes.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e as the workload engine the oversized EC2 instance vanished but the sidecar remained. Also, Kubernetes offers a very standardized way to consume logs from containers: stdout and stderr of the container are written to file on the Kubernetes worker hosts by the container runtime. And files can be easily consumed.\u003c/p\u003e\u003cp id=\"50b9\"\u003eWe did exactly that and used one of the established tools in that domain — filebeat — which is capable of reading the mentioned files and enriching the resulting events with metadata from Kubernetes — e.g. pod and container name and namespace.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eDetails on the log shipping process\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"f7ef\"\u003eThis was the first optimization. The second was how events will be sent to downstream systems.\u003c/p\u003e\u003cp id=\"9518\"\u003eOperating in cloud environments requires a fast shipping of events away from nodes as these nodes can vanish at any time.\u003c/p\u003e\u003cp id=\"15f7\"\u003eA common design pattern for this is to use a message queue as the first persistent layer. This can protect downstream systems in case of event bursts. It also decouples the individual parts from each other which can be helpful for maintenance or even replacements of tools.\u003c/p\u003e\u003cp id=\"bcc3\"\u003eMost of the time, the message queue used for that is an \u003ca href=\"https://kafka.apache.org/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eApache Kafka\u003c/a\u003e installation that is capable of storing events at scale. As we already used a Kafka setup to store business events from multiple sources, we went that route without digging further into alternative persistent layers.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eSending events to a message queue\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"1fe2\"\u003eOnce the events are in the queue, they can be parsed and ingested into a central system.\u003c/p\u003e\u003ch2 id=\"a58a\"\u003eParsing and Storing\u003c/h2\u003e\u003cp id=\"5731\"\u003eIn our first take on this, we planned to set up the central log file management system inside our cloud environment. When doing that, there are 2 major options to go: do something with \u003ca href=\"https://www.elastic.co/elasticsearch\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eElasticsearch\u003c/a\u003e or use \u003ca href=\"https://grafana.com/oss/loki/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGrafana Loki\u003c/a\u003e as the backend.\u003c/p\u003e\u003ch2 id=\"30e8\"\u003eThe very first take\u003c/h2\u003e\u003cp id=\"7190\"\u003eWe’ve started with \u003ca href=\"https://aws.amazon.com/opensearch-service/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAWS OpenSearch\u003c/a\u003e service as a backend and \u003ca href=\"https://www.elastic.co/logstash\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLogstash\u003c/a\u003e to feed events from Kafka into our OpenSearch cluster.\u003c/p\u003e\u003cp id=\"ce48\"\u003eAs we run most of our software on Kubernetes, we also set up Logstash on Kubernetes and soon discovered all the joy of running a JVM inside containers. We suffered a lot of out-of-memory kills of that component.\u003c/p\u003e\u003cp id=\"a834\"\u003eStoring and indexing a massive amount of data into OpenSearch leads to massive indexes that soon have not been manageable anymore. This was caused by the vast amount of non-standardized fields in the application logging. A lot of heterogeneity in the fields and the contents leads to a lot of parsing errors. The most prominent example is the time and date format. Some applications have been logging unix timestamps, whereas others are using a string representation.\u003c/p\u003e\u003cp id=\"a0b9\"\u003eWe discovered that if we don’t control the sources, a solution based on OpenSearch will not service us well. Going to control the source by evangelizing a common log scheme throughout all applications would have been the only way to make this work.\u003c/p\u003e\u003ch2 id=\"a072\"\u003eThe overhaul\u003c/h2\u003e\u003cp id=\"5430\"\u003eWe started to look into an alternative to Logstash to get rid of the memory issues. We started to replace it with vector.dev which has a smaller footprint, a more flexible configuration, and it can also send to more possible backends. Logstash, without any modification, is tied closely to the OpenSearch ecosystem. But as spoiled above there is another major option to save log events: Grafana Loki.\u003c/p\u003e\u003cp id=\"db3a\"\u003eWith the replacement of Logstash, we got rid of the constant restart but not of the constant indexing errors.\u003c/p\u003e\u003cp id=\"d6c7\"\u003eSoon we started to look into Loki as an alternative. Also, we considered the hosting option as the running and maintenance of a log management system is not one of our core tasks. Running that system is more or less a commodity and takes away precious time that could be spent otherwise.\u003c/p\u003e\u003cp id=\"59bb\"\u003eFocusing on our core tasks as the SRE team is also beneficial for customers of Prezi.\u003c/p\u003e\u003ch2 id=\"fec6\"\u003eOptimizing the central systems\u003c/h2\u003e\u003cp id=\"9181\"\u003eLooking at log management systems is in most cases also a buy or make (host) decision: Do one want to have the whole aggregation systems self-hosted or can this be offloaded to some 3rd party vendor?\u003c/p\u003e\u003cp id=\"60fa\"\u003eSecurity and compliance concerns aside, this mostly boils down to the question of “How much can we spend?”.\u003c/p\u003e\u003cp id=\"70d8\"\u003eWith the security clearing to send logs to a 3rd party vendor and the budget to do so, we started to look at the hosted version of Loki. It turned out to be within our cost range and it can service us well: They had no issues with our ingestion rate. The way Loki stores log events as streams was perfect as it moved the problem Opensearch had with the variety of field contents from indexing time to later. With Loki, those differences surface at query time and can be tackled by predefined dashboards. This way we don’t lose any events by parsing errors.\u003c/p\u003e\u003cp id=\"aeae\"\u003eThe way to consume events that are stored in Loki is a very common user interface: Grafana, which is a well-known dashboarding solution and is already in use. With that, engineers can rely on existing tool knowledge.\u003c/p\u003e\u003cp id=\"ffc0\"\u003eOffloading logs to an external vendor also removes them from your direct control. To avoid any issues with any retention period, we also started to write logs additionally to S3 to archive them. That way, we have control over them and can use them in case we need them later.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003eDetails on parsing and storing\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"9f04\"\u003eWith that last piece in place, we have been able to shut down the above-mentioned original log management system at the end of 2023.\u003c/p\u003e\u003ch2 id=\"4ff5\"\u003eThe result\u003c/h2\u003e\u003cp id=\"9ed5\"\u003eLooking at the completely new log management system, we went from a very homegrown solution to a modern stack:\u003c/p\u003e\u003cul\u003e\u003cli id=\"81d8\"\u003eWe consume logs via a standard API of the container runtime\u003c/li\u003e\u003cli id=\"a2d3\"\u003eSending the events to Kafka enables us to consume them decoupled from the creation time. Kafka also stores events for a certain period, so any downtime of downstream systems does not cause data loss.\u003c/li\u003e\u003cli id=\"38e4\"\u003eVector enables us to feed events into multiple sinks. Even though not outlined before, it enables us to make certain parsing and routing decisions when parsing events. But that is part of another story.\u003c/li\u003e\u003cli id=\"3a85\"\u003eLoki enables us to consume event streams via the well-known Grafana UI and query a vast amount of data in real time.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption\u003eThe whole process\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"d79e\"\u003eLessons learned\u003c/h2\u003e\u003cp id=\"6dc9\"\u003eThe whole project took us most of a year until we shut down the old solution. We took this amount of time to verify all is well set up, all engineers are onboarded and familiar with the solution, and the solution is capable of handling all different peak situations.\u003c/p\u003e\u003cul\u003e\u003cli id=\"129a\"\u003eKeeping the old system running has been a good decision. By that, we have been able to optimize the new system until it was able to handle the load and satisfy our needs\u003c/li\u003e\u003cli id=\"f04d\"\u003eStarting to advertise a common logging scheme through a company is beneficial. That scheme makes collecting and analyzing events simpler. It gives a better user experience, too, because a timestamp is always in the same format for example.\u003c/li\u003e\u003cli id=\"8e51\"\u003eControlling log levels and the understanding of the various log levels are also crucial. What one engineer sees as debug another emits as info. Creating a common understanding is helpful.\u003c/li\u003e\u003cli id=\"9726\"\u003eDecoupling the different components from one another enables us to change them if we have other requirements or find better solutions. E.g. if we start to get unhappy with Vector, we can replace it without any hassle as the interface between the log source and Vector is Kafka.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2024-02-08T15:26:37.121Z",
  "modifiedTime": null
}
