{
  "id": "d28584e8-a431-42e0-be66-b493e2c9e2cd",
  "title": "Meta’s open AI hardware vision",
  "link": "https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/",
  "description": "At the Open Compute Project (OCP) Global Summit 2024, we’re showcasing our latest open AI hardware designs with the OCP community. These innovations include a new AI platform, cutting-edge open rack designs, and advanced network fabrics and components.  By sharing our designs, we hope to inspire collaboration and foster innovation. If you’re passionate about building [...] Read More... The post Meta’s open AI hardware vision appeared first on Engineering at Meta.",
  "author": "",
  "published": "Tue, 15 Oct 2024 17:00:13 +0000",
  "source": "https://engineering.fb.com/feed/",
  "categories": [
    "Data Center Engineering",
    "Data Infrastructure",
    "DevInfra",
    "ML Applications",
    "Networking \u0026 Traffic",
    "Open Source"
  ],
  "byline": "By Dan Rabinovitsj, Omar Baldonado",
  "length": 8460,
  "excerpt": "At the Open Compute Project (OCP) Global Summit 2024, we’re showcasing our latest open AI hardware designs with the OCP community. These innovations include a new AI platform, cutting-edge open rac…",
  "siteName": "Engineering at Meta",
  "favicon": "",
  "text": "At the Open Compute Project (OCP) Global Summit 2024, we’re showcasing our latest open AI hardware designs with the OCP community. These innovations include a new AI platform, cutting-edge open rack designs, and advanced network fabrics and components.  By sharing our designs, we hope to inspire collaboration and foster innovation. If you’re passionate about building the future of AI, we invite you to engage with us and OCP to help shape the next generation of open hardware for AI. AI has been at the core of the experiences Meta has been delivering to people and businesses for years, including AI modeling innovations to optimize and improve on features like Feed and our ads system. As we develop and release new, advanced AI models, we are also driven to advance our infrastructure to support our new and emerging AI workloads. For example, Llama 3.1 405B, Meta’s largest model, is a dense transformer with 405B parameters and a context window of up to 128k tokens. To train a large language model (LLM) of this magnitude, with over 15 trillion tokens, we had to make substantial optimizations to our entire training stack. This effort pushed our infrastructure to operate across more than 16,000 NVIDIA H100 GPUs, making Llama 3.1 405B the first model in the Llama series to be trained at such a massive scale.  Prior to Llama, our largest AI jobs ran on 128 NVIDIA A100 GPUs. ​But things have rapidly accelerated. ​Over the course of 2023, we rapidly scaled up our training clusters from 1K, 2K, 4K, to eventually 16K GPUs to support our AI workloads. Today, we’re training our models on two 24K-GPU clusters. We don’t expect this upward trajectory for AI clusters to slow down any time soon. In fact, we expect the amount of compute needed for AI training will grow significantly from where we are today. Building AI clusters requires more than just GPUs. Networking and bandwidth play an important role in ensuring the clusters’ performance. Our systems consist of a tightly integrated HPC compute system and an isolated high-bandwidth compute network that connects all our GPUs and domain-specific accelerators. This design is necessary to meet our injection needs and address the challenges posed by our need for bisection bandwidth. In the next few years, we anticipate greater injection bandwidth on the order of a terabyte per second, per accelerator, with equal normalized bisection bandwidth. This represents a growth of more than an order of magnitude compared to today’s networks! To support this growth, we need a high-performance, multi-tier, non-blocking network fabric that can utilize modern congestion control to behave predictably under heavy load. This will enable us to fully leverage the power of our AI clusters and ensure they continue to perform optimally as we push the boundaries of what is possible with AI. Scaling AI at this speed requires open hardware solutions. Developing new architectures, network fabrics, and system designs is the most efficient and impactful when we can build it on principles of openness. By investing in open hardware, we unlock AI’s full potential and propel ongoing innovation in the field. Introducing Catalina: Open Architecture for AI Infra Catalina front view (left) and rear view (right). Today, we announced the upcoming release of Catalina, our new high-powered rack designed for AI workloads, to the OCP community. Catalina is based on the NVIDIA Blackwell platform full rack-scale solution, with a focus on modularity and flexibility. It is built to support the latest NVIDIA GB200 Grace Blackwell Superchip, ensuring it meets the growing demands of modern AI infrastructure.  The growing power demands of GPUs means open rack solutions need to support higher power capability. With Catalina we’re introducing the Orv3, a high-power rack (HPR) capable of supporting up to 140kW. The full solution is liquid cooled and consists of a power shelf that supports a compute tray, switch tray, the Orv3 HPR, the Wedge 400 fabric switch, a management switch, battery backup unit, and a rack management controller. We aim for Catalina’s modular design to empower others to customize the rack to meet their specific AI workloads while leveraging both existing and emerging industry standards. The Grand Teton Platform now supports AMD accelerators In 2022, we announced Grand Teton, our next-generation AI platform (the follow-up to our Zion-EX platform). Grand Teton is designed with compute capacity to support the demands of memory-bandwidth-bound workloads, such as Meta’s deep learning recommendation models (DLRMs), as well as compute-bound workloads like content understanding. Now, we have expanded the Grand Teton platform to support the AMD Instinct MI300X and will be contributing this new version to OCP. Like its predecessors, this new version of Grand Teton features a single monolithic system design with fully integrated power, control, compute, and fabric interfaces. This high level of integration simplifies system deployment, enabling rapid scaling with increased reliability for large-scale AI inference workloads. In addition to supporting a range of accelerator designs, now including the AMD Instinct MI300x, Grand Teton offers significantly greater compute capacity, allowing faster convergence on a larger set of weights. This is complemented by expanded memory to store and run larger models locally, along with increased network bandwidth to scale up training cluster sizes efficiently. ​Open Disaggregated Scheduled Fabric  Developing open, vendor-agnostic networking backend is going to play an important role going forward as we continue to push the performance of our AI training clusters. Disaggregating our network allows us to work with vendors from across the industry to design systems that are innovative as well as scalable, flexible, and efficient. Our new Disaggregated Scheduled Fabric (DSF) for our next-generation AI clusters offers several advantages over our existing switches. By opening up our network fabric we can overcome limitations in scale, component supply options, and power density. DSF is powered by the open OCP-SAI standard and FBOSS, Meta’s own network operating system for controlling network switches. It also supports an open and standard Ethernet-based RoCE interface to endpoints and accelerators across several GPUS and NICS from several different vendors, including our partners at NVIDIA, Broadcom, and AMD. In addition to DSF, we have also developed and built new 51T fabric switches based on Broadcom and Cisco ASICs. Finally, we are sharing our new FBNIC, a new NIC module that contains our first Meta-design network ASIC. In order to meet the growing needs of our AI  Meta and Microsoft: Driving Open Innovation Together Meta and Microsoft have a long-standing partnership within OCP, beginning with the development of the Switch Abstraction Interface (SAI) for data centers in 2018. Over the years together, we’ve contributed to key initiatives such as the Open Accelerator Module (OAM) standard and SSD standardization, showcasing our shared commitment to advancing open innovation. Our current collaboration focuses on Mount Diablo, a new disaggregated power rack. It’s a cutting-edge solution featuring a scalable 400 VDC unit that enhances efficiency and scalability. This innovative design allows more AI accelerators per IT rack, significantly advancing AI infrastructure. We’re excited to continue our collaboration through this contribution. The open future of AI infra Meta is committed to open source AI. We believe that open source will put the benefits and opportunities of AI into the hands of people all over the word.  AI won’t realize its full potential without collaboration. We need open software frameworks to drive model innovation, ensure portability, and promote transparency in AI development. We must also prioritize open and standardized models so we can leverage collective expertise, make AI more accessible, and work towards minimizing biases in our systems.​ Just as important, we also need open AI hardware systems. These systems are necessary for delivering the kind of high-performance, cost-effective, and adaptable infrastructure necessary for AI advancement.​ We encourage anyone who wants to help advance the future of AI hardware systems to engage with the OCP community. By addressing AI’s infrastructure needs together, we can unlock the true promise of open AI for everyone.​",
  "image": "https://engineering.fb.com/wp-content/uploads/2024/10/Omar-Baldonado-OCP-2024-Keynote-HERO-small.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAt the Open Compute Project (OCP) Global Summit 2024, we’re showcasing our latest open AI hardware designs with the OCP community.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThese innovations include a new AI platform, cutting-edge open rack designs, and advanced network fabrics and components. \u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eBy sharing our designs, we hope to inspire collaboration and foster innovation. If you’re passionate about building the future of AI, we invite you to engage with us and OCP to help shape the next generation of open hardware for AI.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eAI has been at the core of the experiences Meta has been delivering to people and businesses for years, including AI modeling innovations to optimize and improve on features like \u003c/span\u003e\u003ca href=\"https://ai.meta.com/blog/facebook-feed-improvements-ai-show-more-less/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eFeed\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and our \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2024/07/10/data-infrastructure/machine-learning-ml-prediction-robustness-meta/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eads system\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. As we develop and release new, advanced AI models, we are also driven to advance our infrastructure to support our new and emerging AI workloads.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor example, \u003c/span\u003e\u003ca href=\"https://ai.meta.com/blog/meta-llama-3-1/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eLlama 3.1 405B\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, Meta’s largest model, is a dense transformer with 405B parameters and a context window of up to 128k tokens. To train a large language model (LLM) of this magnitude, with over 15 trillion tokens, we had to make substantial optimizations to our entire training stack. This effort pushed our infrastructure to operate across more than 16,000 NVIDIA H100 GPUs, making Llama 3.1 405B the first model in the Llama series to be trained at such a massive scale. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePrior to Llama, our largest AI jobs ran on 128 NVIDIA A100 GPUs. ​But things have rapidly accelerated. ​Over the course of 2023, we rapidly scaled up our training clusters from 1K, 2K, 4K, to eventually 16K GPUs to support our AI workloads. Today, we’re training our models on two\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003e24K-GPU clusters\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe don’t expect this upward trajectory for AI clusters to slow down any time soon. In fact, we expect the amount of compute needed for AI training will grow significantly from where we are today.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBuilding AI clusters requires more than just GPUs. Networking and bandwidth play an important role in ensuring the clusters’ performance. Our systems consist of a tightly integrated HPC compute system and an isolated high-bandwidth compute network that connects all our GPUs and domain-specific accelerators. This design is necessary to meet our injection needs and address the challenges posed by our need for bisection bandwidth.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn the next few years, we anticipate greater injection bandwidth on the order of a terabyte per second, per accelerator, with equal normalized bisection bandwidth. This represents a growth of more than an order of magnitude compared to today’s networks!\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo support this growth, we need a high-performance, multi-tier, non-blocking network fabric that can utilize modern congestion control to behave predictably under heavy load. This will enable us to fully leverage the power of our AI clusters and ensure they continue to perform optimally as we push the boundaries of what is possible with AI.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eScaling AI at this speed requires open hardware solutions. Developing new architectures, network fabrics, and system designs is the most efficient and impactful when we can build it on principles of openness. By investing in open hardware, we unlock AI’s full potential and propel ongoing innovation in the field.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eIntroducing Catalina: Open Architecture for AI Infra\u003c/span\u003e\u003c/h2\u003e\n\u003cfigure id=\"attachment_21841\" aria-describedby=\"caption-attachment-21841\"\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?w=683\" alt=\"\" width=\"456\" height=\"683\" srcset=\"https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png 720w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=611,916 611w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=683,1024 683w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=96,144 96w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=192,288 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-21841\"\u003eCatalina front view (left) and rear view (right).\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eToday, we announced the upcoming release of Catalina, our new high-powered rack designed for AI workloads, to the OCP community. Catalina is based on the \u003ca href=\"https://nvidianews.nvidia.com/news/nvidia-contributes-blackwell-platform-design-to-open-hardware-ecosystem-accelerating-ai-infrastructure-innovation\" target=\"_blank\" rel=\"noopener\"\u003eNVIDIA Blackwell platform full rack-scale solution\u003c/a\u003e, with a focus on modularity and flexibility. It is built to support the latest NVIDIA GB200 Grace Blackwell Superchip, ensuring it meets the growing demands of modern AI infrastructure. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe growing power demands of GPUs means open rack solutions need to support higher power capability. With Catalina we’re introducing the Orv3, a high-power rack (HPR) capable of supporting up to 140kW.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe full solution is liquid cooled and consists of a power shelf that supports a compute tray, switch tray, the Orv3 HPR, the\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2021/11/09/data-center-engineering/ocp-summit-2021/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eWedge 400\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e fabric switch, a management switch, battery backup unit, and a rack management controller.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe aim for Catalina’s modular design to empower others to customize the rack to meet their specific AI workloads while leveraging both existing and emerging industry standards.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe Grand Teton Platform now supports AMD accelerators\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?w=916\" alt=\"\" width=\"600\" height=\"436\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png 1109w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=916,665 916w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=768,557 768w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=1024,743 1024w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=96,70 96w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=192,139 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn 2022, we announced \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eGrand Teton\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, our next-generation AI platform (the follow-up to our Zion-EX platform). Grand Teton is designed with compute capacity to support the demands of memory-bandwidth-bound workloads, such as Meta’s \u003ca href=\"https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/\" target=\"_blank\" rel=\"noopener\"\u003edeep learning recommendation models (\u003c/a\u003e\u003c/span\u003e\u003cspan\u003eDLRMs\u003c/span\u003e\u003cspan\u003e), as well as compute-bound workloads like content understanding.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eNow, we have expanded the Grand Teton platform to support the AMD Instinct MI300X and will be contributing this new version to OCP. Like its predecessors, this new version of Grand Teton\u003c/span\u003e\u003cspan\u003e features a single monolithic system design with fully integrated power, control, compute, and fabric interfaces. This high level of integration simplifies system deployment, enabling rapid scaling with increased reliability for large-scale AI inference workloads.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to supporting a range of accelerator designs, now including the AMD Instinct MI300x, Grand Teton offers significantly greater compute capacity, allowing faster convergence on a larger set of weights. This is complemented by expanded memory to store and run larger models locally, along with increased network bandwidth to scale up training cluster sizes efficiently.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003e​Open Disaggregated Scheduled Fabric \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?w=1024\" alt=\"\" width=\"1024\" height=\"508\" srcset=\"https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png 1871w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=916,454 916w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=768,381 768w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=1024,508 1024w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=1536,762 1536w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=96,48 96w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=192,95 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDeveloping open, vendor-agnostic networking backend is going to play an important role going forward as we continue to push the performance of our AI training clusters. Disaggregating our network allows us to work with vendors from across the industry to design systems that are innovative as well as scalable, flexible, and efficient.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur new \u003ca href=\"https://engineering.fb.com/2024/10/15/data-infrastructure/open-future-networking-hardware-ai-ocp-2024-meta/\" target=\"_blank\" rel=\"noopener\"\u003eDisaggregated Scheduled Fabric (DSF)\u003c/a\u003e for our next-generation AI clusters offers several advantages over our existing switches. By opening up our network fabric we can overcome limitations in scale, component supply options, and power density. DSF is powered by the open\u003c/span\u003e\u003ca href=\"https://github.com/opencomputeproject/SAI\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003e OCP-SAI\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e standard and\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2018/09/04/data-infrastructure/research-in-brief-building-switch-software-at-scale-and-in-the-open/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003e FBOSS\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, Meta’s own network operating system for controlling network switches. It also supports an open and standard Ethernet-based RoCE interface to endpoints and accelerators across several GPUS and NICS from several different vendors, including our partners at NVIDIA, Broadcom, and AMD.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to DSF, we have also developed and built new 51T fabric switches based on Broadcom and Cisco ASICs. Finally, we are sharing our new FBNIC, a new NIC module that contains our first Meta-design network ASIC. In order to meet the growing needs of our AI \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eMeta and Microsoft: Driving Open Innovation Together\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eMeta and Microsoft have a long-standing partnership within OCP, beginning with the development of the \u003c/span\u003e\u003ca href=\"https://www.opencompute.org/documents/switch-abstraction-interface-ocp-specification-v0-2-pdf\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eSwitch Abstraction Interface (SAI)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for data centers in 2018. Over the years together, we’ve contributed to key initiatives such as the \u003c/span\u003e\u003ca href=\"https://www.opencompute.org/blog/new-open-accelerator-infrastructure-oai-sub-project-to-launch-within-the-ocp-server-project\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eOpen Accelerator Module (OAM)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e standard and SSD standardization, showcasing our shared commitment to advancing open innovation.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur current \u003c/span\u003e\u003ca href=\"https://azure.microsoft.com/en-us/blog/accelerating-industry-wide-innovations-in-datacenter-infrastructure-and-security/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ecollaboration focuses on Mount Diablo\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, a new disaggregated power rack. It’s a cutting-edge solution featuring a scalable 400 VDC unit that enhances efficiency and scalability. This innovative design allows more AI accelerators per IT rack, significantly advancing AI infrastructure. We’re excited to continue our collaboration through this contribution.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe open future of AI infra\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eMeta is committed to open source AI\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. We believe that open source will put the benefits and opportunities of AI into the hands of people all over the word. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAI won’t realize its full potential without collaboration. We need open software frameworks to drive model innovation, ensure portability, and promote transparency in AI development. We must also prioritize open and standardized models so we can leverage collective expertise, make AI more accessible, and work towards minimizing biases in our systems.​\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eJust as important, we also need open AI hardware systems. These systems are necessary for delivering the kind of high-performance, cost-effective, and adaptable infrastructure necessary for AI advancement.​\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe encourage anyone who wants to help advance the future of AI hardware systems to engage with the OCP community. By addressing AI’s infrastructure needs together, we can unlock the true promise of open AI for everyone.​\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-10-15T17:00:13Z",
  "modifiedTime": "2024-10-15T17:33:48Z"
}
