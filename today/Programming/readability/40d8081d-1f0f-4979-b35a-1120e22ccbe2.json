{
  "id": "40d8081d-1f0f-4979-b35a-1120e22ccbe2",
  "title": "Break Stuff on Purpose",
  "link": "https://slack.engineering/break-stuff-on-purpose/",
  "description": "“A complex system can fail in an infinite number of ways.” -“Systemantics” by John Gall   Incidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong.  Our team found ourselves in… The post Break Stuff on Purpose appeared first on Engineering at Slack.",
  "author": "Sean Madden",
  "published": "Tue, 10 Dec 2024 09:00:41 +0000",
  "source": "https://slack.engineering/feed",
  "categories": [
    "Uncategorized"
  ],
  "byline": "",
  "length": 7548,
  "excerpt": "“A complex system can fail in an infinite number of ways.” -“Systemantics” by John Gall Incidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong. Our team found ourselves in…",
  "siteName": "Engineering at Slack",
  "favicon": "https://slack.engineering/wp-content/uploads/sites/7/2020/05/cropped-octothrope-1.png?w=192",
  "text": "“A complex system can fail in an infinite number of ways.” -“Systemantics” by John Gall Incidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong.  Our team found ourselves in this position when a service we use internally for dashboards went down, recovery failed, and we lost our teammates configurations. However, with creativity and a dash of mischievousness, we developed an exercise that addressed the cause of the problem, energized our teammates, and brought excitement and fun to the dry job of system maintenance. Come along as we share our journey from incident panic to peace of mind.  The incident Slack engineers use Kibana with Elasticsearch to save custom dashboards and visualizations of important application performance data. On January 29th, 2024, our Kibana cluster—and subsequently, the dashboards—started to fail due to a lack of disk space. We began investigating and realized this was the unfortunate downstream effect of an earlier architectural decision. You can configure Elasticsearch as a stand-alone cluster for Kibana to use, which decouples the object storage from the Kibana application itself. However, our Kibana cluster was configured to use an Elasticsearch instance on the same hosts as the Kibana application. This tied the storage and the application together on the same nodes, and those nodes were now failing. Slack engineers couldn’t load the data they needed to ensure their applications were healthy. Eventually, the cluster got into such a bad state that it couldn’t be saved, and we had to rebuild it from a clean slate. We thought we could stand up a new cluster by cycling in new hosts and restoring the Kibana objects from a backup. However, we were shocked and disappointed to discover our most recent backup was almost two years old. The backup and restore method hadn’t gotten a lot of love after its first configuration, and it didn’t have alerts to tell us if it wasn’t running correctly. On top of that, our runbook was out of date, and the old backup failed when we tried to restore from it. We lost our internal employees’ links and visualizations, we were forced to recreate indexes and index patterns by hand. Explaining to our teammates that our recovery procedure had failed and their data was lost was not fun. We didn’t notice our backups were failing until it was too late.  No one is immune to situations like these. Unless you actively exercise your processes, procedures, and runbooks, they will become obsolete and fail when you need them the most. Incident response is about restoring service as quickly as possible, but what you do when the dust settles determines whether they are ultimately a benefit or a liability. Breaking stuff is fun We were determined to turn this incident into tangible benefits. Our post-incident tasks included making sure that our Elasticsearch clusters in every environment were backed up with a scheduled backup script, fixing our runbooks based on the experience, and checking that the Amazon S3 retention policies were set correctly. We wanted to test our improvements to make sure they worked. Our team came up with an unconventional but exciting idea: we would break one of our development Kibana clusters and try the new backup and restore process. The development cluster is configured similarly to production clusters, and it would provide a realistic environment for testing. To ensure success, we carefully planned which cluster we would break, how we would break it, and how we would restore service. Running the exercise We planned the testing event for a quiet Thursday morning and invited the whole team. Folks showed up energized and delighted at the opportunity to break something at work on purpose. We filled the disk on our Kibana nodes, watched them fail in real time, and successfully triggered our alerts. We worked through the new runbook steps and cycled the entire cluster into a fresh rebuild. Our system recovered successfully from our staged incident. Although the recovery was successful, we fell short of our goal of being able to recover in less than one hour. A lot of the commands in the runbook were not well understood and hard to grok during a stressful incident. Even trying to copy and paste from the runbook was a challenge due to formatting issues. Despite these rough edges, the backups ended up restoring the cluster state completely. Additionally, we found some firewall rules that needed to be added to our infrastructure as code. This was a bonus discovery from running the exercise — we did not expect to find firewall issues, but fixing them saved us future headaches. In a final test of our new recovery process, we migrated the general development Kibana instance and Elasticsearch cluster to run on Kubernetes. This was an excellent opportunity to test our improved backup script on a high-use Kibana cluster. Thanks to our improved understanding of the process, and the updated provisioning scripts, we successfully completed the migration with about 30 minutes of downtime. During both exercises, we ran into minor issues with our new runbooks and restoration process. We spent time figuring out where the runbook was lacking and improved it. Inspired by the exercise, we took it upon ourselves to automate the entire process by updating the scheduled backup script tool to be a full-featured CLI backup and restore program. Now we are able to completely restore a Kibana backup from cloud storage with a single command. “Breaking stuff” wasn’t just fun: it was an incredibly valuable investment of our time to save us from future stress. Chaos is everywhere—might as well use it “Complex systems usually operate in failure mode.” – John Gall Every production system is broken in a way that hasn’t been uncovered yet. Yes, even yours. Take the time and effort to find those issues and plan how to recover from them before it’s critical. Generate a lot of traffic and load test services before customers do. Turn services off to simulate unexpected outages. Upgrade dependencies often. Routine maintenance in software is often neglected because it can be dry and boring, but we pay for it when an incident inevitably hits. We discovered we can make system testing and maintenance exciting and fresh with strategic chaos: planned opportunities to break things. Not only is it simply exciting to diverge from the usual job of fixing, it puts us in unique and realistic situations we would have never discovered if we had approached maintenance the traditional way. We encourage you to take the time to break your own systems. Restore them and then do it again. Each iteration will make the process and tooling better for when you inevitably have to use it in a stressful situation. Finally, remember to celebrate World Backup Day every March 31st. I know we will! Acknowledgments Kyle Sammons – for pairing with me on the planning and execution of the recovery exercise Mark Carey and Renning Bruns – for getting the tooling functioning properly and automating the process Emma Montross, Shelly Wu, and Bryan Burkholder – for incident response and support during the recovery exercise George Luong and Ryan Katkov – for giving us the autonomy to make things better Interested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring! 💼    Apply Now",
  "image": "https://slack.engineering/wp-content/uploads/sites/7/2024/12/pot-breaking.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\u003cp\u003e\u003cspan\u003e “A complex system can fail in an infinite number of ways.”\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e-“Systemantics” by John Gall\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003eIncidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur team found ourselves in this position when a service we use internally for dashboards went down, recovery failed, and we lost our teammates configurations. However, with creativity and a dash of mischievousness, we developed an exercise that addressed the cause of the problem, energized our teammates, and brought excitement and fun to the dry job of system maintenance. Come along as we share our journey from incident panic to peace of mind. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe incident\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eSlack engineers use \u003c/span\u003e\u003ca href=\"https://www.elastic.co/kibana\"\u003e\u003cspan\u003eKibana with Elasticsearch\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to save custom dashboards and visualizations of important application performance data. On January 29th, 2024, our Kibana cluster—and subsequently, the dashboards—started to fail due to a lack of disk space. We began investigating and realized this was the unfortunate downstream effect of an earlier architectural decision. You can configure Elasticsearch as a stand-alone cluster for Kibana to use, which decouples the object storage from the Kibana application itself. However, our Kibana cluster was configured to use an Elasticsearch instance on the same hosts as the Kibana application. This tied the storage and the application together on the same nodes, and those nodes were now failing. Slack engineers couldn’t load the data they needed to ensure their applications were healthy.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEventually, the cluster got into such a bad state that it couldn’t be saved, and we had to rebuild it from a clean slate. We thought we could stand up a new cluster by cycling in new hosts and restoring the Kibana objects from a backup. However, we were shocked and disappointed to discover our most recent backup was almost two years old. The backup and restore method hadn’t gotten a lot of love after its first configuration, and it didn’t have alerts to tell us if it wasn’t running correctly. On top of that, our runbook was out of date, and the old backup failed when we tried to restore from it. We lost our internal employees’ links and visualizations, we were forced to recreate indexes and index patterns by hand.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eExplaining to our teammates that our recovery procedure had failed and their data was lost was not fun. We didn’t notice our backups were failing until it was too late. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eNo one is immune to situations like these. Unless you actively exercise your processes, procedures, and runbooks, they will become obsolete and fail when you need them the most. Incident response is about restoring service as quickly as possible, but what you do when the dust settles determines whether they are ultimately a benefit or a liability.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eBreaking stuff is fun\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe were determined to turn this incident into tangible benefits. Our post-incident tasks included making sure that our Elasticsearch clusters in every environment were backed up with a scheduled backup script, fixing our runbooks based on the experience, and checking that the Amazon S3 retention policies were set correctly.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe wanted to test our improvements to make sure they worked. Our team came up with an unconventional but exciting idea: we would break one of our development Kibana clusters and try the new backup and restore process. The development cluster is configured similarly to production clusters, and it would provide a realistic environment for testing. To ensure success, we carefully planned which cluster we would break, how we would break it, and how we would restore service.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eRunning the exercise\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe planned the testing event for a quiet Thursday morning and invited the whole team. Folks showed up energized and delighted at the opportunity to break something at work on purpose. We filled the disk on our Kibana nodes, watched them fail in real time, and successfully triggered our alerts. We worked through the new runbook steps and cycled the entire cluster into a fresh rebuild. Our system recovered successfully from our staged incident.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAlthough the recovery was successful, we fell short of our goal of being able to recover in less than one hour. A lot of the commands in the runbook were not well understood and hard to grok during a stressful incident. Even trying to copy and paste from the runbook was a challenge due to formatting issues. Despite these rough edges, the backups ended up restoring the cluster state completely. Additionally, we found some firewall rules that needed to be added to our infrastructure as code. This was a bonus discovery from running the exercise \u003c/span\u003e\u003cspan\u003e— \u003c/span\u003e\u003cspan\u003ewe did not expect to find firewall issues, but fixing them saved us future headaches.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn a final test of our new recovery process, we migrated the general development Kibana instance and Elasticsearch cluster to run on Kubernetes. This was an excellent opportunity to test our improved backup script on a high-use Kibana cluster. Thanks to our improved understanding of the process, and the updated provisioning scripts, we successfully completed the migration with about 30 minutes of downtime.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDuring both exercises, we ran into minor issues with our new runbooks and restoration process. We spent time figuring out where the runbook was lacking and improved it. Inspired by the exercise, we took it upon ourselves to automate the entire process by updating the scheduled backup script tool to be a full-featured CLI backup and restore program. Now we are able to completely restore a Kibana backup from cloud storage with a single command. “Breaking stuff” wasn’t just fun: it was an incredibly valuable investment of our time to save us from future stress.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eChaos is everywhere—might as well use it\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003e“Complex systems usually operate in failure mode.”\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e– John Gall\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003eEvery production system is broken in a way that hasn’t been uncovered yet. Yes, even yours. Take the time and effort to find those issues and plan how to recover from them before it’s critical. Generate a lot of traffic and load test services before customers do. Turn services off to simulate unexpected outages. Upgrade dependencies often. Routine maintenance in software is often neglected because it can be dry and boring, but we pay for it when an incident inevitably hits.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe discovered we can make system testing and maintenance exciting and fresh with strategic chaos: planned opportunities to break things. Not only is it simply exciting to diverge from the usual job of fixing, it puts us in unique and realistic situations we would have never discovered if we had approached maintenance the traditional way.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe encourage you to take the time to break your own systems. Restore them and then do it again. Each iteration will make the process and tooling better for when you inevitably have to use it in a stressful situation.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFinally, remember to celebrate World Backup Day every March 31st. I know we will!\u003c/span\u003e\u003c/p\u003e\n\u003ch4\u003e\u003cspan\u003eAcknowledgments\u003c/span\u003e\u003c/h4\u003e\n\u003cp\u003e\u003cb\u003eKyle Sammons\u003c/b\u003e\u003cspan\u003e – for pairing with me on the planning and execution of the recovery exercise\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eMark Carey\u003c/b\u003e\u003cspan\u003e and \u003c/span\u003e\u003cb\u003eRenning Bruns\u003c/b\u003e\u003cspan\u003e – for getting the tooling functioning properly and automating the process\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eEmma Montross\u003c/b\u003e\u003cspan\u003e, \u003c/span\u003e\u003cb\u003eShelly Wu\u003c/b\u003e\u003cspan\u003e, and \u003c/span\u003e\u003cb\u003eBryan Burkholder\u003c/b\u003e\u003cspan\u003e – for incident response and support during the recovery exercise\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eGeorge Luong\u003c/b\u003e\u003cspan\u003e and \u003c/span\u003e\u003cb\u003eRyan Katkov\u003c/b\u003e\u003cspan\u003e – for giving us the autonomy to make things better\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eInterested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring! 💼    \u003c/span\u003e\u003c/i\u003e\u003ca href=\"https://slack.com/careers/dept/engineering\"\u003e\u003cspan\u003eApply Now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\n\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-12-10T09:00:41Z",
  "modifiedTime": "2024-12-09T16:40:12Z"
}
