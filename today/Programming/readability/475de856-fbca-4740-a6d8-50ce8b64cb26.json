{
  "id": "475de856-fbca-4740-a6d8-50ce8b64cb26",
  "title": "Gemini 2.0: Level Up Your Apps with Real-Time Multimodal Interactions",
  "link": "https://developers.googleblog.com/en/gemini-2-0-level-up-your-apps-with-real-time-multimodal-interactions/",
  "description": "The Multimodal Live API for Gemini 2.0 enables real-time multimodal interactions between humans and computers, and can be used to build real-time virtual assistants and adaptive educational tools.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ivan Solovyev",
  "length": 5349,
  "excerpt": "The Multimodal Live API for Gemini 2.0 enables real-time multimodal interactions between humans and computers, and can be used to build real-time virtual assistants and adaptive educational tools.",
  "siteName": "",
  "favicon": "",
  "text": "Products More Solutions Events Learn Community Developer Program Blog Human-to-human communication is naturally multimodal, involving a mix of spoken words, visual cues, and real-time adjustments. With the Multimodal Live API for Gemini we've achieved this same level of naturalness in human-computer interaction. Imagine AI conversations that feel more interactive, where you can use visual inputs and receive context-aware solutions in real-time, seamlessly blending text, audio, and video. The Multimodal Live API for Gemini 2.0 enables this type of interaction and is available in Google AI Studio and Gemini API. This technology allows you to build applications that respond to the world as it happens, leveraging real-time data.How it worksThe Multimodal Live API is a stateful API utilizing WebSockets to facilitate low-latency, server-to-server communication. This API supports tools such as function calling, code execution, search grounding, and the combination of multiple tools within a single request, enabling comprehensive responses without the need for multiple prompts. This allows developers to create more efficient and complex AI interactions.Key features of the Multimodal Live API include:Bidirectional streaming: Allows for concurrent sending and receiving of text, audio and video data.Sub-second latency: Outputs the first token in 600 milliseconds aligning reaction times with human expectation for seamless response.Natural voice conversations: Supports human-like voice interactions, including the ability to interrupt and features like voice activity detection, enabling more fluid dialogue with AI.Video understanding: Provides the ability to process and understand video input, enabling the model to combine both audio and video contexts for a more informed and nuanced response. This contextual awareness brings another layer of richness to the interaction.Tool integration: Facilitates the integration of multiple tools within a single API call, extending the API's capabilities and allowing it to perform actions on behalf of the user to solve complex tasks.Steerable voices: Offers a selection of five distinct voices with a high level of expressiveness, capable of conveying a wide spectrum of emotions. This allows for a more personalized and engaging user experience.Multimodal live streaming in ActionThe Multimodal Live API enables a variety of real-time, interactive applications. Here are a few examples of use cases where this API can be effectively applied:Real-Time Virtual Assistants: Imagine an assistant that observes your screen and offers tailored advice in real-time, telling you where to find what you are looking for or executing actions or your behalf.Adaptive Educational Tools: The API supports the development of educational applications that can adapt to a student's learning pace, for example, a language learning app could adjust the difficulty of exercises based on a student's real-time pronunciation and comprehension.To help you explore this new functionality and kick start your own exploration we've created a bunch of demo applications showcasing realtime streaming capabilities:A starter web application for streaming mic, camera or screen input. A perfect base for your creativity: Full code and a getting started guide available on Github: https://github.com/google-gemini/multimodal-live-api-web-console.Chat with Gemini about the weather. Select a location and have a gemini powered character explaining the weather in that location. You can interrupt and ask a follow up question anytime. Getting Started with the Multimodal Live APIReady to dive in? Experiment with Multimodal Live Streaming directly in Google AI Studio for a hands-on experience. Or, for full control, grab the detailed documentation and code samples to start building with the API today.We've also partnered with Daily, to provide a seamless integration via their pipecat framework, enabling you to add real-time capabilities to your apps effortlessly. Daily.co, creators of the pipecat framework, is a video and audio API platform that makes it easy for developers to add real-time video and audio streaming to their websites and apps. Check out Daily's integration guide to get started building.We're excited to see your creations - share your feedback and the amazing applications you build with the new API!",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Stream-meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\n        \n        \n        \n\n        \n\n\t\t\t\t\n        \n\n\n\n\n\u003cdiv top-level-nav=\"\"\u003e\n  \u003cnav aria-label=\"Side menu\"\u003e\n    \n    \u003cdiv\u003e\n        \u003cul\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/products\" data-label=\"Tab: Products\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Products\n             \u003c/span\u003e\n            \u003c/a\u003e\n            \u003cul\u003e\n              \u003cli\u003e\n                \u003cspan tabindex=\"0\" data-label=\"More Products\"\u003e\n                  \u003cspan menu=\"Products\"\u003e\n                    More\n                  \u003c/span\u003e\n                  \u003cspan menu=\"Products\"\u003e\n                    \n                  \u003c/span\u003e\n                \u003c/span\u003e\n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/solutions/catalog\" data-label=\"Tab: Solutions\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Solutions\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/events\" data-label=\"Tab: Events\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Events\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/learn\" data-label=\"Tab: Learn\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Learn\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/community\" data-label=\"Tab: Community\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Community\n             \u003c/span\u003e\n            \u003c/a\u003e\n            \u003cul\u003e\n              \u003cli\u003e\n                \n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/profile/u/me\" data-label=\"Tab: Developer Program\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Developer Program\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.googleblog.com/\" data-label=\"Tab: Blog\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Blog\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/div\u003e\n  \u003c/nav\u003e\n  \u003c/div\u003e\n\n\n\n        \n  \u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"7qnwr\"\u003eHuman-to-human communication is naturally multimodal, involving a mix of spoken words, visual cues, and real-time adjustments. With the Multimodal Live API for Gemini we\u0026#39;ve achieved this same level of naturalness in human-computer interaction. Imagine AI conversations that feel more interactive, where you can use visual inputs and receive context-aware solutions in real-time, seamlessly blending text, audio, and video. The Multimodal Live API for Gemini 2.0 enables this type of interaction and is available in Google AI Studio and Gemini API. This technology allows you to build applications that respond to the world as it happens, leveraging real-time data.\u003c/p\u003e\u003ch2 data-block-key=\"995hv\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eHow it works\u003c/h2\u003e\u003cp data-block-key=\"1m7ug\"\u003eThe Multimodal Live API is a stateful API utilizing WebSockets to facilitate low-latency, server-to-server communication. This API supports tools such as function calling, code execution, search grounding, and the combination of multiple tools within a single request, enabling comprehensive responses without the need for multiple prompts. This allows developers to create more efficient and complex AI interactions.\u003c/p\u003e\u003cp data-block-key=\"8andg\"\u003eKey features of the Multimodal Live API include:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"bf9rg\"\u003eBidirectional streaming: Allows for concurrent sending and receiving of text, audio and video data.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6k42e\"\u003eSub-second latency: Outputs the first token in 600 milliseconds aligning reaction times with human expectation for seamless response.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"5bscp\"\u003eNatural voice conversations: Supports human-like voice interactions, including the ability to interrupt and features like voice activity detection, enabling more fluid dialogue with AI.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"75ihk\"\u003eVideo understanding: Provides the ability to process and understand video input, enabling the model to combine both audio and video contexts for a more informed and nuanced response. This contextual awareness brings another layer of richness to the interaction.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"embef\"\u003eTool integration: Facilitates the integration of multiple tools within a single API call, extending the API\u0026#39;s capabilities and allowing it to perform actions on behalf of the user to solve complex tasks.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4s9ph\"\u003eSteerable voices: Offers a selection of five distinct voices with a high level of expressiveness, capable of conveying a wide spectrum of emotions. This allows for a more personalized and engaging user experience.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"fh0cc\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eMultimodal live streaming in Action\u003c/h2\u003e\u003cp data-block-key=\"dkujn\"\u003eThe Multimodal Live API enables a variety of real-time, interactive applications. Here are a few examples of use cases where this API can be effectively applied:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"732g1\"\u003eReal-Time Virtual Assistants: Imagine an assistant that observes your screen and offers tailored advice in real-time, telling you where to find what you are looking for or executing actions or your behalf.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"dpni8\"\u003eAdaptive Educational Tools: The API supports the development of educational applications that can adapt to a student\u0026#39;s learning pace, for example, a language learning app could adjust the difficulty of exercises based on a student\u0026#39;s real-time pronunciation and comprehension.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"53tkl\"\u003eTo help you explore this new functionality and kick start your own exploration we\u0026#39;ve created a bunch of demo applications showcasing realtime streaming capabilities:\u003c/p\u003e\u003cp data-block-key=\"4j5bt\"\u003eA starter web application for streaming mic, camera or screen input. A perfect base for your creativity:\u003c/p\u003e\n\u003c/div\u003e    \u003cdiv\u003e\n    \u003cp data-block-key=\"tgj1c\"\u003eFull code and a getting started guide available on Github: \u003ca href=\"https://github.com/google-gemini/multimodal-live-api-web-console\"\u003ehttps://github.com/google-gemini/multimodal-live-api-web-console\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"953b6\"\u003e\u003cbr/\u003eChat with Gemini about the weather. Select a location and have a gemini powered character explaining the weather in that location. You can interrupt and ask a follow up question anytime.\u003c/p\u003e\n\u003c/div\u003e    \u003cdiv\u003e\n    \u003ch2 data-block-key=\"tgj1c\"\u003eGetting Started with the Multimodal Live API\u003c/h2\u003e\u003cp data-block-key=\"df4t0\"\u003eReady to dive in? Experiment with Multimodal Live Streaming directly in Google AI Studio for a hands-on experience. Or, for full control, grab the detailed \u003ca href=\"https://ai.google.dev/api/multimodal-live\"\u003edocumentation\u003c/a\u003e and \u003ca href=\"https://github.com/google-gemini/cookbook\"\u003ecode samples\u003c/a\u003e to start building with the API today.\u003c/p\u003e\u003cp data-block-key=\"a9du5\"\u003eWe\u0026#39;ve also partnered with Daily, to provide a seamless integration via their \u003ca href=\"https://github.com/pipecat-ai/pipecat\"\u003epipecat\u003c/a\u003e framework, enabling you to add real-time capabilities to your apps effortlessly. \u003ca href=\"http://daily.co/\"\u003eDaily.co\u003c/a\u003e, creators of the \u003ca href=\"https://github.com/pipecat-ai/pipecat\"\u003epipecat\u003c/a\u003e framework, is a video and audio API platform that makes it easy for developers to add real-time video and audio streaming to their websites and apps. Check out Daily\u0026#39;s \u003ca href=\"https://www.daily.co/products/gemini/multimodal-live-api/\"\u003eintegration guide\u003c/a\u003e to get started building.\u003c/p\u003e\u003cp data-block-key=\"7inpo\"\u003eWe\u0026#39;re excited to see your creations - share your feedback and the amazing applications you build with the new API!\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\n\n\n\t\t\t\t\n\t\t\t\t\n\n\n\n\n\n        \n\t\t\t\t\n\n        \n        \n        \n        \n\n        \n\n        \n  \n\n    \n\n\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-12-23T00:00:00Z",
  "modifiedTime": null
}
