{
  "id": "f93a1365-84f1-49c5-98ff-513cc2582807",
  "title": "Hello Deep Learning",
  "link": "https://robertheaton.com/hello-deep-learning/",
  "description": "Introductions to deep learning are too complicated and spend too much time trying to thrill you with details and real-world applications. This makes them a frustrating place to start. You already know that deep learning is amazing and that it actually works on real problems. You know that most of the hard work in industry is in the data cleaning. You don’t want to set up a new environment, or play with parameters, or get dirty in the data. The actual first thing you want to do is to train a model, as soon as possible, and it doesn’t matter how simple it is. Once you’ve trained your own model you’d be more than happy to learn about overfitting, data cleaning, and splitting strategies as well. But first you just want to create something yourself and see it work. Hello Deep Learning is the missing introduction to deep learning. It’s a series of challenges, each of which gives you a task and a perfect, synthetic dataset and asks you to train and play with a trivial model. The challenges cover image generation, text classification, and tabular data, and each one: Runs on your laptop Trains in a few seconds Uses perfect, noiseless, synthetic data that takes seconds to generate Has absolutely no sidebars Hello Deep Learning allows you to rapidly experiment with simple models and take your first steps in a calm, kindhearted environment. It gets you ready to leap into the detail and chaos of the real-world. You can get the challenges, data generation scripts, and setup instructions on GitHub. Let me know how you get on; if they’re useful then I’ll make more! The challenges 1. Image classification Challenge Train a classifier that distinguishes between red squares and yellow circles. Your program should be able to: Train a model that can distinguish between squares and circles Use it to run a few individual predictions on specific images Display the confusion matrix Data generation The repo includes a script that generates 200 images of circles and 200 of rectangles and saves them in the data/shapes/ directory. Tips I did this using a fastai vision_learner based on the resnet18 pretrained model. I mostly copied and stitched together code snippets from chapter 2 of the fastai book 2. Text classification Challenge Train a classifier that distinguishes between text inputs of positive and negative words, for example \"happy chirpy awesome\" and \"awful terrible heinous\". Your program should be able to: Train a model that can distinguish between this type of positive and negative input Use it to run a few individual predictions on specific inputs Data generation The repo includes a script that generates 1000 text files containing positive words and 1000 containing negative words and saves them in the data/sentiment_text/ directory. Tips I did this using a fastai language_model_learner based on the AWD_LSTM pretrained model, and a fastai text_classifier_learner. I copied and stitched together code snippets from chapter 10 of the fastai book 3. Decision trees Challenge Train decision trees that reverse-engineer the rules from src/generators/random_tabular.py that were used to randomly generate a tabular dataset. Your program should be able to Train a decision tree that reverse-engineers the rules Train a random forest that reverse-engineers the rules Uses these models it to run a few individual predictions on specific inputs Calculates the RMS error on a validation set Visualises the decision tree, using (for example) the dtreeviz library Data generation The repo contains a script that generates 1 JSON file containing 10,000 data points and saves it in the data/random_tabular/data.json file.. Each data point contains: 6 features: a, b, c, d, e, and f. Each of these is a random integer between 0 and 100. 1 label: y. This label is derived deterministically from the features using simple rules contained in src/generators/random_tabular.py . Tips I did this using an sklearn DecisionTreeRegressor and RandomForestRegressor. I copied and stitched together code snippets from chapter 9 of the fastai book My solutions My solutions are in src/examples/ in the repo, although they’re not the only way to solve the challenges, and they’re almost certainly not the best way to solve them either. Get the challenges, data generation scripts, and setup instructions on GitHub. Let me know how you get on; if they’re useful then I’ll make more!",
  "author": "",
  "published": "Fri, 13 Oct 2023 00:00:00 +0000",
  "source": "https://robertheaton.com/feed.xml",
  "categories": null,
  "byline": "",
  "length": 4481,
  "excerpt": "Introductions to deep learning are too complicated and spend too much time trying to thrill you with details and real-world applications.",
  "siteName": "Robert Heaton",
  "favicon": "https://robertheaton.com/images/fav.png",
  "text": "Introductions to deep learning are too complicated and spend too much time trying to thrill you with details and real-world applications. This makes them a frustrating place to start. You already know that deep learning is amazing and that it actually works on real problems. You know that most of the hard work in industry is in the data cleaning. You don’t want to set up a new environment, or play with parameters, or get dirty in the data. The actual first thing you want to do is to train a model, as soon as possible, and it doesn’t matter how simple it is. Once you’ve trained your own model you’d be more than happy to learn about overfitting, data cleaning, and splitting strategies as well. But first you just want to create something yourself and see it work. Hello Deep Learning is the missing introduction to deep learning. It’s a series of challenges, each of which gives you a task and a perfect, synthetic dataset and asks you to train and play with a trivial model. The challenges cover image generation, text classification, and tabular data, and each one: Runs on your laptop Trains in a few seconds Uses perfect, noiseless, synthetic data that takes seconds to generate Has absolutely no sidebars Hello Deep Learning allows you to rapidly experiment with simple models and take your first steps in a calm, kindhearted environment. It gets you ready to leap into the detail and chaos of the real-world. You can get the challenges, data generation scripts, and setup instructions on GitHub. Let me know how you get on; if they’re useful then I’ll make more! The challenges 1. Image classification Challenge Train a classifier that distinguishes between red squares and yellow circles. Your program should be able to: Train a model that can distinguish between squares and circles Use it to run a few individual predictions on specific images Display the confusion matrix Data generation The repo includes a script that generates 200 images of circles and 200 of rectangles and saves them in the data/shapes/ directory. Tips I did this using a fastai vision_learner based on the resnet18 pretrained model. I mostly copied and stitched together code snippets from chapter 2 of the fastai book 2. Text classification Challenge Train a classifier that distinguishes between text inputs of positive and negative words, for example \"happy chirpy awesome\" and \"awful terrible heinous\". Your program should be able to: Train a model that can distinguish between this type of positive and negative input Use it to run a few individual predictions on specific inputs Data generation The repo includes a script that generates 1000 text files containing positive words and 1000 containing negative words and saves them in the data/sentiment_text/ directory. Tips I did this using a fastai language_model_learner based on the AWD_LSTM pretrained model, and a fastai text_classifier_learner. I copied and stitched together code snippets from chapter 10 of the fastai book 3. Decision trees Challenge Train decision trees that reverse-engineer the rules from src/generators/random_tabular.py that were used to randomly generate a tabular dataset. Your program should be able to Train a decision tree that reverse-engineers the rules Train a random forest that reverse-engineers the rules Uses these models it to run a few individual predictions on specific inputs Calculates the RMS error on a validation set Visualises the decision tree, using (for example) the dtreeviz library Data generation The repo contains a script that generates 1 JSON file containing 10,000 data points and saves it in the data/random_tabular/data.json file.. Each data point contains: 6 features: a, b, c, d, e, and f. Each of these is a random integer between 0 and 100. 1 label: y. This label is derived deterministically from the features using simple rules contained in src/generators/random_tabular.py . Tips I did this using an sklearn DecisionTreeRegressor and RandomForestRegressor. I copied and stitched together code snippets from chapter 9 of the fastai book My solutions My solutions are in src/examples/ in the repo, although they’re not the only way to solve the challenges, and they’re almost certainly not the best way to solve them either. Get the challenges, data generation scripts, and setup instructions on GitHub. Let me know how you get on; if they’re useful then I’ll make more!",
  "image": "https://robertheaton.com/images/hello-deep-learning-cover.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eIntroductions to deep learning are too complicated and spend too much time trying to thrill you with details and real-world applications.\u003c/p\u003e\n\n\u003cp\u003eThis makes them a frustrating place to start. You already know that deep learning is amazing and that it actually works on real problems. You know that most of the hard work in industry is in the data cleaning. You don’t want to set up a new environment, or play with parameters, or get dirty in the data.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://robertheaton.com/images/hello-deep-learning-cover.png\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThe actual first thing you want to do is to train a model, as soon as possible, and it doesn’t matter how simple it is. Once you’ve trained your own model you’d be more than happy to learn about overfitting, data cleaning, and splitting strategies as well. But first you just want to create something yourself and see it work.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/robert/hello-deep-learning/\"\u003e\u003cem\u003eHello Deep Learning\u003c/em\u003e\u003c/a\u003e is the missing introduction to deep learning. It’s a series of challenges, each of which gives you a task and a perfect, synthetic dataset and asks you to train and play with a trivial model. The challenges cover image generation, text classification, and tabular data, and each one:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eRuns on your laptop\u003c/li\u003e\n  \u003cli\u003eTrains in a few seconds\u003c/li\u003e\n  \u003cli\u003eUses perfect, noiseless, synthetic data that takes seconds to generate\u003c/li\u003e\n  \u003cli\u003eHas absolutely no sidebars\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/robert/hello-deep-learning/\"\u003e\u003cem\u003eHello Deep Learning\u003c/em\u003e\u003c/a\u003e allows you to rapidly experiment with simple models and take your first steps in a calm, kindhearted environment. It gets you ready to leap into the detail and chaos of the real-world.\u003c/p\u003e\n\n\u003cp\u003eYou can get the challenges, data generation scripts, and setup instructions \u003ca href=\"https://github.com/robert/hello-deep-learning/\"\u003eon GitHub\u003c/a\u003e. Let me know how you get on; if they’re useful then I’ll make more!\u003c/p\u003e\n\n\u003ch2 id=\"the-challenges\"\u003eThe challenges\u003c/h2\u003e\n\n\u003ch3 id=\"1-image-classification\"\u003e1. Image classification\u003c/h3\u003e\n\n\u003ch4 id=\"challenge\"\u003eChallenge\u003c/h4\u003e\n\n\u003cp\u003eTrain a classifier that distinguishes between red squares and yellow circles. Your program should be able to:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eTrain a model that can distinguish between squares and circles\u003c/li\u003e\n  \u003cli\u003eUse it to run a few individual predictions on specific images\u003c/li\u003e\n  \u003cli\u003eDisplay the confusion matrix\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch4 id=\"data-generation\"\u003eData generation\u003c/h4\u003e\n\n\u003cp\u003eThe repo includes a \u003ca href=\"https://github.com/robert/hello-deep-learning/blob/master/src/generators/shape_images.py\"\u003escript\u003c/a\u003e that generates 200 images of circles and 200 of rectangles and saves them in the \u003ccode\u003edata/shapes/\u003c/code\u003e directory.\u003c/p\u003e\n\n\u003ch4 id=\"tips\"\u003eTips\u003c/h4\u003e\n\n\u003cul\u003e\n  \u003cli\u003eI did this using a fastai \u003ccode\u003evision_learner\u003c/code\u003e based on the \u003ccode\u003eresnet18\u003c/code\u003e pretrained model.\u003c/li\u003e\n  \u003cli\u003eI mostly copied and stitched together code snippets from \u003ca href=\"https://github.com/fastai/fastbook/blob/master/02_production.ipynb\"\u003echapter 2 of the fastai book\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"2-text-classification\"\u003e2. Text classification\u003c/h3\u003e\n\n\u003ch4 id=\"challenge-1\"\u003eChallenge\u003c/h4\u003e\n\n\u003cp\u003eTrain a classifier that distinguishes between text inputs of positive and negative words, for example \u003ccode\u003e\u0026#34;happy chirpy awesome\u0026#34;\u003c/code\u003e and \u003ccode\u003e\u0026#34;awful terrible heinous\u0026#34;\u003c/code\u003e. Your program should be able to:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eTrain a model that can distinguish between this type of positive and negative input\u003c/li\u003e\n  \u003cli\u003eUse it to run a few individual predictions on specific inputs\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch4 id=\"data-generation-1\"\u003eData generation\u003c/h4\u003e\n\n\u003cp\u003eThe repo includes \u003ca href=\"https://github.com/robert/hello-deep-learning/blob/master/src/generators/sentiment_text.py\"\u003ea script\u003c/a\u003e that generates 1000 text files containing positive words and 1000 containing negative words and saves them in the \u003ccode\u003edata/sentiment_text/\u003c/code\u003e directory.\u003c/p\u003e\n\n\u003ch4 id=\"tips-1\"\u003eTips\u003c/h4\u003e\n\n\u003cul\u003e\n  \u003cli\u003eI did this using a fastai \u003ccode\u003elanguage_model_learner\u003c/code\u003e based on the \u003ccode\u003eAWD_LSTM\u003c/code\u003e pretrained model, and a fastai \u003ccode\u003etext_classifier_learner\u003c/code\u003e.\u003c/li\u003e\n  \u003cli\u003eI copied and stitched together code snippets from \u003ca href=\"https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb\"\u003echapter 10 of the fastai book\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"3-decision-trees\"\u003e3. Decision trees\u003c/h3\u003e\n\n\u003ch4 id=\"challenge-2\"\u003eChallenge\u003c/h4\u003e\n\n\u003cp\u003eTrain decision trees that reverse-engineer the rules from \u003ca href=\"https://github.com/robert/hello-deep-learning/blob/master/src/generators/random_tabular.py\"\u003esrc/generators/random_tabular.py\u003c/a\u003e that were used to randomly generate a tabular dataset. Your program should be able to\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eTrain a decision tree that reverse-engineers the rules\u003c/li\u003e\n  \u003cli\u003eTrain a random forest that reverse-engineers the rules\u003c/li\u003e\n  \u003cli\u003eUses these models it to run a few individual predictions on specific inputs\u003c/li\u003e\n  \u003cli\u003eCalculates the RMS error on a validation set\u003c/li\u003e\n  \u003cli\u003eVisualises the decision tree, using (for example) the \u003ca href=\"https://github.com/parrt/dtreeviz\"\u003e\u003ccode\u003edtreeviz\u003c/code\u003e\u003c/a\u003e library\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch4 id=\"data-generation-2\"\u003eData generation\u003c/h4\u003e\n\n\u003cp\u003eThe repo contains \u003ca href=\"https://github.com/robert/hello-deep-learning/blob/master/src/generators/random_tabular.py\"\u003ea script\u003c/a\u003e that generates 1 JSON file containing 10,000 data points and saves it in the \u003ccode\u003edata/random_tabular/data.json\u003c/code\u003e file.. Each data point contains:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e6 features: \u003ccode\u003ea\u003c/code\u003e, \u003ccode\u003eb\u003c/code\u003e, \u003ccode\u003ec\u003c/code\u003e, \u003ccode\u003ed\u003c/code\u003e, \u003ccode\u003ee\u003c/code\u003e, and \u003ccode\u003ef\u003c/code\u003e. Each of these is a random integer between 0 and 100.\u003c/li\u003e\n  \u003cli\u003e1 label: \u003ccode\u003ey\u003c/code\u003e. This label is derived deterministically from the features using simple rules contained in  \u003ca href=\"https://github.com/robert/hello-deep-learning/blob/master/src/generators/random_tabular.py\"\u003esrc/generators/random_tabular.py\u003c/a\u003e .\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"tips-2\"\u003eTips\u003c/h4\u003e\n\n\u003cul\u003e\n  \u003cli\u003eI did this using an sklearn \u003ccode\u003eDecisionTreeRegressor\u003c/code\u003e and \u003ccode\u003eRandomForestRegressor\u003c/code\u003e.\u003c/li\u003e\n  \u003cli\u003eI copied and stitched together code snippets from \u003ca href=\"https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\"\u003echapter 9 of the fastai book\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"my-solutions\"\u003eMy solutions\u003c/h2\u003e\n\n\u003cp\u003eMy solutions are in \u003ca href=\"https://github.com/robert/hello-deep-learning/blob/master/src/examples/\"\u003e\u003ccode\u003esrc/examples/\u003c/code\u003e\u003c/a\u003e in the repo, although they’re not the only way to solve the challenges, and they’re almost certainly not the best way to solve them either.\u003c/p\u003e\n\n\u003cp\u003eGet the challenges, data generation scripts, and setup instructions \u003ca href=\"https://github.com/robert/hello-deep-learning/\"\u003eon GitHub\u003c/a\u003e. Let me know how you get on; if they’re useful then I’ll make more!\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": null,
  "modifiedTime": null
}
