{
  "id": "f3a3b1e6-2d32-4783-af04-ba09445d4ae4",
  "title": "Unlocking Efficiency and Performance: Navigating the Spark 3 and EMR 6 Upgrade Journey at Slack",
  "link": "https://slack.engineering/unlocking-efficiency-and-performance-navigating-the-spark-3-and-emr-6-upgrade-journey-at-slack/",
  "description": "Slack Data Engineering recently underwent data workload migration from AWS EMR 5 (Spark 2/Hive 2 processing engine) to EMR 6 (Spark 3 processing engine). In this blog, we will share our migration journey, challenges, and the performance gains we observed in the process. This blog aims to assist Data Engineers, Data Infrastructure Engineers, and Product […] The post Unlocking Efficiency and Performance: Navigating the Spark 3 and EMR 6 Upgrade Journey at Slack appeared first on Slack Engineering.",
  "author": "Nilanjana Mukherjee",
  "published": "Tue, 02 Jul 2024 09:00:00 +0000",
  "source": "https://slack.engineering/feed",
  "categories": [
    "Uncategorized",
    "analytics",
    "aws",
    "big-data",
    "data-engineering",
    "data-infrastructure",
    "spark",
    "sql"
  ],
  "byline": "Nilanjana Mukherjee Staff Software Engineer, Metrics Foundations",
  "length": 13236,
  "excerpt": "Slack Data Engineering recently underwent data workload migration from AWS EMR 5 (Spark 2/Hive 2 processing engine) to EMR 6 (Spark 3 processing engine). In this blog, we will share our migration journey, challenges, and the performance gains we observed in the process. This blog aims to assist Data Engineers, Data Infrastructure Engineers, and Product …",
  "siteName": "Slack Engineering",
  "favicon": "https://slack.engineering/wp-content/uploads/sites/7/2020/05/cropped-octothrope-1.png?w=192",
  "text": "Slack Data Engineering recently underwent data workload migration from AWS EMR 5 (Spark 2/Hive 2 processing engine) to EMR 6 (Spark 3 processing engine). In this blog, we will share our migration journey, challenges, and the performance gains we observed in the process. This blog aims to assist Data Engineers, Data Infrastructure Engineers, and Product Managers who may be considering migrating to EMR 6/Spark 3. In Data Engineering, our primary objective is to support internal teams—such as Product Engineering, Machine Learning, and Data Science—by providing essential datasets and a reliable data infrastructure to facilitate the creation of their own datasets. We ensure the reliability and timeliness of critical billing and usage data for our clients. Maintaining Landing Time SLAs (Service Level Agreements) serves as a measure to keep up these promises. Over time, the rapid expansion of our data volume frequently led to the violation of our critical data pipeline’s SLAs. As we sought alternatives to Spark 2 and Hive 2, Spark 3 emerged as a compelling solution for all our data processing needs, notably due to its  Adaptive Query Execution (AQE) feature that could improve performance for some of our skewed datasets. We embarked on this EMR 6/Spark 3 migration due to enhanced performance, enhanced security—with updated log4j libraries—and the potential for significant cost savings. This year-long project consisted of two major phases: Phase 1: Upgrade EMR from 5.3x to 6.x. Phase 2: Upgrade from Hive 2.x/Spark 2.x to Spark 3.x. Migration journey Current landscape  We at Slack Data Engineering use a federated AWS EMR cluster model to manage all data analytics requirements. The data that lives in the data warehouse is physically stored in S3 and its metadata is stored in Hive Metastore schema on an RDS database. SQL handles most of our use cases. Additionally, we rely on Scala/PySpark for certain complex workloads. We use Apache Airflow to orchestrate our workflows and have designed custom Spark Airflow operators for submitting SparkSQL, PySpark and Scala jobs to the EMR cluster via Livy Batches API using authenticated HTTP requests. Here is an example of our hierarchical custom Airflow Spark operators: BaseAirflowOperator → SparkBaseAirflowOperator → CustomPySparkAirflowOperator or CustomSparkSqlAirflowOperator Here is an example of how we use CustomSparkSqlAirflowOperator to schedule Airflow task: Below is a pictorial representation of all the components working together: Our data warehouse infrastructure comprises over 60 EMR clusters, catering to the needs of over 40 teams and supporting thousands of Airflow Directed Acyclic Graphs (DAGs). Prior to this migration, all workloads were executed on EMR 5.36, Spark 2.4.8, and Hive 2.3.9. Migration challenges As the majority of our workloads were managed by Hive 2, making the transition to Hive 3 in EMR 6 was the preferred choice for our internal customers due to minimal changes required in the codebase. However, we opted to consolidate into a single compute engine, Spark 3. This strategic decision was made to leverage Spark 3 Adaptive Query Execution (AQE) feature, develop expertise in Spark 3 across our teams, and fine-tune Hadoop clusters exclusively for Spark operations for efficiency. Given the scale of this migration, a phased approach was essential. Thus, we decided to support both AWS EMR 5 and EMR 6 versions until the migration was complete, allowing us to transition workloads without disrupting roadmaps for existing teams. However, maintaining two different cluster settings (Hive 2.x/Spark 2.x in EMR 5.x and Spark 3.x in EMR 6) presented several challenges for us: How can we support the same Hive catalog across Spark 2/Spark 3 workloads? How can we provision different versions of EMR clusters? How can we control cost? How can we support different versions of our job libraries across these clusters? How can we submit and route jobs across these different versions of clusters? Pre-migration planning Hive catalog migration How can we support the same Hive catalog across Spark 2/Spark 3 workloads? We needed to use the same Hive Metastore catalog for our workloads across EMR 5/Spark 2 and EMR 6/Spark 3 as migration of our pipelines from Spark 2 to Spark 3 would take multiple quarters. We solved this problem by migrating our existing HMS 2.3.0 catalog to HMS 3.1.0 catalog, using Hive Schema Tool. We executed the following commands on the EMR 5 master host connected to the catalog database. Before migration we took backups of our Hive Metastore database, and also took some downtime on job processing during migration for schema upgrade. Post schema upgrade both our EMR 5 and EMR 6 clusters could talk to the same upgraded HMS 3 catalog DB as it was backward compatible with Hive 2 and Spark 2 applications. EMR cluster provisioning How can we provision different versions of EMR clusters? How can we control cost?  We use EMR’s golang SDK to launch EMR clusters via the RunJobFlow api. This API accepts a JSON-based launch configuration for an EMR cluster. We maintain a base JSON config for all clusters and override custom parameters like InstanceFleets, Capacity, and Release Label at the cluster configuration level. We created specific EMR 6 configurations for new EMR 6 clusters with auto-scaling enabled and low minimum capacity to keep costs under control. During the process of migration, we created more such EMR 6 cluster configurations for each new cluster. We regulated the capacity and overall cluster usage costs by gradually reducing EMR 5 fleet size and increasing EMR 6 fleets based on usage. Job builds across different Spark versions How can we support different versions of our job libraries across these clusters? We use Bazel as the primary tool to build our codebase. Using Bazel, we implemented parallel build streams for Spark JARs across versions 2.x and 3.x. We propagated all ongoing config changes to both Spark 2 and Spark 3 JARs for consistency. Enabling the build --config=spark3 flag in the .bazelrc file allowed building local JARs with the required version for testing. In our airflow pipelines, as we migrated jobs to EMR 6, the airflow operator would pick Spark 3 jars automatically based on the flag approach described below. Airflow operators enhancement How can we submit and route jobs across these different versions of clusters? We enhanced our custom Airflow Spark operator to route jobs to different versions of clusters by using a boolean flag. This flag offered the convenience of submitting jobs to either pre-migration and post-migration cluster by a simple toggle. Additionally we introduced four logical groups of Spark config sizing options (SMALL, DEFAULT, LARGE and EXTRA_LARGE) embedded in the Airflow Spark operator. Each option has its own executor memory, driver memory, and executor ranges. Sizing options helped some of our end users to migrate existing Hive jobs with minimal understanding of Spark configurations. This is an example of our enhanced CustomSparkSqlAirflowOperator: Code changes  For most cases, the existing Hive and Spark 2 code ran fine in Spark 3. There were few cases where we had to make changes to the code to make it Spark 3 compatible. One example of a code change from Hive to Spark 3 would be the use of a salting function for skewed joins. While some code used bulky subqueries to generate salt keys, others used RAND() in the joining key as a workaround for handling skew. While RAND() in the joining key works in Hive, it throws an error in Spark 3: org.apache.spark.sql.AnalysisException: nondeterministic expressions are only allowed in Project, Filter, Aggregate, or Window. We removed all skew-handling code and let Spark 3’s Adaptive Query Execution (AQE) take care of the data skew. More about AQE in the ‘Migration gain and impact’ section. Additionally, Spark 3 threw errors for certain data type casting scenarios that worked well in Spark 2. We had to change the default value of a few Spark 3 configurations. One example is setting spark.sql.storeAssignmentPolicy to ‘Legacy’ instead of default Spark 3 value ‘ANSI’. We faced a few instances where the Spark 3 job inferred the schema from the Hive Metastore but failed to consolidate schemas, erroring with java.lang.StackOverflowError.  This occurred due to a lack of synchronization between the underlying Parquet data and the Hive metastore schema. By setting spark.sql.hive.convertMetastoreParquet to False, we successfully resolved the issue. Post-migration data validation  We compared two tables: prod_table_hive2_or_spark2 (EMR 5 table) test_table_spark3 (EMR 6 table) We aimed for an exact data match between the tables rather than relying on sampling, particularly because some of our data, such as customer billing data, is mission-critical. We used config files and macros to enable our SQL script to read from the production schema and write to the test schema in the test environment. This helped us to populate the exact prod data in the test schema using Spark 3 for easy comparison. We then ran except and count SQL queries between prod_table_hive2_or_spark2 and test_table_spark3 in Trino to speed up the validation process. In case of mismatch in except or count query output, we used our in-house Python framework with the Trino engine for detailed analysis. We continuously monitored post migration production runtime of our pipelines using Airflow metadata DB tables and tuned pipelines as required. There were few sources of uncertainties in the validation process. For example: When the code relied on the current timestamp, it caused variations between production and development runs. We excluded timestamp related columns while validating those tables. Random rows appeared when there’s no differentiable order by clause in the code to resolve ties. We fixed the code to have a differentiable order by clause for future. Discrepancies appeared in the behavior of certain built-in functions between Hive and Spark. For instance, functions like Greatest, which is used to return the greatest value of the list of arguments, exhibit different behavior when one of the arguments is NULL. We made code changes to adhere to the correct business logic. Migration gain and impact After migration, we observed substantial runtime performance improvements across the majority of our pipeline tasks. Most of our Airflow tasks showed improvements ranging from 30% to 60%, with some jobs experiencing an impressive 90% boost in runtime efficiency. We used Airflow metadata DB tables (duration column in task_instance table) to get runtime comparison numbers. Here is an example of how the runtime of one of our critical tasks improved significantly post migration: EMR 6 EMRFS S3-optimized committer fixed the problem of incomplete writes and misleading SUCCESS statuses for some of our Spark jobs that handled text-based input and output format. It also improves application performance by avoiding list and rename operations done in S3 during job and task commit phases. Prior to EMR 6.4.0, this feature was only available for Apache Parquet file format. From EMR 6.4.0 it was extended to all common formats, including parquet, ORC, and text-based formats (including CSV and JSON). As expected, we noticed several Adaptive Query Execution(AQE) improvements in the query execution plan. One of the key improvements was dynamically optimizing skew join. This helped us to remove several lines of skew handling logic from our codebase and replace them by simple join condition between the keys. Below is an example which shows AQE (skew=true) hint in the query plan. Another improvement was in dynamically coalescing shuffle partitions. This feature simplified the tuning of the shuffle partition number by selecting the correct shuffle partition number at runtime. We only had to provide a large enough initial shuffle partition number using spark.sql.adaptive.coalescePartitions.initialPartitionNum configuration. Below is a query plan which shows partition count going from 3000 to 348 using AQE. Conclusion The migration to EMR 6 has resulted in significant improvement in the runtime performance, efficiency, and reliability of our data processing pipelines. AQE improvements, such as dynamically optimizing skew joins and coalescing shuffle partitions, have simplified query optimization and reduced the need for manual intervention in tuning parameters. S3-optimized committer has addressed issues related to incomplete writes and misleading statuses in Spark jobs, leading to improved stability. The entire process of migration described here ran quite smoothly and did not cause any incidents in any of the steps! We improved our pipeline codebase along the way, making it easier for new engineers to onboard on a clean foundation and work entirely off Spark 3 engine. The migration has also laid the foundation for a more scalable lakehouse with availability of modern table formats like Iceberg and Hudi in EMR 6. We recommend data organizations to invest in such long-term modernization projects as it brings efficiencies across the board. Interested in joining our Data Engineering team? Apply now",
  "image": "https://slack.engineering/wp-content/uploads/sites/7/2024/07/entering_a_room_for_migration_left_door_says_apache_spark_right_door_says_aws_emr_science_fiction-1.jpeg?w=640",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"content-area\"\u003e\n\t\t\u003cmain id=\"primary\"\u003e\n\t\t\t\u003carticle id=\"post-16763\"\u003e\n\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\t\t\u003cp\u003eSlack Data Engineering recently underwent data workload migration from \u003ca href=\"https://aws.amazon.com/emr/\"\u003eAWS\u003c/a\u003e \u003ca href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html\"\u003eEMR 5\u003c/a\u003e (Spark 2/\u003ca href=\"https://hive.apache.org/\"\u003eHive 2\u003c/a\u003e processing engine) to \u003ca href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-6x.html\"\u003eEMR 6 \u003c/a\u003e(\u003ca href=\"https://spark.apache.org/news/spark-3-0-0-released.html\"\u003eSpark 3\u003c/a\u003e processing engine). In this blog, we will share our migration journey, challenges, and the performance gains we observed in the process. This blog aims to assist Data Engineers, Data Infrastructure Engineers, and Product Managers who may be considering migrating to EMR 6/Spark 3.\u003c/p\u003e\n\u003cp\u003eIn Data Engineering, our primary objective is to support internal teams—such as Product Engineering, Machine Learning, and Data Science—by providing essential datasets and a reliable data infrastructure to facilitate the creation of their own datasets. We ensure the reliability and timeliness of critical billing and usage data for our clients. Maintaining Landing Time SLAs (Service Level Agreements) serves as a measure to keep up these promises\u003cb\u003e.\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003eOver time, the rapid expansion of our data volume frequently led to the violation of our critical data pipeline’s SLAs. As we sought alternatives to Spark 2 and Hive 2, Spark 3 emerged as a compelling solution for all our data processing needs, notably due to its  \u003ca href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html\"\u003eAdaptive Query Execution (AQE)\u003c/a\u003e feature that could improve performance for some of our skewed datasets. We embarked on this EMR 6/Spark 3 migration due to enhanced performance, enhanced security—with updated \u003ca href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-log4j-vulnerability.html\"\u003elog4j\u003c/a\u003e libraries—and the potential for significant \u003ca href=\"https://aws.amazon.com/blogs/big-data/upgrade-amazon-emr-hive-metastore-from-5-x-to-6-x/\"\u003ecost\u003c/a\u003e savings.\u003c/p\u003e\n\u003cp\u003eThis year-long project consisted of two major phases:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cb\u003ePhase 1\u003c/b\u003e: Upgrade EMR from 5.3x to 6.x.\u003c/li\u003e\n\u003cli\u003e\u003cb\u003ePhase 2\u003c/b\u003e: Upgrade from Hive 2.x/Spark 2.x to Spark 3.x.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cstrong\u003eMigration journey\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" width=\"1280\" height=\"1280\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/07/entering_a_room_for_migration_left_door_says_apache_spark_right_door_says_aws_emr_science_fiction-1.jpeg?w=640\" alt=\"\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/07/entering_a_room_for_migration_left_door_says_apache_spark_right_door_says_aws_emr_science_fiction-1.jpeg 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/07/entering_a_room_for_migration_left_door_says_apache_spark_right_door_says_aws_emr_science_fiction-1.jpeg?resize=160,160 160w, https://slack.engineering/wp-content/uploads/sites/7/2024/07/entering_a_room_for_migration_left_door_says_apache_spark_right_door_says_aws_emr_science_fiction-1.jpeg?resize=640,640 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/07/entering_a_room_for_migration_left_door_says_apache_spark_right_door_says_aws_emr_science_fiction-1.jpeg?resize=768,768 768w\" sizes=\"(max-width: 959px) 688px, (max-width: 1023px) 768px, 1172px\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eCurrent landscape \u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eWe at Slack Data Engineering use a federated AWS \u003ca href=\"https://aws.amazon.com/emr\"\u003eEMR\u003c/a\u003e cluster model to manage all data analytics requirements. The data that lives in the data warehouse is physically stored in \u003ca href=\"https://aws.amazon.com/pm/serv-s3/?gclid=Cj0KCQjwxeyxBhC7ARIsAC7dS39RjvF_KIxEF_GegmFglhJi6k5YOKB5B4OPDm0e57XpvfIyMAJ43ZUaAm2HEALw_wcB\u0026amp;trk=20e04791-939c-4db9-8964-ee54c41bc6ad\u0026amp;sc_channel=ps\u0026amp;ef_id=Cj0KCQjwxeyxBhC7ARIsAC7dS39RjvF_KIxEF_GegmFglhJi6k5YOKB5B4OPDm0e57XpvfIyMAJ43ZUaAm2HEALw_wcB:G:s\u0026amp;s_kwcid=AL!4422!3!651751060962!e!!g!!amazon%20s3%20account!19852662362!145019251177\"\u003eS3\u003c/a\u003e and its metadata is stored in \u003ca href=\"https://hive.apache.org/\"\u003eHive Metastore\u003c/a\u003e schema on an \u003ca href=\"https://aws.amazon.com/rds/\"\u003eRDS\u003c/a\u003e database. SQL handles most of our use cases. Additionally, we rely on Scala/PySpark for certain complex workloads. We use \u003ca href=\"https://airflow.apache.org/\"\u003eApache Airflow\u003c/a\u003e to orchestrate our workflows and have designed custom Spark Airflow operators for submitting SparkSQL, PySpark and Scala jobs to the EMR cluster via \u003ca href=\"https://livy.incubator.apache.org/docs/latest/rest-api.html\"\u003eLivy\u003c/a\u003e Batches API using authenticated HTTP requests.\u003c/p\u003e\n\u003cp\u003eHere is an example of our hierarchical custom Airflow Spark operators:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eBaseAirflowOperator → SparkBaseAirflowOperator → CustomPySparkAirflowOperator or CustomSparkSqlAirflowOperator\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eHere is an example of how we use CustomSparkSqlAirflowOperator to schedule Airflow task:\u003c/p\u003e\n\u003cp\u003e\u003cimg decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/25.png?w=640\" alt=\"\" width=\"512\" height=\"177\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/25.png 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/25.png?resize=640,222 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/25.png?resize=768,266 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/25.png?resize=1280,443 1280w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003cp\u003eBelow is a pictorial representation of all the components working together:\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/12.png?w=640\" alt=\"\" width=\"656\" height=\"414\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/12.png 1602w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/12.png?resize=640,403 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/12.png?resize=768,484 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/12.png?resize=1280,807 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/12.png?resize=1536,968 1536w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003cp\u003eOur data warehouse infrastructure comprises over 60 EMR clusters, catering to the needs of over 40 teams and supporting thousands of Airflow Directed Acyclic Graphs (DAGs). Prior to this migration, all workloads were executed on EMR 5.36, Spark 2.4.8, and Hive 2.3.9.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eMigration challenges\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eAs the majority of our workloads were managed by Hive 2, making the transition to Hive 3 in EMR 6 was the preferred choice for our internal customers due to minimal changes required in the codebase. However, we opted to consolidate into a single compute engine, Spark 3. This strategic decision was made to leverage Spark 3 \u003ca href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\"\u003eAdaptive Query Execution\u003c/a\u003e (AQE) feature, develop expertise in Spark 3 across our teams, and fine-tune Hadoop clusters exclusively for Spark operations for efficiency.\u003c/p\u003e\n\u003cp\u003eGiven the scale of this migration, a phased approach was essential. Thus, we decided to support both AWS EMR 5 and EMR 6 versions until the migration was complete, allowing us to transition workloads without disrupting roadmaps for existing teams.\u003c/p\u003e\n\u003cp\u003eHowever, maintaining two different cluster settings \u003cb\u003e(Hive 2.x/Spark 2.x \u003c/b\u003ein\u003cb\u003e EMR 5.x \u003c/b\u003eand\u003cb\u003e Spark 3.x \u003c/b\u003ein\u003cb\u003e EMR 6\u003c/b\u003e) presented several challenges for us:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHow can we support the same Hive catalog across Spark 2/Spark 3 workloads?\u003c/li\u003e\n\u003cli\u003eHow can we provision different versions of EMR clusters?\u003c/li\u003e\n\u003cli\u003eHow can we control cost?\u003c/li\u003e\n\u003cli\u003eHow can we support different versions of our job libraries across these clusters?\u003c/li\u003e\n\u003cli\u003eHow can we submit and route jobs across these different versions of clusters?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\u003cstrong\u003ePre-migration planning\u003c/strong\u003e\u003c/h3\u003e\n\u003ch4\u003eHive catalog migration\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eHow can we support the same Hive catalog across Spark 2/Spark 3 workloads? \u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe needed to use the same Hive Metastore catalog for our workloads across EMR 5/Spark 2 and EMR 6/Spark 3 as migration of our pipelines from Spark 2 to Spark 3 would take multiple quarters. We solved this problem by migrating our existing HMS 2.3.0 catalog to HMS 3.1.0 catalog, using \u003ca href=\"https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool\"\u003eHive Schema Tool\u003c/a\u003e. We executed the following commands on the EMR 5 master host connected to the catalog database.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/26.png?w=640\" alt=\"\" width=\"665\" height=\"88\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/26.png 1830w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/26.png?resize=640,85 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/26.png?resize=768,102 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/26.png?resize=1280,169 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/26.png?resize=1536,203 1536w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/code\u003e\u003ccode\u003e\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003eBefore migration we took backups of our Hive Metastore database, and also took some downtime on job processing during migration for schema upgrade.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/13.png?w=640\" alt=\"\" width=\"663\" height=\"247\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/13.png 1520w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/13.png?resize=640,238 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/13.png?resize=768,286 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/13.png?resize=1280,477 1280w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003cp\u003ePost schema upgrade both our EMR 5 and EMR 6 clusters could talk to the same upgraded HMS 3 catalog DB as it was backward compatible with Hive 2 and Spark 2 applications.\u003c/p\u003e\n\u003ch4\u003eEMR cluster provisioning\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eHow can we provision different versions of EMR clusters? How can we control cost? \u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe use EMR’s golang \u003ca href=\"https://docs.aws.amazon.com/sdk-for-go/api/service/emr/\"\u003eSDK\u003c/a\u003e to launch EMR clusters via the \u003ca href=\"https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html\"\u003eRunJobFlow\u003c/a\u003e api. This API accepts a JSON-based launch configuration for an EMR cluster. We maintain a base JSON config for all clusters and override custom parameters like \u003ccode\u003eInstanceFleets\u003c/code\u003e, \u003ccode\u003eCapacity\u003c/code\u003e, and \u003ccode\u003eRelease Label\u003c/code\u003e at the cluster configuration level. We created specific EMR 6 configurations for new EMR 6 clusters with auto-scaling enabled and low minimum capacity to keep costs under control. During the process of migration, we created more such EMR 6 cluster configurations for each new cluster. We regulated the capacity and overall cluster usage costs by gradually reducing EMR 5 fleet size and increasing EMR 6 fleets based on usage.\u003c/p\u003e\n\u003ch4\u003eJob builds across different Spark versions\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eHow can we support different versions of our job libraries across these clusters?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe use \u003ca href=\"https://bazel.build/\"\u003eBazel\u003c/a\u003e as the primary tool to build our codebase. Using Bazel, we implemented parallel build streams for Spark JARs across versions 2.x and 3.x. We propagated all ongoing config changes to both Spark 2 and Spark 3 JARs for consistency. Enabling the build \u003ccode\u003e--config=spark3\u003c/code\u003e flag in the .bazelrc file allowed building local JARs with the required version for testing. In our airflow pipelines, as we migrated jobs to EMR 6, the airflow operator would pick Spark 3 jars automatically based on the flag approach described below.\u003c/p\u003e\n\u003ch4\u003eAirflow operators enhancement\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eHow can we submit and route jobs across these different versions of clusters?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe enhanced our custom Airflow Spark operator to route jobs to different versions of clusters by using a boolean flag. This flag offered the convenience of submitting jobs to either pre-migration and post-migration cluster by a simple toggle.\u003c/p\u003e\n\u003cp\u003eAdditionally we introduced four logical groups of Spark config sizing options (SMALL, DEFAULT, LARGE and EXTRA_LARGE) embedded in the Airflow Spark operator. Each option has its own executor memory, driver memory, and executor ranges. Sizing options helped some of our end users to migrate existing Hive jobs with minimal understanding of Spark configurations.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png?w=640\" alt=\"\" width=\"714\" height=\"140\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png 2426w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png?resize=640,126 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png?resize=768,151 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png?resize=1280,251 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png?resize=1536,301 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png?resize=2048,402 2048w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/33.png?resize=1920,377 1920w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003cp\u003eThis is an example of our enhanced CustomSparkSqlAirflowOperator:\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/24.png?w=640\" alt=\"\" width=\"497\" height=\"216\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/24.png 1704w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/24.png?resize=640,278 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/24.png?resize=768,334 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/24.png?resize=1280,556 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/24.png?resize=1536,667 1536w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003eCode changes \u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eFor most cases, the existing Hive and Spark 2 code ran fine in Spark 3. There were few cases where we had to make changes to the code to make it Spark 3 compatible.\u003c/p\u003e\n\u003cp\u003eOne example of a code change from Hive to Spark 3 would be the use of a salting function for skewed joins. While some code used bulky subqueries to generate salt keys, others used RAND() in the joining key as a workaround for handling skew. While RAND() in the joining key works in Hive, it throws an error in Spark 3: \u003ci\u003eorg.apache.spark.sql.AnalysisException: nondeterministic expressions are only allowed in Project, Filter, Aggregate, or Window. \u003c/i\u003eWe removed all skew-handling code and let Spark 3’s \u003ca href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\"\u003eAdaptive Query Execution\u003c/a\u003e (AQE) take care of the data skew. More about AQE in the ‘\u003cb\u003eMigration gain and impact\u003c/b\u003e’ section.\u003c/p\u003e\n\u003cp\u003eAdditionally, Spark 3 threw errors for certain data type casting scenarios that worked well in Spark 2. We had to change the default value of a few Spark 3 configurations. One example is setting \u003ca href=\"https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html\"\u003e\u003cb\u003espark.sql.storeAssignmentPolicy\u003c/b\u003e\u003c/a\u003e to ‘Legacy’ instead of default Spark 3 value ‘ANSI’.\u003c/p\u003e\n\u003cp\u003eWe faced a few instances where the Spark 3 job inferred the schema from the Hive Metastore but failed to consolidate schemas, erroring with \u003ccode\u003ejava.lang.StackOverflowError\u003c/code\u003e.  This occurred due to a lack of synchronization between the underlying Parquet data and the Hive metastore schema. By setting \u003ca href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\"\u003espark.sql.hive.convertMetastoreParquet\u003c/a\u003e to \u003ccode\u003eFalse\u003c/code\u003e, we successfully resolved the issue.\u003c/p\u003e\n\u003ch3\u003e\u003cstrong\u003ePost-migration data validation \u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eWe compared two tables:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eprod_table_hive2_or_spark2 (EMR 5 table)\u003c/li\u003e\n\u003cli\u003etest_table_spark3 (EMR 6 table)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe aimed for an exact data match between the tables rather than relying on sampling, particularly because some of our data, such as customer billing data, is mission-critical.\u003c/p\u003e\n\u003cp\u003eWe used config files and macros to enable our SQL script to read from the production schema and write to the test schema in the test environment. This helped us to populate the exact prod data in the test schema using Spark 3 for easy comparison. We then ran \u003ccode\u003eexcept\u003c/code\u003e and \u003ccode\u003ecount\u003c/code\u003e SQL queries between prod_table_hive2_or_spark2 and test_table_spark3 in \u003ca href=\"https://trino.io/\"\u003eTrino\u003c/a\u003e to speed up the validation process.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/27.png?w=640\" alt=\"\" width=\"470\" height=\"220\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/27.png 1660w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/27.png?resize=640,300 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/27.png?resize=768,360 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/27.png?resize=1280,600 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/27.png?resize=1536,720 1536w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/14.png?w=640\" alt=\"\" width=\"512\" height=\"358\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/14.png 1672w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/14.png?resize=640,447 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/14.png?resize=768,536 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/14.png?resize=1280,894 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/14.png?resize=1536,1073 1536w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003cp\u003eIn case of mismatch in \u003ccode\u003eexcept\u003c/code\u003e or \u003ccode\u003ecount\u003c/code\u003e query output, we used our in-house Python framework with the Trino engine for detailed analysis. We continuously monitored post migration production runtime of our pipelines using Airflow metadata DB tables and tuned pipelines as required.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/11.png?w=640\" alt=\"\" width=\"590\" height=\"346\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/11.png 1972w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/11.png?resize=640,376 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/11.png?resize=768,451 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/11.png?resize=1280,752 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/11.png?resize=1536,902 1536w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/11.png?resize=1920,1127 1920w\" sizes=\"(max-width: 479px) 90vw, (max-width: 599px) 432px, 536px\"/\u003e\u003c/p\u003e\n\u003cp\u003eThere were few sources of uncertainties in the validation process. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen the code relied on the current timestamp, it caused variations between production and development runs. We excluded timestamp related columns while validating those tables.\u003c/li\u003e\n\u003cli\u003eRandom rows appeared when there’s no differentiable \u003ccode\u003eorder by\u003c/code\u003e clause in the code to resolve ties. We fixed the code to have a differentiable \u003ccode\u003eorder by\u003c/code\u003e clause for future.\u003c/li\u003e\n\u003cli\u003eDiscrepancies appeared in the behavior of certain built-in functions between Hive and Spark. For instance, functions like \u003ca href=\"https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.greatest.html\"\u003eGreatest\u003c/a\u003e, which is used to return the greatest value of the list of arguments, exhibit different behavior when one of the arguments is \u003ccode\u003eNULL\u003c/code\u003e. We made code changes to adhere to the correct business logic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cstrong\u003eMigration gain and impact\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eAfter migration, we observed substantial runtime performance improvements across the majority of our pipeline tasks. Most of our Airflow tasks showed improvements ranging from 30% to 60%, with some jobs experiencing an impressive 90% boost in runtime efficiency. We used Airflow \u003ca href=\"https://airflow.apache.org/docs/apache-airflow/stable/database-erd-ref.html\"\u003emetadata DB\u003c/a\u003e tables (\u003ccode\u003eduration\u003c/code\u003e column in task_instance table) to get runtime comparison numbers. Here is an example of how the runtime of one of our critical tasks improved significantly post migration:\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1588\" height=\"988\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/15.png?w=640\" alt=\"\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/15.png 1588w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/15.png?resize=640,398 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/15.png?resize=768,478 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/15.png?resize=1280,796 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/15.png?resize=1536,956 1536w\" sizes=\"(max-width: 959px) 688px, (max-width: 1023px) 768px, 1172px\"/\u003e\u003c/p\u003e\n\u003cp\u003eEMR 6 \u003ca href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-s3-optimized-committer.html\"\u003eEMRFS S3-optimized committer\u003c/a\u003e fixed the problem of incomplete writes and misleading SUCCESS statuses for some of our Spark jobs that handled text-based input and output format. It also improves application performance by avoiding list and rename operations done in S3 during job and task commit phases. Prior to EMR 6.4.0, this feature was only available for Apache Parquet file format. From EMR 6.4.0 it was extended to all common formats, including parquet, ORC, and text-based formats (including CSV and JSON).\u003c/p\u003e\n\u003cp\u003eAs expected, we noticed several \u003ca href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html\"\u003eAdaptive Query Execution(AQE)\u003c/a\u003e improvements in the query execution plan. One of the key improvements was dynamically optimizing skew join. This helped us to remove several lines of skew handling logic from our codebase and replace them by simple join condition between the keys. Below is an example which shows AQE (skew=true) hint in the query plan.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1672\" height=\"1428\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/16.png?w=640\" alt=\"\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/16.png 1672w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/16.png?resize=640,547 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/16.png?resize=768,656 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/16.png?resize=1280,1093 1280w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/16.png?resize=1536,1312 1536w\" sizes=\"(max-width: 959px) 688px, (max-width: 1023px) 768px, 1172px\"/\u003e\u003c/p\u003e\n\u003cp\u003eAnother improvement was in dynamically coalescing shuffle partitions. This feature simplified the tuning of the shuffle partition number by selecting the correct shuffle partition number at runtime. We only had to provide a large enough initial shuffle partition number using \u003ca href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html\"\u003e\u003cb\u003espark.sql.adaptive.coalescePartitions.initialPartitionNum\u003c/b\u003e\u003c/a\u003e configuration. Below is a query plan which shows partition count going from 3000 to 348 using AQE.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1418\" height=\"1010\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/18.png?w=640\" alt=\"\" srcset=\"https://slack.engineering/wp-content/uploads/sites/7/2024/06/18.png 1418w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/18.png?resize=640,456 640w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/18.png?resize=768,547 768w, https://slack.engineering/wp-content/uploads/sites/7/2024/06/18.png?resize=1280,912 1280w\" sizes=\"(max-width: 959px) 688px, (max-width: 1023px) 768px, 1172px\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eThe migration to EMR 6 has resulted in significant improvement in the runtime performance, efficiency, and reliability of our data processing pipelines.\u003c/p\u003e\n\u003cp\u003eAQE improvements, such as dynamically optimizing skew joins and coalescing shuffle partitions, have simplified query optimization and reduced the need for manual intervention in tuning parameters. S3-optimized committer has addressed issues related to incomplete writes and misleading statuses in Spark jobs, leading to improved stability. The entire process of migration described here ran quite smoothly and did not cause any incidents in any of the steps! We improved our pipeline codebase along the way, making it easier for new engineers to onboard on a clean foundation and work entirely off Spark 3 engine. The migration has also laid the foundation for a more scalable lakehouse with availability of modern table formats like Iceberg and Hudi in EMR 6. We recommend data organizations to invest in such long-term modernization projects as it brings efficiencies across the board.\u003c/p\u003e\n\t\t\u003cp\u003e\n\t\t\tInterested in joining our Data Engineering team?\t\t\t\u003ca href=\"https://slack.com/careers\" target=\"_blank\" data-clog-click=\"\" data-clog-trigger=\"trigger=\" data-clog-ui-element=\"\" data-clog-ui-component=\"\"\u003eApply now\u003c/a\u003e\n\t\t\u003c/p\u003e\n\t\n\t\t\t\t\t\u003c/div\u003e\n\t\t\t\t\n\t\t\t\u003c/article\u003e\n\n\t\t\n\t\t\u003c/main\u003e\n\n\t\t\n\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": "2024-07-02T09:00:00Z",
  "modifiedTime": "2024-07-02T20:20:04Z"
}
