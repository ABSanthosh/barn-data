{
  "id": "bcd39534-6878-48dd-8cf7-3fa4cc0489cd",
  "title": "QCon SF 2024 - Scaling Large Language Model Serving Infrastructure at Meta",
  "link": "https://www.infoq.com/news/2024/11/qconsf-scaling-meta/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "At QCon SF 2024, Ye (Charlotte) Qi of Meta tackled the complexities of scaling large language model (LLM) infrastructure, highlighting the \"AI Gold Rush\" challenge. She emphasized efficient hardware integration, latency optimization, and production readiness, alongside Meta's innovative approaches like hierarchical caching and automation to enhance AI performance and reliability. By Andrew Hoblitzell",
  "author": "Andrew Hoblitzell",
  "published": "Tue, 26 Nov 2024 17:36:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Artificial Intelligence",
    "Machine Learning",
    "QCon San Francisco 2024",
    "Facebook",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Andrew Hoblitzell",
  "length": 3881,
  "excerpt": "At QCon SF 2024, Ye (Charlotte) Qi of Meta tackled the complexities of scaling large language model (LLM) infrastructure, highlighting the \u0026quot;AI Gold Rush\u0026quot; challenge. She emphasized efficient hardware i",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241126075425/apple-touch-icon.png",
  "text": "At QCon San Francisco Conference 2024, Ye (Charlotte) Qi from Meta spoke about scaling large language model (LLM) serving infrastructure. Her talk explored the complexities of deploying LLMs, underscoring the unique challenges posed by their size, computational demands, and integration into production systems. Qi framed the current landscape as an \"AI Gold Rush,\" where organizations are grappling with unprecedented compute demands and resource constraints. Deploying LLMs at scale requires not only fitting models onto hardware but also optimizing their performance and cost. She emphasized that the work involves not just infrastructure techniques but also close collaboration with model developers to achieve end-to-end optimization. One of the first challenges addressed was the need to fit models onto hardware efficiently. LLMs, especially those with billions of parameters, often exceed the capacity of a single GPU. Meta employs tensor parallelism and pipeline parallelism to partition models across GPUs and nodes. She explained that understanding hardware constraints and runtime requirements is critical, as mismatches between model architecture and hardware can drastically limit performance. \"Don't just grab your training runtime or your favorite framework. Find a runtime specialized for inference serving and understand your AI problem deeply to pick the right optimizations.\" – Qi Performance optimization emerged as another focal point. Qi discussed how first token latency and overall generation throughput are key metrics for real-time applications. Techniques like continuous batching help improve responsiveness and throughput. Quantization, the practice of reducing model precision to unlock hardware efficiency, was highlighted as a major lever for performance gains, often achieving 2–4x improvements. The transition from prototype to production revealed a new layer of challenges. Real-world applications experience fluctuating workloads, latency requirements, and fault tolerance needs. Qi emphasized that scaling LLMs is not just about deploying larger clusters of GPUs but also managing the intricate trade-offs between latency, reliability, and cost. Disaggregated deployments, hierarchical caching, and request scheduling all play crucial roles in maintaining performance under production conditions. Qi shared Meta's approach to handling production-specific issues, such as caching strategies tailored to LLM workloads. Hierarchical caching systems, where common data is stored in high-speed memory tiers and less-used data in slower tiers, significantly reduce latency and resource consumption. She also detailed how consistent hashing ensures related requests are routed to the same host, maximizing cache hit rates. Qi underscored the importance of automation and observability, highlighting Meta’s investment in tools that benchmark performance, optimize resource allocation, and monitor system behavior. She described Meta's custom deployment solver, which integrates auto-scaling and placement logic to meet demand while minimizing costs. Qi emphasized the importance of stepping back to see the bigger picture when scaling AI infrastructure. By adopting this broader perspective, businesses can identify more effective approaches that deliver real value and focus their resources on these priorities. This mindset also clarifies which efforts yield meaningful results during continuous evaluation, allowing organizations to refine their systems at every stage for sustained performance and reliability. Developers interested in learning more about Qi’s presentation may watch the InfoQ website where a video of her presentation will be available in the coming weeks. About the Author Andrew Hoblitzell",
  "image": "https://res.infoq.com/news/2024/11/qconsf-scaling-meta/en/headerimage/generatedHeaderImage-1732474209067.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eAt \u003cspan\u003e\u003cspan data-testid=\"comment-base-item-66865\"\u003eQCon San Francisco Conference 2024\u003c/span\u003e\u003c/span\u003e, \u003ca href=\"https://qconsf.com/speakers/yecharlotteqi\"\u003eYe (Charlotte) Qi\u003c/a\u003e from Meta spoke about scaling large language model (LLM) serving infrastructure. Her talk explored the complexities of deploying LLMs, underscoring the unique challenges posed by their size, computational demands, and integration into production systems.\u003c/p\u003e\n\n\u003cp\u003eQi framed the current landscape as an \u0026#34;AI Gold Rush,\u0026#34; where organizations are grappling with unprecedented compute demands and resource constraints. Deploying LLMs at scale requires not only fitting models onto hardware but also optimizing their performance and cost. She emphasized that the work involves not just infrastructure techniques but also close collaboration with model developers to achieve end-to-end optimization.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/11/qconsf-scaling-meta/en/resources/1IMG_1872-1732470903223.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/qconsf-scaling-meta/en/resources/1IMG_1872-1732470903223.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eOne of the first challenges addressed was the need to fit models onto hardware efficiently. LLMs, especially those with billions of parameters, often exceed the capacity of a single GPU. Meta employs \u003ca href=\"https://pytorch.org/docs/stable/distributed.tensor.parallel.html\"\u003etensor parallelism\u003c/a\u003e and \u003ca href=\"https://pytorch.org/docs/stable/distributed.pipelining.html\"\u003epipeline parallelism\u003c/a\u003e to partition models across GPUs and nodes. She explained that understanding hardware constraints and runtime requirements is critical, as mismatches between model architecture and hardware can drastically limit performance.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#34;Don\u0026#39;t just grab your training runtime or your favorite framework. Find a runtime specialized for inference serving and understand your AI problem deeply to pick the right optimizations.\u0026#34; – Qi\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003ePerformance optimization emerged as another focal point. Qi discussed how first token latency and overall generation throughput are key metrics for real-time applications. Techniques like \u003ca href=\"https://insujang.github.io/2024-01-07/llm-inference-continuous-batching-and-pagedattention/\"\u003econtinuous batching\u003c/a\u003e help improve responsiveness and throughput. \u003ca href=\"https://www.ibm.com/think/topics/quantization\"\u003eQuantization\u003c/a\u003e, the practice of reducing model precision to unlock hardware efficiency, was highlighted as a major lever for performance gains, often achieving 2–4x improvements.\u003c/p\u003e\n\n\u003cp\u003eThe transition from prototype to production revealed a new layer of challenges. Real-world applications experience fluctuating workloads, latency requirements, and fault tolerance needs. Qi emphasized that scaling LLMs is not just about deploying larger clusters of GPUs but also managing the intricate trade-offs between latency, reliability, and cost. Disaggregated deployments, hierarchical caching, and request scheduling all play crucial roles in maintaining performance under production conditions.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"\" data-src=\"news/2024/11/qconsf-scaling-meta/en/resources/1IMG_1888-1732470903223.jpg\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2024/11/qconsf-scaling-meta/en/resources/1IMG_1888-1732470903223.jpg\" rel=\"share\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eQi shared Meta\u0026#39;s approach to handling production-specific issues, such as caching strategies tailored to LLM workloads. Hierarchical caching systems, where common data is stored in high-speed memory tiers and less-used data in slower tiers, significantly reduce latency and resource consumption. She also detailed how consistent hashing ensures related requests are routed to the same host, maximizing cache hit rates.\u003c/p\u003e\n\n\u003cp\u003eQi underscored the importance of automation and observability, highlighting Meta’s investment in tools that benchmark performance, optimize resource allocation, and monitor system behavior. She described Meta\u0026#39;s custom deployment solver, which integrates auto-scaling and placement logic to meet demand while minimizing costs.\u003c/p\u003e\n\n\u003cp\u003eQi emphasized the importance of stepping back to see the bigger picture when scaling AI infrastructure. By adopting this broader perspective, businesses can identify more effective approaches that deliver real value and focus their resources on these priorities. This mindset also clarifies which efforts yield meaningful results during continuous evaluation, allowing organizations to refine their systems at every stage for sustained performance and reliability.\u003c/p\u003e\n\n\u003cp\u003eDevelopers interested in learning more about Qi’s presentation may watch the \u003ca href=\"https://qconsf.com/\"\u003eInfoQ website\u003c/a\u003e where a video of her presentation will be available in the coming weeks.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Andrew-Hoblitzell\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAndrew Hoblitzell\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-11-26T00:00:00Z",
  "modifiedTime": null
}
