{
  "id": "58b599d3-5ef4-4bf0-819f-90c8b0d6b3b7",
  "title": "Data Platform Explained Part II",
  "link": "https://engineering.atspotify.com/2024/05/data-platform-explained-part-ii/",
  "description": "Check out Data Platform Explained Part I, where we started sharing the journey of building a data platform, its building [...] The post Data Platform Explained Part II  appeared first on Spotify Engineering.",
  "author": "Spotify Engineering",
  "published": "Tue, 28 May 2024 18:44:35 +0000",
  "source": "https://labs.spotify.com/feed/",
  "categories": [
    "Data",
    "Data Science",
    "Platform"
  ],
  "byline": "Spotify Engineering",
  "length": 7938,
  "excerpt": "Data Platform Explained Part II - Spotify Engineering",
  "siteName": "Spotify Engineering",
  "favicon": "https://storage.googleapis.com/production-eng/1/2021/03/cropped-Engineering-Icon-RGB-512x512-light-250x250.png",
  "text": "May 28, 2024 Check out Data Platform Explained Part I, where we started sharing the journey of building a data platform, its building blocks, and the motivation for investing into building a platformized solution at Spotify. Introduction In Data Platform Explained Part I, we shared the first steps in the journey to build a data platform, the insights that indicate it’s time to start building one, and how we are organized to succeed on it. In this article, we will take one step further into the why, what, and how of our data platform, introduce you to the domains underneath it that are responsible for the platform’s building blocks — here we will talk about scalability, the tooling we use and provide, alongside the value each building block brings to a data platform — and finally our strategy to navigate the complexity of a data ecosystem by building a strong community around it. Data Collection  When it comes to scalability, Spotify’s Data Collection platform collects more than 1 trillion events per day. Its event delivery architecture is constantly evolving through numerous iterations. To learn more about  the event delivery evolution, its inception, and subsequent improvements, check out this blog post.Data Collection is needed, so we can:  Understand what content is relevant to Spotify users  Directly respond to user feedback Have a deeper understanding of user interactions to enhance their experience Figure 1: The event delivery infrastructure is a significant topic that deserves its own dedicated article (coming soon). Nevertheless, here’s an overview of the main components handled by our event delivery infrastructure. When a team at Spotify decides to instrument their functionality with event delivery, aside from writing code using our SDs, they only need to define the event schemas. The infrastructure then automatically deploys a new set of event-specific components (such as PubSub queues, anonymization pipelines, and streaming jobs) using K8 operators. Any changes to the event schema triggers the deployment of corresponding resources. Anonymization solutions, including internal key-handling systems, are covered in detail in this article.  The balance between centralized and distributed ownership allows most updates to be managed by consumers of the consumption dataset, without requiring intervention from the infrastructure team. Today, over 1800 different event types — or signals representing interactions from Spotify users — are being published. In terms of team structure, the data collection area is organized to focus on the event delivery infrastructure, supporting and enhancing client SDKs for event transmission, and building the high quality datasets that represent the user journey experience, as well as the infrastructure needed behind it. Data Management \u0026 Data Processing Our Data Processing efforts focus on empowering Spotify to utilize data effectively, while Data Management is dedicated to ensuring data integrity through tool creation and collaborative efforts. With more than 38,000 actively scheduled pipelines handling both hourly and daily tasks, scalability is a key consideration. Data Management and Data Processing are essential for Spotify to effectively manage its extensive data and pipelines. It’s crucial to maintain data traceability (lineage), searchability (metadata), and accessibility, while implementing access controls and retention policies to manage storage costs and comply with regulations. These functions enable Spotify to extract maximum value from its data assets while upholding operational efficiency and regulatory standards. Figure 2: These domains, like Event Delivery, warrant their own comprehensive blog posts. This article provides a closer look at the tools we use, and our organizational structure. The scheduling and orchestration of workflows are essential components of Data Processing. Once a workflow is picked up by the scheduler, it’s executed on BigQuery, or either Flink or Dataflow clusters. Most pipelines utilize Scio, a Scala API for Beam. Data pipelines generate data endpoints, each adhering to a specific schema and possibly containing multiple partitions. These endpoints are equipped with retention policies, access controls, lineage tracking, and quality checks. Defining a workflow or endpoint involves custom K8 operators, which help us to easily deploy and maintain complex structures. In that manner, the resource definition lives in the same repo as the pipeline code and gets deployed and maintained by the codeowners. Monitoring options include alerts for data lateness, long-running or failing workflows, and endpoints. Backstage integration facilitates easy resource management, monitoring, cost analysis, and quality assurance. Building a culture around the data platform Building a data platform is non-trivial — it needs to be flexible enough to satisfy a variety of different use cases, aligning with cost effectiveness and return on investment goals, and at the same time keeping the developer experience lean. The data platform needs to be easy to onboard to and have seamless upgrade paths (nobody likes to be disrupted by platform upgrades and breaking changes). And the platform needs to be reliable — if teams  have the expectation to build business critical logic on top of your platform, we treat the platform as a critical use case as well.  There are multiple ways to elevate engagement with your product: Documentation (which is easy to find). We all have been in situations where, “I remember reading about it, but I don’t remember where.” It should be easier to find documentation than to ask a question (considering the waiting time). Onboard teams. There is no better way to learn about your product than to start using it yourself. Go to users and embed there. Learn about different use cases, make sure that your product is easy to use in all possible environments, and bring the learnings back to the platform. Fleetshift the changes. People love evolving and making changes to their infrastructure and having the code being highlighted as deprecated, right? Not really. That is why we should automate all possible toils and migrations. Plan to deal with risks. Make time to support your customers. Build a community where people are free to ask questions and where there are dedicated goalies to answer these questions. Answering community questions should not be left to free will, but should instead be encouraged and taken seriously. At Spotify we have a slack channel #data-support, where all data questions are addressed. Wrapping up  Our Data Platform has come a long way, and continues to evolve. At the very beginning, we were a few people, part of one team. We ran the pipelines on-premise, operating the largest Hadoop cluster in Europe. We are now 100+ engineers working on building the Spotify data platform on GCP, with data collection, management, and processing capabilities. There is no formula or script to set up a data platform. A good way to start is by aligning your organizational needs with your investments. These needs become the drivers for your platform’s building blocks, and may change over time. Make sure the challenges are clear — define clear goals and set clear expectations — it will help you to have the right support from your organization and to be on the path for success. Get closer to your users, have a clear way through which customers and stakeholders can reach out and give you direct feedback — it will set the stage to create a community around your platform. Finally, you do not have to start big: just start somewhere then evolve, iterate, and learn. Tags: Data",
  "image": "https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-630.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 28, 2024\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2024/05/data-platform-explained-part-ii/\" title=\"Data Platform Explained Part II \"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590.png\" alt=\"\" decoding=\"async\" fetchpriority=\"high\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590.png 2501w, https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590-250x123.png 250w, https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590-700x344.png 700w, https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590-768x378.png 768w, https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590-1536x755.png 1536w, https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590-2048x1007.png 2048w, https://storage.googleapis.com/production-eng/1/2024/05/EN219-DataPlatform_Part2_BlogPost-1200-x-590-120x59.png 120w\" sizes=\"(max-width: 2501px) 100vw, 2501px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eCheck out \u003ca href=\"https://engineering.atspotify.com/2024/04/data-platform-explained/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eData Platform Explained Part I\u003c/a\u003e, where we started sharing the journey of building a data platform, its building blocks, and the motivation for investing into building a platformized solution at Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://engineering.atspotify.com/2024/04/data-platform-explained/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eData Platform Explained Part I\u003c/a\u003e, we shared the first steps in the journey to build a data platform, the insights that indicate it’s time to start building one, and how we are organized to succeed on it. In this article, we will take one step further into the why, what, and how of our data platform, introduce you to the domains underneath it that are responsible for the platform’s building blocks — here we will talk about scalability, the tooling we use and provide, alongside the value each building block brings to a data platform — and finally our strategy to navigate the complexity of a data ecosystem by building a strong community around it.\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1590\" height=\"593\" src=\"https://storage.googleapis.com/production-eng/1/2024/05/image3.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/05/image3.png 1590w, https://storage.googleapis.com/production-eng/1/2024/05/image3-250x93.png 250w, https://storage.googleapis.com/production-eng/1/2024/05/image3-700x261.png 700w, https://storage.googleapis.com/production-eng/1/2024/05/image3-768x286.png 768w, https://storage.googleapis.com/production-eng/1/2024/05/image3-1536x573.png 1536w, https://storage.googleapis.com/production-eng/1/2024/05/image3-120x45.png 120w\" sizes=\"(max-width: 1590px) 100vw, 1590px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003ch2\u003eData Collection \u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eWhen it comes to scalability, Spotify’s Data Collection platform collects more than 1 trillion events per day. Its event delivery architecture is constantly evolving through numerous iterations. To learn more about  the event delivery evolution, its \u003ca href=\"https://engineering.atspotify.com/2016/02/spotifys-event-delivery-the-road-to-the-cloud-part-i/\"\u003einception\u003c/a\u003e, and subsequent improvements, check out \u003ca href=\"https://engineering.atspotify.com/2021/10/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/\"\u003ethis\u003c/a\u003e blog post.\u003c/p\u003e\u003cp\u003eData Collection is needed, so we can: \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eUnderstand what content is relevant to Spotify users \u003c/li\u003e\n\n\n\n\u003cli\u003eDirectly respond to user feedback\u003c/li\u003e\n\n\n\n\u003cli\u003eHave a deeper understanding of user interactions to enhance their experience\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg decoding=\"async\" width=\"1999\" height=\"856\" src=\"https://storage.googleapis.com/production-eng/1/2024/05/image2.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/05/image2.png 1999w, https://storage.googleapis.com/production-eng/1/2024/05/image2-250x107.png 250w, https://storage.googleapis.com/production-eng/1/2024/05/image2-700x300.png 700w, https://storage.googleapis.com/production-eng/1/2024/05/image2-768x329.png 768w, https://storage.googleapis.com/production-eng/1/2024/05/image2-1536x658.png 1536w, https://storage.googleapis.com/production-eng/1/2024/05/image2-120x51.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e\u003cfigcaption\u003eFigure 1: The event delivery infrastructure is a significant topic that deserves its own dedicated article (coming soon). Nevertheless, here’s an overview of the main components handled by our event delivery infrastructure.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eWhen a team at Spotify decides to instrument their functionality with event delivery, aside from writing code using our SDs, they only need to define the event schemas. The infrastructure then automatically deploys a new set of event-specific components (such as PubSub queues, anonymization pipelines, and streaming jobs) using K8 operators. Any changes to the event schema triggers the deployment of corresponding resources. Anonymization solutions, including internal key-handling systems, are covered in detail in \u003ca href=\"https://engineering.atspotify.com/2018/09/scalable-user-privacy/\"\u003ethis article\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe balance between centralized and distributed ownership allows most updates to be managed by consumers of the consumption dataset, without requiring intervention from the infrastructure team.\u003c/p\u003e\n\n\n\n\u003cp\u003eToday, over 1800 different event types — or signals representing interactions from Spotify users — are being published. In terms of team structure, the data collection area is organized to focus on the event delivery infrastructure, supporting and enhancing client SDKs for event transmission, and building the high quality datasets that represent the user journey experience, as well as the infrastructure needed behind it.\u003c/p\u003e\n\n\n\n\u003ch2\u003eData Management \u0026amp; Data Processing\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur Data Processing efforts focus on empowering Spotify to utilize data effectively, while Data Management is dedicated to ensuring data integrity through tool creation and collaborative efforts. With more than 38,000 actively scheduled pipelines handling both hourly and daily tasks, scalability is a key consideration. Data Management and Data Processing are essential for Spotify to effectively manage its extensive data and pipelines. It’s crucial to maintain data traceability (lineage), searchability (metadata), and accessibility, while implementing access controls and retention policies to manage storage costs and comply with regulations. These functions enable Spotify to extract maximum value from its data assets while upholding operational efficiency and regulatory standards.\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" width=\"1999\" height=\"1069\" src=\"https://storage.googleapis.com/production-eng/1/2024/05/image1-1.png\" alt=\"\" srcset=\"https://storage.googleapis.com/production-eng/1/2024/05/image1-1.png 1999w, https://storage.googleapis.com/production-eng/1/2024/05/image1-1-250x134.png 250w, https://storage.googleapis.com/production-eng/1/2024/05/image1-1-700x374.png 700w, https://storage.googleapis.com/production-eng/1/2024/05/image1-1-768x411.png 768w, https://storage.googleapis.com/production-eng/1/2024/05/image1-1-1536x821.png 1536w, https://storage.googleapis.com/production-eng/1/2024/05/image1-1-120x64.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e\u003cfigcaption\u003eFigure 2: These domains, like Event Delivery, warrant their own comprehensive blog posts. This article provides a closer look at the tools we use, and our organizational structure.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\u003cp\u003eThe scheduling and orchestration of workflows are essential components of Data Processing. Once a workflow is picked up by the scheduler, it’s executed on BigQuery, or either Flink or Dataflow clusters. Most pipelines utilize \u003ca href=\"https://spotify.github.io/scio/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eScio\u003c/a\u003e, a Scala API for Beam.\u003c/p\u003e\n\n\n\n\u003cp\u003eData pipelines generate data endpoints, each adhering to a specific schema and possibly containing multiple partitions. These endpoints are equipped with retention policies, access controls, lineage tracking, and quality checks.\u003c/p\u003e\n\n\n\n\u003cp\u003eDefining a workflow or endpoint involves custom \u003ca href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eK8 operators\u003c/a\u003e, which help us to easily deploy and maintain complex structures. In that manner, the resource definition lives in the same repo as the pipeline code and gets deployed and maintained by the codeowners.\u003c/p\u003e\n\n\n\n\u003cp\u003eMonitoring options include alerts for data lateness, long-running or failing workflows, and endpoints. \u003ca href=\"https://engineering.atspotify.com/2020/03/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e integration facilitates easy resource management, monitoring, cost analysis, and quality assurance.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBuilding a culture around the data platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eBuilding a data platform is non-trivial — it needs to be flexible enough to satisfy a variety of different use cases, aligning with cost effectiveness and return on investment goals, and at the same time keeping the developer experience lean. The data platform needs to be easy to onboard to and have seamless upgrade paths (nobody likes to be disrupted by platform upgrades and breaking changes). And the platform needs to be reliable — if teams  have the expectation to build business critical logic on top of your platform, we treat the platform as a critical use case as well. \u003c/p\u003e\n\n\n\n\u003cp\u003eThere are multiple ways to elevate engagement with your product:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://backstage.io/docs/features/techdocs/\"\u003e\u003cstrong\u003eDocumentation\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e (which is easy to find).\u003c/strong\u003e We all have been in situations where, “I remember reading about it, but I don’t remember where.” It should be easier to find documentation than to ask a question (considering the waiting time).\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eOnboard teams.\u003c/strong\u003e There is no better way to learn about your product than to start using it yourself. Go to users and embed there. Learn about different use cases, make sure that your product is easy to use in all possible environments, and bring the learnings back to the platform.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003ca href=\"https://engineering.atspotify.com/2023/05/fleet-management-at-spotify-part-3-fleet-wide-refactoring/\"\u003e\u003cstrong\u003eFleetshift\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e the changes.\u003c/strong\u003e People love evolving and making changes to their infrastructure and having the code being highlighted as deprecated, right? Not really. That is why we should automate all possible toils and migrations. Plan to deal with risks. Make time to support your customers.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eBuild a community\u003c/strong\u003e where people are free to ask questions and where there are dedicated goalies to answer these questions. Answering community questions should not be left to free will, but should instead be encouraged and taken seriously. At Spotify we have a slack channel #data-support, where all data questions are addressed.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2\u003eWrapping up \u003c/h2\u003e\n\n\n\n\u003cp\u003eOur Data Platform has come a long way, and continues to evolve. At the very beginning, we were a few people, part of one team. We ran the pipelines on-premise, operating the\u003ca href=\"https://engineering.atspotify.com/2016/02/spotifys-event-delivery-the-road-to-the-cloud-part-i/\"\u003e largest Hadoop cluster in Europe\u003c/a\u003e. We are now 100+ engineers working on building the Spotify data platform on GCP, with data collection, management, and processing capabilities.\u003c/p\u003e\n\n\n\n\u003cp\u003eThere is no formula or script to set up a data platform. A good way to start is by aligning your organizational needs with your investments. These needs become the drivers for your platform’s building blocks, and may change over time. Make sure the challenges are clear — define clear goals and set clear expectations — it will help you to have the right support from your organization and to be on the path for success.\u003c/p\u003e\n\n\n\n\u003cp\u003eGet closer to your users, have a clear way through which customers and stakeholders can reach out and give you direct feedback — it will set the stage to create a community around your platform. Finally, you do not have to start big: just start somewhere then evolve, iterate, and learn.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": "2024-05-28T18:44:35Z",
  "modifiedTime": "2024-07-10T16:07:10Z"
}
