{
  "id": "d47fef4e-4570-40ea-a6ca-6235ca94a66b",
  "title": "OpenAI Releases Improved Image Generation in GPT-4o",
  "link": "https://www.infoq.com/news/2025/04/gpt-4o-images/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "OpenAI released a new version of GPT-4o with native image generation capability. The model can modify uploaded images or create new ones from prompts and exhibits multi-turn consistency when refining images and improved generation of text in images. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 01 Apr 2025 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "OpenAI",
    "Generative AI",
    "ChatGPT",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3360,
  "excerpt": "OpenAI released a new version of GPT-4o with native image generation capability. The model can modify uploaded images or create new ones from prompts and exhibits multi-turn consistency when refining",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250328105617-1/apple-touch-icon.png",
  "text": "OpenAI released a new version of GPT-4o with native image generation capability. The model can modify uploaded images or create new ones from prompts and exhibits multi-turn consistency when refining images and improved generation of text in images. OpenAI's CEO Sam Altman announced the release in a recent livestream. Unlike the previous iteration of the chat model, which invoked an external model like DALL-E to generate images, the new model is trained to handle image output as a native modality. It uses an autoregressive generation method, while models like DALL-E and Stable Diffusion use a diffusion method. According to OpenAI: GPT‑4o image generation excels at accurately rendering text, precisely following prompts, and leveraging 4o’s inherent knowledge base and chat context—including transforming uploaded images or using them as visual inspiration. These capabilities make it easier to create exactly the image you envision, helping you communicate more effectively through visuals and advancing image generation into a practical tool with precision and power. OpenAI trained the new model on a combination of image and text data, including \"aggressive post-training.\" While OpenAI did not release technical details about the model or its performance on benchmarks, they released several sample images and the prompts used to generate them. OpenAI claims that the model can generate images with \"up to 10-20 different objects,\" although it may \"struggle to accurately render more.\" As a safety feature, the images generated by GPT-4o include C2PA tags showing that they were generated by AI. OpenAI also built an internal tool to help determine if an image was generated by their models. OpenAI will block generation of images that violate their content policies, but Kevin Weil, CPO of OpenAI, wrote on X that: If you explicitly ask for something edgy (within reason), the model should respect your intent. As we said in our model spec, giving users creative control matters, and we'll continue listening and adapting based on feedback. OpenAI updated the 4o model's system card to describe its potential risks and the mitigations taken, including extensive red-teaming exercises. The system card also lists cases where the model will refuse to generate images: for example, it will refuse prompts that ask for images in the style of a living artist. However, in a change to previous policy, the model will generate images of a public figure, as long as the images do not otherwise violate OpenAI policy. Hacker News users commented on the quality of the generated images, particularly mentioning its ability to correctly render text in images. One user wrote: It very much looks like a side effect of this new architecture. In my experience, text looks much better in recent DALL-E images (so what ChatGPT was using before), but it [was] still noticeably mangled when printing more than a few letters. This model update seems to improve text rendering by a lot, at least as long as the content is clearly specified. OpenAI noted that the model \"struggles\" with rendering languages that use non-Latin characters and might produce text that is \"inaccurate or hallucinated.\" About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2025/04/gpt-4o-images/en/headerimage/generatedHeaderImage-1743345640467.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eOpenAI released a \u003ca href=\"https://openai.com/index/introducing-4o-image-generation/\"\u003enew version of GPT-4o\u003c/a\u003e with native image generation capability. The model can modify uploaded images or create new ones from prompts and exhibits multi-turn consistency when refining images and improved generation of text in images.\u003c/p\u003e\n\n\u003cp\u003eOpenAI\u0026#39;s CEO Sam Altman \u003ca href=\"https://www.youtube.com/watch?v=2f3K43FHRKo\"\u003eannounced the release in a recent livestream\u003c/a\u003e. Unlike the previous iteration of the chat model, which invoked an external model like DALL-E to generate images, the new model is trained to handle image output as a native modality. It uses an autoregressive generation method, while models like DALL-E and Stable Diffusion use a diffusion method. According to OpenAI:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGPT‑4o image generation excels at accurately rendering text, precisely following prompts, and leveraging 4o’s inherent knowledge base and chat context—including transforming uploaded images or using them as visual inspiration. These capabilities make it easier to create exactly the image you envision, helping you communicate more effectively through visuals and advancing image generation into a practical tool with precision and power.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOpenAI trained the new model on a combination of image and text data, including \u0026#34;aggressive post-training.\u0026#34; While OpenAI did not release technical details about the model or its performance on benchmarks, they released several sample images and the prompts used to generate them. OpenAI claims that the model can generate images with \u0026#34;up to 10-20 different objects,\u0026#34; although it may \u0026#34;struggle to accurately render more.\u0026#34;\u003c/p\u003e\n\n\u003cp\u003eAs a safety feature, the images generated by GPT-4o include C2PA tags showing that they were generated by AI. OpenAI also built an internal tool to help determine if an image was generated by their models. OpenAI will block generation of images that violate their content policies, but Kevin Weil, CPO of OpenAI, \u003ca href=\"https://x.com/kevinweil/status/1904595752380465645\"\u003ewrote on X\u003c/a\u003e that:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eIf you explicitly ask for something edgy (within reason), the model should respect your intent. As we said in our model spec, giving users creative control matters, and we\u0026#39;ll continue listening and adapting based on feedback.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOpenAI updated the 4o model\u0026#39;s \u003ca href=\"https://openai.com/index/gpt-4o-image-generation-system-card-addendum/\"\u003esystem card\u003c/a\u003e to describe its potential risks and the mitigations taken, including extensive red-teaming exercises. The system card also lists cases where the model will refuse to generate images: for example, it will refuse prompts that ask for images in the style of a living artist. However, in a change to previous policy, the model will generate images of a public figure, as long as the images do not otherwise violate OpenAI policy.\u003c/p\u003e\n\n\u003cp\u003eHacker News users \u003ca href=\"https://news.ycombinator.com/item?id=43474112\"\u003ecommented on the quality of the generated images\u003c/a\u003e, particularly mentioning its ability to correctly render text in images. One user wrote:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eIt very much looks like a side effect of this new architecture. In my experience, text looks much better in recent DALL-E images (so what ChatGPT was using before), but it [was] still noticeably mangled when printing more than a few letters. This model update seems to improve text rendering by a lot, at least as long as the content is clearly specified.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eOpenAI noted that the model \u0026#34;struggles\u0026#34; with rendering languages that use non-Latin characters and might produce text that is \u0026#34;inaccurate or hallucinated.\u0026#34;\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-04-01T00:00:00Z",
  "modifiedTime": null
}
