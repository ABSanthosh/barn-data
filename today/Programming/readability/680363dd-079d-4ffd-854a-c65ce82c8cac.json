{
  "id": "680363dd-079d-4ffd-854a-c65ce82c8cac",
  "title": "T5Gemma: A new collection of encoder-decoder Gemma models",
  "link": "https://developers.googleblog.com/en/t5gemma/",
  "description": "T5Gemma is a new family of encoder-decoder LLMs developed by converting and adapting pretrained decoder-only models based on the Gemma 2 framework, offering superior performance and efficiency compared to its decoder-only counterparts, particularly for tasks requiring deep input understanding, like summarization and translation.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Biao Zhang, Paul Suganthan, Ben Hora",
  "length": 6956,
  "excerpt": "T5Gemma is a new family of encoder-decoder LLMs developed by converting and adapting pretrained decoder-only models based on the Gemma 2 framework, offering superior performance and efficiency compared to its decoder-only counterparts, particularly for tasks requiring deep input understanding, like summarization and translation.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "In the rapidly evolving landscape of large language models (LLMs), the spotlight has largely focused on the decoder-only architecture. While these models have shown impressive capabilities across a wide range of generation tasks, the classic encoder-decoder architecture, such as T5 (The Text-to-Text Transfer Transformer), remains a popular choice for many real-world applications. Encoder-decoder models often excel at summarization, translation, QA, and more due to their high inference efficiency, design flexibility, and richer encoder representation for understanding input. Nevertheless, the powerful encoder-decoder architecture has received little relative attention.Today, we revisit this architecture and introduce T5Gemma, a new collection of encoder-decoder LLMs developed by converting pretrained decoder-only models into the encoder-decoder architecture through a technique called adaptation. T5Gemma is based on the Gemma 2 framework, including adapted Gemma 2 2B and 9B models as well as a set of newly trained T5-sized models (Small, Base, Large and XL). We are excited to release pretrained and instruction-tuned T5Gemma models to the community to unlock new opportunities for research and development.From decoder-only to encoder-decoderIn T5Gemma, we ask the following question: can we build top-tier encoder-decoder models based on pretrained decoder-only models? We answer this question by exploring a technique called model adaptation. The core idea is to initialize the parameters of an encoder-decoder model using the weights of an already pretrained decoder-only model, and then further adapt them via UL2 or PrefixLM-based pre-training. An overview of our approach, showing how we initialize a new encoder-decoder model using the parameters from a pretrained, decoder-only model. This adaptation method is highly flexible, allowing for creative combinations of model sizes. For instance, we can pair a large encoder with a small decoder (e.g., a 9B encoder with a 2B decoder) to create an \"unbalanced\" model. This allows us to fine-tune the quality-efficiency trade-off for specific tasks, such as summarization, where a deep understanding of the input is more critical than the complexity of the generated output.Towards better quality-efficiency trade-offHow does T5Gemma perform?In our experiments, T5Gemma models achieve comparable or better performance than their decoder-only Gemma counterparts, nearly dominating the quality-inference efficiency pareto frontier across several benchmarks, such as SuperGLUE which measures the quality of the learned representation. Encoder-decoder models consistently offer better performance for a given level of inference compute, leading the quality-efficiency frontier across a range of benchmarks. This performance advantage isn't just theoretical; it translates to real-world quality and speed too. When measuring the actual latency for GSM8K (math reasoning), T5Gemma provided a clear win. For example, T5Gemma 9B-9B achieves higher accuracy than Gemma 2 9B but with a similar latency. Even more impressively, T5Gemma 9B-2B delivers a significant accuracy boost over the 2B-2B model, yet its latency is nearly identical to the much smaller Gemma 2 2B model. Ultimately, these experiments showcase that encoder-decoder adaptation offers a flexible, powerful way to balance across quality and inference speed.Unlocking foundational and fine-tuned capabilitiesCould encoder-decoder LLMs have similar capabilities to decoder-only models?Yes, T5Gemma shows promising capabilities both before and after instruction tuning.After pre-training, T5Gemma achieves impressive gains on complex tasks that require reasoning. For instance, T5Gemma 9B-9B scores over 9 points higher on GSM8K (math reasoning) and 4 points higher on DROP (reading comprehension) than the original Gemma 2 9B model. This pattern demonstrates that the encoder-decoder architecture, when initialized via adaptation, has the potential to create a more capable, performant foundational model. Detailed results for pretrained models, illustrating how adapted models have significant gains on several reasoning-intensive benchmarks compared to decoder-only Gemma 2. These foundational improvements from pre-training set the stage for even more dramatic gains after instruction tuning. For example, comparing Gemma 2 IT to T5Gemma IT, the performance gap widens significantly across the board. T5Gemma 2B-2B IT sees its MMLU score jump by nearly 12 points over the Gemma 2 2B, and its GSM8K score increases from 58.0% to 70.7%. The adapted architecture not only potentially provides a better starting point but also responds more effectively to instruction-tuning, ultimately leading to a substantially more capable and helpful final model. Detailed results for fine-tuned + RLHFed models, illustrating the capabilities of post-training to significantly amplify the performance advantages of the encoder-decoder architecture. Explore our models: Releasing T5Gemma checkpointsWeâ€™re very excited to present this new method of building powerful, general purpose encoder-decoder models by adapting from pretrained decoder-only LLMs like Gemma 2. To help accelerate further research and allow the community to build on this work, we are excited to release a suite of our T5Gemma checkpoints.The release includes:Multiple Sizes: Checkpoints for T5-sized models (Small, Base, Large, and XL), the Gemma 2-based models (2B and 9B), as well as an additional model in between T5 Large and T5 XL.Multiple Variants: Pretrained and instruction-tuned models.Flexible Configurations: A powerful and efficient unbalanced 9B-2B checkpoint to explore the trade-offs between encoder and decoder size.Different Training Objectives: Models trained with either PrefixLM or UL2 objectives to provide either state-of-the-art generative performance or representation quality.We hope these checkpoints will provide a valuable resource for investigating model architecture, efficiency, and performance.Getting started with T5GemmaWe can't wait to see what you build with T5Gemma. Please see the following links for more information:Learn about the research behind this project by reading the paper.Download the models: Find the model weights on Hugging Face and Kaggle.Explore the models capabilities or fine-tune them for your own use cases with the Colab notebook.Run inference with the models on Vertex AI.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/HeroBlog-meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"9jscd\"\u003eIn the rapidly evolving landscape of large language models (LLMs), the spotlight has largely focused on the decoder-only architecture. While these models have shown impressive capabilities across a wide range of generation tasks, the classic encoder-decoder architecture, such as T5 (The Text-to-Text Transfer Transformer), remains a popular choice for many real-world applications. Encoder-decoder models often excel at summarization, translation, QA, and more due to their high inference efficiency, design flexibility, and richer encoder representation for understanding input. Nevertheless, the powerful encoder-decoder architecture has received little relative attention.\u003c/p\u003e\u003cp data-block-key=\"7f8nq\"\u003eToday, we revisit this architecture and introduce \u003ca href=\"https://arxiv.org/abs/2504.06225\"\u003eT5Gemma\u003c/a\u003e, a new collection of encoder-decoder LLMs developed by converting pretrained decoder-only models into the encoder-decoder architecture through a technique called adaptation. T5Gemma is based on the Gemma 2 framework, including adapted Gemma 2 2B and 9B models as well as a set of newly trained T5-sized models (Small, Base, Large and XL). We are excited to release pretrained and instruction-tuned T5Gemma models to the community to unlock new opportunities for research and development.\u003c/p\u003e\u003ch2 data-block-key=\"cboz8\" id=\"from-decoder-only-to-encoder-decoder\"\u003e\u003cbr/\u003eFrom decoder-only to encoder-decoder\u003c/h2\u003e\u003cp data-block-key=\"8iu0b\"\u003eIn T5Gemma, we ask the following question: \u003ci\u003ecan we build top-tier encoder-decoder models based on pretrained decoder-only models?\u003c/i\u003e We answer this question by exploring a technique called \u003ci\u003emodel adaptation\u003c/i\u003e. The core idea is to initialize the parameters of an encoder-decoder model using the weights of an already pretrained decoder-only model, and then further adapt them via UL2 or PrefixLM-based pre-training.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n            \n                \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Chart-1.original.png\" alt=\"decoder-only model\"/\u003e\u003c/p\u003e\u003cp\u003e\n                        An overview of our approach, showing how we initialize a new encoder-decoder model using the parameters from a pretrained, decoder-only model.\n                    \u003c/p\u003e\n                \n            \n        \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"9jscd\"\u003eThis adaptation method is highly flexible, allowing for creative combinations of model sizes. For instance, we can pair a large encoder with a small decoder (e.g., a 9B encoder with a 2B decoder) to create an \u0026#34;unbalanced\u0026#34; model. This allows us to fine-tune the quality-efficiency trade-off for specific tasks, such as summarization, where a deep understanding of the input is more critical than the complexity of the generated output.\u003c/p\u003e\u003ch2 data-block-key=\"vzh5y\" id=\"towards-better-quality-efficiency-trade-off\"\u003e\u003cbr/\u003eTowards better quality-efficiency trade-off\u003c/h2\u003e\u003cp data-block-key=\"1vosl\"\u003e\u003ci\u003eHow does T5Gemma perform?\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"c8rrg\"\u003eIn our experiments, T5Gemma models achieve comparable or better performance than their decoder-only Gemma counterparts, nearly dominating the quality-inference efficiency pareto frontier across several benchmarks, such as SuperGLUE which measures the quality of the learned representation.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n            \n                \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Encoder-decoder_models_benchmarks.original.png\" alt=\"Encoder-decoder models benchmarks\"/\u003e\u003c/p\u003e\u003cp\u003e\n                        Encoder-decoder models consistently offer better performance for a given level of inference compute, leading the quality-efficiency frontier across a range of benchmarks.\n                    \u003c/p\u003e\n                \n            \n        \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"9jscd\"\u003eThis performance advantage isn\u0026#39;t just theoretical; it translates to real-world quality and speed too. When measuring the actual latency for GSM8K (math reasoning), T5Gemma provided a clear win. For example, T5Gemma 9B-9B achieves higher accuracy than Gemma 2 9B but with a similar latency. Even more impressively, T5Gemma 9B-2B delivers a significant accuracy boost over the 2B-2B model, yet its latency is nearly identical to the much smaller Gemma 2 2B model. Ultimately, these experiments showcase that encoder-decoder adaptation offers a flexible, powerful way to balance across quality and inference speed.\u003c/p\u003e\u003ch2 data-block-key=\"jqmq1\" id=\"unlocking-foundational-and-fine-tuned-capabilities\"\u003e\u003cbr/\u003eUnlocking foundational and fine-tuned capabilities\u003c/h2\u003e\u003cp data-block-key=\"1vl20\"\u003e\u003ci\u003eCould encoder-decoder LLMs have similar capabilities to decoder-only models?\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"bi8uk\"\u003eYes, T5Gemma shows promising capabilities both before and after instruction tuning.\u003c/p\u003e\u003cp data-block-key=\"e16k2\"\u003eAfter pre-training, T5Gemma achieves impressive gains on complex tasks that require reasoning. For instance, T5Gemma 9B-9B scores over 9 points higher on GSM8K (math reasoning) and 4 points higher on DROP (reading comprehension) than the original Gemma 2 9B model. This pattern demonstrates that the encoder-decoder architecture, when initialized via adaptation, has the potential to create a more capable, performant foundational model.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n            \n                \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/pretrained-model-results.original.png\" alt=\"Detailed results for pretrained models\"/\u003e\u003c/p\u003e\u003cp\u003e\n                        Detailed results for pretrained models, illustrating how adapted models have significant gains on several reasoning-intensive benchmarks compared to decoder-only Gemma 2.\n                    \u003c/p\u003e\n                \n            \n        \u003c/div\u003e\n  \u003cp data-block-key=\"9jscd\"\u003eThese foundational improvements from pre-training set the stage for even more dramatic gains after instruction tuning. For example, comparing Gemma 2 IT to T5Gemma IT, the performance gap widens significantly across the board. T5Gemma 2B-2B IT sees its MMLU score jump by nearly 12 points over the Gemma 2 2B, and its GSM8K score increases from 58.0% to 70.7%. The adapted architecture not only potentially provides a better starting point but also responds more effectively to instruction-tuning, ultimately leading to a substantially more capable and helpful final model.\u003c/p\u003e   \n\n\n    \n    \u003cdiv\u003e\n            \n                \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/results-for-fine-tuned-RLHFed-models.original.png\" alt=\"Results for fine-tuned + RLHFed models\"/\u003e\u003c/p\u003e\u003cp\u003e\n                        Detailed results for fine-tuned + RLHFed models, illustrating the capabilities of post-training to significantly amplify the performance advantages of the encoder-decoder architecture.\n                    \u003c/p\u003e\n                \n            \n        \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"kfhz1\" id=\"explore-our-models:-releasing-t5gemma-checkpoints\"\u003eExplore our models: Releasing T5Gemma checkpoints\u003c/h2\u003e\u003cdiv data-block-key=\"3jo2n\"\u003e\u003cp\u003eWeâ€™re very excited to present this new method of building powerful, general purpose encoder-decoder models by adapting from pretrained decoder-only LLMs like Gemma 2. To help accelerate further research and allow the community to build on this work, we are excited to release a suite of our T5Gemma checkpoints.\u003c/p\u003e\u003cp\u003eThe release includes:\u003c/p\u003e\u003c/div\u003e\u003cul\u003e\u003cli data-block-key=\"9s5qe\"\u003e\u003cb\u003eMultiple Sizes:\u003c/b\u003e Checkpoints for T5-sized models (Small, Base, Large, and XL), the Gemma 2-based models (2B and 9B), as well as an additional model in between T5 Large and T5 XL.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"8jkal\"\u003e\u003cb\u003eMultiple Variants\u003c/b\u003e: Pretrained and instruction-tuned models.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"dvs6f\"\u003e\u003cb\u003eFlexible Configurations:\u003c/b\u003e A powerful and efficient unbalanced 9B-2B checkpoint to explore the trade-offs between encoder and decoder size.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"b4k4v\"\u003e\u003cb\u003eDifferent Training Objectives:\u003c/b\u003e Models trained with either PrefixLM or UL2 objectives to provide either state-of-the-art generative performance or representation quality.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"f2552\"\u003e\u003cbr/\u003eWe hope these checkpoints will provide a valuable resource for investigating model architecture, efficiency, and performance.\u003c/p\u003e\u003ch2 data-block-key=\"lby7w\" id=\"getting-started-with-t5gemma\"\u003e\u003cbr/\u003eGetting started with T5Gemma\u003c/h2\u003e\u003cp data-block-key=\"5q2d7\"\u003eWe can\u0026#39;t wait to see what you build with T5Gemma. Please see the following links for more information:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"1ejoc\"\u003eLearn about the research behind this project by reading \u003ca href=\"https://arxiv.org/abs/2504.06225\"\u003ethe paper\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ba685\"\u003eDownload the models: Find the model weights on \u003ca href=\"https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86\"\u003eHugging Face\u003c/a\u003e and \u003ca href=\"https://www.kaggle.com/models/google/t5gemma\"\u003eKaggle\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"57rc9\"\u003eExplore the models capabilities or fine-tune them for your own use cases with the \u003ca href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/Research/%5BT5Gemma%5DExample.ipynb\"\u003eColab notebook\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ismn\"\u003eRun inference with the models on \u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/t5gemma\"\u003eVertex AI\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-07-09T00:00:00Z",
  "modifiedTime": null
}
