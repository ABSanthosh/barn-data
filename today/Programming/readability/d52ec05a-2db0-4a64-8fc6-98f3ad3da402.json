{
  "id": "d52ec05a-2db0-4a64-8fc6-98f3ad3da402",
  "title": "TensorFlow Lite is now LiteRT",
  "link": "https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/",
  "description": "TensorFlow Lite, now named LiteRT, is still the same high-performance runtime for on-device AI, but with an expanded vision to support models authored in PyTorch, JAX, and Keras.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Google AI Edge team",
  "length": 4759,
  "excerpt": "TensorFlow Lite, now named LiteRT, is still the same high-performance runtime for on-device AI, but with an expanded vision to support models authored in PyTorch, JAX, and Keras.",
  "siteName": "",
  "favicon": "",
  "text": "LiteRT (short for Lite Runtime) is the new name for TensorFlow Lite (TFLite). While the name is new, it's still the same trusted, high-performance runtime for on-device AI, now with an expanded vision.Since its debut in 2017, TFLite has enabled developers to bring ML-powered experiences to over 100K apps running on 2.7B devices. More recently, TFLite has grown beyond its TensorFlow roots to support models authored in PyTorch, JAX, and Keras with the same leading performance. The name LiteRT captures this multi-framework vision for the future: enabling developers to start with any popular framework and run their model on-device with exceptional performance.LiteRT, part of the Google AI Edge suite of tools, is the runtime that lets you seamlessly deploy ML and AI models on Android, iOS, and embedded devices. With AI Edge's robust model conversion and optimization tools, you can ready both open-source and custom models for on-device development. This change will roll out progressively. Starting today, you’ll see the LiteRT name reflected in the developer documentation, which is moving to ai.google.dev/edge/litert, and in other references across the AI Edge website. The documentation at tensorflow.org/lite now redirects to corresponding pages at ai.google.dev/edge/litert.The main TensorFlow brand will not be affected, nor will apps already using TensorFlow Lite.How to access LiteRTOur goal is that this change is minimally disruptive, requiring as few code changes from developers as possible.If you currently use TensorFlow Lite via packages, you’ll need to update any dependencies to use the new LiteRT from Maven, PyPi, Cocoapods.If you currently use TensorFlow Lite via Google Play Services, no change is necessary at this time.If you currently build TensorFlow Lite from source, please continue building from the TensorFlow repo until code has been fully moved to the new LiteRT repo later this year.Frequently asked questions1. What is changing beyond the new name, LiteRT?For now, the only change is the new name, LiteRT. Your production apps will not be affected. With a new name and refreshed vision, look out for more updates coming to LiteRT, improving how you deploy classic ML models, LLMs, and diffusion models with GPU and NPU acceleration across platforms.2. What’s happening to the TensorFlow Lite Support Library (including TensorFlow Lite Tasks)?The TensorFlow Lite support library and TensorFlow Lite Tasks will remain in the /tensorflow repository at this time. We encourage you to use MediaPipe Tasks for future development.3. What’s happening to TensorFlow Lite Model Maker?You can continue to access TFLite Model Maker via https://pypi.org/project/tflite-model-maker/4. What if I want to contribute code?For now, please contribute code to the existing TensorFlow Lite repository. We’ll make a separate announcement when we’re ready for contributions to the LiteRT repository.5. What’s happening to the .tflite file extension and file format?No changes are being made to the .tflite file extension or format. Conversion tools will continue to output .tflite flatbuffer files, and .tflite files will be readable by LiteRT.6. How do I convert models to .tflite format?For Tensorflow, Keras and Jax you can continue to use the same flows. For PyTorch support check out ai-edge-torch.7. Will there be any changes to classes and methods?No. Aside from package names, you won’t have to change any code you’ve written for now.8. Will there be any changes to TensorFlow.js?No, TensorFlow.js will continue to function independently as part of the Tensorflow codebase.9. My production app uses TensorFlow Lite. Will it be affected?Apps that have already deployed TensorFlow Lite will not be affected. This includes apps that access TensorFlow Lite via Google Play Services. (TFLite is compiled into the apps at build time, so once they’re deployed, apps have no dependency.)10. Why “LiteRT”?“LiteRT” (short for Lite Runtime) reflects the legacy of TensorFlow Lite, a pioneering “lite”, on-device runtime, plus Google’s commitment to supporting today’s thriving multi-framework ecosystem.11. Is TensorFlow Lite still being actively developed?Yes, but under the name LiteRT. Active development will continue on the runtime (now called LiteRT), as well as the conversion and optimization tools. To ensure you're using the most up-to-date version of the runtime, please use LiteRT.12. Where can I see examples of LiteRT in practice?You can find examples for Python, Android, and iOS in the official LiteRT samples repo.We’re excited for the future of on-device ML, and are committed to our vision of making LiteRT the easiest to use, highest performance runtime for a wide range of models.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/LiteRT_BlogGraphics_1600x873px_1.2e16d0ba.fill-1200x600.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"zb45t\"\u003eLiteRT (short for Lite Runtime) is the new name for TensorFlow Lite (TFLite). While the name is new, it\u0026#39;s still the same trusted, high-performance runtime for on-device AI, now with an expanded vision.\u003c/p\u003e\u003cp data-block-key=\"e20v2\"\u003eSince its debut in 2017, TFLite has enabled developers to bring ML-powered experiences to over 100K apps running on 2.7B devices. More recently, TFLite has grown beyond its TensorFlow roots to support models authored in \u003ca href=\"https://ai.google.dev/edge/lite/models/convert_pytorch\"\u003ePyTorch\u003c/a\u003e, \u003ca href=\"https://ai.google.dev/edge/lite/models/convert_jax\"\u003eJAX\u003c/a\u003e, and \u003ca href=\"https://ai.google.dev/edge/lite/models/convert\"\u003eKeras\u003c/a\u003e with the same leading performance. The name LiteRT captures this multi-framework vision for the future: enabling developers to start with any popular framework and run their model on-device with exceptional performance.\u003c/p\u003e\u003cp data-block-key=\"f5r0t\"\u003eLiteRT, part of the \u003ca href=\"https://ai.google.dev/edge\"\u003eGoogle AI Edge\u003c/a\u003e suite of tools, is the runtime that lets you seamlessly deploy ML and AI models on Android, iOS, and embedded devices. With AI Edge\u0026#39;s robust model conversion and optimization tools, you can ready both open-source and custom models for on-device development.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Lite-RT-feature.original.png\" alt=\"Lite-RT-feature\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"zb45t\"\u003eThis change will roll out progressively. Starting today, you’ll see the LiteRT name reflected in the developer documentation, which is moving to \u003ca href=\"http://ai.google.dev/edge/litert\"\u003eai.google.dev/edge/litert\u003c/a\u003e, and in other references across the AI Edge website. The documentation at \u003ca href=\"http://tensorflow.org/lite\"\u003etensorflow.org/lite\u003c/a\u003e now redirects to corresponding pages at \u003ca href=\"http://ai.google.dev/edge/litert\"\u003eai.google.dev/edge/litert\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"40on7\"\u003eThe main TensorFlow brand will not be affected, nor will apps already using TensorFlow Lite.\u003c/p\u003e\u003ch2 data-block-key=\"7dk9q\"\u003e\u003cbr/\u003e\u003cb\u003eHow to access LiteRT\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"9b956\"\u003eOur goal is that this change is minimally disruptive, requiring as few code changes from developers as possible.\u003c/p\u003e\u003cp data-block-key=\"cj1ed\"\u003e\u003cb\u003eIf you currently use TensorFlow Lite via packages,\u003c/b\u003e you’ll need to update any dependencies to use the new LiteRT from \u003ca href=\"https://maven.google.com/web/index.html?#com.google.ai.edge.litert\"\u003eMaven\u003c/a\u003e, \u003ca href=\"https://pypi.org/project/ai-edge-litert\"\u003ePyPi\u003c/a\u003e, \u003ca href=\"https://cocoapods.org/pods/TensorFlowLiteSwift\"\u003eCocoapods\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"dgvg\"\u003e\u003cb\u003eIf you currently use TensorFlow Lite via Google Play Services\u003c/b\u003e, no change is necessary at this time.\u003c/p\u003e\u003cp data-block-key=\"f1q2g\"\u003e\u003cb\u003eIf you currently build TensorFlow Lite from source,\u003c/b\u003e please continue building from the \u003ca href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite\"\u003eTensorFlow repo\u003c/a\u003e until code has been fully moved to the new\u003ca href=\"https://github.com/google-ai-edge/litert\"\u003e LiteRT repo\u003c/a\u003e later this year.\u003c/p\u003e\u003ch2 data-block-key=\"a57g1\"\u003e\u003cbr/\u003e\u003cb\u003eFrequently asked questions\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"bd9g7\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e1. \u003cb\u003eWhat is changing beyond the new name, LiteRT?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"1h53v\"\u003eFor now, the only change is the new name, LiteRT. Your production apps will not be affected. With a new name and refreshed vision, look out for more updates coming to LiteRT, improving how you deploy classic ML models, LLMs, and diffusion models with GPU and NPU acceleration across platforms.\u003c/p\u003e\u003cp data-block-key=\"adflu\"\u003e\u003cbr/\u003e2.\u003cb\u003e What’s happening to the\u003c/b\u003e \u003ca href=\"https://github.com/tensorflow/tflite-support\"\u003e\u003cb\u003eTensorFlow Lite Support Library\u003c/b\u003e\u003c/a\u003e\u003cb\u003e (including TensorFlow Lite Tasks)?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"3mpgt\"\u003eThe TensorFlow Lite support library and TensorFlow Lite Tasks will remain in the /tensorflow repository at this time. We encourage you to use \u003ca href=\"https://ai.google.dev/edge/mediapipe/solutions/tasks\"\u003eMediaPipe Tasks\u003c/a\u003e for future development.\u003c/p\u003e\u003cp data-block-key=\"1j1mc\"\u003e\u003cbr/\u003e3. \u003cb\u003eWhat’s happening to TensorFlow Lite Model Maker?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"4hfuf\"\u003eYou can continue to access TFLite Model Maker via \u003ca href=\"https://pypi.org/project/tflite-model-maker/\"\u003ehttps://pypi.org/project/tflite-model-maker/\u003c/a\u003e\u003c/p\u003e\u003cp data-block-key=\"d31cu\"\u003e\u003cbr/\u003e4. \u003cb\u003eWhat if I want to contribute code?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"4l54\"\u003eFor now, please contribute code to the existing TensorFlow Lite repository. We’ll make a separate announcement when we’re ready for contributions to the LiteRT repository.\u003c/p\u003e\u003cp data-block-key=\"brf4k\"\u003e\u003cbr/\u003e5. \u003cb\u003eWhat’s happening to the .tflite file extension and file format?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"e8qtn\"\u003eNo changes are being made to the .tflite file extension or format. Conversion tools will continue to output .tflite flatbuffer files, and .tflite files will be readable by LiteRT.\u003c/p\u003e\u003cp data-block-key=\"72gfs\"\u003e\u003cbr/\u003e6. \u003cb\u003eHow do I convert models to .tflite format?\u003c/b\u003e\u003cbr/\u003eFor Tensorflow, Keras and Jax you can continue to use the same flows. For PyTorch support check out \u003ca href=\"https://github.com/google-ai-edge/ai-edge-torch\"\u003eai-edge-torch\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"cbv9m\"\u003e\u003cbr/\u003e7. \u003cb\u003eWill there be any changes to classes and methods?\u003c/b\u003e\u003cbr/\u003eNo. Aside from package names, you won’t have to change any code you’ve written for now.\u003c/p\u003e\u003cp data-block-key=\"6769n\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e8.\u003cb\u003e Will there be any changes to\u003c/b\u003e \u003ca href=\"https://www.tensorflow.org/js\"\u003e\u003cb\u003eTensorFlow.js\u003c/b\u003e\u003c/a\u003e\u003cb\u003e?\u003c/b\u003e\u003cbr/\u003eNo, TensorFlow.js will continue to function independently as part of the Tensorflow codebase.\u003c/p\u003e\u003cp data-block-key=\"6htf2\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e9.\u003cb\u003e My production app uses TensorFlow Lite. Will it be affected?\u003c/b\u003e\u003cbr/\u003eApps that have already deployed TensorFlow Lite will not be affected. This includes apps that access TensorFlow Lite via Google Play Services. (TFLite is compiled into the apps at build time, so once they’re deployed, apps have no dependency.)\u003c/p\u003e\u003cp data-block-key=\"3p6pd\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e10.\u003cb\u003e Why “LiteRT”?\u003c/b\u003e\u003cbr/\u003e“LiteRT” (short for Lite Runtime) reflects the legacy of TensorFlow Lite, a pioneering “lite”, on-device runtime, plus Google’s commitment to supporting today’s thriving multi-framework ecosystem.\u003c/p\u003e\u003cp data-block-key=\"bfej6\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e11.\u003cb\u003e Is TensorFlow Lite still being actively developed?\u003c/b\u003e\u003cbr/\u003eYes, but under the name LiteRT. Active development will continue on the runtime (now called LiteRT), as well as the conversion and optimization tools. To ensure you\u0026#39;re using the most up-to-date version of the runtime, please use \u003ca href=\"https://ai.google.dev/edge/litert\"\u003eLiteRT\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"53v2t\"\u003e\u003cbr/\u003e12. \u003cb\u003eWhere can I see examples of LiteRT in practice?\u003c/b\u003e\u003c/p\u003e\u003cp data-block-key=\"aq536\"\u003eYou can find examples for Python, Android, and iOS in the official \u003ca href=\"https://github.com/google-ai-edge/litert-samples\"\u003eLiteRT samples repo\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"4bm06\"\u003e\u003cbr/\u003eWe’re excited for the future of on-device ML, and are committed to our vision of making LiteRT the easiest to use, highest performance runtime for a wide range of models.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2024-09-04T00:00:00Z",
  "modifiedTime": null
}
