{
  "id": "8560c8d4-b986-4791-ba34-1b904b5c09d4",
  "title": "Protecting user data through source code analysis at scale",
  "link": "https://engineering.fb.com/2025/02/18/security/protecting-user-data-through-source-code-analysis/",
  "description": "Meta’s Anti Scraping team focuses on preventing unauthorized scraping as part of our ongoing work to combat data misuse. In order to protect Meta’s changing codebase from scraping attacks, we have introduced static analysis tools into our workflow. These tools allow us to detect potential scraping vectors at scale across our Facebook, Instagram, and even [...] Read More... The post Protecting user data through source code analysis at scale appeared first on Engineering at Meta.",
  "author": "",
  "published": "Tue, 18 Feb 2025 20:30:49 +0000",
  "source": "https://engineering.fb.com/feed/",
  "categories": [
    "Security"
  ],
  "byline": "By Steven Hsieh, Mudita Khurana",
  "length": 5884,
  "excerpt": "Meta’s Anti Scraping team focuses on preventing unauthorized scraping as part of our ongoing work to combat data misuse. In order to protect Meta’s changing codebase from scraping attacks, we have …",
  "siteName": "Engineering at Meta",
  "favicon": "",
  "text": "Meta’s Anti Scraping team focuses on preventing unauthorized scraping as part of our ongoing work to combat data misuse. In order to protect Meta’s changing codebase from scraping attacks, we have introduced static analysis tools into our workflow. These tools allow us to detect potential scraping vectors at scale across our Facebook, Instagram, and even parts of our Reality Labs codebases.  What is scraping?  Scraping is the automated collection of data from a website or app and can be either authorized or unauthorized. Unauthorized scrapers commonly hide themselves by mimicking the ways users would normally use a product. As a result, unauthorized scraping can be difficult to detect. At Meta, we take a number of steps to combat scraping and have a number of methods to distinguish unauthorized automated activity from legitimate usage.  Proactive detection Meta’s Anti-Scraping team learns about scrapers (entities attempting to scrape our systems) through many different sources. For example, we investigate suspected unauthorized scraping activity and take actions against such entities, including sending cease-and-desist letters and disabling accounts. Part of our strategy is to further develop proactive measures to mitigate the risk of scraping over and above our reactive approaches. One way we do this is by turning our attack vector criteria into static analysis rules that run automatically on our entire code base. Those static analysis tools, which include Zoncolan for Hack and Pysa for Python, run automatically for their respective codebases and are built in-house, allowing us to customize them for Anti-Scraping purposes. This approach can identify potential issues early and ensure product development teams have an opportunity to remediate prior to launch. Static analysis tools enable us to apply learnings across events to systematically prevent similar issues from existing in our codebase. They also help us create best practices when developing code to combat unauthorized scraping. Developing static analysis rules Our static analysis tools (like Zoncolan and Pysa) focus on tracking data flow through a program. Engineers define classes of issues using the following: Sources are where the data originates. For potential scraping issues, these are mostly user-controlled parameters, as these are the avenues in which scrapers control the data they could receive. Sinks are where the data flows to. For scraping, the sink is usually when the data flows back to the user. An Issue is found when our tools detect a possibility of data flow from a source to a sink. For example, assume the “source” to be the user-controlled “count” parameter that determines the number of results loaded, and “the sink” to be the data that is returned to the user. Here, the user controlled “count” parameter is an entrypoint for a scraper who can manipulate its value to extract more data than intended by the application. When our tools suspect that there is a code flow between such sources and sinks, it alerts the team for further triage. An example of static analysis Building on the example above, see the below mock code excerpt loading the number of followers for a page: # views/followers.py async def get_followers(request: HttpRequest) -\u003e HttpResponse: viewer = request.GET['viewer_id'] target = request.GET['target_id'] count = request.GET['count'] if(can_see(viewer, target)): followers = load_followers(target, count) return followers # controller/followers.py async def load_followers(target_id: int, count: int): ... In the example above, the mock endpoint backed by get_followers is a potential scraping attack vector since the “user” and “count” variables control whose information is to be loaded and number of followers returned. Under usual circumstances, the endpoint would be called with suitable parameters that match what the user is browsing on screen. However, scrapers can abuse such an endpoint by specifying arbitrary users and large counts which can result in their entire follower lists returned in a single request. By doing so, scrapers can try to evade rate limiting systems which limit how many requests a user can send to our systems in a defined timeframe. These systems are set in place to stop any scraping attempts at a high level. Since our static analysis systems run automatically on our codebase, the Anti-Scraping team can identify such scraping vectors proactively and make remediations before the code is introduced to our production systems. For example, the recommended fix for the code above is to cap the maximum number of results that can be returned at a time: # views/followers.py async def get_followers(request: HttpRequest) -\u003e HttpResponse: viewer = request.Get['viewer_id'] target = request.GET['target_id'] count = min(request.GET['count'], MAX_FOLLOWERS_RESULTS) if(can_see(viewer, target)): followers = load_followers(target, count) return followers # controller/followers.py async def load_followers(target_id: int, count: int): ... Following the fix, the maximum number of results retrieved by each request is limited to MAX_FOLLOWERS_RESULTS. Such a change would not affect regular users and only interfere with scrapers, forcing them to send magnitudes more requests that would then trigger our rate limiting systems. The limitations of static analysis in combating unauthorized scraping  Static analysis tools are not designed to catch all possible unauthorized scraping issues. Because unauthorized scrapers can mimic the legitimate ways that people use Meta’s products, we cannot fully prevent all unauthorized scraping without affecting people’s ability to use our apps and websites the way they enjoy. Since unauthorized scraping is both a common and complex challenge to solve, we combat scraping by taking a more holistic approach to staying ahead of scraping actors.",
  "image": "https://engineering.fb.com/wp-content/uploads/2023/08/Eng-Blog-Self-Serve-Hero-Images-PRIVACY-103-Navy-1.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eMeta’s Anti Scraping team focuses on preventing \u003c/span\u003e\u003ca href=\"https://about.fb.com/news/2021/04/how-we-combat-scraping/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eunauthorized scraping\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e as part of our ongoing work to combat data misuse. In order to protect Meta’s \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2022/11/16/culture/meta-code-review-time-improving/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003echanging codebase\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e from scraping attacks, we have introduced static analysis tools into our workflow. These tools allow us to \u003c/span\u003e\u003cb\u003edetect potential scraping vectors\u003c/b\u003e\u003cspan\u003e at scale across our Facebook, Instagram, and even parts of our Reality Labs codebases. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhat is scraping? \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://about.fb.com/news/2021/04/how-we-combat-scraping/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eScraping\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is the automated collection of data from a website or app and can be either authorized or unauthorized. Unauthorized scrapers commonly hide themselves by mimicking the ways users would normally use a product. As a result, unauthorized scraping can be difficult to detect. At Meta, we take a number of steps to \u003c/span\u003e\u003ca href=\"https://about.fb.com/news/2021/04/how-we-combat-scraping/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ecombat scraping\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and have a number of methods to distinguish unauthorized automated activity from legitimate usage. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eProactive detection\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eMeta’s Anti-Scraping team learns about scrapers (entities attempting to scrape our systems) through many different sources. For example, we \u003c/span\u003e\u003ca href=\"https://about.fb.com/news/2021/04/how-we-combat-scraping/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003einvestigate suspected unauthorized scraping activity\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and take actions against such entities, including sending cease-and-desist letters and disabling accounts.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePart of our strategy is to further develop proactive measures to mitigate the risk of scraping over and above our reactive approaches. One way we do this is by \u003c/span\u003e\u003cb\u003eturning our attack vector criteria into static analysis rules\u003c/b\u003e\u003cspan\u003e that run automatically on our entire code base. Those static analysis tools, which include\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2019/08/15/security/zoncolan/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eZoncolan\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for Hack and\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2020/08/07/security/pysa/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ePysa\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for Python, run automatically for their respective codebases and are built in-house, allowing us to customize them for Anti-Scraping purposes. This approach can identify potential issues early and ensure product development teams have an opportunity to remediate prior to launch.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eStatic analysis tools enable us to apply learnings across events to systematically prevent similar issues from existing in our codebase. They also help us create best practices when developing code to combat unauthorized scraping.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eDeveloping static analysis rules\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eOur static analysis tools (like\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2019/08/15/security/zoncolan/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003eZoncolan\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and\u003c/span\u003e \u003ca href=\"https://engineering.fb.com/2020/08/07/security/pysa/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ePysa\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e) focus on tracking data flow through a program.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEngineers define classes of issues using the following:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003ci\u003e\u003cspan\u003eSources\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e are where the data originates. For potential scraping issues, these are mostly user-controlled parameters, as these are the avenues in which scrapers control the data they could receive.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003ci\u003e\u003cspan\u003eSinks\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e are where the data flows to. For scraping, the sink is usually when the data flows back to the user.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAn \u003c/span\u003e\u003ci\u003e\u003cspan\u003eIssue\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e is found when our tools detect a possibility of data flow from a source to a sink.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eFor example, assume the “source” to be the user-controlled “count” parameter that determines the number of results loaded, and “the sink” to be the data that is returned to the user. Here, the user controlled “count” parameter is an entrypoint for a scraper who can manipulate its value to extract more data than intended by the application. When our tools suspect that there is a code flow between such sources and sinks, it alerts the team for further triage.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAn example of static analysis\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eBuilding on the example above, see the below mock code excerpt loading the number of followers for a page:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# views/followers.py\nasync def get_followers(request: HttpRequest) -\u0026gt; HttpResponse:\n\tviewer = request.GET[\u0026#39;viewer_id\u0026#39;]\ntarget = request.GET[\u0026#39;target_id\u0026#39;]\n\tcount = request.GET[\u0026#39;count\u0026#39;]\n\tif(can_see(viewer, target)):\n\t\tfollowers = load_followers(target, count)\n\t\treturn followers\n\n# controller/followers.py\nasync def load_followers(target_id: int, count: int):\n\t...\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eIn the example above, the mock endpoint backed by \u003c/span\u003e\u003cspan\u003eget_followers\u003c/span\u003e\u003cspan\u003e is a potential scraping attack vector since the “user” and “count” variables control whose information is to be loaded and number of followers returned. Under usual circumstances, the endpoint would be called with suitable parameters that match what the user is browsing on screen. However, scrapers can abuse such an endpoint by specifying arbitrary users and large counts which can result in their entire follower lists returned in a single request. By doing so, scrapers can try to evade rate limiting systems which limit how many requests a user can send to our systems in a defined timeframe. These systems are set in place to stop any scraping attempts at a high level.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSince our static analysis systems run automatically on our codebase, the Anti-Scraping team can identify such scraping vectors proactively and make remediations before the code is introduced to our production systems. For example, the recommended fix for the code above is to cap the maximum number of results that can be returned at a time:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# views/followers.py\nasync def get_followers(request: HttpRequest) -\u0026gt; HttpResponse:\n\tviewer = request.Get[\u0026#39;viewer_id\u0026#39;]\n\ttarget = request.GET[\u0026#39;target_id\u0026#39;]\n\tcount = min(request.GET[\u0026#39;count\u0026#39;], MAX_FOLLOWERS_RESULTS)\n\tif(can_see(viewer, target)):\n\t    followers = load_followers(target, count)\n\t\treturn followers\n\n# controller/followers.py\nasync def load_followers(target_id: int, count: int):\n\t...\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eFollowing the fix, the maximum number of results retrieved by each request is limited to \u003c/span\u003e\u003cspan\u003eMAX_FOLLOWERS_RESULTS\u003c/span\u003e\u003cspan\u003e. Such a change would not affect regular users and only interfere with scrapers, forcing them to send magnitudes more requests that would then trigger our rate limiting systems.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe limitations of static analysis in combating unauthorized scraping \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eStatic analysis tools are not designed to catch all possible unauthorized scraping issues. Because unauthorized scrapers can mimic the legitimate ways that people use Meta’s products, we cannot fully prevent all unauthorized scraping without affecting people’s ability to use our apps and websites the way they enjoy. Since unauthorized scraping is both a common and complex challenge to solve, \u003c/span\u003e\u003ca href=\"https://about.fb.com/news/2021/04/how-we-combat-scraping/\" target=\"_blank\" rel=\"noopener\"\u003e\u003cspan\u003ewe combat scraping by taking a more holistic approach\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to staying ahead of scraping actors. \u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-02-18T20:30:49Z",
  "modifiedTime": "2025-02-18T20:03:42Z"
}
