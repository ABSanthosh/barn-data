{
  "id": "83c1db1a-56d9-4abe-bdb5-24ea126f9a66",
  "title": "Google Enhances Data Privacy with Confidential Federated Analytics",
  "link": "https://www.infoq.com/news/2025/03/confidential-federated-analytics/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Google has announced Confidential Federated Analytics (CFA), a technique designed to increase transparency in data processing while maintaining privacy. Building on federated analytics, CFA leverages confidential computing to ensure that only predefined and inspectable computations are performed on user data without exposing raw data to servers or engineers. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Tue, 11 Mar 2025 20:02:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Google",
    "Data Privacy",
    "Privacy",
    "Security",
    "Data Analytics",
    "Mobile",
    "Android",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 3173,
  "excerpt": "Google has announced Confidential Federated Analytics (CFA), a technique designed to increase transparency in data processing while maintaining privacy. Building on federated analytics, CFA leverages",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250311074537/apple-touch-icon.png",
  "text": "Google has announced Confidential Federated Analytics (CFA), a technique designed to increase transparency in data processing while maintaining privacy. Building on federated analytics, CFA leverages confidential computing to ensure that only predefined and inspectable computations are performed on user data without exposing raw data to servers or engineers. Federated analytics allows for distributed data analysis while keeping raw data on user devices. Traditionally, devices respond to queries by sending aggregated statistics rather than individual data points. However, users have had no way to verify how their data is being processed, which presents trust and security challenges. CFA addresses this limitation by using Trusted Execution Environments (TEEs). These restrict computations to predefined analyses and prevent unauthorized access to raw data. CFA also makes all privacy-relevant server-side software publicly inspectable, allowing external verification of the data-handling process. Source: Google Blog Richard Seroter, a director of developer relations at Google Cloud, noted the importance of this advancement, stating: This feels like a real step forward. Federated learning and computation using lots of real devices is very cool but can make privacy-oriented folks nervous. Google has deployed CFA in Gboard, its Android keyboard, to improve new word detection across over 900 languages. Language models require updates to recognize emerging words while filtering out rare, private, or non-standard entries. Previously, Google used LDP-TrieHH, a local differential privacy-based approach. However, this method had limited scalability and required weeks to process updates, particularly for languages with lower user volume. With CFA, the system processed 3,600 missing Indonesian words in two days, reaching more devices and languages while maintaining stronger differential privacy guarantees. CFA operates through a structured, multi-step process that ensures data remains private while enabling meaningful analysis. The workflow consists of the following key stages: Data Collection and Encryption: Devices store relevant data locally and encrypt it before upload. Access Policy Enforcement: Data can only be decrypted for pre-approved computations, defined by structured policies. TEE Execution: Data processing occurs within a TEE, ensuring confidentiality and preventing unauthorized modifications. Differential Privacy Algorithm: The system applies a stability-based histogram approach, adding noise before identifying frequently typed words. External Verifiability: The processing pipeline, software, and cryptographic proofs are logged in a public transparency ledger for external auditing. Google plans to apply Confidential Federated Computations to broader federated learning tasks, enabling AI model training with strict privacy guarantees. The technique is expected to be integrated into Android Private Compute Core and other privacy-focused systems. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/03/confidential-federated-analytics/en/headerimage/generatedHeaderImage-1741722704436.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cdiv\u003e\u003cp\u003eGoogle has announced \u003ca href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\"\u003eConfidential Federated Analytics (CFA)\u003c/a\u003e, a technique designed to increase transparency in data processing while maintaining privacy. Building on federated analytics, CFA leverages confidential computing to ensure that only predefined and inspectable computations are performed on user data without exposing raw data to servers or engineers.\u003c/p\u003e\u003cp\u003e\n\nFederated analytics allows for distributed data analysis while keeping raw data on user devices. Traditionally, devices respond to queries by sending aggregated statistics rather than individual data points. However, users have had no way to verify how their data is being processed, which presents trust and security challenges.\u003c/p\u003e\u003cp\u003e\n\nCFA addresses this limitation by using \u003ca href=\"https://learn.microsoft.com/en-us/azure/confidential-computing/trusted-execution-environment\"\u003eTrusted Execution Environments (TEEs)\u003c/a\u003e. These restrict computations to predefined analyses and prevent unauthorized access to raw data. CFA also makes all privacy-relevant server-side software publicly inspectable, allowing external verification of the data-handling process.\u003c/p\u003e\u003cp\u003e\n\n\u003cimg alt=\"scheme\" data-src=\"news/2025/03/confidential-federated-analytics/en/resources/1Zrzut ekranu 2025-03-11 204621-1741722703473.png\" src=\"https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/news/2025/03/confidential-federated-analytics/en/resources/1Zrzut ekranu 2025-03-11 204621-1741722703473.png\" rel=\"share\"/\u003e\u003c/p\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cem\u003eSource: Google Blog\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eRichard Seroter, a director of developer relations at Google Cloud, noted the importance of this advancement, \u003ca href=\"https://x.com/rseroter/status/1898023668363809130\"\u003estating\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis feels like a real step forward. Federated learning and computation using lots of real devices is very cool but can make privacy-oriented folks nervous.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cdiv\u003e\u003cp\u003eGoogle has deployed CFA in \u003ca href=\"https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin\u0026amp;hl=en_US\u0026amp;pli=1\"\u003eGboard\u003c/a\u003e, its Android keyboard, to improve new word detection across over 900 languages. Language models require updates to recognize emerging words while filtering out rare, private, or non-standard entries.\u003c/p\u003e\u003cp\u003e\n\nPreviously, Google used LDP-TrieHH, a local differential privacy-based approach. However, this method had limited scalability and required weeks to process updates, particularly for languages with lower user volume.\u003c/p\u003e\u003cp\u003e\n\nWith CFA, the system processed 3,600 missing Indonesian words in two days, reaching more devices and languages while maintaining stronger differential privacy guarantees.\u003c/p\u003e\u003cp\u003e\n\nCFA operates through a structured, multi-step process that ensures data remains private while enabling meaningful analysis. The workflow consists of the following key stages:\u003c/p\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\t\u003cli\u003eData Collection and Encryption: Devices store relevant data locally and encrypt it before upload.\u003c/li\u003e\n\t\u003cli\u003eAccess Policy Enforcement: Data can only be decrypted for pre-approved computations, defined by structured policies.\u003c/li\u003e\n\t\u003cli\u003eTEE Execution: Data processing occurs within a TEE, ensuring confidentiality and preventing unauthorized modifications.\u003c/li\u003e\n\t\u003cli\u003eDifferential Privacy Algorithm: The system applies a stability-based histogram approach, adding noise before identifying frequently typed words.\u003c/li\u003e\n\t\u003cli\u003eExternal Verifiability: The processing pipeline, software, and cryptographic proofs are logged in a public transparency ledger for external auditing.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eGoogle plans to apply Confidential Federated Computations to broader federated learning tasks, enabling AI model training with strict privacy guarantees. The technique is expected to be integrated into \u003ca href=\"https://security.googleblog.com/2021/09/introducing-androids-private-compute.html#:~:text=We%20introduced%20Android\u0026#39;s%20Private%20Compute,re%20having%20in%20messaging%20apps\"\u003eAndroid Private Compute Core\u003c/a\u003e and other privacy-focused systems.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-03-11T00:00:00Z",
  "modifiedTime": null
}
