{
  "id": "879d43d6-2251-4f1d-8522-ea6ee0690492",
  "title": "Simplified Dataflow Connectors with Managed I/O",
  "link": "https://developers.googleblog.com/en/simplified-dataflow-connectors-with-managed-io/",
  "description": "Google Cloud Dataflow's Managed I/O simplifies using Apache Beam I/O connectors by automatically updating connectors to the latest versions and providing a standardized API, optimizing connectors specifically for Dataflow, ensuring efficient performance and reducing the need for manual configuration, freeing users to focus on pipeline logic.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Chamikara Jayalath",
  "length": 9889,
  "excerpt": "Google Cloud Dataflow's Managed I/O simplifies using Apache Beam I/O connectors by automatically updating connectors to the latest versions and providing a standardized API, optimizing connectors specifically for Dataflow, ensuring efficient performance and reducing the need for manual configuration, freeing users to focus on pipeline logic.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Google Cloud Dataflow offers a fully managed data processing system for running Apache Beam pipelines on Google Cloud in a highly scalable manner. Due to being a fully managed service, Dataflow users do not have to worry about any service side regressions and versioning. The promise is that you only concern yourself with your pipeline logic while Google takes care of the service infrastructure. While this is certainly true, Apache Beam itself is a very full featured SDK that provides many simple to highly complex transforms for you to use in their pipelines. For example, Apache Beam provides a number of I/O connectors. Many of these connectors are Apache Beam composite transforms from 10s to 100s of steps. Historically, these have been considered \"user code\" from the service's perspective, despite being not authored or maintained by the user. There are several common complications customers run into complex Beam transforms such as I/O connectors.You are on the hook for upgrading Beam to adopt any fixes and improvements to connectors.Connector APIs vary widely and moving from one connector to another usually requires a lot of exploration and learning.While connectors offer a complete API, the API might not be optimized for the Dataflow runner.To alleviate all three of these issues, Dataflow recently introduced a new offering named Managed I/O. With Managed I/O the service itself is able to manage these complexities on your behalf. Hence you can truly focus on their pipelines business logic instead of focussing on the minutiae related to using and configuring a specific connector to suit their needs. Below we detail how each of the above mentioned complexities are addressed via Managed I/O.Automatic SDK upgradesApache Beam is a fully fledged SDK with many transforms, features, and optimization. Like many large pieces of software, upgrading Beam to a new version can be a significant process. Usually upgrading Beam involves upgrading all parts of a pipeline including all I/O connectors. But sometimes, you just need to obtain access to a critical bug fix or an improvement available in the latest version of one or more I/O connectors used in your pipeline.Managed I/O with Dataflow simplifies this by completely taking over the management of the Beam I/O connector version. With Managed I/O, Dataflow will make sure that I/O connectors used by pipelines are always up to date. Dataflow performs this by always upgrading I/O connectors to the latest vetted version during job submission and streaming update via replacement.For example, assume that you use a Beam pipeline that uses Beam 2.x.0 and assume that you use the Managed Apache Iceberg I/O source in your pipeline. Also, assume that the latest vetted version of the Iceberg I/O source supported by Dataflow is 2.y.0. During job submission, Dataflow will replace this specific connector with version 2.y.0 and will keep the rest of the Beam pipeline including any standard (non-managed) I/O connectors at version 2.x.0. After replacement, Dataflow optimizes the updated pipeline and executes it in GCE. To achieve isolation between connectors from different Beam versions, Dataflow deploys an additional Beam SDK container in GCE VMs. So in this case, Beam SDK containers from both versions 2.x.0 and 2.y.0 will be running in each GCE VM used by the Dataflow job.So with Managed I/O you can be assured that I/O connectors used in your pipeline are always up to date. This allows you to focus on improving the business logic of your pipeline without worrying about upgrading the Beam version to simply obtain I/O connector updates.Simplified IO APIAPIs differences across Beam I/O connectors vary greatly. This means that, whenever you try to use a new Beam I/O connector, you would have to learn an API specific to that connector. Some of the APIs can be quite large and non-intuitive. This can be due to:Support for various and in some cases redundant features offered by the underlying system.Maintaining backwards compatibility for legacy (or archaic) features or defaults.Support for customizing the I/O connector to support edge cases and implementation details that may only apply to few customers.Above points result in very large API surfaces for some connectors that are not intuitive for a new customer to use efficiently.Managed I/O offers standardized Java and Python APIs for supported I/O connectors. For example, with Beam Java SDK an I/O connector source can be instantiated in the following standardized form. Managed.read(SOURCE).withConfig(sourceConfig) An I/O connector sink can be instantiated in the following form. Managed.write(SINK).withConfig(sinkConfig) Here SOURCE and SINK are keys specifically identifying the connector while sourceConfig and sinkConfig are maps of configurations used to instantiate the connector source or sink. The map of configurations may also be provided as YAML files available locally or in Google Cloud Storage. Please see the Managed I/O website for more complete examples for supported sources and sinks.Beam Python SDK offers a similarly simplified API.This means that various Beam I/O connectors with different APIs can be instantiated in a very standard way. For example, // Create a Java BigQuery I/O source Map\u003cString, Object\u003e bqReadConfig = ImmutableMap.of(\"query\", \"\u003cquery\u003e\", ...); Managed.read(Managed.BIGQUERY).withConfig(bqReadConfig) // Create a Java Kafka I/O source. Map\u003cString, Object\u003e kafkaReadConfig = ImmutableMap.of(\"bootstrap_servers\", \"\u003cserver\u003e\", \"topic\", \"\u003ctopic\u003e\", ...); Managed.read(Managed.KAFKA).withConfig(kafkaReadConfig) // Create a Java Kafka I/O source but with a YAML based config available in Google Cloud Storage. String kafkaReadYAMLConfig = \"gs://path/to/config.yaml\" Managed.read(Managed.KAFKA).withConfigUrl(kafkaReadYAMLConfig) // Create a Python Iceberg I/O source. iceberg_config = {\"table\": \"\u003ctable\u003e\", ...} managed.Read(managed.ICEBERG, config=iceberg_config) Automatically optimized for DataflowMany Beam connectors offer a comprehensive API for configuring and optimizing the connector to suit a given pipeline and a given Beam runner. One downside of this is that if you specifically want to run on Dataflow, you may have to learn the specific configurations that best suit Dataflow and apply them when setting up your pipeline. Connector related documentation can be long and detailed and specific changes needed might not be intuitive. This might result in connectors used in Dataflow pipelines performing in a sub-optimal way.Manage I/O connectors alleviates this by automatically re-configuring the connectors to incorporate best practices and configure them to best suit Dataflow. Such re-configuration may occur during job submission or streaming update via replacement.For example, Dataflow streaming pipelines offer two modes, exactly-once and at-least-once while BigQuery I/O sink with Storage Write API offer two analogous delivery semantics, exactly-once and at-least-once. BigQuery sink with at-least-once delivery semantics is usually less expensive and results in lower latencies. With standard BigQuery I/O connectors, you are responsible for making sure that you use the appropriate mode when using the BigQuery I/O. With Managed BigQuery I/O sink this is automatically configured for you. Which means that if your streaming pipeline is operating at the at-least-once mode, your Managed I/O BigQuery sink will be automatically configured to use the at-least-once delivery semantics.Real-world pipelinesWe ran several pipelines that wrote data using the Managed Iceberg I/O sink backed by a Hadoop catalog deployed in GCS (please see here for the other supported catalogs). Pipelines were submitted using Beam 2.61.0 and the Managed I/O sink was automatically upgraded by Dataflow to the latest supported version. All benchmarks used n1-standard-4 VMs and the number of VMs used by the pipeline was fixed to 100. Please note that execution time here does not include the startup and shutdown time. As the benchmarks show, Managed Iceberg I/O scaled up nicely and both metrics grew linearly with the data size.We also ran a streaming pipeline that read from Google Pub/Sub and used the Managed I/O Kafka sink to push messages to a Kafka cluster hosted in GCP. The pipeline used Beam 2.61.0 and Dataflow upgraded the Managed Kafka sink to the latest supported version. During the steady state, the pipeline used 10 n1-standard-4 VMs (max 20 VMs). The pipeline was consistently processing messages at a throughput of 250k msgs/sec across all steps and was run for 2 hours. The following graph shows the data throughputs of various steps of the pipeline. Note that throughputs are different here since the element size changes between steps. The pipeline read from Pub/Sub at a rate of 75 MiB/sec (red line) and wrote to Kafka at a rate of 40 MiB/sec (green line). Both latency and backlog was low for the duration of the pipeline execution. The pipeline used VM CPU and memory efficiently. As these results show, the pipeline executed efficiently with the upgraded Managed I/O Kafka sink provided by the Dataflow service.How can I use Managed I/O in my pipeline ?Using Managed I/O is simple as using one of the supported sources and sinks in your pipeline and running the pipeline with Dataflow Runner v2. When you run your pipeline, Dataflow will make sure that the latest vetted versions of the sources and sinks are enabled during job submission and streaming update via replacement, even if you are using an older Beam version for your pipeline.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Cloud-2-meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"hfxj5\"\u003e\u003ca href=\"https://cloud.google.com/products/dataflow\"\u003eGoogle Cloud Dataflow\u003c/a\u003e offers a fully managed data processing system for running \u003ca href=\"https://beam.apache.org/\"\u003eApache Beam\u003c/a\u003e pipelines on Google Cloud in a highly scalable manner. Due to being a fully managed service, Dataflow users do not have to worry about any service side regressions and versioning. The promise is that you only concern yourself with your pipeline logic while Google takes care of the service infrastructure. While this is certainly true, Apache Beam itself is a very full featured SDK that provides many simple to highly complex transforms for you to use in their pipelines. For example, Apache Beam provides a number of I/O connectors. Many of these connectors are Apache Beam composite transforms from 10s to 100s of steps. Historically, these have been considered \u0026#34;user code\u0026#34; from the service\u0026#39;s perspective, despite being not authored or maintained by the user. There are several common complications customers run into complex Beam transforms such as I/O connectors.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"25t4o\"\u003eYou are on the hook for upgrading Beam to adopt any fixes and improvements to connectors.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"nkdh\"\u003eConnector APIs vary widely and moving from one connector to another usually requires a lot of exploration and learning.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4gls4\"\u003eWhile connectors offer a complete API, the API might not be optimized for the Dataflow runner.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"eifj1\"\u003eTo alleviate all three of these issues, Dataflow recently introduced a new offering named \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/managed-io\"\u003eManaged I/O\u003c/a\u003e. With Managed I/O the service itself is able to manage these complexities on your behalf. Hence you can truly focus on their pipelines business logic instead of focussing on the minutiae related to using and configuring a specific connector to suit their needs. Below we detail how each of the above mentioned complexities are addressed via Managed I/O.\u003c/p\u003e\u003ch2 data-block-key=\"7otq1\"\u003e\u003cbr/\u003eAutomatic SDK upgrades\u003c/h2\u003e\u003cp data-block-key=\"cmgq0\"\u003eApache Beam is a fully fledged SDK with many transforms, features, and optimization. Like many large pieces of software, upgrading Beam to a new version can be a significant process. Usually upgrading Beam involves upgrading all parts of a pipeline including all I/O connectors. But sometimes, you just need to obtain access to a critical bug fix or an improvement available in the latest version of one or more I/O connectors used in your pipeline.\u003c/p\u003e\u003cp data-block-key=\"5bkp1\"\u003eManaged I/O with Dataflow simplifies this by completely taking over the management of the Beam I/O connector version. With Managed I/O, Dataflow will make sure that I/O connectors used by pipelines are always up to date. Dataflow performs this by always upgrading I/O connectors to the latest vetted version during job submission and \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching\"\u003estreaming update via replacement\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"8drmi\"\u003eFor example, assume that you use a Beam pipeline that uses Beam 2.x.0 and assume that you use the \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/managed-io-iceberg\"\u003eManaged Apache Iceberg I/O source\u003c/a\u003e in your pipeline. Also, assume that the latest vetted version of the Iceberg I/O source supported by Dataflow is 2.y.0. During job submission, Dataflow will replace this specific connector with version 2.y.0 and will keep the rest of the Beam pipeline including any standard (non-managed) I/O connectors at version 2.x.0.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/diagram1.original.png\" alt=\"dataflow architecture of a Beam pipeline using the managed iceberg I/O source\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"hfxj5\"\u003eAfter replacement, Dataflow optimizes the updated pipeline and executes it in GCE. To achieve isolation between connectors from different Beam versions, Dataflow deploys an additional Beam SDK container in GCE VMs. So in this case, Beam SDK containers from both versions 2.x.0 and 2.y.0 will be running in each GCE VM used by the Dataflow job.\u003c/p\u003e\u003cp data-block-key=\"6gt75\"\u003eSo with Managed I/O you can be assured that I/O connectors used in your pipeline are always up to date. This allows you to focus on improving the business logic of your pipeline without worrying about upgrading the Beam version to simply obtain I/O connector updates.\u003c/p\u003e\u003ch2 data-block-key=\"bfbbo\"\u003e\u003cbr/\u003eSimplified IO API\u003c/h2\u003e\u003cp data-block-key=\"f5jsp\"\u003eAPIs differences across Beam I/O connectors vary greatly. This means that, whenever you try to use a new Beam I/O connector, you would have to learn an API specific to that connector. Some of the APIs can be quite large and non-intuitive. This can be due to:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"9jp96\"\u003eSupport for various and in some cases redundant features offered by the underlying system.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"75epk\"\u003eMaintaining backwards compatibility for legacy (or archaic) features or defaults.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"2ni21\"\u003eSupport for customizing the I/O connector to support edge cases and implementation details that may only apply to few customers.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"bem29\"\u003eAbove points result in very large API surfaces for some connectors that are not intuitive for a new customer to use efficiently.\u003c/p\u003e\u003cp data-block-key=\"b32fv\"\u003e\u003cbr/\u003eManaged I/O offers standardized \u003ca href=\"https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/managed/Managed.html\"\u003eJava\u003c/a\u003e and \u003ca href=\"https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.managed.html#module-apache_beam.transforms.managed\"\u003ePython\u003c/a\u003e APIs for supported I/O connectors. For example, with Beam Java SDK an I/O connector source can be instantiated in the following standardized form.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eread\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eSOURCE\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003ewithConfig\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003esourceConfig\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"hfxj5\"\u003eAn I/O connector sink can be instantiated in the following form.\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ewrite\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eSINK\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003ewithConfig\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003esinkConfig\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"hfxj5\"\u003eHere \u003ccode\u003eSOURCE\u003c/code\u003e and \u003ccode\u003eSINK\u003c/code\u003e are keys specifically identifying the connector while \u003ccode\u003esourceConfig\u003c/code\u003e and \u003ccode\u003esinkConfig\u003c/code\u003e are maps of configurations used to instantiate the connector source or sink. The map of configurations may also be provided as YAML files available locally or in Google Cloud Storage. Please see the Managed I/O website for more complete examples for \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/managed-io#supported\"\u003esupported sources and sinks\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"alj2l\"\u003eBeam Python SDK \u003ca href=\"https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.managed.html#module-apache_beam.transforms.managed\"\u003eoffers\u003c/a\u003e a similarly simplified API.\u003c/p\u003e\u003cp data-block-key=\"1egb7\"\u003eThis means that various Beam I/O connectors with different APIs can be instantiated in a very standard way. For example,\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e// Create a Java BigQuery I/O source\u003c/span\u003e\n\u003cspan\u003eMap\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eString\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eObject\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ebqReadConfig\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eImmutableMap\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eof\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;query\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;query\u0026gt;\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e...);\u003c/span\u003e\n\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eread\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eBIGQUERY\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003ewithConfig\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ebqReadConfig\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e// Create a Java Kafka I/O source.\u003c/span\u003e\n\u003cspan\u003eMap\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003eString\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eObject\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ekafkaReadConfig\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eImmutableMap\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eof\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;bootstrap_servers\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;server\u0026gt;\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;topic\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;topic\u0026gt;\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e...);\u003c/span\u003e\n\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eread\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eKAFKA\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003ewithConfig\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ekafkaReadConfig\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e// Create a Java Kafka I/O source but with a YAML based config available in Google Cloud Storage.\u003c/span\u003e\n\u003cspan\u003eString\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003ekafkaReadYAMLConfig\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;gs://path/to/config.yaml\u0026#34;\u003c/span\u003e\n\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eread\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eManaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eKAFKA\u003c/span\u003e\u003cspan\u003e).\u003c/span\u003e\u003cspan\u003ewithConfigUrl\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ekafkaReadYAMLConfig\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e// Create a Python Iceberg I/O source.\u003c/span\u003e\n\u003cspan\u003eiceberg_config\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\u0026#34;table\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;table\u0026gt;\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e...}\u003c/span\u003e\n\u003cspan\u003emanaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eRead\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emanaged\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eICEBERG\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003econfig\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eiceberg_config\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"hfxj5\"\u003eAutomatically optimized for Dataflow\u003c/h2\u003e\u003cp data-block-key=\"i173\"\u003eMany Beam connectors offer a comprehensive API for configuring and optimizing the connector to suit a given pipeline and a given \u003ca href=\"https://beam.apache.org/documentation/runners/capability-matrix/\"\u003eBeam runner\u003c/a\u003e. One downside of this is that if you specifically want to run on Dataflow, you may have to learn the specific configurations that best suit Dataflow and apply them when setting up your pipeline. Connector related documentation can be long and detailed and specific changes needed might not be intuitive. This might result in connectors used in Dataflow pipelines performing in a sub-optimal way.\u003c/p\u003e\u003cp data-block-key=\"a5nkr\"\u003eManage I/O connectors alleviates this by automatically re-configuring the connectors to incorporate best practices and configure them to best suit Dataflow. Such re-configuration may occur during job submission or streaming update via replacement.\u003c/p\u003e\u003cp data-block-key=\"di0kc\"\u003eFor example, Dataflow streaming pipelines offer two modes, exactly-once and at-least-once while BigQuery I/O sink with Storage Write API offer two analogous delivery semantics, exactly-once and at-least-once. BigQuery sink with at-least-once delivery semantics is usually \u003ca href=\"https://beam.apache.org/documentation/io/built-in/google-bigquery/#at-least-once-semantics\"\u003eless expensive and results in lower latencies\u003c/a\u003e. With standard BigQuery I/O connectors, you are responsible for making sure that you use the appropriate mode when using the BigQuery I/O. With \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/managed-io-bigquery\"\u003eManaged BigQuery I/O sink\u003c/a\u003e this is automatically configured for you. Which means that if your streaming pipeline is operating at the at-least-once mode, your Managed I/O BigQuery sink will be automatically configured to use the at-least-once delivery semantics.\u003c/p\u003e\u003ch2 data-block-key=\"3ga95\"\u003e\u003cbr/\u003eReal-world pipelines\u003c/h2\u003e\u003cp data-block-key=\"68st6\"\u003eWe ran several pipelines that wrote data using the Managed Iceberg I/O sink backed by a Hadoop catalog deployed in GCS (please see \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/managed-io-iceberg\"\u003ehere\u003c/a\u003e for the other supported catalogs). Pipelines were submitted using Beam 2.61.0 and the Managed I/O sink was automatically upgraded by Dataflow to the latest supported version. All benchmarks used n1-standard-4 VMs and the number of VMs used by the pipeline was fixed to 100. Please note that execution time here does not include the startup and shutdown time.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Dataflow-pipeline-performance-benchmarks_1.original.png\" alt=\"Dataflow pipeline performance benchmarks\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"hfxj5\"\u003eAs the benchmarks show, Managed Iceberg I/O scaled up nicely and both metrics grew linearly with the data size.\u003c/p\u003e\u003cp data-block-key=\"2ak8\"\u003eWe also ran a streaming pipeline that read from Google Pub/Sub and used the \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/managed-io-kafka\"\u003eManaged I/O Kafka sink\u003c/a\u003e to push messages to a Kafka cluster hosted in GCP. The pipeline used Beam 2.61.0 and Dataflow upgraded the Managed Kafka sink to the latest supported version. During the steady state, the pipeline used 10 n1-standard-4 VMs (max 20 VMs). The pipeline was consistently processing messages at a throughput of 250k msgs/sec across all steps and was run for 2 hours.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/throughput_elements_sec.original.jpg\" alt=\"Throughput elements sec\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"hfxj5\"\u003eThe following graph shows the data throughputs of various steps of the pipeline. Note that throughputs are different here since the element size changes between steps. The pipeline read from Pub/Sub at a rate of 75 MiB/sec (red line) and wrote to Kafka at a rate of 40 MiB/sec (green line).\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/throghput_bytes_sec.original.jpg\" alt=\"throughput bytes/sec\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"hfxj5\"\u003eBoth latency and backlog was low for the duration of the pipeline execution.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/latency_by_stages.original.jpg\" alt=\"latency by stages\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/backlog_seconds.original.jpg\" alt=\"backlog seconds\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"hfxj5\"\u003eThe pipeline used VM CPU and memory efficiently.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/cpu_utilization.original.jpg\" alt=\"cpu utilization\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/memory_utilization.original.jpg\" alt=\"memory utilization\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"hfxj5\"\u003eAs these results show, the pipeline executed efficiently with the upgraded Managed I/O Kafka sink provided by the Dataflow service.\u003c/p\u003e\u003ch2 data-block-key=\"bmug6\"\u003e\u003cbr/\u003eHow can I use Managed I/O in my pipeline ?\u003c/h2\u003e\u003cp data-block-key=\"bnkm8\"\u003eUsing Managed I/O is simple as using one of the \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/managed-io#supported\"\u003esupported sources and sinks\u003c/a\u003e in your pipeline and running the pipeline with \u003ca href=\"https://cloud.google.com/dataflow/docs/runner-v2\"\u003eDataflow Runner v2\u003c/a\u003e. When you run your pipeline, Dataflow will make sure that the latest vetted versions of the sources and sinks are enabled during job submission and \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching\"\u003estreaming update via replacement\u003c/a\u003e, even if you are using an older Beam version for your pipeline.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2025-04-08T00:00:00Z",
  "modifiedTime": null
}
