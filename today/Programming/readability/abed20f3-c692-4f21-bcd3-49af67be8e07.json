{
  "id": "abed20f3-c692-4f21-bcd3-49af67be8e07",
  "title": "Meta Launches AutoPatchBench to Evaluate LLM Agents on Security Fixes",
  "link": "https://www.infoq.com/news/2025/05/meta-autopatchbench-ai-patching/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "AutoPatchBench is a standardized benchmark designed to help researchers and developers evaluate and compare how effectively LLM agents can automatically patch security vulnerabilities in C/C++ native code. By Sergio De Simone",
  "author": "Sergio De Simone",
  "published": "Wed, 07 May 2025 18:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Automated testing",
    "C++",
    "Large language models",
    "C",
    "Fuzz Testing",
    "Open Source",
    "Development",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Sergio De Simone",
  "length": 3674,
  "excerpt": "AutoPatchBench is a standardized benchmark designed to help researchers and developers evaluate and compare how effectively LLM agents can automatically patch security vulnerabilities in C/C++ native",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20250506220057/apple-touch-icon.png",
  "text": "AutoPatchBench is a standardized benchmark designed to help researchers and developers evaluate and compare how effectively LLM agents can automatically patch security vulnerabilities in C/C++ native code. AutoPatchBench comprises a collection of tests aimed at evaluating the ability of LLMs to autonomously generate security patches for vulnerabilities identified using fuzz testing. This benchmark aims to facilitate a comprehensive understanding of the capabilities and limitations of various AI-driven approaches to repairing fuzzing-found bugs. By offering a consistent set of evaluation criteria, AutoPatchBench fosters transparency and reproducibility in research. Compared to general-purpose benchmarks for evaluating software engineering agents like SWE-Bench and SWE-Bench Verified, AutoPatchBench focuses on the specific challenges posed by bugs uncovered through fuzzing techniques, which often involve security vulnerabilities. AutoPatchBench is based on a subset of ARVO, a dataset of over 5,000 real-world C/C++ vulnerabilities discovered by Google's OSS-Fuzz across more than 250 projects. Each vulnerability in ARVO is paired with a triggering input and the canonical patch the developer wrote to fix the issue. We retained 136 samples for AutoPatchBench that fulfill the necessary conditions for both patch generation and verification. From this refined set, we created a down-sampled subset of 113 AutoPatchBench-Lite samples to provide a focused benchmark for testing AI patch generation tools. These subsets preserves the diversity and complexity of real-world vulnerabilities including 11 distinct crash types, offering a solid foundation for advancing AI-driven security solutions. Fuzz testing is a technique used to uncover security exploits and vulnerabilities by reaching edge cases that are difficult for human testers to encounter. As noted by the creators of OpenSSF’s Fuzz Introspector, fuzz testing is a promising approach, but its challenge lies in writing effective fuzzers that provide good coverage. Additionally, once a crash is uncovered via fuzzing, resolving it is no trivial task requiring a thorough analysis of the crash stack trace to identify the root cause, followed by patching the code and verifying the effectiveness of the fix. This is where AI systems may offer assistance, as demonstrated by Google in its tech report on AI-powered patching and more recently with its GITS-Eval benchmark. One key aspect of patch verification is ensuring the patched program maintains its intended behavior, which goes well beyond checking the program builds and does not crash when fed with the input that originally triggered the crash. To address this concern, AutoPatchBench applies a specific technique to evaluate whether the generated patch produces a program state identical to the ground truth program after the patched function returns. Along with AutoPatchBench, which includes the full set of 136 samples from ARVO, Meta also released AutoPatchBench-Lite, a smaller subset of only 113 samples where the root cause of the crash is confined to a single function. This makes it better suited for tools in early development or those focused on simpler crash scenarios. AutoPatchBench is part of CyberSecEval 4, an extensive benchmark suite for assessing vulnerabilities defensive capabilities of LLMs. Meta open sourced its reference implementation for the community to leverage it in open-source projects employing fuzzing or to build better patching models. About the Author Sergio De Simone",
  "image": "https://res.infoq.com/news/2025/05/meta-autopatchbench-ai-patching/en/headerimage/meta-autopatchbench-1746638935976.jpeg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003e\u003ca href=\"https://engineering.fb.com/2025/04/29/ai-research/autopatchbench-benchmark-ai-powered-security-fixes/\"\u003eAutoPatchBench\u003c/a\u003e is a standardized benchmark designed to help researchers and developers evaluate and compare how effectively LLM agents can automatically patch security vulnerabilities in C/C++ native code.\u003c/p\u003e\n\n\u003cp\u003eAutoPatchBench comprises a collection of tests aimed at evaluating the ability of LLMs to autonomously generate security patches for vulnerabilities identified using \u003ca href=\"https://www.infoq.com/fuzztesting/\"\u003efuzz testing\u003c/a\u003e.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis benchmark aims to facilitate a comprehensive understanding of the capabilities and limitations of various AI-driven approaches to repairing fuzzing-found bugs. By offering a consistent set of evaluation criteria, AutoPatchBench fosters transparency and reproducibility in research.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eCompared to general-purpose benchmarks for evaluating software engineering agents like \u003ca href=\"https://www.swebench.com/\"\u003eSWE-Bench\u003c/a\u003e and \u003ca href=\"https://openai.com/index/introducing-swe-bench-verified/\"\u003eSWE-Bench Verified\u003c/a\u003e, AutoPatchBench focuses on the specific challenges posed by bugs uncovered through fuzzing techniques, which often involve security vulnerabilities.\u003c/p\u003e\n\n\u003cp\u003eAutoPatchBench is based on a subset of \u003ca href=\"https://arxiv.org/abs/2408.02153\"\u003eARVO\u003c/a\u003e, a dataset of over 5,000 real-world C/C++ vulnerabilities discovered by Google\u0026#39;s OSS-Fuzz across more than 250 projects. Each vulnerability in ARVO is paired with a triggering input and the canonical patch the developer wrote to fix the issue.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWe retained 136 samples for AutoPatchBench that fulfill the necessary conditions for both patch generation and verification. From this refined set, we created a down-sampled subset of 113 AutoPatchBench-Lite samples to provide a focused benchmark for testing AI patch generation tools. These subsets preserves the diversity and complexity of real-world vulnerabilities including 11 distinct crash types, offering a solid foundation for advancing AI-driven security solutions.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eFuzz testing is a technique used to uncover security exploits and vulnerabilities by reaching edge cases that are difficult for human testers to encounter. As noted by the creators of OpenSSF’s Fuzz Introspector, \u003ca href=\"https://openssf.org/blog/2022/06/09/introducing-fuzz-introspector-an-openssf-tool-to-improve-fuzzing-coverage/\"\u003efuzz testing is a promising approach, but its challenge lies in writing effective fuzzers\u003c/a\u003e that provide good coverage.\u003c/p\u003e\n\n\u003cp\u003eAdditionally, once a crash is uncovered via fuzzing, resolving it is no trivial task requiring a thorough analysis of the crash stack trace to identify the root cause, followed by patching the code and verifying the effectiveness of the fix. This is where AI systems may offer assistance, as demonstrated by Google in its \u003ca href=\"https://research.google/pubs/ai-powered-patching-the-future-of-automated-vulnerability-fixes/\"\u003etech report on AI-powered patching\u003c/a\u003e and more recently with its \u003ca href=\"https://arxiv.org/abs/2501.07531\"\u003eGITS-Eval benchmark\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eOne key aspect of patch verification is ensuring the patched program maintains its intended behavior, which goes well beyond checking the program builds and does not crash when fed with the input that originally triggered the crash. To address this concern, AutoPatchBench applies a specific technique to evaluate whether the generated patch produces a program state identical to the ground truth program after the patched function returns.\u003c/p\u003e\n\n\u003cp\u003eAlong with AutoPatchBench, which includes the full set of 136 samples from ARVO, Meta also released AutoPatchBench-Lite, a smaller subset of only 113 samples where the root cause of the crash is confined to a single function. This makes it better suited for tools in early development or those focused on simpler crash scenarios.\u003c/p\u003e\n\n\u003cp\u003eAutoPatchBench is part of \u003ca href=\"https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks\"\u003eCyberSecEval 4\u003c/a\u003e, an extensive benchmark suite for assessing vulnerabilities defensive capabilities of LLMs. Meta open sourced its \u003ca href=\"https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks\"\u003ereference implementation\u003c/a\u003e for the community to leverage it in open-source projects employing fuzzing or to build better patching models.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Sergio-De-Simone\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eSergio De Simone\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-05-07T00:00:00Z",
  "modifiedTime": null
}
