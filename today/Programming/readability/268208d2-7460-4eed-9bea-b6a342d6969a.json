{
  "id": "268208d2-7460-4eed-9bea-b6a342d6969a",
  "title": "Mellum: How We Trained a Model to Excel in Code Completion",
  "link": "https://blog.jetbrains.com/ai/2025/04/mellum-how-we-trained-a-model-to-excel-in-code-completion/",
  "description": "Code completion has always been a defining strength of JetBrains products. So, when adding AI into our products, we knew it had to deliver top-tier code completion. This post covers how we trained the model behind our cloud-based completion. Initial research: What about obvious model options? We started by testing both closed-source LLMs via APIs […]",
  "author": "Anton Semenkin",
  "published": "Thu, 24 Apr 2025 12:45:44 +0000",
  "source": "https://blog.jetbrains.com/feed",
  "categories": [
    "jetbrains-ai",
    "ai",
    "mellum"
  ],
  "byline": "Anton Semenkin",
  "length": 15885,
  "excerpt": "Code completion has always been a defining strength of JetBrains products. See how we trained the model behind our cloud-based completion.",
  "siteName": "The JetBrains Blog",
  "favicon": "https://blog.jetbrains.com/wp-content/uploads/2024/01/cropped-mstile-310x310-1-180x180.png",
  "text": "Supercharge your tools with AI-powered features inside many JetBrains products JetBrains AIMellum: How We Trained a Model to Excel in Code Completion Code completion has always been a defining strength of JetBrains products. So, when adding AI into our products, we knew it had to deliver top-tier code completion. This post covers how we trained the model behind our cloud-based completion. Initial research: What about obvious model options? We started by testing both closed-source LLMs via APIs and open-source alternatives. But we quickly hit some roadblocks: As we were aiming for on-the-fly completion, typical chat LLMs proved themselves impractical due to high costs and substantial latency. Such models were also missing critical code completion features, like fill in the middle (FIM) and token healing.  Chat models also tend to provide their outputs in inconsistent format, making it harder to properly process the response and insert suggestions in the editor. One of our biggest concerns was the lack of transparency about the data used for training, which could lead to potential risks related to the use of licensed code. While some open-source models do offer transparency in their data sources, we opted not to use them due to production stability issues. Mellum: A bird’s-eye view of the model After evaluating our options, we concluded that a relatively small in-house code completion model was the way to go. We set a goal of training a high-quality model with reasonable inference costs and latency using transparent data. We determined that staying under 4B parameters would allow us to support efficient inference for all users. Additionally, since our model has been trained primarily on code, its token vocabulary is specialized for coding tasks.  To train the model, we implemented a three-stage process, with each stage bringing in new knowledge and improving the generation quality. We started with basic pre-training on a large corpus of standalone files and then fine-tuned on a smaller number of contextual examples. Finally, to align the model to our product needs and remove undesired generations, we used reinforcement learning with AI feedback (RLAIF).  Let’s take a look at the training steps in detail. Pre-training stage To avoid risks connected with training data, we started training from scratch. The goal of the pre-training stage was to introduce the model to a wide variety of languages, make it learn the syntax, patterns, and general programming concepts.  Dataset We used TheStack as the main source of code data in various languages. It’s not fully up to date, but we addressed this by collecting additional data with fresh code, filtering it by repo and files licenses and cleaning it of personal identifiable information (PII). This ensures that our dataset is both legally compliant and useful. Pre-training process For pre-training, we sampled our combined dataset multiple times to reach approximately 3 trillion tokens. We used an 8192-token context window and split the dataset into chunks of matching size. For half of the files in each chunk, we applied a fill-in-the-middle (FIM) transformation. This involves splitting the file into three parts (prefix, middle, and suffix), then rearranging them so that the model learns to predict the missing middle segment given the surrounding context. This technique encourages the model to consider both the preceding and following code when generating suggestions, which better mimics real-world usage in code editors. The pre-training was conducted on a cluster of 16 nodes with 8 H100 GPUs each, and it took about 15 days to complete. The result was our 4B-parameter Mellum-base model. For comparison, 100M code completion models, which we deploy locally in JetBrains IDEs, typically train in about a week on a single 8 H100 GPUs node. The pre-training produces a general-purpose code completion model with broad knowledge across many programming languages. However, at this step the model is trained to achieve one simple objective: prediction of the next token in a randomly selected segment of a file. Without more context, the model won’t understand your code structure and won’t know when to stop generating. These limitations are precisely what the fine-tuning stage is designed to address. Context-aware fine-tuning Better fill-in-the-middle examples Unlike pre-training – where we randomly select chunks of code for prediction – fine-tuning focuses on slicing the code in more meaningful ways, i.e. by extracting fragments you’re more likely to see “in the wild”. The visual below illustrates this approach. The code shown in blue is what we ask the model to predict. In the second example, for instance, the selected fragment stays within the scope of a single function. This setup better reflects a typical user scenario. Building contextual examples Even with improved fill-in-the-middle splitting, we’re still operating within the scope of a single file, which doesn’t accurately reflect how most developers work. In practice, completing code often requires an understanding of surrounding files and broader project context. One of JetBrains’ superpowers is expertise in symbol resolution, usage search, and other IDE tooling. So, for the sake of scalable data pre-processing, we launched an internal project called Code Engine: a cross-platform SDK providing a lightweight, high-performance CLI tool designed to collect contextual information directly from plain files, without requiring the project to be indexed. Such an SDK allowed us to build contextual examples across thousands of repositories on the internal MapReduce cluster in a reasonable amount of time. However, finding the right algorithms took some trial and error. Here are a few examples to give you an idea of some of the challenges we had to overcome while trying to find the best context collection approach for our model:  Sorting files that are most similar by Jaccard distance on lines Using files from import statements Creating a repomap …and much more.  Language-specific fine-tuning We hypothesized that smaller models like ours could benefit significantly from specialization. While the base model is trained on over 80 programming languages, most users typically work with just one or two (e.g. Java or Python). As a result, we fine-tune separate models for some of the most popular languages, allowing them to better capture language-specific patterns, libraries, and developer workflows: mellum-all – supports the majority of languages and dialects available in JetBrains’ IDEs, but the completion quality is slightly lower than that of specialized models mellum-python – specialized for Python and Jupyter mellum-jotlin – specialized for Java and Kotlin mellum-web – specialized for the web (coming soon!) Refining with RLAIF The final step in our training pipeline focuses on removing undesired behaviors due to misalignment between the model’s training objectives and user expectations. For instance, from a training perspective, it’s perfectly valid to generate placeholders like TODO(“Not implemented”) or pass since these patterns are common in public code repositories. However, these are not likely to be helpful as actual code completion suggestions. To address such issues, we apply an additional training phase using reinforcement learning from AI feedback (RLAIF), incorporating synthetic data crafted from rules and model-generated preferences. We construct a dataset of completion pairs in two main categories: Rule-based completion pairs: We take real examples from our dataset and deliberately degrade them – replacing meaningful code with generic placeholders like pass, comments, or TODO statements. Mellum-generated completion pairs: For a given prompt, we generate two completions – one with low temperature (more deterministic and often higher quality), and another with high temperature (typically more erratic or lower quality). We then use an external LLM to rank the two completions, producing a labeled positive-negative pair. This dataset is then used to train the model to better reflect user preferences. Currently, we use the direct preference optimization (DPO) algorithm, which makes the model more inclined to generate positive or preferred completion examples. This approach not only increases the evaluation score but also reduces the number of annoying generation artifacts. How good is Mellum? Spoiler: the model performs extremely well for its size! Here’s how we evaluated it: First, we evaluated our models on the internal benchmark that we call “JetBrains BigCode”. Then, we ran our models on well-known public benchmarks like SAFIM. Finally, we measured the user-centric metrics by leveraging feature usage logs. Below, we break our feedback loops down into two categories: offline evaluation (with pre-determined datasets) and online evaluation (with real user data). Offline evaluation Discussing datasets is always nice, but what about the metrics? While creating the dataset is challenging enough, it’s even more challenging to create a good metric that compares the ground truth completion with the one proposed by the neural model. We did a little research and ended up with a combination of two main metrics: EM: Exact match – a popular choice for code completion evaluation. The prediction is considered good if the first line of the completion matches the first line of the ground truth with minimal pre-processing.  KK: The metric was named after its authors, Karol and Katya, who designed it and manually annotated the threshold-estimating dataset. The number of suggested lines that are found in the ground truth, divided by the total number of lines in the suggested completion. We also use some additional evaluation techniques, including chrF and LLM-as-a-Judge, for comparing the model variations, but we won’t be covering them here for the sake of keeping the blog post concise. Keep following our updates for more! JetBrains BigCode We evaluated the model using a benchmark dataset derived from our internal tool, JetBrains BigCode, covering the most popular languages supported by Mellum – including Python, Kotlin, and Java. We ensured that our evaluation dataset has no overlap with the training dataset and transformed it to the FIM structure with contexts gathered using Code Engine. One of the key advantages of the JetBrains BigCode dataset is its ability to slice code snippets based on various features, such as repository topic, popularity, age, whether the code is main or test code, and recent activity (e.g. commits in the past year). This is done for two primary reasons. First, a well-performing model should demonstrate strong results across all these slices, not just on a subset of them. Second, public benchmarks often end up in model training datasets over time, leading to evaluation contamination. To mitigate the latter risk, we designed our age and activity slices to better reflect real-world conditions.  As a result, by maintaining full control over our dataset rather than relying on public benchmarks, we can more reliably assess model quality across different coding styles and practices. According to our experiments on JetBrains BigCode, the complicated nature of the base model does pay dividends in terms of performance. So, it’s demonstrably a good idea to have a strong base model, then fine-tune it, and then align it with DPO. Additionally, our JetBrains BigCode evaluation runs show that we are in good company with well-known battle-tested polyglot models, while being smaller and more efficient. Of course, larger models do outperform us, but they come with significantly higher serving costs, making them less practical for our products. Single-line suggestions quality (EM metric) Public benchmarks Besides evaluation on an internal dataset, we compared the capabilities of our models on different public benchmarks like multilingual benchmark SAFIM (syntax-aware fill in the middle). Stay tuned as we will share benchmarks data for Mellum soon! That said, it’s important to remember that benchmarks like JetBrains BigCode and SAFIM, while valuable for scalable offline evaluation, don’t fully capture the experience of a real developer using the model. And ultimately, that’s who we’re building for. Online evaluation To ensure a positive impact on the user experience, we measure several metrics on the feature usage logs, following the same pipeline we described in the paper Full Line Code Completion: Bringing AI to Desktop. Long story short, our main metric is called ratio of completed code (RoCC). It is defined as a ratio of symbols of code written with code completion among all code written in the editor. We generally try to optimize for this metric. The core idea and motivation are simple: the more generated text appears in the editor, the better our code completion is. The good thing about the RoCC is that we can vary the number of code completion contributors that we account for. This allows us to, for instance, calculate a general in-editor RoCC or Mellum-specific RoCC. Another important metric is the acceptance rate (AR), which is the number of accepted suggestions divided by the number of all shown suggestions. This metric is widely used in the community and is also fairly intuitive: the more users accept our suggestions, the better. Below, we provide some online metrics data for various popular languages: RoCC (Mellum + standard completion)RoCC (only Mellum)ARJava46%23%35%Kotlin45%25%31%Python32%23%35%JS/TS39%23%32%C#45%18%32%Go45%30%44%PHP40%26%34%Rust37%24%35% Outcomes and what’s next for Mellum This was a challenging journey for our ML team, but it resulted in one general completion model and several specialized ones that are all available via the JetBrains AI platform and are currently powering code completion inside JetBrains AI Assistant. To conclude the blogpost, here are our few next steps: As mentioned earlier, we are currently working on a specialized model for web development languages that should further boost quality for various languages, and we plan to make this publicly available at some point in the near future. We would also like to scale the number of parameters further, introducing more diverse data to the training set at the same time. This way Mellum becomes capable of other AI-for-code tasks as well. Keep in mind that service performance remains a key for us, so we expect to stay within reasonable size boundaries as we expand the model.  Try coding with Mellum and share your impressions with us! Subscribe to JetBrains AI Blog updates Discover more",
  "image": "https://blog.jetbrains.com/wp-content/uploads/2025/04/jbai-social_share_blog_1280x720_en-1.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"main\"\u003e\n    \u003cdiv\u003e\n                        \u003ca href=\"https://blog.jetbrains.com/ai/\"\u003e\n                            \u003cimg src=\"https://blog.jetbrains.com/wp-content/uploads/2024/01/JetBrains-AI.svg\" alt=\"Ai logo\"/\u003e\n                                                                                                \n                                                                                    \u003c/a\u003e\n                                                    \u003cp\u003eSupercharge your tools with AI-powered features inside many JetBrains products\u003c/p\u003e\n                                            \u003c/div\u003e\n                            \u003csection data-clarity-region=\"article\"\u003e\n                \u003cdiv\u003e\n                    \t\t\t\t\u003cp\u003e\u003ca href=\"https://blog.jetbrains.com/ai/category/jetbrains-ai/\"\u003eJetBrains AI\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"major-updates\"\u003eMellum: How We Trained a Model to Excel in Code Completion\u003c/h2\u003e                    \n                    \n\u003cp\u003eCode completion has always been a defining strength of JetBrains products. So, when adding AI into our products, we knew it had to deliver top-tier code completion. This post covers how we trained the model behind our cloud-based completion.\u003c/p\u003e\n\n\n\n\u003ch2\u003eInitial research: What about obvious model options?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe started by testing both closed-source LLMs via APIs and open-source alternatives. But we quickly hit some roadblocks:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eAs we were aiming for on-the-fly completion, typical chat LLMs proved themselves impractical due to high costs and substantial latency. Such models were also missing critical code completion features, like \u003ca href=\"https://medium.com/@SymeCloud/what-is-fim-and-why-does-it-matter-in-llm-based-ai-53f33385585b\" target=\"_blank\" rel=\"noopener\"\u003efill in the middle (FIM)\u003c/a\u003e and \u003ca href=\"https://guidance.readthedocs.io/en/latest/example_notebooks/tutorials/token_healing.html\" target=\"_blank\" rel=\"noopener\"\u003etoken healing\u003c/a\u003e. \u003c/li\u003e\n\n\n\n\u003cli\u003eChat models also tend to provide their outputs in inconsistent format, making it harder to properly process the response and insert suggestions in the editor.\u003c/li\u003e\n\n\n\n\u003cli\u003eOne of our biggest concerns was the lack of transparency about the data used for training, which could lead to potential risks related to the use of licensed code. While some open-source models do offer transparency in their data sources, we opted not to use them due to production stability issues.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch2\u003eMellum: A bird’s-eye view of the model\u003c/h2\u003e\n\n\n\n\u003cp\u003eAfter evaluating our options, we concluded that a relatively small in-house code completion model was the way to go. We set a goal of training a high-quality model with reasonable inference costs and latency using transparent data. We determined that staying under 4B parameters would allow us to support efficient inference for all users. Additionally, since our model has been trained primarily on code, its token vocabulary is specialized for coding tasks. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo train the model, we implemented a three-stage process, with each stage bringing in new knowledge and improving the generation quality. We started with basic pre-training on a large corpus of standalone files and then fine-tuned on a smaller number of contextual examples. Finally, to align the model to our product needs and remove undesired generations, we used reinforcement learning with AI feedback (RLAIF). \u003c/p\u003e\n\n\n\n\u003cp\u003eLet’s take a look at the training steps in detail.\u003c/p\u003e\n\n\n\n\u003ch3\u003ePre-training stage\u003c/h3\u003e\n\n\n\n\u003cp\u003eTo avoid risks connected with training data, we started training from scratch. The goal of the pre-training stage was to introduce the model to a wide variety of languages, make it learn the syntax, patterns, and general programming concepts. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eDataset\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe used \u003ca href=\"https://huggingface.co/datasets/bigcode/the-stack\" target=\"_blank\" rel=\"noopener\"\u003eTheStack\u003c/a\u003e as the main source of code data in various languages. It’s not fully up to date, but we addressed this by collecting additional data with fresh code, filtering it by repo and files licenses and cleaning it of personal identifiable information (PII). This ensures that our dataset is both legally compliant and useful.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePre-training process\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eFor pre-training, we sampled our combined dataset multiple times to reach approximately 3 trillion tokens. We used an 8192-token context window and split the dataset into chunks of matching size. For half of the files in each chunk, we applied a fill-in-the-middle (FIM) transformation. This involves splitting the file into three parts (prefix, middle, and suffix), then rearranging them so that the model learns to predict the missing middle segment given the surrounding context. This technique encourages the model to consider both the preceding and following code when generating suggestions, which better mimics real-world usage in code editors.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe pre-training was conducted on a cluster of 16 nodes with 8 H100 GPUs each, and it took about 15 days to complete. The result was our 4B-parameter Mellum-base model. For comparison, 100M code completion models, which we deploy locally in JetBrains IDEs, typically train in about a week on a single 8 H100 GPUs node.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe pre-training produces a general-purpose code completion model with broad knowledge across many programming languages. However, at this step the model is trained to achieve one simple objective: prediction of the next token in a randomly selected segment of a file. Without more context, the model won’t understand your code structure and won’t know when to stop generating.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese limitations are precisely what the fine-tuning stage is designed to address.\u003c/p\u003e\n\n\n\n\u003ch3\u003eContext-aware fine-tuning\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eBetter fill-in-the-middle examples\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike pre-training – where we randomly select chunks of code for prediction – fine-tuning focuses on slicing the code in more meaningful ways, i.e. by extracting fragments you’re more likely to see “in the wild”.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe visual below illustrates this approach. The code shown in blue is what we ask the model to predict. In the second example, for instance, the selected fragment stays within the scope of a single function. This setup better reflects a typical user scenario.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdFxWZ-B8tgGUWK5iluOS-bucm7RT6b2fxUjw1hXKGz-Q7zS09J3n8GRmUk3FqiLL8L11-0Ur_i5asPybMOge8eLAQTsVTBPmzt810Q2mqZ_mmqfJrv39pC40p3laLVGGT5kdu-4g?key=3rmPPwdiDGp1IMto6KsKrF14\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eBuilding contextual examples\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eEven with improved fill-in-the-middle splitting, we’re still operating within the scope of a single file, which doesn’t accurately reflect how most developers work. In practice, completing code often requires an understanding of surrounding files and broader project context.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of JetBrains’ superpowers is expertise in symbol resolution, usage search, and other IDE tooling. So, for the sake of scalable data pre-processing, we launched an internal project called Code Engine: a cross-platform SDK providing a lightweight, high-performance CLI tool designed to collect contextual information directly from plain files, without requiring the project to be indexed. Such an SDK allowed us to build contextual examples across thousands of repositories on the internal MapReduce cluster in a reasonable amount of time.\u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, finding the right algorithms took some trial and error. Here are a few examples to give you an idea of some of the challenges we had to overcome while trying to find the best context collection approach for our model: \u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eSorting files that are most similar by Jaccard distance on lines\u003c/li\u003e\n\n\n\n\u003cli\u003eUsing files from import statements\u003c/li\u003e\n\n\n\n\u003cli\u003eCreating a \u003ca href=\"https://aider.chat/docs/repomap.html\" target=\"_blank\" rel=\"noopener\"\u003erepomap\u003c/a\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e…and much more. \u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eLanguage-specific fine-tuning\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe hypothesized that smaller models like ours could benefit significantly from specialization. While the base model is trained on over 80 programming languages, most users typically work with just one or two (e.g. Java or Python). As a result, we fine-tune separate models for some of the most popular languages, allowing them to better capture language-specific patterns, libraries, and developer workflows:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003emellum-all\u003c/strong\u003e – supports the \u003ca href=\"https://gist.github.com/johndolgov/4318d1c7be2a3e39d1f0c8d00b948c00\" target=\"_blank\" rel=\"noopener\"\u003emajority of languages and dialects\u003c/a\u003e available in JetBrains’ IDEs, but the completion quality is slightly lower than that of specialized models\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003emellum-python\u003c/strong\u003e – specialized for Python and Jupyter\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003emellum-jotlin\u003c/strong\u003e – specialized for Java and Kotlin\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003emellum-web\u003c/strong\u003e – specialized for the web (coming soon!)\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003ch3\u003eRefining with RLAIF\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe final step in our training pipeline focuses on removing undesired behaviors due to misalignment between the model’s training objectives and user expectations. For instance, from a training perspective, it’s perfectly valid to generate placeholders like TODO(“Not implemented”) or pass since these patterns are common in public code repositories. However, these are not likely to be helpful as actual code completion suggestions.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo address such issues, we apply an additional training phase using \u003cstrong\u003ereinforcement learning from AI feedback (RLAIF)\u003c/strong\u003e, incorporating synthetic data crafted from rules and model-generated preferences. We construct a dataset of completion pairs in two main categories:\u003c/p\u003e\n\n\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eRule-based completion pairs\u003c/strong\u003e: We take real examples from our dataset and deliberately degrade them – replacing meaningful code with generic placeholders like pass, comments, or TODO statements.\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eMellum-generated completion pairs\u003c/strong\u003e: For a given prompt, we generate two completions – one with low temperature (more deterministic and often higher quality), and another with high temperature (typically more erratic or lower quality). We then use an external LLM to rank the two completions, producing a labeled positive-negative pair.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdyv1pCfpToRcN3IgSjjVfQ8nadjDvndg2Sg8RiOdj362REPz3NNQOgi02UuCY6Z6iqnOWBuzv7lDGJfHAM97q2HmppmR_pOAZ-RCpEsRg2Vt94aOAgQ-Aig2OUILGsgct6k0weSA?key=3rmPPwdiDGp1IMto6KsKrF14\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis dataset is then used to train the model to better reflect user preferences. Currently, we use the \u003ca href=\"https://arxiv.org/abs/2305.18290\" target=\"_blank\" rel=\"noopener\"\u003edirect preference optimization (DPO)\u003c/a\u003e algorithm, which makes the model more inclined to generate positive or preferred completion examples.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis approach not only increases the evaluation score but also reduces the number of annoying generation artifacts.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow good is Mellum?\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpoiler: the model performs extremely well for its size!\u003cstrong\u003e \u003c/strong\u003eHere’s how we evaluated it:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eFirst, we evaluated our models on the internal benchmark that we call “JetBrains BigCode”.\u003c/li\u003e\n\n\n\n\u003cli\u003eThen, we ran our models on well-known public benchmarks like \u003ca href=\"https://safimbenchmark.com/\" target=\"_blank\" rel=\"noopener\"\u003eSAFIM\u003c/a\u003e.\u003c/li\u003e\n\n\n\n\u003cli\u003eFinally, we measured the user-centric metrics by leveraging feature usage logs.\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eBelow, we break our feedback loops down into two categories: offline evaluation (with pre-determined datasets) and online evaluation (with real user data).\u003c/p\u003e\n\n\n\n\u003ch3\u003eOffline evaluation\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eDiscussing datasets is always nice, but what about the metrics?\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile creating the dataset is challenging enough, it’s even more challenging to create a good metric that compares the ground truth completion with the one proposed by the neural model. We did a little research and ended up with a combination of two main metrics:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEM:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eExact match – a popular choice for code completion evaluation.\u003c/li\u003e\n\n\n\n\u003cli\u003eThe prediction is considered good if the first line of the completion matches the first line of the ground truth with minimal pre-processing. \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003cstrong\u003eKK:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eThe metric was named after its authors, Karol and Katya, who designed it and manually annotated the threshold-estimating dataset.\u003c/li\u003e\n\n\n\n\u003cli\u003eThe number of suggested lines that are found in the ground truth, divided by the total number of lines in the suggested completion.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eWe also use some additional evaluation techniques, including chrF and LLM-as-a-Judge, for comparing the model variations, but we won’t be covering them here for the sake of keeping the blog post concise. Keep following our updates for more!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eJetBrains BigCode\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe evaluated the model using a benchmark dataset derived from our internal tool, JetBrains BigCode, covering the most popular languages supported by Mellum – including Python, Kotlin, and Java. We ensured that our evaluation dataset has no overlap with the training dataset and transformed it to the FIM structure with contexts gathered using Code Engine.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of the key advantages of the JetBrains BigCode dataset is its ability to slice code snippets based on various features, such as repository topic, popularity, age, whether the code is main or test code, and recent activity (e.g. commits in the past year). This is done for two primary reasons. First, a well-performing model should demonstrate strong results across all these slices, not just on a subset of them. Second, public benchmarks often end up in model training datasets over time, leading to evaluation contamination. To mitigate the latter risk, we designed our age and activity slices to better reflect real-world conditions. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs a result, by maintaining full control over our dataset rather than relying on public benchmarks, we can more reliably assess model quality across different coding styles and practices. According to our experiments on JetBrains BigCode, the complicated nature of the base model does pay dividends in terms of performance. \u003cstrong\u003eSo, it’s demonstrably a good idea to have a strong base model, then fine-tune it, and then align it with DPO.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcuGjO2HNR0v88vhzrAUNL91nRpPPQfOqRapGjj-L75UBRbqxSyhwx0ks69KY_hGZWPPWbm583T3VjwOXd3Zm1KPgEHt3TJSe6f45wrhrrIMVSN8iXQ1hKH-sKQqiYGozLSKhjuYw?key=3rmPPwdiDGp1IMto6KsKrF14\" alt=\"\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAdditionally, our JetBrains BigCode evaluation runs show that we are in good company with well-known battle-tested polyglot models, while being smaller and more efficient. Of course, larger models do outperform us, but they come with significantly higher serving costs, making them less practical for our products.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" fetchpriority=\"high\" width=\"1967\" height=\"1180\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/04/em_new_dataset.png\" alt=\"\"/\u003e\u003cfigcaption\u003eSingle-line suggestions quality (EM metric)\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePublic benchmarks\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBesides evaluation on an internal dataset, we compared the capabilities of our models on different public benchmarks like multilingual benchmark SAFIM (syntax-aware fill in the middle). Stay tuned as we will share benchmarks data for Mellum soon!\u003c/p\u003e\n\n\n\n\u003cp\u003eThat said, it’s important to remember that benchmarks like JetBrains BigCode and SAFIM, while valuable for scalable offline evaluation, don’t fully capture the experience of a real developer using the model. And ultimately, that’s who we’re building for.\u003c/p\u003e\n\n\n\n\u003ch3\u003eOnline evaluation\u003c/h3\u003e\n\n\n\n\u003cp\u003eTo ensure a positive impact on the user experience, we measure several metrics on the feature usage logs, following the same pipeline we described in the paper \u003cem\u003e\u003ca href=\"https://arxiv.org/abs/2405.08704\" target=\"_blank\" rel=\"noopener\"\u003eFull Line Code Completion: Bringing AI to Desktop\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eLong story short, our main metric is called \u003cstrong\u003eratio of completed code\u003c/strong\u003e (RoCC). It is defined as a ratio of symbols of code written with code completion among all code written in the editor. We generally try to optimize for this metric. The core idea and motivation are simple: the more generated text appears in the editor, the better our code completion is. The good thing about the RoCC is that we can vary the number of code completion contributors that we account for. This allows us to, for instance, calculate a general in-editor RoCC or Mellum-specific RoCC.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnother important metric is the \u003cstrong\u003eacceptance rate\u003c/strong\u003e (AR), which is the number of accepted suggestions divided by the number of all shown suggestions. This metric is widely used in the community and is also fairly intuitive: the more users accept our suggestions, the better.\u003c/p\u003e\n\n\n\n\u003cp\u003eBelow, we provide some online metrics data for various popular languages:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e\u003cstrong\u003eRoCC (Mellum + standard completion)\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e\u003cstrong\u003eRoCC (only Mellum)\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e\u003cstrong\u003eAR\u003c/strong\u003e\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eJava\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e46%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e23%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e35%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eKotlin\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e45%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e25%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e31%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003ePython\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e32%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e23%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e35%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eJS/TS\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e39%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e23%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e32%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eC#\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e45%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e18%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e32%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eGo\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e45%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e30%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e44%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003ePHP\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e40%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e26%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e34%\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e\u003cstrong\u003eRust\u003c/strong\u003e\u003c/td\u003e\u003ctd data-align=\"center\"\u003e37%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e24%\u003c/td\u003e\u003ctd data-align=\"center\"\u003e35%\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eOutcomes and what’s next for Mellum\u003c/h2\u003e\n\n\n\n\u003cp\u003eThis was a challenging journey for our ML team, but it resulted in one general completion model and several specialized ones that are all available via the JetBrains AI platform and are currently powering code completion inside JetBrains AI Assistant. To conclude the blogpost, here are our few next steps:\u003c/p\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003eAs mentioned earlier, we are currently working on a specialized model for web development languages that should further boost quality for various languages, and we plan to make this publicly available at some point in the near future.\u003c/li\u003e\n\n\n\n\u003cli\u003eWe would also like to scale the number of parameters further, introducing more diverse data to the training set at the same time. This way Mellum becomes capable of other AI-for-code tasks as well. Keep in mind that service performance remains a key for us, so we expect to stay within reasonable size boundaries as we expand the model. \u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\u003cp\u003eTry coding with Mellum and share your impressions with us!\u003c/p\u003e\n                    \n                                                                                                                                                                                                                            \u003cdiv\u003e\n                                \u003cdiv\u003e\n                                                                            \u003ch4\u003eSubscribe to JetBrains AI Blog updates\u003c/h4\u003e\n                                                                                                            \n                                \u003c/div\u003e\n                                \n                                \u003cp\u003e\u003cimg src=\"https://blog.jetbrains.com/wp-content/themes/jetbrains/assets/img/img-form.svg\" alt=\"image description\"/\u003e\n                                                                    \u003c/p\u003e\n                            \u003c/div\u003e\n                                                            \u003c/div\u003e\n                \u003ca href=\"#\"\u003e\u003c/a\u003e\n                \n                \n            \u003c/section\u003e\n                    \u003cdiv\u003e\n                \u003cp\u003e\n                    \u003ch2\u003eDiscover more\u003c/h2\u003e\n                \u003c/p\u003e\n                \n            \u003c/div\u003e\n                \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "17 min read",
  "publishedTime": null,
  "modifiedTime": null
}
