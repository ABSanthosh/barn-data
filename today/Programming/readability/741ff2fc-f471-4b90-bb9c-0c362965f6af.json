{
  "id": "741ff2fc-f471-4b90-bb9c-0c362965f6af",
  "title": "Conversational image segmentation with Gemini 2.5",
  "link": "https://developers.googleblog.com/en/conversational-image-segmentation-gemini-2-5/",
  "description": "Gemini's advanced capability for conversational image segmentation allows intuitive interaction with visual data by understanding complex phrases, conditional logic, and abstract concepts, streamlining developer experience and opening doors for new applications in media editing, safety monitoring, and damage assessment.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Paul Voigtlaender, Valentin Gabeur, Rohan Doshi",
  "length": 6218,
  "excerpt": "Gemini's advanced capability for conversational image segmentation allows intuitive interaction with visual data by understanding complex phrases, conditional logic, and abstract concepts, streamlining developer experience and opening doors for new applications in media editing, safety monitoring, and damage assessment.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "The way AI visually understands images has evolved tremendously. Initially, AI could tell us \"where\" an object was using bounding boxes. Then, segmentation models arrived, precisely outlining an object's shape. More recently, open-vocabulary models emerged, allowing us to segment objects using less common labels like \"blue ski boot\" or \"xylophone\" without needing a predefined list of categories.Previous models matched pixels to nouns. However, the real challenge — conversational image segmentation (closely related to referring expression segmentation in the literature) — demands a deeper understanding: parsing complex descriptive phrases. Rather than just identifying \"a car,\" what if we could identify \"the car that is farthest away?\"Today, Gemini's advanced visual understanding brings a new level of conversational image segmentation. Gemini now \"understands\" what you're asking it to \"see.\"Leveraging conversational image segmentation queriesThe magic of this feature lies in the types of questions you can ask. By moving beyond simple single-word labels, you can unlock a more intuitive and powerful way to interact with visual data. Consider the 5 categories of queries below.1. Object relationshipsGemini can now identify objects based on their complex relationships to the objects around them.1: Relational understanding: \"the person holding the umbrella\"2: Ordering: \"the third book from the left\"3: Comparative attributes: \"the most wilted flower in the bouquet\" Sorry, your browser doesn't support playback for this video 2. Conditional logicSometimes you need to query with conditional logic. For example, you can filter with queries like \"food that is vegetarian\". Gemini can also handle queries with negations like \"the people who are not sitting\". 3. Abstract conceptsThis is where Gemini's world knowledge shines. You can ask it to segment things that don't have a simple, fixed visual definition. This includes concepts like \"damage,\" \"a mess,\" or \"opportunity.\" 4. In-image textWhen appearance alone is not enough to distinguish the precise category of an object, the user might refer to it through a written text label present in the image. This requires OCR abilities for the model, one of the strengths of Gemini 2.5. 5. Multi-lingual labelsGemini is not restricted to a single language and can handle labels in many different languages. Conversational image segmentation in actionLet's explore how these query types could enable new use cases.1. Unlocking creativity: Interactive media editingThis capability transforms creative workflows. Instead of using complex selection tools, a designer can now direct software with words. This allows for a more fluid and intuitive process, like when asking to select \"the shadow cast by the building\". 2. Building a safer world: Intelligent safety \u0026 compliance monitoringFor workplace safety, you need to identify situations, not just objects. With a prompt like, \"Highlight any employees on the factory floor not wearing a hard hat\", Gemini comprehends the entire conditional instruction as a single query, producing a final, precise mask of only the non-compliant individuals. 3. The future of claims: Nuanced insurance damage assessment\"Damage\" is an abstract concept with many visual forms. An insurance adjuster can now use prompts like, \"Segment the homes with weather damage” and Gemini will use its world knowledge to identify the specific dents and textures associated with that type of damage, distinguishing it from a simple reflection or rust. Why this matters for developers1: Flexible Language: Move beyond rigid, predefined classes. The natural language approach gives you the flexibility to build solutions for the \"long tail\" of visual queries that are specific to your industry and users.2: Simplified Developer Experience: Get started in minutes with a single API. There is no need to find, train, and host separate, specialized segmentation models. This accessibility lowers the barrier to entry for building sophisticated vision applications.Start building todayWe believe that giving language a direct, pixel-level connection to vision will unlock a new generation of intelligent applications. We are incredibly excited to see what you will build.Get started right away in Google AI Studio via our interactive:Spatial Understanding demoOr if you’d prefer a Python environment, feel free to start with our interactive Spatial Understanding colab.To start building with the Gemini API, visit our developer guide and read more about starting with segmentation. You can also join our developer forum to meet other builders, discuss your use cases, and get help from the Gemini API team.Recommended best practicesFor best results, we recommend following the following best practices:1: Use the gemini-2.5-flash model2: Disable thinking set (thinkingBudget=0)3: Stay close to the recommended prompt, and request JSON as output format. Give the segmentation masks for the objects. Output a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and the text label in the key \"label\". Use descriptive labels. Plain text Copied AcknowledgementsWe thank Weicheng Kuo, Rich Munoz, and Huizhong Chen for their work on Gemini segmentation, Junyan Xu for work on infrastructure, Guillaume Vernade for work on documentation and code samples, and the entire Gemini image understanding team, culminating in this release. Finally, we would like to thank image understanding leads Xi Chen and Fei Xia and multimodal understanding lead Jean-Baptiste Alayrac.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini-conversational-image-segme.2e16d0ba.fill-1200x600_7jj4SoJ.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cdiv data-block-key=\"cy6fv\"\u003e\u003cp\u003eThe way AI visually understands images has evolved tremendously. Initially, AI could tell us \u0026#34;where\u0026#34; an object was using bounding boxes. Then, segmentation models arrived, precisely outlining an object\u0026#39;s shape. More recently, \u003ci\u003eopen-vocabulary\u003c/i\u003e models emerged, allowing us to segment objects using less common labels like \u0026#34;blue ski boot\u0026#34; or \u0026#34;xylophone\u0026#34; without needing a predefined list of categories.\u003c/p\u003e\u003cp\u003ePrevious models matched pixels to nouns. However, the real challenge — \u003cb\u003econversational image segmentation\u003c/b\u003e (closely related to \u003ci\u003ereferring expression segmentation\u003c/i\u003e in the literature) — demands a deeper understanding: parsing complex descriptive phrases. Rather than just identifying \u0026#34;a car,\u0026#34; what if we could identify \u0026#34;the car that is farthest away?\u0026#34;\u003c/p\u003e\u003c/div\u003e\u003cp data-block-key=\"9acdg\"\u003eToday, Gemini\u0026#39;s advanced visual understanding brings a new level of conversational image segmentation. Gemini now \u0026#34;understands\u0026#34; what you\u0026#39;re asking it to \u0026#34;see.\u0026#34;\u003c/p\u003e\u003ch2 data-block-key=\"glnbk\" id=\"leveraging-conversational-image-segmentation-queries\"\u003e\u003cbr/\u003eLeveraging conversational image segmentation queries\u003c/h2\u003e\u003cp data-block-key=\"duohs\"\u003eThe magic of this feature lies in the \u003ci\u003etypes\u003c/i\u003e of questions you can ask. By moving beyond simple single-word labels, you can unlock a more intuitive and powerful way to interact with visual data. Consider the 5 categories of queries below.\u003c/p\u003e\u003ch3 data-block-key=\"35g4l\" id=\"1.-object-relationships\"\u003e\u003cbr/\u003e\u003cb\u003e1. Object relationships\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"4spa0\"\u003eGemini can now identify objects based on their complex relationships to the objects around them.\u003c/p\u003e\u003cp data-block-key=\"d9ck1\"\u003e1:\u003cb\u003e Relational understanding:\u003c/b\u003e \u003ccode\u003e\u0026#34;the person holding the umbrella\u0026#34;\u003c/code\u003e\u003c/p\u003e\u003cp data-block-key=\"752dj\"\u003e2:\u003cb\u003e Ordering:\u003c/b\u003e \u003ccode\u003e\u0026#34;the third book from the left\u0026#34;\u003c/code\u003e\u003c/p\u003e\u003cp data-block-key=\"efe9i\"\u003e3:\u003cb\u003e Comparative attributes:\u003c/b\u003e \u003ccode\u003e\u0026#34;the most wilted flower in the bouquet\u0026#34;\u003c/code\u003e\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-f2_fv4eq_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/frisbee_6s.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"c0cip\" id=\"2.-conditional-logic\"\u003e\u003cb\u003e2. Conditional logic\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"3b09v\"\u003eSometimes you need to query with conditional logic. For example, you can filter with queries like \u003ccode\u003e\u0026#34;food that is vegetarian\u0026#34;\u003c/code\u003e. Gemini can also handle queries with negations like \u003ccode\u003e\u0026#34;the people who are not sitting\u0026#34;\u003c/code\u003e.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/people_seating.original.png\" alt=\"Within an office meeting, the natural language query \u0026#34;the people who are not sitting\u0026#34; is used to overlay segmentation masks on the two individuals who are standing.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"ggw9c\" id=\"3.-abstract-concepts\"\u003e\u003cb\u003e3. Abstract concepts\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"avo8n\"\u003eThis is where Gemini\u0026#39;s world knowledge shines. You can ask it to segment things that don\u0026#39;t have a simple, fixed visual definition. This includes concepts like \u0026#34;damage,\u0026#34; \u0026#34;a mess,\u0026#34; or \u0026#34;opportunity.\u0026#34;\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/spill_seg.original.png\" alt=\"On a kitchen counter, a natural language segmentation overlay highlights a spill in response to the abstract query, \u0026#34;area that should be cleaned up\u0026#34;.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"jzu8d\" id=\"4.-in-image-text\"\u003e\u003cb\u003e4. In-image text\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"nrq7\"\u003eWhen appearance alone is not enough to distinguish the precise category of an object, the user might refer to it through a written text label present in the image. This requires OCR abilities for the model, one of the strengths of Gemini 2.5.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/baklava_seg.original.png\" alt=\"In a bakery setting, the model uses natural language segmentation to overlay masks on \u0026#34;the pistachio baklava\u0026#34; , distinguishing it from other nearby pastries based on in-image text.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"khksw\" id=\"5.-multi-lingual-labels\"\u003e\u003cb\u003e5. Multi-lingual labels\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"4iktf\"\u003eGemini is not restricted to a single language and can handle labels in many different languages.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/breakfast_francais_2.original.png\" alt=\"A plate of food has natural language segmentation overlays identifying various components, with the model providing corresponding labels in French as requested by the prompt \u0026#34;tous les objects en français\u0026#34;.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"dbeop\" id=\"conversational-image-segmentation-in-action\"\u003eConversational image segmentation in action\u003c/h2\u003e\u003cp data-block-key=\"bnqo6\"\u003eLet\u0026#39;s explore how these query types could enable new use cases.\u003c/p\u003e\u003ch3 data-block-key=\"umtfc\" id=\"1.-unlocking-creativity:-interactive-media-editing\"\u003e\u003cbr/\u003e\u003cb\u003e1. Unlocking creativity: Interactive media editing\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"dgdpc\"\u003eThis capability transforms creative workflows. Instead of using complex selection tools, a designer can now direct software with words. This allows for a more fluid and intuitive process, like when asking to select \u003ccode\u003e\u0026#34;the shadow cast by the building\u0026#34;\u003c/code\u003e.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/shadow_seg.original.png\" alt=\"An aerial view of a park demonstrates a natural language segmentation overlay identifying \u0026#34;the shadow of the building\u0026#34;.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"2uwn5\" id=\"2.-building-a-safer-world:-intelligent-safety-and-compliance-monitoring\"\u003e\u003cb\u003e2. Building a safer world: Intelligent safety \u0026amp; compliance monitoring\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"aoivg\"\u003eFor workplace safety, you need to identify situations, not just objects. With a prompt like, \u003ccode\u003e\u0026#34;Highlight any employees on the factory floor not wearing a hard hat\u0026#34;\u003c/code\u003e, Gemini comprehends the entire conditional instruction as a single query, producing a final, precise mask of only the non-compliant individuals.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/construction_seg.original.png\" alt=\"At a construction site, a natural language segmentation overlay is applied to identify \u0026#34;the people not wearing a hard hat\u0026#34;.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"4hjps\" id=\"3.-the-future-of-claims:-nuanced-insurance-damage-assessment\"\u003e\u003cb\u003e3. The future of claims: Nuanced insurance damage assessment\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"2ss5d\"\u003e\u0026#34;Damage\u0026#34; is an abstract concept with many visual forms. An insurance adjuster can now use prompts like, \u003ccode\u003e\u0026#34;Segment the homes with weather damage”\u003c/code\u003e and Gemini will use its world knowledge to identify the specific dents and textures associated with that type of damage, distinguishing it from a simple reflection or rust.\u003c/p\u003e\n\u003c/div\u003e   \n\n\n    \n    \u003cdiv\u003e\n        \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/tornado3_seg.original.png\" alt=\"In an aerial photo of a subdivision, natural language segmentation is used to overlay masks on each \u0026#34;damaged house\u0026#34;.\"/\u003e\n            \n            \n        \u003c/p\u003e\n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"7m5rf\" id=\"why-this-matters-for-developers\"\u003eWhy this matters for developers\u003c/h2\u003e\u003cp data-block-key=\"fcumt\"\u003e1:\u003cb\u003e Flexible Language\u003c/b\u003e: Move beyond rigid, predefined classes. The natural language approach gives you the flexibility to build solutions for the \u0026#34;long tail\u0026#34; of visual queries that are specific to your industry and users.\u003c/p\u003e\u003cp data-block-key=\"d6j0b\"\u003e2: \u003cb\u003eSimplified Developer Experience:\u003c/b\u003e Get started in minutes with a single API. There is no need to find, train, and host separate, specialized segmentation models. This accessibility lowers the barrier to entry for building sophisticated vision applications.\u003c/p\u003e\u003ch2 data-block-key=\"phq2y\" id=\"start-building-today\"\u003e\u003cbr/\u003eStart building today\u003c/h2\u003e\u003cp data-block-key=\"9rh03\"\u003eWe believe that giving language a direct, pixel-level connection to vision will unlock a new generation of intelligent applications. We are incredibly excited to see what you will build.\u003c/p\u003e\u003cp data-block-key=\"17ng5\"\u003eGet started right away in Google AI Studio via our interactive:\u003c/p\u003e\u003ch3 data-block-key=\"tnph8\" id=\"spatial-understanding-demo\"\u003e\u003ca href=\"https://aistudio.google.com/app/apps/bundled/spatial-understanding?showPreview=true\u0026amp;appParams=task%3Dsegmentation-masks\"\u003e\u003cb\u003eSpatial Understanding demo\u003c/b\u003e\u003c/a\u003e\u003c/h3\u003e\u003cp data-block-key=\"1bnmn\"\u003eOr if you’d prefer a Python environment, feel free to start with our interactive \u003ca href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb#scrollTo=WQJTJ8wdGOKx\"\u003eSpatial Understanding colab\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"6929l\"\u003eTo start building with the Gemini API, visit our \u003ca href=\"https://ai.google.dev/docs/gemini_api_overview#vision\"\u003edeveloper guide\u003c/a\u003e and \u003ca href=\"https://ai.google.dev/gemini-api/docs/image-understanding#segmentation\"\u003eread more about starting with segmentation\u003c/a\u003e. You can also join our \u003ca href=\"https://discuss.ai.google.dev/\"\u003edeveloper forum\u003c/a\u003e to meet other builders, discuss your use cases, and get help from the Gemini API team.\u003c/p\u003e\u003ch2 data-block-key=\"35um6\" id=\"recommended-best-practices\"\u003e\u003cbr/\u003eRecommended best practices\u003c/h2\u003e\u003cp data-block-key=\"83pom\"\u003eFor best results, we recommend following the following best practices:\u003c/p\u003e\u003cp data-block-key=\"4n85p\"\u003e1: Use the gemini-2.5-flash model\u003c/p\u003e\u003cp data-block-key=\"e79kn\"\u003e2: Disable thinking set (\u003ccode\u003ethinkingBudget=0\u003c/code\u003e)\u003c/p\u003e\u003cp data-block-key=\"1qag1\"\u003e3: Stay close to the recommended prompt, and request JSON as output format.\u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cpre\u003e\u003ccode\u003eGive the segmentation masks for the objects. \nOutput a JSON list of segmentation masks where each entry contains the 2D bounding box in the key \u0026#34;box_2d\u0026#34;, the segmentation mask in key \u0026#34;mask\u0026#34;, and the text label in the key \u0026#34;label\u0026#34;. \nUse descriptive labels.\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e\n        Plain text\n    \u003c/p\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"dfvhf\" id=\"\"\u003e\u003cb\u003eAcknowledgements\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"d45u1\"\u003e\u003csup\u003eWe thank\u003c/sup\u003e \u003csup\u003eWeicheng Kuo,\u003c/sup\u003e \u003csup\u003eRich Munoz, and\u003c/sup\u003e \u003csup\u003eHuizhong Chen for their work on Gemini segmentation,\u003c/sup\u003e \u003csup\u003eJunyan Xu for work on infrastructure,\u003c/sup\u003e \u003csup\u003eGuillaume Vernade for work on documentation and code samples, and the entire Gemini image understanding team, culminating in this release. Finally, we would like to thank image understanding leads\u003c/sup\u003e \u003csup\u003eXi Chen and\u003c/sup\u003e \u003csup\u003eFei Xia and multimodal understanding lead\u003c/sup\u003e \u003csup\u003eJean-Baptiste Alayrac.\u003c/sup\u003e\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "7 min read",
  "publishedTime": "2025-07-21T00:00:00Z",
  "modifiedTime": null
}
