{
  "id": "22c64e06-8ab0-4f69-9766-da405c6b86de",
  "title": "Announcing the general availability of Llama 4 MaaS on Vertex AI",
  "link": "https://developers.googleblog.com/en/llama-4-ga-maas-vertex-ai/",
  "description": "Llama 4, Meta's advanced large language model, is now generally available as a fully managed API on Vertex AI, simplifying deployment and management. The Llama 3.3 70B managed API is also generally available, offering users greater flexibility.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ivan Nardini",
  "length": 7017,
  "excerpt": "Llama 4, Meta's advanced large language model, is now generally available as a fully managed API on Vertex AI, simplifying deployment and management. The Llama 3.3 70B managed API is also generally available, offering users greater flexibility.",
  "siteName": "",
  "favicon": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/meta/apple-touch-icon.png",
  "text": "Deploying and managing Llama 4 models involves multiple steps: navigating complex infrastructure setup, managing GPU availability, ensuring scalability, and handling ongoing operational overhead. What if you could address these challenges and focus directly on building your applications? It’s possible with Vertex AI.We're thrilled to announce that Llama 4, the latest generation of Meta’s open large language models, is now generally available (GA) as a fully managed API endpoint in Vertex AI! In addition to Llama 4, we’re also announcing the general availability of the Llama 3.3 70B managed API in Vertex AI.Llama 4 reaches new performance peaks compared to previous Llama models, with multimodal capabilities and a highly efficient Mixture-of-Experts (MoE) architecture. Llama 4 Scout is more powerful than all previous generations of Llama models while also delivering significant efficiency for multimodal tasks and is optimized to run in a single-GPU environment. Llama 4 Maverick is the most intelligent model option Meta provides today, designed for reasoning, complex image understanding, and demanding generative tasks.With Llama 4 as a fully managed API endpoint, you can now leverage Llama 4's advanced reasoning, coding, and instruction-following capabilities with the ease, scalability, and reliability of Vertex AI to build more sophisticated and impactful AI-powered applications.This post will guide you through getting started with Llama 4 as a Model-as-a-Service (MaaS), highlight the key benefits, show you how simple it is to use, and touch upon cost considerations.Discover Llama 4 MaaS in Vertex AI Model GardenVertex AI Model Garden is your central hub for discovering and deploying foundation models on Google Cloud via managed APIs. It offers a curated selection of Google's own models (like Gemini), open-source models, and third-party models — all accessible through simplified interfaces. The addition of Llama 4 (GA) as a managed service expands this selection, offering you more flexibility. Accessing Llama 4 as a Model-as-a-Service (MaaS) on Vertex AI has the following advantages:1: Zero infrastructure management: Google Cloud handles the underlying infrastructure, GPU provisioning, software dependencies, patching, and maintenance. You interact with a simple API endpoint.2: Guaranteed performance: Processing capacity assigned for these models, ensuring high availability.3: Enterprise-grade security and compliance: Benefit from Google Cloud's robust security, data encryption, access controls, and compliance certifications.Getting started with Llama 4 MaaSGetting started with Llama 4 MaaS on Vertex AI only requires you to navigate to the Llama 4 model card within the Vertex AI Model Garden and accept the Llama Community License Agreement; you cannot call the API without completing this step.Once you have accepted the Llama Community License Agreement in the Model Garden, find the specific Llama 4 MaaS model you wish to use within the Vertex AI Model Garden (e.g., \"Llama 4 17B Instruct MaaS\"). Take note of its unique Model ID (like meta/llama-4-scout-17b-16e-instruct-maas), as you'll need this ID when calling the API.Then you can directly call the Llama 4 MaaS endpoint using the ChatCompletion API. There's no separate \"deploy\" step required for the MaaS offering – Google Cloud manages the endpoint provisioning. Below is an example of how to use Llama 4 Scout using the ChatCompletion API for Python. import openai from google.auth import default, transport import os # --- Configuration --- PROJECT_ID = \"\u003cYOUR_PROJECT_ID\u003e\" LOCATION = \"us-east5\" MODEL_ID = \"meta/llama-4-scout-17b-16e-instruct-maas\" # Obtain Application Default Credentials (ADC) token credentials, _ = default() auth_request = transport.requests.Request() credentials.refresh(auth_request) gcp_token = credentials.token # Construct the Vertex AI MaaS endpoint URL for OpenAI library vertex_ai_endpoint_url = ( f\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/\" f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi\" ) # Initialize the client to use ChatCompletion API pointing to Vertex AI MaaS client = openai.OpenAI( base_url=vertex_ai_endpoint_url, api_key=gcp_token, # Use the GCP token as the API key ) # Example: Multimodal request (text + image from Cloud Storage) prompt_text = \"Describe this landmark and its significance.\" image_gcs_uri = \"gs://cloud-samples-data/vision/landmark/eiffel_tower.jpg\" messages = [ { \"role\": \"user\", \"content\": [ { \"type\": \"image_url\", \"image_url\": {\"url\": image_gcs_uri}, }, {\"type\": \"text\", \"text\": prompt_text}, ], } ] # Optional parameters (refer to model card for specifics) max_tokens_to_generate = 1024 request_temperature = 0.7 request_top_p = 1.0 # Call the ChatCompletion API response = client.chat.completions.create( model=MODEL_ID, # Specify the Llama 4 MaaS model ID messages=messages, max_tokens=max_tokens_to_generate, temperature=request_temperature, top_p=request_top_p, # stream=False # Set to True for streaming responses ) generated_text = response.choices[0].message.content print(generated_text) # The image contains... Copied Important: Always consult the specific Llama 4 model card in Vertex AI Model Garden. It contains crucial information about:The exact input/output schema expected by the model.Supported parameters (like temperature, top_p, max_tokens) and their valid ranges.Any specific formatting requirements for prompts or multimodal inputs.Cost and quota considerationsUsing the Llama 4 as Model-as-a-Service on Vertex AI operates on a predictable model combining pay-as-you-go pricing with usage quotas. Understanding both the pricing structure and your service quotas is essential for scaling your application and managing costs effectively when using the Llama 4 MaaS on Vertex AI.In regards to pricing, you pay only for the prediction requests you make. The underlying infrastructure, scaling, and management costs are incorporated into the API usage price. Refer to the Vertex AI pricing page for details.To ensure service stability and fair usage, your use of Llama 4 as Model-as-service on Vertex AI is subject to quotas. These are limits on factors such as the number of requests per minute (RPM) your project can make to the specific model endpoint. Refer to our quota documentation for more details.What’s nextWith Llama 4 now generally available as a Model-as-a-Service on Vertex AI, you can leverage one of the most advanced open LLMs without managing required infrastructure.Explore Llama 4 in Model GardenCheck out the documentationReview pricing \u0026 quotasWe are excited to see what applications you will build with Llama 4 on Vertex AI. Share your feedback and experiences through our Google Cloud community forum.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Llama-4-MaaS-Blog-Meta.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"q5gcq\"\u003eDeploying and managing Llama 4 models involves multiple steps: navigating complex infrastructure setup, managing GPU availability, ensuring scalability, and handling ongoing operational overhead. What if you could address these challenges and focus directly on building your applications? It’s possible with Vertex AI.\u003c/p\u003e\u003cp data-block-key=\"9irfd\"\u003eWe\u0026#39;re thrilled to announce that Llama 4, the latest generation of Meta’s open large language models, is now generally available (GA) as a \u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-4-maverick-17b-128e-instruct-maas\"\u003efully managed API endpoint\u003c/a\u003e in Vertex AI! In addition to Llama 4, we’re also announcing the general availability of the \u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.3-70b-instruct-maas\"\u003eLlama 3.3 70B managed API\u003c/a\u003e in Vertex AI.\u003c/p\u003e\u003cp data-block-key=\"3hd0q\"\u003eLlama 4 reaches new performance peaks compared to previous Llama models, with multimodal capabilities and a highly efficient Mixture-of-Experts (MoE) architecture. Llama 4 Scout is more powerful than all previous generations of Llama models while also delivering significant efficiency for multimodal tasks and is optimized to run in a single-GPU environment. Llama 4 Maverick is the most intelligent model option Meta provides today, designed for reasoning, complex image understanding, and demanding generative tasks.\u003c/p\u003e\u003cp data-block-key=\"b5ftq\"\u003eWith Llama 4 as a fully managed API endpoint, you can now leverage Llama 4\u0026#39;s advanced reasoning, coding, and instruction-following capabilities with the ease, scalability, and reliability of Vertex AI to build more sophisticated and impactful AI-powered applications.\u003c/p\u003e\u003cp data-block-key=\"7orgp\"\u003eThis post will guide you through getting started with Llama 4 as a Model-as-a-Service (MaaS), highlight the key benefits, show you how simple it is to use, and touch upon cost considerations.\u003c/p\u003e\u003ch2 data-block-key=\"018tc\" id=\"\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eDiscover Llama 4 MaaS in Vertex AI Model Garden\u003c/h2\u003e\u003cp data-block-key=\"fuek\"\u003e\u003ca href=\"https://cloud.google.com/model-garden\"\u003eVertex AI Model Garden\u003c/a\u003e is your central hub for discovering and deploying foundation models on Google Cloud via managed APIs. It offers a curated selection of Google\u0026#39;s own models (like Gemini), open-source models, and third-party models — all accessible through simplified interfaces. The addition of Llama 4 (GA) as a managed service expands this selection, offering you more flexibility.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/llama-4-maas-in-vertex-ai-model-garden.original.png\" alt=\"Llama 4 MaaS in Vertex AI Model Garden\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"q5gcq\"\u003eAccessing Llama 4 as a Model-as-a-Service (MaaS) on Vertex AI has the following advantages:\u003c/p\u003e\u003cp data-block-key=\"7d4bk\"\u003e1:\u003cb\u003e Zero infrastructure management:\u003c/b\u003e Google Cloud handles the underlying infrastructure, GPU provisioning, software dependencies, patching, and maintenance. You interact with a simple API endpoint.\u003c/p\u003e\u003cp data-block-key=\"66vbj\"\u003e2:\u003cb\u003e Guaranteed performance:\u003c/b\u003e Processing capacity assigned for these models, ensuring high availability.\u003c/p\u003e\u003cp data-block-key=\"aa1bi\"\u003e3:\u003cb\u003e Enterprise-grade security and compliance:\u003c/b\u003e Benefit from Google Cloud\u0026#39;s robust security, data encryption, access controls, and compliance certifications.\u003c/p\u003e\u003ch2 data-block-key=\"ofyk0\" id=\"getting-started-with-llama-4-maas\"\u003e\u003cbr/\u003eGetting started with Llama 4 MaaS\u003c/h2\u003e\u003cp data-block-key=\"8fdks\"\u003eGetting started with Llama 4 MaaS on Vertex AI only requires you to navigate\u003cb\u003e to the\u003c/b\u003e \u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-4-maverick-17b-128e-instruct-maas\"\u003e\u003cb\u003eLlama 4 model card within the Vertex AI Model Garden\u003c/b\u003e\u003c/a\u003e\u003cb\u003e and accept the Llama Community License Agreement\u003c/b\u003e; you cannot call the API without completing this step.\u003c/p\u003e\u003cp data-block-key=\"3v7n0\"\u003eOnce you have accepted the Llama Community License Agreement in the Model Garden, find the specific Llama 4 MaaS model you wish to use within the Vertex AI Model Garden (e.g., \u0026#34;Llama 4 17B Instruct MaaS\u0026#34;). Take note of its unique \u003cb\u003eModel ID\u003c/b\u003e (like meta/llama-4-scout-17b-16e-instruct-maas), as you\u0026#39;ll need this ID when calling the API.\u003c/p\u003e\u003cp data-block-key=\"fk22o\"\u003eThen you can directly call the Llama 4 MaaS endpoint using the ChatCompletion API. There\u0026#39;s no separate \u0026#34;deploy\u0026#34; step required for the MaaS offering – Google Cloud manages the endpoint provisioning. Below is an example of how to use Llama 4 Scout using the ChatCompletion API for Python.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eopenai\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003egoogle.auth\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003edefault\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003etransport\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eos\u003c/span\u003e\n\n\u003cspan\u003e# --- Configuration ---\u003c/span\u003e\n\u003cspan\u003ePROJECT_ID\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026#34;\u003c/span\u003e \n\u003cspan\u003eLOCATION\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;us-east5\u0026#34;\u003c/span\u003e\n\u003cspan\u003eMODEL_ID\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;meta/llama-4-scout-17b-16e-instruct-maas\u0026#34;\u003c/span\u003e \n\n\u003cspan\u003e# Obtain Application Default Credentials (ADC) token\u003c/span\u003e\n\u003cspan\u003ecredentials\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e_\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003edefault\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n\u003cspan\u003eauth_request\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003etransport\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003erequests\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eRequest\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n\u003cspan\u003ecredentials\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003erefresh\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eauth_request\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \n\u003cspan\u003egcp_token\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ecredentials\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003etoken\u003c/span\u003e\n\n\u003cspan\u003e# Construct the Vertex AI MaaS endpoint URL for OpenAI library\u003c/span\u003e\n\u003cspan\u003evertex_ai_endpoint_url\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;https://\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003eLOCATION\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e-aiplatform.googleapis.com/v1beta1/\u0026#34;\u003c/span\u003e\n    \u003cspan\u003ef\u003c/span\u003e\u003cspan\u003e\u0026#34;projects/\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003ePROJECT_ID\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e/locations/\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\u003cspan\u003eLOCATION\u003c/span\u003e\u003cspan\u003e}\u003c/span\u003e\u003cspan\u003e/endpoints/openapi\u0026#34;\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Initialize the client to use ChatCompletion API pointing to Vertex AI MaaS\u003c/span\u003e\n\u003cspan\u003eclient\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eopenai\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eOpenAI\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003ebase_url\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003evertex_ai_endpoint_url\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003eapi_key\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003egcp_token\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e# Use the GCP token as the API key\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Example: Multimodal request (text + image from Cloud Storage)\u003c/span\u003e\n\u003cspan\u003eprompt_text\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;Describe this landmark and its significance.\u0026#34;\u003c/span\u003e\n\u003cspan\u003eimage_gcs_uri\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;gs://cloud-samples-data/vision/landmark/eiffel_tower.jpg\u0026#34;\u003c/span\u003e\n\n\u003cspan\u003emessages\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\n    \u003cspan\u003e{\u003c/span\u003e\n        \u003cspan\u003e\u0026#34;role\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;user\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\n            \u003cspan\u003e{\u003c/span\u003e\n                \u003cspan\u003e\u0026#34;type\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;image_url\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n                \u003cspan\u003e\u0026#34;image_url\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\u0026#34;url\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eimage_gcs_uri\u003c/span\u003e\u003cspan\u003e},\u003c/span\u003e\n            \u003cspan\u003e},\u003c/span\u003e\n            \u003cspan\u003e{\u003c/span\u003e\u003cspan\u003e\u0026#34;type\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#34;text\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e\u0026#34;text\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eprompt_text\u003c/span\u003e\u003cspan\u003e},\u003c/span\u003e\n        \u003cspan\u003e],\u003c/span\u003e\n    \u003cspan\u003e}\u003c/span\u003e\n\u003cspan\u003e]\u003c/span\u003e\n\n\u003cspan\u003e# Optional parameters (refer to model card for specifics)\u003c/span\u003e\n\u003cspan\u003emax_tokens_to_generate\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e1024\u003c/span\u003e\n\u003cspan\u003erequest_temperature\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e0.7\u003c/span\u003e\n\u003cspan\u003erequest_top_p\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e1.0\u003c/span\u003e\n\n\u003cspan\u003e# Call the ChatCompletion API\u003c/span\u003e\n\u003cspan\u003eresponse\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eclient\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003echat\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ecompletions\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ecreate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eMODEL_ID\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e# Specify the Llama 4 MaaS model ID\u003c/span\u003e\n        \u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003emessages\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003emax_tokens\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003emax_tokens_to_generate\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003etemperature\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003erequest_temperature\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003etop_p\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003erequest_top_p\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003e# stream=False # Set to True for streaming responses\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003egenerated_text\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eresponse\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003echoices\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emessage\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003econtent\u003c/span\u003e\n\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egenerated_text\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e# The image contains...\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n    \u003cp\u003e\u003cspan\u003eCopied\u003c/span\u003e\n        \n    \u003c/p\u003e\n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"q5gcq\"\u003e\u003cb\u003eImportant:\u003c/b\u003e Always consult the specific Llama 4 model card in Vertex AI Model Garden. It contains crucial information about:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"9ksav\"\u003eThe exact input/output schema expected by the model.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"3j1pm\"\u003eSupported parameters (like temperature, top_p, max_tokens) and their valid ranges.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"d21gm\"\u003eAny specific formatting requirements for prompts or multimodal inputs.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"cedow\" id=\"cost-and-quota-considerations\"\u003e\u003cbr/\u003eCost and quota considerations\u003c/h2\u003e\u003cp data-block-key=\"eh53p\"\u003eUsing the Llama 4 as Model-as-a-Service on Vertex AI operates on a predictable model combining pay-as-you-go pricing with usage quotas. Understanding both the pricing structure and your service quotas is essential for scaling your application and managing costs effectively when using the Llama 4 MaaS on Vertex AI.\u003c/p\u003e\u003cp data-block-key=\"2s2dh\"\u003eIn regards to pricing, you pay only for the prediction requests you make. The underlying infrastructure, scaling, and management costs are incorporated into the API usage price. Refer to the \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/pricing#meta-models\"\u003eVertex AI pricing page\u003c/a\u003e for details.\u003c/p\u003e\u003cp data-block-key=\"75jv9\"\u003eTo ensure service stability and fair usage, your use of Llama 4 as Model-as-service on Vertex AI is subject to quotas. These are limits on factors such as the number of requests per minute (RPM) your project can make to the specific model endpoint. Refer to our \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama#regions-quotas\"\u003equota documentation\u003c/a\u003e for more details.\u003c/p\u003e\u003ch2 data-block-key=\"uiq1w\" id=\"what\u0026#39;s-next\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhat’s next\u003c/h2\u003e\u003cp data-block-key=\"44dhb\"\u003eWith Llama 4 now generally available as a Model-as-a-Service on Vertex AI, you can leverage one of the most advanced open LLMs without managing required infrastructure.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"5vrhb\"\u003e\u003ca href=\"https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-4-maverick-17b-128e-instruct-maas\"\u003e\u003cb\u003eExplore Llama 4 in Model Garden\u003c/b\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"c4e5p\"\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama\"\u003e\u003cb\u003eCheck out the documentation\u003c/b\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"21mud\"\u003e\u003cb\u003eReview\u003c/b\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/pricing#meta-models\"\u003e\u003cb\u003epricing\u003c/b\u003e\u003c/a\u003e\u003cb\u003e \u0026amp;\u003c/b\u003e \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama#regions-quotas\"\u003e\u003cb\u003equotas\u003c/b\u003e\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"9hag3\"\u003e\u003cbr/\u003eWe are excited to see what applications you will build with Llama 4 on Vertex AI. Share your feedback and experiences through our Google Cloud community forum.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2025-04-29T00:00:00Z",
  "modifiedTime": null
}
