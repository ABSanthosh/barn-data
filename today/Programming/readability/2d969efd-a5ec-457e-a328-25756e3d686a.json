{
  "id": "2d969efd-a5ec-457e-a328-25756e3d686a",
  "title": "Enhancing Netflix Reliability with Service-Level Prioritized Load Shedding",
  "link": "https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Tue, 25 Jun 2024 22:58:09 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "chaos-engineering",
    "load-shedding",
    "reliability",
    "distributed-systems",
    "netflix"
  ],
  "byline": "Netflix Technology Blog",
  "length": 17832,
  "excerpt": "In November 2020, we introduced the concept of prioritized load shedding at the API gateway level in our blog post, Keeping Netflix Reliable Using Prioritized Load Shedding. Today, we’re excited to…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "Applying Quality of Service techniques at the application levelAnirudh Mendiratta, Kevin Wang, Joey Lynch, Javier Fernandez-Ivern, Benjamin FedorkaIntroductionIn November 2020, we introduced the concept of prioritized load shedding at the API gateway level in our blog post, Keeping Netflix Reliable Using Prioritized Load Shedding. Today, we’re excited to dive deeper into how we’ve extended this strategy to the individual service level, focusing on the video streaming control plane and data plane, to further enhance user experience and system resilience.The Evolution of Load Shedding at NetflixAt Netflix, ensuring a seamless viewing experience for millions of users simultaneously is paramount. Our initial approach for prioritized load shedding was implemented at the Zuul API gateway layer. This system effectively manages different types of network traffic, ensuring that critical playback requests receive priority over less critical telemetry traffic.Building on this foundation, we recognized the need to apply a similar prioritization logic deeper within our architecture, specifically at the service layer where different types of requests within the same service could be prioritized differently. The advantages of applying these techniques at the service level in addition to our edge API gateway are:Service teams can own their prioritization logic and can apply finer grained prioritization.This can be used for backend to backend communication, i.e. for services not sitting behind our edge API gateway.Services can use cloud capacity more efficiently by combining different request types into one cluster and shedding low priority requests when necessary instead of maintaining separate clusters for failure isolation.Introducing Service-Level Prioritized Load SheddingPlayAPI is a critical backend service on the video streaming control plane, responsible for handling device initiated manifest and license requests necessary to start playback. We categorize these requests into two types based on their criticality:User-Initiated Requests (critical): These requests are made when a user hits play and directly impact the user’s ability to start watching a show or a movie.Pre-fetch Requests (non-critical): These requests are made optimistically when a user browses content without the user hitting play, to reduce latency should the user decide to watch a particular title. A failure in only pre-fetch requests does not result in a playback failure, but slightly increases the latency between pressing play and video appearing on screen.The ProblemIn order to handle large traffic spikes, high backend latency, or an under-scaled backend service, PlayAPI previously used a concurrency limiter to throttle requests that would reduce the availability of both user-initiated and prefetch requests equally. This was not ideal because:Spikes in pre-fetch traffic reduced availability for user-initiated requestsIncreased backend latency reduced availability for user-initiated requests and pre-fetch requests equally, when the system had enough capacity to serve all user-initiated requests.Sharding the critical and non-critical requests into separate clusters was an option, which addressed problem 1 and added failure isolation between the two types of requests, however it came with a higher compute cost. Another disadvantage of sharding is that it adds some operational overhead — engineers need to make sure CI/CD, auto-scaling, metrics, and alerts are enabled for the new cluster.Option 1 — No isolationOption 2 — Isolation but higher compute costOur SolutionWe implemented a concurrency limiter within PlayAPI that prioritizes user-initiated requests over prefetch requests without physically sharding the two request handlers. This mechanism uses the partitioning functionality of the open source Netflix/concurrency-limits Java library. We create two partitions in our limiter:User-Initiated Partition: Guaranteed 100% throughput.Pre-fetch Partition: Utilizes only excess capacity.Option 3 — Single cluster with prioritized load-shedding offers application-level isolation with lower compute cost. Each instance serves both types of requests and has a partition whose size adjusts dynamically to ensure that pre-fetch requests only get excess capacity. This allows user-initiated requests to “steal” pre-fetch capacity when necessary.The partitioned limiter is configured as a pre-processing Servlet Filter that uses HTTP headers sent by devices to determine a request’s criticality, thus avoiding the need to read and parse the request body for rejected requests. This ensures that the limiter is not itself a bottleneck and can effectively reject requests while using minimal CPU. As an example, the filter can be initialized asFilter filter = new ConcurrencyLimitServletFilter( new ServletLimiterBuilder() .named(\"playapi\") .partitionByHeader(\"X-Netflix.Request-Name\") .partition(\"user-initiated\", 1.0) .partition(\"pre-fetch\", 0.0) .build());Note that in steady state, there is no throttling and the prioritization has no effect on the handling of pre-fetch requests. The prioritization mechanism only kicks in when a server is at the concurrency limit and needs to reject requests.TestingIn order to validate that our load-shedding worked as intended, we used Failure Injection Testing to inject 2 second latency in pre-fetch calls, where the typical p99 latency for these calls is \u003c 200 ms. The failure was injected on one baseline instance with regular load shedding and one canary instance with prioritized load shedding. Some internal services that PlayAPI calls use separate clusters for user-initiated and pre-fetch requests and run pre-fetch clusters hotter. This test case simulates a scenario where a pre-fetch cluster for a downstream service is experiencing high latency.Without prioritized load-shedding, both user-initiated and prefetch availability drop when latency is injected. However, after adding prioritized load-shedding, user-initiated requests maintain a 100% availability and only prefetch requests are throttled.We were ready to roll this out to production and see how it performed in the wild!Real-World Application and ResultsNetflix engineers work hard to keep our systems available, and it was a while before we had a production incident that tested the efficacy of our solution. A few months after deploying prioritized load shedding, we had an infrastructure outage at Netflix that impacted streaming for many of our users. Once the outage was fixed, we got a 12x spike in pre-fetch requests per second from Android devices, presumably because there was a backlog of queued requests built up.Spike in Android pre-fetch RPSThis could have resulted in a second outage as our systems weren’t scaled to handle this traffic spike. Did prioritized load-shedding in PlayAPI help us here?Yes! While the availability for prefetch requests dropped as low as 20%, the availability for user-initiated requests was \u003e 99.4% due to prioritized load-shedding.Availability of pre-fetch and user-initiated requestsAt one point we were throttling more than 50% of all requests but the availability of user-initiated requests continued to be \u003e 99.4%.Generic service work prioritizationBased on the success of this approach, we have created an internal library to enable services to perform prioritized load shedding based on pluggable utilization measures, with multiple priority levels.Unlike API gateway, which needs to handle a large volume of requests with varying priorities, most microservices typically receive requests with only a few distinct priorities. To maintain consistency across different services, we have introduced four predefined priority buckets inspired by the Linux tc-prio levels:CRITICAL: Affect core functionality — These will never be shed if we are not in complete failure.DEGRADED: Affect user experience — These will be progressively shed as the load increases.BEST_EFFORT: Do not affect the user — These will be responded to in a best effort fashion and may be shed progressively in normal operation.BULK: Background work, expect these to be routinely shed.Services can either choose the upstream client’s priority or map incoming requests to one of these priority buckets by examining various request attributes, such as HTTP headers or the request body, for more precise control. Here is an example of how services can map requests to priority buckets:ResourceLimiterRequestPriorityProvider requestPriorityProvider() { return contextProvider -\u003e { if (contextProvider.getRequest().isCritical()) { return PriorityBucket.CRITICAL; } else if (contextProvider.getRequest().isHighPriority()) { return PriorityBucket.DEGRADED; } else if (contextProvider.getRequest().isMediumPriority()) { return PriorityBucket.BEST_EFFORT; } else { return PriorityBucket.BULK; } }; }Generic CPU based load-sheddingMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load after hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.For example, if a cluster targets a 60% CPU utilization for auto-scaling, it can be configured to start shedding requests when the CPU utilization exceeds this threshold. When a traffic spike causes the cluster’s CPU utilization to significantly surpass this threshold, it will gradually shed low-priority traffic to conserve resources for high-priority traffic. This approach also allows more time for auto-scaling to add additional instances to the cluster. Once more instances are added, CPU utilization will decrease, and low-priority traffic will resume being served normally.Percentage of requests (Y-axis) being load-shed based on CPU utilization (X-axis) for different priority bucketsExperiments with CPU based load-sheddingWe ran a series of experiments sending a large request volume at a service which normally targets 45% CPU for auto scaling but which was prevented from scaling up for the purpose of monitoring CPU load shedding under extreme load conditions. The instances were configured to shed noncritical traffic after 60% CPU and critical traffic after 80%.As RPS was dialed up past 6x the autoscale volume, the service was able to shed first noncritical and then critical requests. Latency remained within reasonable limits throughout, and successful RPS throughput remained stable.Experimental behavior of CPU based load-shedding using synthetic traffic.P99 latency stayed within a reasonable range throughout the experiment, even as RPS surpassed 6x the autoscale target.Anti-patterns with load-sheddingAnti-pattern 1 — No sheddingIn the above graphs, the limiter does a good job keeping latency low for the successful requests. If there was no shedding here, we’d see latency increase for all requests, instead of a fast failure in some requests that can be retried. Further, this can result in a death spiral where one instance becomes unhealthy, resulting in more load on other instances, resulting in all instances becoming unhealthy before auto-scaling can kick in.No load-shedding: In the absence of load-shedding, increased latency can degrade all requests instead of rejecting some requests (that can be retried), and can make instances unhealthyAnti-pattern 2 — Congestive failureAnother anti-pattern to watch out for is congestive failure or shedding too aggressively. If the load-shedding is due to an increase in traffic, the successful RPS should not drop after load-shedding. Here is an example of what congestive failure looks like:Congestive failure: After 16:57, the service starts rejecting most requests and is not able to sustain a successful 240 RPS that it was before load-shedding kicked in. This can be seen in fixed concurrency limiters or when load-shedding consumes too much CPU preventing any other work from being doneWe can see in the Experiments with CPU based load-shedding section above that our load-shedding implementation avoids both these anti-patterns by keeping latency low and sustaining as much successful RPS during load-shedding as before.Generic IO based load-sheddingSome services are not CPU-bound but instead are IO-bound by backing services or datastores that can apply back pressure via increased latency when they are overloaded either in compute or in storage capacity. For these services we re-use the prioritized load shedding techniques, but we introduce new utilization measures to feed into the shedding logic. Our initial implementation supports two forms of latency based shedding in addition to standard adaptive concurrency limiters (themselves a measure of average latency):The service can specify per-endpoint target and maximum latencies, which allow the service to shed when the service is abnormally slow regardless of backend.The Netflix storage services running on the Data Gateway return observed storage target and max latency SLO utilization, allowing services to shed when they overload their allocated storage capacity.These utilization measures provide early warning signs that a service is generating too much load to a backend, and allow it to shed low priority work before it overwhelms that backend. The main advantage of these techniques over concurrency limits alone is they require less tuning as our services already must maintain tight latency service-level-objectives (SLOs), for example a p50 \u003c 10ms and p100 \u003c 500ms. So, rephrasing these existing SLOs as utilizations allows us to shed low priority work early to prevent further latency impact to high priority work. At the same time, the system will accept as much work as it can while maintaining SLO’s.To create these utilization measures, we count how many requests are processed slower than our target and maximum latency objectives, and emit the percentage of requests failing to meet those latency goals. For example, our KeyValue storage service offers a 10ms target with 500ms max latency for each namespace, and all clients receive utilization measures per data namespace to feed into their prioritized load shedding. These measures look like:utilization(namespace) = { overall = 12 latency = { slo_target = 12, slo_max = 0 } system = { storage = 17, compute = 10, }}In this case, 12% of requests are slower than the 10ms target, 0% are slower than the 500ms max latency (timeout), and 17% of allocated storage is utilized. Different use cases consult different utilizations in their prioritized shedding, for example batches that write data daily may get shed when system storage utilization is approaching capacity as writing more data would create further instability.An example where the latency utilization is useful is for one of our critical file origin services which accepts writes of new files in the AWS cloud and acts as an origin (serves reads) for those files to our Open Connect CDN infrastructure. Writes are the most critical and should never be shed by the service, but when the backing datastore is getting overloaded, it is reasonable to progressively shed reads to files which are less critical to the CDN as it can retry those reads and they do not affect the product experience.To achieve this goal, the origin service configured a KeyValue latency based limiter that starts shedding reads to files which are less critical to the CDN when the datastore reports a target latency utilization exceeding 40%. We then stress tested the system by generating over 50Gbps of read traffic, some of it to high priority files and some of it to low priority files:In this test, there are a nominal number of critical writes and a high number of reads to both low and high priority files. In the top-left graph we ramp to 2000 read/second of ~4MiB files until we can trigger overload of the backend store at over 50Gbps in the top-center graph. When that happens, the top-right graph shows that even under significant load, the origin only sheds low priority read work to preserve high-priority writes and reads. Before this change when we hit breaking points, critical writes and reads would fail along with low priority reads. During this test the CPU load of the file serving service was nominal (\u003c10%), so in this case only IO based limiters are able to protect the system. It is also important to note that the origin will serve more traffic as long as the backing datastore continues accepting it with low latency, preventing the problems we had with concurrency limits in the past where they would either shed too early when nothing was actually wrong or too late when we had entered congestive failure.Conclusion and Future DirectionsThe implementation of service-level prioritized load shedding has proven to be a significant step forward in maintaining high availability and excellent user experience for Netflix customers, even during unexpected system stress.Stay tuned for more updates as we innovate to keep your favorite shows streaming smoothly, no matter what SLO busters lie in wait.AcknowledgementsWe would like to acknowledge the many members of the Netflix consumer product, platform, and open connect teams who have designed, implemented, and tested these prioritization techniques. In particular: Xiaomei Liu, Raj Ummadisetty, Shyam Gala, Justin Guerra, William Schor, Tony Ghita et al.",
  "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*2KByIB47RWng5UNH",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ch2 id=\"14a1\"\u003eApplying Quality of Service techniques at the application level\u003c/h2\u003e\u003cdiv\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page-----e735e6ce8f7d--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page-----e735e6ce8f7d--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cp id=\"0656\"\u003e\u003ca href=\"https://www.linkedin.com/in/amendira/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAnirudh Mendiratta\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/kzwang\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eKevin Wang\u003c/a\u003e, \u003ca href=\"https://jolynch.github.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJoey Lynch\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/ivern\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJavier Fernandez-Ivern\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/benjamin-fedorka\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBenjamin Fedorka\u003c/a\u003e\u003c/p\u003e\u003ch2 id=\"37a8\"\u003eIntroduction\u003c/h2\u003e\u003cp id=\"8aa0\"\u003eIn November 2020, we introduced the concept of prioritized load shedding at the API gateway level in our blog post, \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/keeping-netflix-reliable-using-prioritized-load-shedding-6cc827b02f94\"\u003eKeeping Netflix Reliable Using Prioritized Load Shedding\u003c/a\u003e. Today, we’re excited to dive deeper into how we’ve extended this strategy to the individual service level, focusing on the video streaming control plane and data plane, to further enhance user experience and system resilience.\u003c/p\u003e\u003ch2 id=\"5443\"\u003eThe Evolution of Load Shedding at Netflix\u003c/h2\u003e\u003cp id=\"b404\"\u003eAt Netflix, ensuring a seamless viewing experience for millions of users simultaneously is paramount. Our initial approach for prioritized load shedding was implemented at the Zuul API gateway layer. This system effectively manages different types of network traffic, ensuring that critical playback requests receive priority over less critical telemetry traffic.\u003c/p\u003e\u003cp id=\"9d2f\"\u003eBuilding on this foundation, we recognized the need to apply a similar prioritization logic deeper within our architecture, specifically at the service layer where different types of requests within the same service could be prioritized differently. The advantages of applying these techniques at the service level in addition to our edge API gateway are:\u003c/p\u003e\u003col\u003e\u003cli id=\"c118\"\u003eService teams can own their prioritization logic and can apply finer grained prioritization.\u003c/li\u003e\u003cli id=\"32f7\"\u003eThis can be used for backend to backend communication, i.e. for services not sitting behind our edge API gateway.\u003c/li\u003e\u003cli id=\"c6f4\"\u003eServices can use cloud capacity more efficiently by combining different request types into one cluster and shedding low priority requests when necessary instead of maintaining separate clusters for failure isolation.\u003c/li\u003e\u003c/ol\u003e\u003ch2 id=\"d410\"\u003eIntroducing Service-Level Prioritized Load Shedding\u003c/h2\u003e\u003cp id=\"4cd7\"\u003ePlayAPI is a critical backend service on the video streaming control plane, responsible for handling device initiated manifest and license requests necessary to start playback. We categorize these requests into two types based on their criticality:\u003c/p\u003e\u003col\u003e\u003cli id=\"daf3\"\u003e\u003cstrong\u003eUser-Initiated Requests (critical):\u003c/strong\u003e These requests are made when a user hits play and directly impact the user’s ability to start watching a show or a movie.\u003c/li\u003e\u003cli id=\"9efd\"\u003e\u003cstrong\u003ePre-fetch Requests (non-critical):\u003c/strong\u003e These requests are made optimistically when a user browses content without the user hitting play, to reduce latency should the user decide to watch a particular title. A failure in only pre-fetch requests does not result in a playback failure, but slightly increases the latency between pressing play and video appearing on screen.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv\u003e\u003ch2 id=\"ca91\"\u003eThe Problem\u003c/h2\u003e\u003cp id=\"991e\"\u003eIn order to handle large traffic spikes, high backend latency, or an under-scaled backend service, PlayAPI previously used a concurrency limiter to throttle requests that would reduce the availability of both user-initiated and prefetch requests equally. This was not ideal because:\u003c/p\u003e\u003col\u003e\u003cli id=\"844a\"\u003eSpikes in pre-fetch traffic reduced availability for user-initiated requests\u003c/li\u003e\u003cli id=\"174b\"\u003eIncreased backend latency reduced availability for user-initiated requests and pre-fetch requests equally, when the system had enough capacity to serve all user-initiated requests.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"47f1\"\u003eSharding the critical and non-critical requests into separate clusters was an option, which addressed problem 1 and added failure isolation between the two types of requests, however it came with a higher compute cost. Another disadvantage of sharding is that it adds some operational overhead — engineers need to make sure CI/CD, auto-scaling, metrics, and alerts are enabled for the new cluster.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003e\u003cem\u003eOption 1\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e — No isolation\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003e\u003cem\u003eOption 2\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e — Isolation but higher compute cost\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"c84b\"\u003eOur Solution\u003c/h2\u003e\u003cp id=\"8318\"\u003eWe implemented a concurrency limiter within PlayAPI that prioritizes user-initiated requests over prefetch requests without physically sharding the two request handlers. This mechanism uses the partitioning functionality of the open source \u003ca href=\"https://github.com/Netflix/concurrency-limits\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNetflix/concurrency-limits\u003c/a\u003e Java library. We create two partitions in our limiter:\u003c/p\u003e\u003cul\u003e\u003cli id=\"28e9\"\u003e\u003cstrong\u003eUser-Initiated Partition:\u003c/strong\u003e Guaranteed 100% throughput.\u003c/li\u003e\u003cli id=\"542f\"\u003e\u003cstrong\u003ePre-fetch Partition:\u003c/strong\u003e Utilizes only excess capacity.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cstrong\u003e\u003cem\u003eOption 3\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e — Single cluster with prioritized load-shedding offers application-level isolation with lower compute cost. Each instance serves both types of requests and has a partition whose size adjusts dynamically to ensure that pre-fetch requests only get excess capacity. This allows user-initiated requests to “steal” pre-fetch capacity when necessary.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"5eea\"\u003eThe partitioned limiter is configured as a pre-processing \u003ca href=\"https://github.com/Netflix/concurrency-limits/blob/master/concurrency-limits-servlet/src/main/java/com/netflix/concurrency/limits/servlet/ConcurrencyLimitServletFilter.java\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eServlet Filter\u003c/a\u003e that uses HTTP headers sent by devices to determine a request’s criticality, thus avoiding the need to read and parse the request body for rejected requests. This ensures that the limiter is not itself a bottleneck and can effectively reject requests while using minimal CPU. As an example, the filter can be initialized as\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"65ee\"\u003eFilter filter = new ConcurrencyLimitServletFilter(\u003cbr/\u003e        new ServletLimiterBuilder()\u003cbr/\u003e                .named(\u0026#34;playapi\u0026#34;)\u003cbr/\u003e                .partitionByHeader(\u0026#34;X-Netflix.Request-Name\u0026#34;)\u003cbr/\u003e                .partition(\u0026#34;user-initiated\u0026#34;, 1.0)\u003cbr/\u003e                .partition(\u0026#34;pre-fetch\u0026#34;, 0.0)\u003cbr/\u003e                .build());\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"a211\"\u003eNote that in steady state, there is no throttling and the prioritization has no effect on the handling of pre-fetch requests. The prioritization mechanism only kicks in when a server is at the concurrency limit and needs to reject requests.\u003c/p\u003e\u003ch2 id=\"24b8\"\u003eTesting\u003c/h2\u003e\u003cp id=\"ecef\"\u003eIn order to validate that our load-shedding worked as intended, we used Failure Injection Testing to inject 2 second latency in pre-fetch calls, where the typical p99 latency for these calls is \u0026lt; 200 ms. The failure was injected on one baseline instance with regular load shedding and one canary instance with prioritized load shedding. Some internal services that PlayAPI calls use separate clusters for user-initiated and pre-fetch requests and run pre-fetch clusters hotter. This test case simulates a scenario where a pre-fetch cluster for a downstream service is experiencing high latency.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"2b2e\"\u003eWithout prioritized load-shedding, both user-initiated and prefetch availability drop when latency is injected. However, after adding prioritized load-shedding, user-initiated requests maintain a 100% availability and only prefetch requests are throttled.\u003c/p\u003e\u003cp id=\"cc2a\"\u003eWe were ready to roll this out to production and see how it performed in the wild!\u003c/p\u003e\u003ch2 id=\"d734\"\u003eReal-World Application and Results\u003c/h2\u003e\u003cp id=\"8504\"\u003eNetflix engineers work hard to keep our systems available, and it was a while before we had a production incident that tested the efficacy of our solution. A few months after deploying prioritized load shedding, we had an infrastructure outage at Netflix that impacted streaming for many of our users. Once the outage was fixed, we got a 12x spike in pre-fetch requests per second from Android devices, presumably because there was a backlog of queued requests built up.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eSpike in Android pre-fetch RPS\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"d570\"\u003eThis could have resulted in a second outage as our systems weren’t scaled to handle this traffic spike. Did prioritized load-shedding in PlayAPI help us here?\u003c/p\u003e\u003cp id=\"8fbe\"\u003eYes! While the availability for prefetch requests dropped as low as 20%, the availability for user-initiated requests was \u0026gt; 99.4% due to prioritized load-shedding.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eAvailability of pre-fetch and user-initiated requests\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"e7c2\"\u003eAt one point we were throttling more than 50% of all requests but the availability of user-initiated requests continued to be \u0026gt; 99.4%.\u003c/p\u003e\u003ch2 id=\"60a9\"\u003eGeneric service work prioritization\u003c/h2\u003e\u003cp id=\"bbc2\"\u003eBased on the success of this approach, we have created an internal library to enable services to perform prioritized load shedding based on pluggable utilization measures, with multiple priority levels.\u003c/p\u003e\u003cp id=\"5062\"\u003eUnlike API gateway, which needs to handle a large volume of requests with varying priorities, most microservices typically receive requests with only a few distinct priorities. To maintain consistency across different services, we have introduced four predefined priority buckets inspired by the \u003ca href=\"https://linux.die.net/man/8/tc-prio\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLinux tc-prio levels\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli id=\"b0ff\"\u003e\u003cstrong\u003eCRITICAL\u003c/strong\u003e: Affect core functionality — These will never be shed if we are not in complete failure.\u003c/li\u003e\u003cli id=\"36f6\"\u003e\u003cstrong\u003eDEGRADED\u003c/strong\u003e: Affect user experience — These will be progressively shed as the load increases.\u003c/li\u003e\u003cli id=\"21ce\"\u003e\u003cstrong\u003eBEST_EFFORT\u003c/strong\u003e: Do not affect the user — These will be responded to in a best effort fashion and may be shed progressively in normal operation.\u003c/li\u003e\u003cli id=\"2f38\"\u003e\u003cstrong\u003eBULK\u003c/strong\u003e: Background work, expect these to be routinely shed.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"0a3c\"\u003eServices can either choose the upstream client’s priority \u003cem\u003eor\u003c/em\u003e map incoming requests to one of these priority buckets by examining various request attributes, such as HTTP headers or the request body, for more precise control. Here is an example of how services can map requests to priority buckets:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a188\"\u003eResourceLimiterRequestPriorityProvider requestPriorityProvider() {\u003cbr/\u003e    return contextProvider -\u0026gt; {\u003cbr/\u003e        if (contextProvider.getRequest().isCritical()) {\u003cbr/\u003e              return PriorityBucket.CRITICAL;\u003cbr/\u003e          } else if (contextProvider.getRequest().isHighPriority()) {\u003cbr/\u003e              return PriorityBucket.DEGRADED;\u003cbr/\u003e          } else if (contextProvider.getRequest().isMediumPriority()) {\u003cbr/\u003e              return PriorityBucket.BEST_EFFORT;\u003cbr/\u003e          } else {\u003cbr/\u003e              return PriorityBucket.BULK;\u003cbr/\u003e          }\u003cbr/\u003e        };\u003cbr/\u003e    }\u003c/span\u003e\u003c/pre\u003e\u003ch2 id=\"46c9\"\u003eGeneric CPU based load-shedding\u003c/h2\u003e\u003cp id=\"6762\"\u003eMost services at Netflix autoscale on CPU utilization, so it is a natural measure of system load to tie into the prioritized load shedding framework. Once a request is mapped to a priority bucket, services can determine when to shed traffic from a particular bucket based on CPU utilization. In order to maintain the signal to autoscaling that scaling is needed, prioritized shedding only starts shedding load \u003cem\u003eafter\u003c/em\u003e hitting the target CPU utilization, and as system load increases, more critical traffic is progressively shed in an attempt to maintain user experience.\u003c/p\u003e\u003cp id=\"fc7c\"\u003eFor example, if a cluster targets a 60% CPU utilization for auto-scaling, it can be configured to start shedding requests when the CPU utilization exceeds this threshold. When a traffic spike causes the cluster’s CPU utilization to significantly surpass this threshold, it will gradually shed low-priority traffic to conserve resources for high-priority traffic. This approach also allows more time for auto-scaling to add additional instances to the cluster. Once more instances are added, CPU utilization will decrease, and low-priority traffic will resume being served normally.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003ePercentage of requests (Y-axis) being load-shed based on CPU utilization (X-axis) for different priority buckets\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"df97\"\u003eExperiments with CPU based load-shedding\u003c/h2\u003e\u003cp id=\"afe6\"\u003eWe ran a series of experiments sending a large request volume at a service which normally targets 45% CPU for auto scaling but which was prevented from scaling up for the purpose of monitoring CPU load shedding under extreme load conditions. The instances were configured to shed noncritical traffic after 60% CPU and critical traffic after 80%.\u003c/p\u003e\u003cp id=\"0fe9\"\u003eAs RPS was dialed up past 6x the autoscale volume, the service was able to shed first noncritical and then critical requests. Latency remained within reasonable limits throughout, and successful RPS throughput remained stable.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eExperimental behavior of CPU based load-shedding using synthetic traffic.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eP99 latency stayed within a reasonable range throughout the experiment, even as RPS surpassed 6x the autoscale target.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch2 id=\"ffc7\"\u003eAnti-patterns with load-shedding\u003c/h2\u003e\u003cp id=\"6712\"\u003e\u003cstrong\u003eAnti-pattern 1 — No shedding\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"3332\"\u003eIn the above graphs, the limiter does a good job keeping latency low for the successful requests. If there was no shedding here, we’d see latency increase for all requests, instead of a fast failure in some requests that can be retried. Further, this can result in a death spiral where one instance becomes unhealthy, resulting in more load on other instances, resulting in all instances becoming unhealthy before auto-scaling can kick in.\u003c/p\u003e\u003cfigure\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eNo load-shedding: In the absence of load-shedding, increased latency can degrade all requests instead of rejecting some requests (that can be retried), and can make instances unhealthy\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"34a6\"\u003e\u003cstrong\u003eAnti-pattern 2 — Congestive failure\u003c/strong\u003e\u003c/p\u003e\u003cp id=\"2d12\"\u003eAnother anti-pattern to watch out for is congestive failure or shedding too aggressively. If the load-shedding is due to an increase in traffic, the successful RPS should not drop after load-shedding. Here is an example of what congestive failure looks like:\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption\u003e\u003cem\u003eCongestive failure: After 16:57, the service starts rejecting most requests and is not able to sustain a successful 240 RPS that it was before load-shedding kicked in. This can be seen in fixed concurrency limiters or when load-shedding consumes too much CPU preventing any other work from being done\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"8f93\"\u003eWe can see in the \u003cstrong\u003eExperiments with CPU based load-shedding\u003c/strong\u003e section above that our load-shedding implementation avoids both these anti-patterns by keeping latency low and sustaining as much successful RPS during load-shedding as before.\u003c/p\u003e\u003ch2 id=\"9b9d\"\u003eGeneric IO based load-shedding\u003c/h2\u003e\u003cp id=\"cadc\"\u003eSome services are not CPU-bound but instead are IO-bound by backing services or datastores that can apply back pressure via increased latency when they are overloaded either in compute or in storage capacity. For these services we re-use the prioritized load shedding techniques, but we introduce new utilization measures to feed into the shedding logic. Our initial implementation supports two forms of latency based shedding in addition to standard adaptive concurrency limiters (themselves a measure of average latency):\u003c/p\u003e\u003col\u003e\u003cli id=\"48b6\"\u003eThe service can specify per-endpoint target and maximum latencies, which allow the service to shed when the service is abnormally slow regardless of backend.\u003c/li\u003e\u003cli id=\"ece4\"\u003eThe Netflix storage services running on the \u003ca href=\"https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6\" rel=\"noopener\"\u003eData Gateway\u003c/a\u003e return observed storage target and max latency SLO utilization, allowing services to shed when they overload their allocated storage capacity.\u003c/li\u003e\u003c/ol\u003e\u003cp id=\"ace0\"\u003eThese utilization measures provide early warning signs that a service is generating too much load to a backend, and allow it to shed low priority work before it overwhelms that backend. The main advantage of these techniques over concurrency limits alone is they require less tuning as our services already must maintain tight latency service-level-objectives (SLOs), for example a p50 \u0026lt; 10ms and p100 \u0026lt; 500ms. So, rephrasing these existing SLOs as utilizations allows us to shed low priority work early to prevent further latency impact to high priority work. At the same time, the system \u003cem\u003ewill accept as much work as it can\u003c/em\u003e while maintaining SLO’s.\u003c/p\u003e\u003cp id=\"3a01\"\u003eTo create these utilization measures, we count how many requests are processed \u003cem\u003eslower\u003c/em\u003e than our target and maximum latency objectives, and emit the percentage of requests failing to meet those latency goals. For example, our KeyValue storage service offers a 10ms target with 500ms max latency for each namespace, and all clients receive utilization measures per data namespace to feed into their prioritized load shedding. These measures look like:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"cc68\"\u003eutilization(namespace) = {\u003cbr/\u003e  overall = 12\u003cbr/\u003e  latency = {\u003cbr/\u003e    slo_target = 12,\u003cbr/\u003e    slo_max = 0\u003cbr/\u003e  }\u003cbr/\u003e  system = {\u003cbr/\u003e    storage = 17,\u003cbr/\u003e    compute = 10,\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"d8cd\"\u003eIn this case, 12% of requests are slower than the 10ms target, 0% are slower than the 500ms max latency (timeout), and 17% of allocated storage is utilized. Different use cases consult different utilizations in their prioritized shedding, for example batches that write data daily may get shed when system storage utilization is approaching capacity as writing more data would create further instability.\u003c/p\u003e\u003cp id=\"6c98\"\u003eAn example where the latency utilization is useful is for one of our critical file origin services which accepts writes of new files in the AWS cloud and acts as an origin (serves reads) for those files to our Open Connect CDN infrastructure. Writes are the most critical and should never be shed by the service, but when the backing datastore is getting overloaded, it is reasonable to progressively shed reads to files which are less critical to the CDN as it can retry those reads and they do not affect the product experience.\u003c/p\u003e\u003cp id=\"6791\"\u003eTo achieve this goal, the origin service configured a KeyValue latency based limiter that starts shedding reads to files which are less critical to the CDN when the datastore reports a target latency utilization exceeding 40%. We then stress tested the system by generating over 50Gbps of read traffic, some of it to high priority files and some of it to low priority files:\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"a68b\"\u003eIn this test, there are a nominal number of critical writes and a high number of reads to both low and high priority files. In the top-left graph we ramp to 2000 read/second of ~4MiB files until we can trigger overload of the backend store at over 50Gbps in the top-center graph. When that happens, the top-right graph shows that even under significant load, the origin \u003cem\u003eonly\u003c/em\u003e sheds low priority read work to preserve high-priority writes and reads. Before this change when we hit breaking points, critical writes \u003cem\u003eand\u003c/em\u003e reads would fail along with low priority reads. During this test the CPU load of the file serving service was nominal (\u0026lt;10%), so in this case only IO based limiters are able to protect the system. It is also important to note that the origin will serve more traffic as long as the backing datastore continues accepting it with low latency, preventing the problems we had with concurrency limits in the past where they would either shed too early when nothing was actually wrong or too late when we had entered congestive failure.\u003c/p\u003e\u003ch2 id=\"eb96\"\u003eConclusion and Future Directions\u003c/h2\u003e\u003cp id=\"e80b\"\u003eThe implementation of service-level prioritized load shedding has proven to be a significant step forward in maintaining high availability and excellent user experience for Netflix customers, even during unexpected system stress.\u003c/p\u003e\u003cp id=\"8513\"\u003eStay tuned for more updates as we innovate to keep your favorite shows streaming smoothly, no matter what SLO busters lie in wait.\u003c/p\u003e\u003ch2 id=\"ebf8\"\u003eAcknowledgements\u003c/h2\u003e\u003cp id=\"48fd\"\u003eWe would like to acknowledge the many members of the Netflix consumer product, platform, and open connect teams who have designed, implemented, and tested these prioritization techniques. In particular: \u003ca href=\"https://www.linkedin.com/in/xiaomei-liu-b475711\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eXiaomei Liu\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/rummadis\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRaj Ummadisetty\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/shyam-gala-5891224/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eShyam Gala\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/justin-guerra-3282262b\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJustin Guerra\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/william-schor\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWilliam Schor\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/tonyghita\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTony Ghita\u003c/a\u003e et al.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "19 min read",
  "publishedTime": "2024-06-25T04:23:56.271Z",
  "modifiedTime": null
}
