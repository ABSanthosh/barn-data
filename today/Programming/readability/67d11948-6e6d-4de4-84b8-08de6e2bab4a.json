{
  "id": "67d11948-6e6d-4de4-84b8-08de6e2bab4a",
  "title": "Introducing Keras Hub: Your one-stop shop for pretrained models",
  "link": "https://developers.googleblog.com/en/introducing-keras-hub-for-pretrained-models/",
  "description": "KerasHub is a new unified library for pretrained models fostering a more cohesive ecosystem for developers.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Divyashree Sreepathihalli, Luciano Martins",
  "length": 10107,
  "excerpt": "KerasHub is a new unified library for pretrained models fostering a more cohesive ecosystem for developers.",
  "siteName": "",
  "favicon": "",
  "text": "The world of deep learning is rapidly evolving, with pretrained models becoming increasingly crucial for a wide range of tasks. Keras, known for its user-friendly API and focus on accessibility, has been at the forefront of this movement with specialized libraries like KerasNLP for text-based models and KerasCV for computer vision models.However, as models increasingly blur the lines between modalities – think of powerful chat LLMs with image inputs or vision tasks leveraging text encoders – maintaining these separate domains is less practical. The division between NLP and CV can hinder the development and deployment of truly multimodal models, leading to redundant efforts and a fragmented user experience. To address this, we're excited to announce a major evolution in the Keras ecosystem: KerasHub, a unified, comprehensive library for pretrained models, streamlining access to both cutting-edge NLP and CV architectures. KerasHub is a central repository where you can seamlessly explore and utilize state-of-the-art models like BERT for text analysis alongside EfficientNet for image classification, all within a consistent and familiar Keras framework.A unified developer experienceThis unification not only simplifies model discovery and usage but also fosters a more cohesive ecosystem. With KerasHub, you can leverage advanced features like effortless model publishing and sharing, LoRA fine-tuning for resource-efficient adaptation, quantization for optimized performance, and robust multi-host training for tackling large-scale datasets, all applicable across diverse modalities. This marks a significant step towards democratizing access to powerful AI tools and accelerating the development of innovative multimodal applications.First steps with KerasHubLet's get started by installing KerasHub on your system. From there, you can explore the extensive collection of readily available models and different implementations of popular architectures. You'll then be ready to easily load and incorporate these pre-trained models into your own projects and fine-tune them for optimal performance according to your specific requirements.Installing KerasHubTo install the latest KerasHub release with Keras 3, simply run: $ pip install --upgrade keras-hub Now you can start exploring the available models. The standard environment setup to start working with Keras 3 doesn't change at all to start using KerasHub: import os # Define the Keras 3 backend you want to use - \"jax\", \"tensorflow\" or \"torch\" os.environ[\"KERAS_BACKEND\"] = \"jax\" # Import Keras 3 and KerasHub modules import keras import keras_hub Using computer vision and natural language models with KerasHubNow you are ready to start with KerasHub to access and use the models available at Keras 3 ecosystem. Some examples below:GemmaGemma is a collection of cutting-edge, yet accessible, open models developed by Google. Leveraging the same research and technology behind the Gemini models, Gemma's base models excel at various text generation tasks. These include answering questions, summarizing information, and engaging in logical reasoning. Furthermore, they can be customized to address specific needs.In this example you use Keras and KerasHub to load and start generating contents using Gemma2 2B parameters. For more details about Gemma variants, take a look at the Gemma model card at Kaggle. # Load Gemma 2 2B preset from Kaggle models gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(\"gemma_2b_en\") # Start generating contents with Gemma 2 2B gemma_lm.generate(\"Keras is a\", max_length=32) PaliGemmaPaliGemma is a compact, open model that understands both images and text. Drawing inspiration from PaLI-3 and built on open-source components like the SigLIP vision model and the Gemma language model, PaliGemma can provide detailed and insightful answers to questions about images. This allows for a deeper understanding of visual content, enabling capabilities such as generating captions for images and short videos, identifying objects, and even reading text within images. import os # Define the Keras 3 backend you want to use - \"jax\", \"tensorflow\" or \"torch\" os.environ[\"KERAS_BACKEND\"] = \"jax\" # Import Keras 3 and KerasHub modules import keras import keras_hub from keras.utils import get_file, load_img, img_to_array # Import PaliGemma 3B fine tuned with 224x224 images pali_gemma_lm = keras_hub.models.PaliGemmaCausalLM.from_preset( \"pali_gemma_3b_mix_224\" ) # Download a test image and prepare it for usage with KerasHub url = 'https://storage.googleapis.com/keras-cv/models/paligemma/cow_beach_1.png' img_path = get_file(origin=url) img = img_to_array(load_img(image_path)) # Create the prompt with the question about the image prompt = 'answer where is the cow standing?' # Generate the contents with PaliGemma output = pali_gemma_lm.generate( inputs={ \"images\": img, \"prompts\": prompt, } ) For more details about the available pre-trained models on Keras 3, check out the list of models in Keras on Kaggle.Stability.ai Stable Diffusion 3You have the computer vision models available for usage too. As an example you can use stability.ai Stable Diffusion 3 with KerasHub: from PIL import Image from keras.utils import array_to_img from keras_hub.models import StableDiffusion3TextToImage text_to_image = StableDiffusion3TextToImage.from_preset( \"stable_diffusion_3_medium\", height=1024, width=1024, dtype=\"float16\", ) # Generate images with SD3 image = text_to_image.generate( \"photograph of an astronaut riding a horse, detailed, 8k\", ) # Display the generated image img = array_to_img(image) img For more details about the available pre-trained computer vision models on Keras 3, check the list of models in Keras.What changes for KerasNLP developers?The transition from KerasNLP to KerasHub is a straightforward process. It solely requires updating the import statements from keras_nlp to keras_hub.Example: Previously if you were importing keras_nlp to use a BERT model like below import keras_nlp # Load a BERT model classifier = keras_nlp.models.BertClassifier.from_preset( \"bert_base_en_uncased\", num_classes=2, ) Adjust the import, and you are ready to go with KerasHub: import keras_hub # Load a BERT model classifier = keras_hub.models.BertClassifier.from_preset( \"bert_base_en_uncased\", num_classes=2, ) What changes for KerasCV developers?If you are a current KerasCV user, updating to KerasHub gives you these benefits:Simplified Model Loading: KerasHub offers a consistent API for loading models, which can simplify your code if you're working with both KerasCV and KerasNLP.Framework Flexibility: If you're interested in exploring different frameworks like JAX or PyTorch, KerasHub makes it easier to use KerasCV and KerasNLP models with them.Centralized Repository: Finding and accessing models is easier with KerasHub's unified model repository and is where new architectures will be added in the future.How to adapt my code to KerasHub?ModelsKerasCV models are currently being ported to KerasHub. While most are already available, a few are still a work in progress. Please note that the Centerpillar model will not be ported. You should be able to use any vision model in KerasHub with: import keras_hub # Load a model using preset Model = keras_hub.models.\u003cmodel_name\u003e.from_preset('preset_name`) # or load a custom model by specifying the backbone and preprocessor Model = keras_hub.models.\u003cmodel_name\u003e(backbone=backbone, preprocessor=preprocessor) KerasHub introduces exciting new features for KerasCV developers, offering greater flexibility and expanded capabilities. It includes:Built in preprocessingEach model is accompanied by a bespoke preprocessor that addresses routine tasks including resizing, rescaling, and more, streamlining your workflow.Prior to this, the input preprocessing was performed manually prior to providing the inputs to the model. # Preprocess inputs for example def preprocess_inputs(image, label): # Resize rescale or do more preprocessing on inputs return preprocessed_inputs backbone = keras_cv.models.ResNet50V2Backbone.from_preset( \"resnet50_v2_imagenet\", ) model = keras_cv.models.ImageClassifier( backbone=backbone, num_classes=4, ) output = model(preprocessed_input) Currently, the task models' preprocessing is integrated within the established presets. The inputs undergo preprocessing, where sample images undergo resizing and rescaling within the preprocessor. The preprocessor is an intrinsic component of the task model. Notwithstanding, one has the option to utilize a personalized preprocessor. classifier = keras_hub.models.ImageClassifier.from_preset('resnet_18_imagenet') classifier.predict(inputs) Loss functionsSimilar to augmentation layers, loss functions previously in KerasCV are now available in Keras through keras.losses.\u003closs_function\u003e. For example, if you are currently using FocalLoss function: import keras import keras_cv keras_cv.losses.FocalLoss( alpha=0.25, gamma=2, from_logits=False, label_smoothing=0, **kwargs ) You just need to adjust your loss function definition code to use keras.losses instead of keras_cv.losses: import keras keras.losses.FocalLoss( alpha=0.25, gamma=2, from_logits=False, label_smoothing=0, **kwargs ) Get started with KerasHubDive into the world of KerasHub today:Get started with the documentation: https://keras.io/keras_hub/Check the getting started guides for KerasHub: https://keras.io/guides/keras_hub/Experiment with the pretrained models: https://keras.io/api/keras_hub/models/Explore the source code and contribute: https://github.com/keras-team/keras-hub/Drive into Keras on Kaggle: https://www.kaggle.com/organizations/kerasJoin the Keras community and unlock the power of unified, accessible, and efficient deep learning models. The future of AI is multimodal, and KerasHub is your gateway to it!",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Keras-GfD.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"u50tg\"\u003eThe world of deep learning is rapidly evolving, with pretrained models becoming increasingly crucial for a wide range of tasks. Keras, known for its user-friendly API and focus on accessibility, has been at the forefront of this movement with specialized libraries like KerasNLP for text-based models and KerasCV for computer vision models.\u003c/p\u003e\u003cp data-block-key=\"as7cn\"\u003eHowever, as models increasingly blur the lines between modalities – think of powerful chat LLMs with image inputs or vision tasks leveraging text encoders – maintaining these separate domains is less practical. The division between NLP and CV can hinder the development and deployment of truly multimodal models, leading to redundant efforts and a fragmented user experience.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_ljbDif4.original.png\" alt=\"keras-team/keras-hub, a unified, comprehensive library for pretrained models\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"u50tg\"\u003eTo address this, we\u0026#39;re excited to announce a major evolution in the Keras ecosystem: \u003ca href=\"https://keras.io/keras_hub/\"\u003eKerasHub\u003c/a\u003e, a unified, comprehensive library for pretrained models, streamlining access to both cutting-edge NLP and CV architectures. KerasHub is a central repository where you can seamlessly explore and utilize state-of-the-art models like BERT for text analysis alongside EfficientNet for image classification, all within a consistent and familiar Keras framework.\u003c/p\u003e\u003ch3 data-block-key=\"620os\"\u003e\u003cbr/\u003e\u003cb\u003eA unified developer experience\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"fklj1\"\u003eThis unification not only simplifies model discovery and usage but also fosters a more cohesive ecosystem. With KerasHub, you can leverage advanced features like effortless model publishing and sharing, LoRA fine-tuning for resource-efficient adaptation, quantization for optimized performance, and robust multi-host training for tackling large-scale datasets, all applicable across diverse modalities. This marks a significant step towards democratizing access to powerful AI tools and accelerating the development of innovative multimodal applications.\u003c/p\u003e\u003ch3 data-block-key=\"6vq37\"\u003e\u003cbr/\u003e\u003cb\u003eFirst steps with KerasHub\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"8n85p\"\u003eLet\u0026#39;s get started by installing KerasHub on your system. From there, you can explore the extensive collection of \u003ca href=\"https://keras.io/api/keras_hub/models/\"\u003ereadily available models\u003c/a\u003e and different implementations of popular architectures. You\u0026#39;ll then be ready to easily load and incorporate these pre-trained models into your own projects and fine-tune them for optimal performance according to your specific requirements.\u003c/p\u003e\u003ch3 data-block-key=\"4cu9q\"\u003e\u003cbr/\u003e\u003cb\u003eInstalling KerasHub\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"8btr5\"\u003eTo install the latest KerasHub release with Keras 3, simply run:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e$\u003cspan\u003e \u003c/span\u003epip\u003cspan\u003e \u003c/span\u003einstall\u003cspan\u003e \u003c/span\u003e--upgrade\u003cspan\u003e \u003c/span\u003ekeras-hub\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"u50tg\"\u003eNow you can start exploring the available models. The standard environment setup to start working with Keras 3 doesn\u0026#39;t change at all to start using KerasHub:\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eos\u003c/span\u003e\n\n\u003cspan\u003e# Define the Keras 3 backend you want to use - \u0026#34;jax\u0026#34;, \u0026#34;tensorflow\u0026#34; or \u0026#34;torch\u0026#34;\u003c/span\u003e\n\u003cspan\u003eos\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eenviron\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#34;KERAS_BACKEND\u0026#34;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;jax\u0026#34;\u003c/span\u003e\n\n\u003cspan\u003e# Import Keras 3 and KerasHub modules\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"u50tg\"\u003eUsing computer vision and natural language models with KerasHub\u003c/h2\u003e\u003cp data-block-key=\"728q8\"\u003eNow you are ready to start with KerasHub to access and use the models available at Keras 3 ecosystem. Some examples below:\u003c/p\u003e\u003ch3 data-block-key=\"6b0r3\"\u003e\u003cb\u003e\u003cbr/\u003eGemma\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"87ia\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/base\"\u003eGemma\u003c/a\u003e is a collection of cutting-edge, yet accessible, open models developed by Google. Leveraging the same research and technology behind the Gemini models, Gemma\u0026#39;s base models excel at various text generation tasks. These include answering questions, summarizing information, and engaging in logical reasoning. Furthermore, they can be customized to address specific needs.\u003c/p\u003e\u003cp data-block-key=\"fgnpl\"\u003eIn this example you use Keras and KerasHub to load and start generating contents using Gemma2 2B parameters. For more details about Gemma variants, take a look at the \u003ca href=\"https://www.kaggle.com/models/google/gemma/\"\u003eGemma model card\u003c/a\u003e at Kaggle.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e# Load Gemma 2 2B preset from Kaggle models \u003c/span\u003e\n\u003cspan\u003egemma_lm\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eGemmaCausalLM\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;gemma_2b_en\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Start generating contents with Gemma 2 2B\u003c/span\u003e\n\u003cspan\u003egemma_lm\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;Keras is a\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003emax_length\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e32\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"u50tg\"\u003e\u003cb\u003ePaliGemma\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"cj0hi\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/paligemma\"\u003ePaliGemma\u003c/a\u003e is a compact, open model that understands both images and text. Drawing inspiration from \u003ca href=\"https://arxiv.org/abs/2310.09199\"\u003ePaLI-3\u003c/a\u003e and built on open-source components like the \u003ca href=\"https://arxiv.org/abs/2303.15343\"\u003eSigLIP vision model\u003c/a\u003e and the \u003ca href=\"https://arxiv.org/abs/2403.08295\"\u003eGemma language model\u003c/a\u003e, PaliGemma can provide detailed and insightful answers to questions about images. This allows for a deeper understanding of visual content, enabling capabilities such as generating captions for images and short videos, identifying objects, and even reading text within images.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eos\u003c/span\u003e\n\n\u003cspan\u003e# Define the Keras 3 backend you want to use - \u0026#34;jax\u0026#34;, \u0026#34;tensorflow\u0026#34; or \u0026#34;torch\u0026#34;\u003c/span\u003e\n\u003cspan\u003eos\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eenviron\u003c/span\u003e\u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#34;KERAS_BACKEND\u0026#34;\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;jax\u0026#34;\u003c/span\u003e\n\n\u003cspan\u003e# Import Keras 3 and KerasHub modules\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ekeras.utils\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eget_file\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eload_img\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eimg_to_array\u003c/span\u003e\n\n\n\u003cspan\u003e# Import PaliGemma 3B fine tuned with 224x224 images\u003c/span\u003e\n\u003cspan\u003epali_gemma_lm\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ePaliGemmaCausalLM\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;pali_gemma_3b_mix_224\u0026#34;\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Download a test image and prepare it for usage with KerasHub\u003c/span\u003e\n\u003cspan\u003eurl\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#39;https://storage.googleapis.com/keras-cv/models/paligemma/cow_beach_1.png\u0026#39;\u003c/span\u003e\n\u003cspan\u003eimg_path\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eget_file\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eorigin\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eurl\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003eimg\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eimg_to_array\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eload_img\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eimage_path\u003c/span\u003e\u003cspan\u003e))\u003c/span\u003e\n\n\u003cspan\u003e# Create the prompt with the question about the image\u003c/span\u003e\n\u003cspan\u003eprompt\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#39;answer where is the cow standing?\u0026#39;\u003c/span\u003e\n\n\u003cspan\u003e# Generate the contents with PaliGemma\u003c/span\u003e\n\u003cspan\u003eoutput\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003epali_gemma_lm\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003einputs\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e{\u003c/span\u003e\n        \u003cspan\u003e\u0026#34;images\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eimg\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003e\u0026#34;prompts\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eprompt\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e}\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"u50tg\"\u003eFor more details about the available pre-trained models on Keras 3, check out the \u003ca href=\"https://www.kaggle.com/organizations/keras/models\"\u003elist of models in Keras\u003c/a\u003e on Kaggle.\u003c/p\u003e\u003ch3 data-block-key=\"fcv95\"\u003e\u003cbr/\u003e\u003cb\u003eStability.ai Stable Diffusion 3\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"56ia0\"\u003eYou have the computer vision models available for usage too. As an example you can use stability.ai \u003ca href=\"https://stability.ai/news/stable-diffusion-3\"\u003eStable Diffusion 3\u003c/a\u003e with KerasHub:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ePIL\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eImage\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ekeras.utils\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003earray_to_img\u003c/span\u003e\n\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003ekeras_hub.models\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eStableDiffusion3TextToImage\u003c/span\u003e\n\n\u003cspan\u003etext_to_image\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eStableDiffusion3TextToImage\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;stable_diffusion_3_medium\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003eheight\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003ewidth\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e1024\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003edtype\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;float16\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Generate images with SD3\u003c/span\u003e\n\u003cspan\u003eimage\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003etext_to_image\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003egenerate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;photograph of an astronaut riding a horse, detailed, 8k\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003e# Display the generated image\u003c/span\u003e\n\u003cspan\u003eimg\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003earray_to_img\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003eimg\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"u50tg\"\u003eFor more details about the available pre-trained computer vision models on Keras 3, check the \u003ca href=\"https://keras.io/api/keras_hub/models/\"\u003elist of models in Keras\u003c/a\u003e.\u003c/p\u003e\u003ch2 data-block-key=\"6bgmm\"\u003e\u003cbr/\u003eWhat changes for KerasNLP developers?\u003c/h2\u003e\u003cp data-block-key=\"b4vep\"\u003eThe transition from KerasNLP to KerasHub is a straightforward process. It solely requires updating the import statements from \u003ccode\u003ekeras_nlp\u003c/code\u003e to \u003ccode\u003ekeras_hub\u003c/code\u003e.\u003c/p\u003e\u003cp data-block-key=\"8mod1\"\u003eExample: Previously if you were importing keras_nlp to use a BERT model like below\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras_nlp\u003c/span\u003e\n\n\u003cspan\u003e# Load a BERT model \u003c/span\u003e\n\u003cspan\u003eclassifier\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_nlp\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eBertClassifier\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;bert_base_en_uncased\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \n    \u003cspan\u003enum_classes\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"u50tg\"\u003eAdjust the import, and you are ready to go with KerasHub:\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\n\n\u003cspan\u003e# Load a BERT model \u003c/span\u003e\n\u003cspan\u003eclassifier\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eBertClassifier\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;bert_base_en_uncased\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \n    \u003cspan\u003enum_classes\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"u50tg\"\u003eWhat changes for KerasCV developers?\u003c/h2\u003e\u003cp data-block-key=\"deqts\"\u003eIf you are a current KerasCV user, updating to KerasHub gives you these benefits:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"8g3j5\"\u003e\u003cb\u003eSimplified Model Loading:\u003c/b\u003e KerasHub offers a consistent API for loading models, which can simplify your code if you\u0026#39;re working with both KerasCV and KerasNLP.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"f8le9\"\u003e\u003cb\u003eFramework Flexibility:\u003c/b\u003e If you\u0026#39;re interested in exploring different frameworks like JAX or PyTorch, KerasHub makes it easier to use KerasCV and KerasNLP models with them.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"76h9c\"\u003e\u003cb\u003eCentralized Repository:\u003c/b\u003e Finding and accessing models is easier with KerasHub\u0026#39;s unified model repository and is where new architectures will be added in the future.\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"e5gb4\"\u003e\u003cbr/\u003e\u003cb\u003eHow to adapt my code to KerasHub?\u003c/b\u003e\u003c/h3\u003e\u003ch3 data-block-key=\"13otm\"\u003eModels\u003c/h3\u003e\u003cp data-block-key=\"107ui\"\u003eKerasCV models are currently being ported to KerasHub. While most are already available, a few are still a work in progress. Please note that the \u003ca href=\"https://www.kaggle.com/models/keras/centerpillar\"\u003eCenterpillar\u003c/a\u003e model will not be ported. You should be able to use any vision model in KerasHub with:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\n\n\u003cspan\u003e# Load a model using preset\u003c/span\u003e\n\u003cspan\u003eModel\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u0026lt;\u003c/span\u003e\u003cspan\u003emodel_name\u003c/span\u003e\u003cspan\u003e\u0026gt;.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;preset_name`)\u003c/span\u003e\n\n\u003cspan\u003e# or load a custom model by specifying the backbone and preprocessor\u003c/span\u003e\n\u003cspan\u003eModel\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u0026lt;\u003c/span\u003e\u003cspan\u003emodel_name\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ebackbone\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003ebackbone\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epreprocessor\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003epreprocessor\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"u50tg\"\u003eKerasHub introduces exciting new features for KerasCV developers, offering greater flexibility and expanded capabilities. It includes:\u003c/p\u003e\u003ch3 data-block-key=\"fela8\"\u003e\u003cbr/\u003eBuilt in preprocessing\u003c/h3\u003e\u003cp data-block-key=\"6sjsv\"\u003eEach model is accompanied by a bespoke preprocessor that addresses routine tasks including resizing, rescaling, and more, streamlining your workflow.\u003c/p\u003e\u003cp data-block-key=\"ai30i\"\u003ePrior to this, the input preprocessing was performed manually prior to providing the inputs to the model.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e# Preprocess inputs for example\u003c/span\u003e\n\u003cspan\u003edef\u003c/span\u003e \u003cspan\u003epreprocess_inputs\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eimage\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003elabel\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e\n    \u003cspan\u003e# Resize rescale or do more preprocessing on inputs\u003c/span\u003e\n    \u003cspan\u003ereturn\u003c/span\u003e \u003cspan\u003epreprocessed_inputs\u003c/span\u003e\n\u003cspan\u003ebackbone\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_cv\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eResNet50V2Backbone\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e\u0026#34;resnet50_v2_imagenet\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003emodel\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_cv\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eImageClassifier\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003ebackbone\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003ebackbone\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003enum_classes\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003eoutput\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epreprocessed_input\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"u50tg\"\u003eCurrently, the task models\u0026#39; preprocessing is integrated within the established presets. The inputs undergo preprocessing, where sample images undergo resizing and rescaling within the preprocessor. The preprocessor is an intrinsic component of the task model. Notwithstanding, one has the option to utilize a personalized preprocessor.\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eclassifier\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003ekeras_hub\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003emodels\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eImageClassifier\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_preset\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;resnet_18_imagenet\u0026#39;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003eclassifier\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003epredict\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einputs\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"u50tg\"\u003eLoss functions\u003c/h3\u003e\u003cp data-block-key=\"5qhd\"\u003eSimilar to augmentation layers, loss functions previously in KerasCV are now available in Keras through \u003ccode\u003ekeras.losses.\u0026lt;loss_function\u0026gt;\u003c/code\u003e. For example, if you are currently using \u003ca href=\"https://keras.io/api/keras_cv/losses/focal_loss/\"\u003eFocalLoss function\u003c/a\u003e:\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras\u003c/span\u003e\n\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras_cv\u003c/span\u003e\n\n\u003cspan\u003ekeras_cv\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elosses\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eFocalLoss\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003ealpha\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0.25\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003egamma\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003efrom_logits\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003elabel_smoothing\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e**\u003c/span\u003e\u003cspan\u003ekwargs\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"u50tg\"\u003eYou just need to adjust your loss function definition code to use \u003ccode\u003ekeras.losses\u003c/code\u003e instead of \u003ccode\u003ekeras_cv.losses\u003c/code\u003e:\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003ekeras\u003c/span\u003e\n\n\u003cspan\u003ekeras\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003elosses\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eFocalLoss\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003ealpha\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0.25\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003egamma\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003efrom_logits\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003elabel_smoothing\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e**\u003c/span\u003e\u003cspan\u003ekwargs\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"u50tg\"\u003e\u003cb\u003eGet started with KerasHub\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"e16fi\"\u003eDive into the world of KerasHub today:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"22sc3\"\u003e\u003cb\u003eGet started with the documentation:\u003c/b\u003e\u003ca href=\"https://keras.io/keras_hub/\"\u003e https://keras.io/keras_hub/\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"bsh6s\"\u003e\u003cb\u003eCheck the getting started guides for KerasHub:\u003c/b\u003e \u003ca href=\"https://keras.io/guides/keras_hub/\"\u003ehttps://keras.io/guides/keras_hub/\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"atjro\"\u003e\u003cb\u003eExperiment with the pretrained models:\u003c/b\u003e \u003ca href=\"https://keras.io/api/keras_hub/models/\"\u003ehttps://keras.io/api/keras_hub/models/\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"6vhaq\"\u003e\u003cb\u003eExplore the source code and contribute:\u003c/b\u003e\u003ca href=\"https://github.com/keras-team/keras-hub/\"\u003e https://github.com/keras-team/keras-hub/\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"19vfn\"\u003e\u003cb\u003eDrive into Keras on Kaggle:\u003c/b\u003e \u003ca href=\"https://www.kaggle.com/organizations/keras\"\u003ehttps://www.kaggle.com/organizations/keras\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"3adku\"\u003e\u003cbr/\u003eJoin the Keras community and unlock the power of unified, accessible, and efficient deep learning models. The future of AI is multimodal, and KerasHub is your gateway to it!\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "11 min read",
  "publishedTime": "2024-10-22T00:00:00Z",
  "modifiedTime": null
}
