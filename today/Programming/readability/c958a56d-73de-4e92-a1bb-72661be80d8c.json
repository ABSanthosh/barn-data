{
  "id": "c958a56d-73de-4e92-a1bb-72661be80d8c",
  "title": "Secure Access To Opensearch on AWS",
  "link": "https://buffer.com/resources/secure-access-to-opensearch-on-aws/",
  "description": "With the surprising swap of Elasticsearch with Opensearch on AWS. Learn how the team at Buffer achieved secure access without AWS credentials.",
  "author": "Peter Emil",
  "published": "Mon, 18 Apr 2022 13:49:52 GMT",
  "source": "https://buffer.com/resources/overflow/rss/",
  "categories": [
    "Overflow"
  ],
  "byline": "Joe Birch",
  "length": 6660,
  "excerpt": "With the surprising swap of Elasticsearch with Opensearch on AWS. Learn how the team at Buffer achieved secure access without AWS credentials.",
  "siteName": "Buffer: All-you-need social media toolkit for small businesses",
  "favicon": "https://buffer.com/static/icons/apple-touch-icon.png",
  "text": "At Buffer, we’ve been working on a better admin dashboard for our customer advocacy team. This admin dashboard included a much more powerful search functionality. Nearing the end of the project’s timeline, we’ve been prompted with the replacement of managed Elasticsearch on AWS with managed Opensearch. Our project has been built on top of newer versions of the elasticsearch client which suddenly didn’t support Opensearch.To add more fuel to the fire, OpenSearch clients for the languages we use, did not yet support transparent AWS Sigv4 signatures. AWS Sigv4 signing is a requirement to authenticate to the OpenSearch cluster using AWS credentials.This meant that the path forward was riddled with one of these optionsLeave our search cluster open to the world without authentication, then it would work with the OpenSearch client. Needless to say, this is a huge NO GO for obvious reasons.Refactor our code to send raw HTTP requests and implement the AWS Sigv4 mechanism ourselves on these requests. This is infeasible, and we wouldn’t want to reinvent a client library ourselves!Build a plugin/middleware for the client that implements AWS Sigv4 signing. This would work at first, but Buffer is not a big team and with constant service upgrades, this is not something we can reliably maintain.Switch our infrastructure to use an elasticsearch cluster hosted on Elastic’s cloud. This entailed a huge amount of effort as we examined Elastic’s Terms of Service, pricing, requirements for a secure networking setup and other time-expensive measures.It seemed like this project was stuck in it for the long haul! Or was it?Looking at the situation, here are the constants we can’t feasibly change.We can’t use the elasticsearch client anymore.Switching to the OpenSearch client would work if the cluster was open and required no authentication.We can’t leave the OpenSearch cluster open to the world for obvious reasons.Wouldn’t it be nice if the OpenSearch cluster was open ONLY to the applications that need it?If this can be accomplished, then those applications would be able to connect to the cluster without authentication allowing them to use the existing OpenSearch client, but for everything else, the cluster would be unreachable.With that end goal in mind, we architected the following solution.Piggybacking off our recent migration from self-managed Kubernetes to Amazon EKSWe recently migrated our computational infrastructure from a self-managed Kubernetes cluster to another cluster that’s managed by Amazon EKS.With this migration, we exchanged our container networking interface (CNI) from flannel to VPC CNI. This entails that we eliminated the overlay/underlay networks split and that all our pods were now getting VPC routable IP addresses.This will become more relevant going forward.Block cluster access from the outside worldWe created an OpenSearch cluster in a private VPC (no internet-facing IP addresses). This means the cluster’s IP addresses would not be reachable over the internet but only to internal VPC routable IP addresses.We added three security groups to the cluster to control which VPC IP addresses are allowed to reach the cluster.Build automations to control what is allowed to access the clusterWe built two automations running as AWS lambdas.Security Group Manager: This automation can execute two processes on-demand.-\u003e Add an IP address to one of those three security groups (the one with the least number of rules at the time of addition).-\u003e Remove an IP address everywhere it appears in those three security groups.Pod Lifecycle Auditor: This automation runs on schedule and we’ll get to what it does in a moment.How it all connects togetherWe added an InitContainer to all pods needing access to the OpenSearch cluster that, on-start, will execute the Security Group Manager automation and ask it to add the pod’s IP address to one of the security groups. This allows it to reach the OpenSearch cluster.In real life, things happen and pods get killed and they get new IP addresses.Therefore, on schedule, the Pod Lifecycle Auditor runs and checks all the whitelisted IP addresses in the three security groups that enable access to cluster. It then checks which IP addresses should not be there and reconciles the security groups by asking the Security Group Manager to remove those IP addresses. Here is a diagram of how it all connects togetherDiagram for our solution to tackling Opensearch access problems through automated whitelisting, source: Peter Emil on behalf of Buffer's Infrastructure TeamWhy did we create three security groups to manage access to the OpenSearch cluster?Because security groups have a maximum limit of 50 ingress/egress rules. We anticipate that we won’t have more than 70-90 pods at any given time needing access to the cluster. Having three security groups sets the limit at 150 rules which feels like a safe spot for us to start with.Do I need to host the Opensearch cluster in the same VPC as the EKS cluster?It depends on your networking setup! If your VPC has private subnets with NAT gateways, then you can host it in any VPC you like. If you don’t have private subnets, you need to host both clusters in the same VPC because VPC CNI by default NATs VPC-external pod traffic to the hosting node’s IP address which invalidates this solution. If you turn off the NAT configuration, then your pods can’t reach the internet which is a bigger problem.If a pod gets stuck in CrashLoopBackoff state, won’t the huge volume of restarts exhaust the 150 rules limit?No, because container crashes within a pod get restarted with the same IP address within the same pod. The IP Address isn’t changed.Aren’t those automations a single-point-of-failure?Yes they are, which is why it’s important to approach them with an SRE mindset. Adequate monitoring of these automations mixed with rolling deployments is crucial to having reliability here. Ever since these automations were instated, they’ve been very stable and we didn’t get any incidents. However, I sleep easy at night knowing that if one of them breaks for any reason I’ll get notified way before it becomes a noticeable problem.ConclusionI acknowledge that this solution isn’t perfect but it was the quickest and easiest solution to implement without requiring continuous maintenance and without delving into the process of on-boarding a new cloud provider.Over to youWhat do you think of the approach we adopted here? Have you encountered similar situations in your organization? Send us a tweet!Try Buffer for free140,000+ small businesses like yours use Buffer to build their brand on social media every monthGet started now",
  "image": "https://buffer.com/resources/content/images/2022/04/kari-shea-1SAnrIxw5OY-unsplash.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eAt Buffer, we’ve been working on a better admin dashboard for our customer advocacy team. This admin dashboard included a much more powerful search functionality. Nearing the end of the project’s timeline, we’ve been prompted with the replacement of managed Elasticsearch on AWS with managed Opensearch. Our project has been built on top of newer versions of the elasticsearch client which \u003ca href=\"https://aws.amazon.com/blogs/opensource/keeping-clients-of-opensearch-and-elasticsearch-compatible-with-open-source/\" rel=\"noreferrer nofollow noopener\"\u003esuddenly didn’t support\u003c/a\u003e Opensearch.\u003c/p\u003e\u003cp\u003eTo add more fuel to the fire, OpenSearch clients for the languages we use, did not yet support transparent AWS Sigv4 signatures. AWS Sigv4 signing is a requirement to authenticate to the OpenSearch cluster using AWS credentials.\u003c/p\u003e\u003cp\u003eThis meant that the path forward was riddled with one of these options\u003c/p\u003e\u003cul\u003e\u003cli\u003eLeave our search cluster open to the world without authentication, then it would work with the OpenSearch client. Needless to say, this is a huge NO GO for obvious reasons.\u003c/li\u003e\u003cli\u003eRefactor our code to send raw HTTP requests and implement the AWS Sigv4 mechanism ourselves on these requests. This is infeasible, and we wouldn’t want to reinvent a client library ourselves!\u003c/li\u003e\u003cli\u003eBuild a plugin/middleware for the client that implements AWS Sigv4 signing. This would work at first, but Buffer is not a big team and with constant service upgrades, this is not something we can reliably maintain.\u003c/li\u003e\u003cli\u003eSwitch our infrastructure to use an elasticsearch cluster hosted on Elastic’s cloud. This entailed a huge amount of effort as we examined Elastic’s Terms of Service, pricing, requirements for a secure networking setup and other time-expensive measures.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr/\u003eIt seemed like this project was stuck in it for the long haul! Or was it?\u003c/p\u003e\u003cp\u003eLooking at the situation, here are the constants we can’t feasibly change.\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe can’t use the elasticsearch client anymore.\u003c/li\u003e\u003cli\u003eSwitching to the OpenSearch client would work if the cluster was open and required no authentication.\u003c/li\u003e\u003cli\u003eWe can’t leave the OpenSearch cluster open to the world for obvious reasons.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr/\u003eWouldn’t it be nice if the OpenSearch cluster was open ONLY to the applications that need it?\u003c/p\u003e\u003cp\u003eIf this can be accomplished, then those applications would be able to connect to the cluster without authentication allowing them to use the existing OpenSearch client, but for everything else, the cluster would be unreachable.\u003cbr/\u003eWith that end goal in mind, we architected the following solution.\u003c/p\u003e\u003ch2 id=\"piggybacking-off-our-recent-migration-from-self-managed-kubernetes-to-amazon-eks\"\u003ePiggybacking off our recent migration from self-managed Kubernetes to Amazon EKS\u003c/h2\u003e\u003cp\u003eWe recently migrated our computational infrastructure from a self-managed Kubernetes cluster to another cluster that’s managed by Amazon EKS.\u003cbr/\u003eWith this migration, we exchanged our container networking interface (CNI) from flannel to VPC CNI. This entails that we eliminated the overlay/underlay networks split and that all our pods were now getting VPC routable IP addresses.\u003cbr/\u003eThis will become more relevant going forward.\u003c/p\u003e\u003ch2 id=\"block-cluster-access-from-the-outside-world\"\u003eBlock cluster access from the outside world\u003c/h2\u003e\u003cp\u003eWe created an OpenSearch cluster in a private VPC (no internet-facing IP addresses). This means the cluster’s IP addresses would not be reachable over the internet but only to internal VPC routable IP addresses.\u003cbr/\u003eWe added three security groups to the cluster to control which VPC IP addresses are allowed to reach the cluster.\u003c/p\u003e\u003ch2 id=\"build-automations-to-control-what-is-allowed-to-access-the-cluster\"\u003eBuild automations to control what is allowed to access the cluster\u003c/h2\u003e\u003cp\u003eWe built two automations running as AWS lambdas.\u003c/p\u003e\u003cul\u003e\u003cli\u003eSecurity Group Manager: This automation can execute two processes on-demand.\u003c/li\u003e\u003cli\u003e-\u0026gt; Add an IP address to one of those three security groups (the one with the least number of rules at the time of addition).\u003c/li\u003e\u003cli\u003e-\u0026gt; Remove an IP address everywhere it appears in those three security groups.\u003c/li\u003e\u003cli\u003ePod Lifecycle Auditor: This automation runs on schedule and we’ll get to what it does in a moment.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"how-it-all-connects-together\"\u003eHow it all connects together\u003c/h2\u003e\u003cp\u003eWe added an \u003ca href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\" rel=\"noreferrer nofollow noopener\"\u003eInitContainer\u003c/a\u003e to all pods needing access to the OpenSearch cluster that, on-start, will execute the Security Group Manager automation and ask it to add the pod’s IP address to one of the security groups. This allows it to reach the OpenSearch cluster.\u003cbr/\u003eIn real life, things happen and pods get killed and they get new IP addresses.Therefore, on schedule, the Pod Lifecycle Auditor runs and checks all the whitelisted IP addresses in the three security groups that enable access to cluster. It then checks which IP addresses should not be there and reconciles the security groups by asking the Security Group Manager to remove those IP addresses. \u003cbr/\u003eHere is a diagram of how it all connects together\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://buffer.com/resources/content/images/2022/04/Buffer-s-Automation-for-OpenSearch---Page-2--2-.png\" alt=\"Diagram for our solution to tackling Opensearch access problems through automated Whitelisting, source: Peter Emil on behalf of Buffer\u0026#39;s Infrastructure Team\" loading=\"lazy\" width=\"2000\" height=\"1287\" srcset=\"https://buffer.com/resources/content/images/size/w600/2022/04/Buffer-s-Automation-for-OpenSearch---Page-2--2-.png 600w, https://buffer.com/resources/content/images/size/w1000/2022/04/Buffer-s-Automation-for-OpenSearch---Page-2--2-.png 1000w, https://buffer.com/resources/content/images/size/w1600/2022/04/Buffer-s-Automation-for-OpenSearch---Page-2--2-.png 1600w, https://buffer.com/resources/content/images/size/w2400/2022/04/Buffer-s-Automation-for-OpenSearch---Page-2--2-.png 2400w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003cfigcaption\u003eDiagram for our solution to tackling Opensearch access problems through automated whitelisting, source: Peter Emil on behalf of Buffer\u0026#39;s Infrastructure Team\u003c/figcaption\u003e\u003c/figure\u003e\u003ch3 id=\"why-did-we-create-three-security-groups-to-manage-access-to-the-opensearch-cluster\"\u003eWhy did we create three security groups to manage access to the OpenSearch cluster?\u003c/h3\u003e\u003cp\u003eBecause security groups have a maximum limit of 50 ingress/egress rules. We anticipate that we won’t have more than 70-90 pods at any given time needing access to the cluster. Having three security groups sets the limit at 150 rules which feels like a safe spot for us to start with.\u003c/p\u003e\u003ch3 id=\"do-i-need-to-host-the-opensearch-cluster-in-the-same-vpc-as-the-eks-cluster\"\u003eDo I need to host the Opensearch cluster in the same VPC as the EKS cluster?\u003c/h3\u003e\u003cp\u003eIt depends on your networking setup! If your VPC has private subnets with NAT gateways, then you can host it in any VPC you like. If you don’t have private subnets, you need to host both clusters in the same VPC because VPC CNI by default \u003ca href=\"https://docs.aws.amazon.com/eks/latest/userguide/external-snat.html\" rel=\"noreferrer nofollow noopener\"\u003eNATs VPC-external pod traffic\u003c/a\u003e to the hosting node’s IP address which invalidates this solution. If you turn off the NAT configuration, then your pods can’t reach the internet which is a bigger problem.\u003c/p\u003e\u003ch3 id=\"if-a-pod-gets-stuck-in-crashloopbackoff-state-won%E2%80%99t-the-huge-volume-of-restarts-exhaust-the-150-rules-limit\"\u003eIf a pod gets stuck in CrashLoopBackoff state, won’t the huge volume of restarts exhaust the 150 rules limit?\u003c/h3\u003e\u003cp\u003eNo, because container crashes within a pod get restarted with the same IP address within the same pod. The IP Address isn’t changed.\u003c/p\u003e\u003ch3 id=\"aren%E2%80%99t-those-automations-a-single-point-of-failure\"\u003eAren’t those automations a single-point-of-failure?\u003c/h3\u003e\u003cp\u003eYes they are, which is why it’s important to approach them with an SRE mindset. Adequate monitoring of these automations mixed with rolling deployments is crucial to having reliability here. Ever since these automations were instated, they’ve been very stable and we didn’t get any incidents. However, I sleep easy at night knowing that if one of them breaks for any reason I’ll get notified way before it becomes a noticeable problem.\u003c/p\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\u003cp\u003eI acknowledge that this solution isn’t perfect but it was the quickest and easiest solution to implement without requiring continuous maintenance and without delving into the process of on-boarding a new cloud provider.\u003c/p\u003e\u003ch2 id=\"over-to-you\"\u003eOver to you\u003c/h2\u003e\u003cp\u003eWhat do you think of the approach we adopted here? Have you encountered similar situations in your organization? \u003ca href=\"https://twitter.com/buffer\" rel=\"noreferrer nofollow noopener\"\u003eSend us a tweet!\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"midpost-cta\"\u003e\u003ch3\u003eTry Buffer for free\u003c/h3\u003e\u003cp\u003e140,000+ small businesses like yours use Buffer to build their brand on social media every month\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://login.buffer.com/signup?product=buffer\u0026amp;plan=free\u0026amp;cycle=year\u0026amp;cta=bufferBlogLibrary-post-midCTA-signup-1\" role=\"button\"\u003eGet started now\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": null,
  "modifiedTime": null
}
