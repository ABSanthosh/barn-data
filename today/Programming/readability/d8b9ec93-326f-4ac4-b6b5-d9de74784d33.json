{
  "id": "d8b9ec93-326f-4ac4-b6b5-d9de74784d33",
  "title": "Gemma explained: RecurrentGemma architecture",
  "link": "https://developers.googleblog.com/en/gemma-explained-recurrentgemma-architecture/",
  "description": "RecurrentGemma architecture showcases a hybrid model that mixes gated linear recurrences with local sliding window attention; a highly valuable feature when you're concerned about exhausting your LLM's context window.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ju-yeong Ji, Ravin Kumar",
  "length": 11077,
  "excerpt": "RecurrentGemma architecture showcases a hybrid model that mixes gated linear recurrences with local sliding window attention; a highly valuable feature when you're concerned about exhausting your LLM's context window.",
  "siteName": "",
  "favicon": "",
  "text": "Ravin Kumar Google Data Scientist Language Applications In the previous post of the Gemma explained series, we discussed the latest Gemma 2 architecture. In this post, you will explore the RecurrentGemma architecture. Let’s get started!RecurrentGemma 2B, 9BRecurrentGemma is based on Griffin, a hybrid model that mixes gated linear recurrences with local sliding window attention. This change improves computation and memory and it's better suited for long context prompts. However it comes with the downside of reduced needle in haystack performance due to the fixed-sized state of the Griffin architecture. While it is possible to provide the entire text from a book as input, this approach may not be optimal. Recurrent Neural Networks (RNNs) can encounter difficulties in learning long-range dependencies in exceedingly long sequences, and the model has a limited context window. This means that it can only effectively consider a certain number of preceding tokens when making predictions.Moreover, recurrent models have not yet received as much attention in terms of inference time optimizations compared to their transformer counterparts. And there’s less research and community support available compared to the well-established transformer architecture.So, this model will be highly valuable in scenarios when you are concerned about exhausting your LLM’s context window. By prioritizing the most recent information and strategically discarding older data, RecurrentGemma ensures that the LLM's performance remains strong as the context expands.Below is the architecture diagram for the Recurrent Gemma 2B model. Griffin follows the same residual pattern and MLP block as other Transformer baseline. However, unlike both the MQA Transformer baseline and the Hawk model, Griffin uses a blend of recurrent and MQA blocks. Griffin uses a layered structure by alternating two residual blocks with a recurrent block, followed by a residual block that incorporates the local MQA attention block.The core parameters of the architecture are summarized in the table below. Non-Embedding params and Embedding paramsNon-embedding parameters are distributed throughout the hidden layers of the model, in components like attention mechanisms and feedforward networks.Note: The naming of the model “2B” comes from this parameterEmbedding Parameters are usually found in the dedicated layer called an embedding layer. This layer is responsible for mapping discrete tokens (like words or characters) into continuous vector representations (embeddings).Note: 0.7B can be calculated as 256k (vocabulary size) x 2560 (model width)Model width and RNN widthModel width refers to the size of the hidden layers in the model, determining the model’s capacity to represent complex patterns, just like the base Gemma Models.Recurrent neural network (RNN) width is the size of the hidden state maintained by the Real-Gated Linear Recurrent Unit (RG-LRU). Unlike traditional Transformers, the recurrent block maintains a fixed-size internal state, regardless of the input length. This allows RecurrentGemma to process longer sequences with less memory, making it more efficient for tasks like generating long articles or code.MLP expansion factorIt’s the same as feedforward hidden dimensions in the base Gemma model. For simplicity, we applied an expansion factor of 3 in the Recurrent Gemma model, resulting in an MLP dimension of 7680 (calculated as 2560 x 3).Local attention window sizeThe state maintained by RecurrentGemma has a finite size and does not grow with sequences longer than the local attention window of 2k tokens. This means that while the maximum length of samples generated autoregressively by Gemma is limited by the host system's memory capacity, RecurrentGemma can generate sequences of arbitrary length, overcoming this constraint. RecurrentGemmaForCausalLM( (model): RecurrentGemmaModel( (embed_tokens): Embedding(256000, 2560, padding_idx=0) (layers): ModuleList( (0-1): 2 x RecurrentGemmaDecoderLayer( (temporal_pre_norm): RecurrentGemmaRMSNorm() (temporal_block): RecurrentGemmaRecurrentBlock( (linear_y): Linear(in_features=2560, out_features=2560, bias=True) (linear_x): Linear(in_features=2560, out_features=2560, bias=True) (linear_out): Linear(in_features=2560, out_features=2560, bias=True) (conv_1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560) (rg_lru): RecurrentGemmaRglru() (act_fn): PytorchGELUTanh() ) (channel_pre_norm): RecurrentGemmaRMSNorm() (mlp_block): RecurrentGemmaMlp( (gate_proj): Linear(in_features=2560, out_features=7680, bias=True) (up_proj): Linear(in_features=2560, out_features=7680, bias=True) (down_proj): Linear(in_features=7680, out_features=2560, bias=True) (act_fn): PytorchGELUTanh() ) ) (2): RecurrentGemmaDecoderLayer( (temporal_pre_norm): RecurrentGemmaRMSNorm() (temporal_block): RecurrentGemmaSdpaAttention( (q_proj): Linear(in_features=2560, out_features=2560, bias=False) (k_proj): Linear(in_features=2560, out_features=256, bias=False) (v_proj): Linear(in_features=2560, out_features=256, bias=False) (o_proj): Linear(in_features=2560, out_features=2560, bias=True) (rotary_emb): RecurrentGemmaRotaryEmbedding() ) (channel_pre_norm): RecurrentGemmaRMSNorm() (mlp_block): RecurrentGemmaMlp( (gate_proj): Linear(in_features=2560, out_features=7680, bias=True) (up_proj): Linear(in_features=2560, out_features=7680, bias=True) (down_proj): Linear(in_features=7680, out_features=2560, bias=True) (act_fn): PytorchGELUTanh() ) ) : (23): RecurrentGemmaDecoderLayer( (temporal_pre_norm): RecurrentGemmaRMSNorm() (temporal_block): RecurrentGemmaSdpaAttention( (q_proj): Linear(in_features=2560, out_features=2560, bias=False) (k_proj): Linear(in_features=2560, out_features=256, bias=False) (v_proj): Linear(in_features=2560, out_features=256, bias=False) (o_proj): Linear(in_features=2560, out_features=2560, bias=True) (rotary_emb): RecurrentGemmaRotaryEmbedding() ) (channel_pre_norm): RecurrentGemmaRMSNorm() (mlp_block): RecurrentGemmaMlp( (gate_proj): Linear(in_features=2560, out_features=7680, bias=True) (up_proj): Linear(in_features=2560, out_features=7680, bias=True) (down_proj): Linear(in_features=7680, out_features=2560, bias=True) (act_fn): PytorchGELUTanh() ) ) (24-25): 2 x RecurrentGemmaDecoderLayer( (temporal_pre_norm): RecurrentGemmaRMSNorm() (temporal_block): RecurrentGemmaRecurrentBlock( (linear_y): Linear(in_features=2560, out_features=2560, bias=True) (linear_x): Linear(in_features=2560, out_features=2560, bias=True) (linear_out): Linear(in_features=2560, out_features=2560, bias=True) (conv_1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560) (rg_lru): RecurrentGemmaRglru() (act_fn): PytorchGELUTanh() ) (channel_pre_norm): RecurrentGemmaRMSNorm() (mlp_block): RecurrentGemmaMlp( (gate_proj): Linear(in_features=2560, out_features=7680, bias=True) (up_proj): Linear(in_features=2560, out_features=7680, bias=True) (down_proj): Linear(in_features=7680, out_features=2560, bias=True) (act_fn): PytorchGELUTanh() ) ) ) (final_norm): RecurrentGemmaRMSNorm() ) (lm_head): Linear(in_features=2560, out_features=256000, bias=False) ) embed_tokens (Embedding Layer)Takes the input text as a sequence of tokens and maps each token to a continuous vector representation of size 2560. It has a vocabulary size of 256000 which is the same with base Gemma models.layersThere are 26 decoder layers in total, grouped into repeating patterns.The model begins with two residual blocks with a recurrent block (0-1). This sequence is then followed by a residual block (2) and a series of continuous blocks that alternate until the end of the layer (25). Residual block with a recurrent blockIn the recurrent block (Temporal mixing block), the model takes the input of dimension (Model width) 2560 and applies two linear layers with output dimension (RNN width) 2560 in parallel, creating two branches.On the first branch (right side), it applies a small separable Conv1D layer with a temporal filter dimension of 4. And the RG-LRU(Real-Gated Linear Recurrent Unit) layer follows.On the second branch (left side), it applies a GeLU nonlinearity.And then merge the branches by element-wise multiplication, apply a final linear layer with output dimension (Model width) 2560. After applying RMSNorm, the MLP block follows.Residual Block with a local MQAAfter having two residual blocks with a recurrent block (0-1), a residual block with a local MQA (2) follows. One of the key disadvantages of using global attention is that its computational complexity grows quadratically in the sequence length. To address this, RecurrentGemma uses a local sliding window attention. It allows each position to attend only to a fixed number of tokens in the past.In the local MQA block (Temporal mixing block), the model takes the input of dimension (Model width) 2560. It uses linear projections (q_proj, k_proj, v_proj, o_proj) to create query, key, value, and output representations. Note that out_features for k_proj and v_proj is 256 as they share the same head with a size of 256, while q_proj and o_proj have 10 heads (256 x 10 = 2560) in parallel.It incorporates rotary_emb (RecurrentGemmaRotaryEmbedding) for rotary positional embeddings (RoPE) just like the base Gemma models.Applying RMSNorm and the MLP block is the same with the previous residual block.What’s Next?In this article, you learned about RecurrentGemma.In the next post, you will explore PaliGemma which is a lightweight open vision-language model (VLM).Stay tuned and thank you for reading!ReferencesPapersGriffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language ModelsRecurrentGemma: Moving Past Transformers for Efficient Open Language ModelsCode ExamplesGetting started with RecurrentGemma📋 The complete Gemma architecture seriesGemma explained: An overview of Gemma model family architecturesGemma explained: What’s new in Gemma 2Gemma explained: RecurrentGemma architectureGemma explained: PaliGemma architecture",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma_metadata_image_1600_x_873_p.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Ravin+Kumar\"\u003eRavin Kumar\u003c/a\u003e\n            \n              \u003cspan\u003eGoogle Data Scientist\u003c/span\u003e\n            \n            \n              \u003cspan\u003eLanguage Applications\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"j612x\"\u003eIn the \u003ca href=\"https://developers.googleblog.com/en/gemma-explained-new-in-gemma-2/\"\u003eprevious post\u003c/a\u003e of the Gemma explained series, we discussed the latest Gemma 2 architecture. In this post, you will explore the RecurrentGemma architecture. Let’s get started!\u003c/p\u003e\u003ch2 data-block-key=\"2119b\"\u003e\u003cbr/\u003eRecurrentGemma 2B, 9B\u003c/h2\u003e\u003cp data-block-key=\"4juh8\"\u003eRecurrentGemma is based on \u003ca href=\"https://arxiv.org/abs/2402.19427\"\u003eGriffin\u003c/a\u003e, a hybrid model that mixes gated linear recurrences with local sliding window attention. This change improves computation and memory and it\u0026#39;s better suited for long context prompts.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_REGZE1l.original.png\" alt=\"Griffin hybrid model architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"r6w58\"\u003eHowever it comes with the downside of reduced needle in haystack performance due to the fixed-sized state of the Griffin architecture. While it is possible to provide the entire text from a book as input, this approach may not be optimal. Recurrent Neural Networks (RNNs) can encounter difficulties in learning long-range dependencies in exceedingly long sequences, and the model has a limited context window. This means that it can only effectively consider a certain number of preceding tokens when making predictions.\u003c/p\u003e\u003cp data-block-key=\"4m6q8\"\u003eMoreover, recurrent models have not yet received as much attention in terms of inference time optimizations compared to their transformer counterparts. And there’s less research and community support available compared to the well-established transformer architecture.\u003c/p\u003e\u003cp data-block-key=\"4e9mg\"\u003eSo, this model will be highly valuable in scenarios when you are concerned about exhausting your LLM’s context window. By prioritizing the most recent information and strategically discarding older data, RecurrentGemma ensures that the LLM\u0026#39;s performance remains strong as the context expands.\u003c/p\u003e\u003cp data-block-key=\"101ki\"\u003eBelow is the architecture diagram for the Recurrent Gemma 2B model.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image7_oX4tL8v.original.png\" alt=\"Recurrent Gemma 2B model architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"r6w58\"\u003eGriffin follows the same residual pattern and MLP block as other Transformer baseline. However, unlike both the MQA Transformer baseline and the Hawk model, Griffin uses a blend of recurrent and MQA blocks.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_MB1Gg03.original.png\" alt=\"Layered structure of recurrent and MQA blocks\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"r6w58\"\u003eGriffin uses a layered structure by alternating two residual blocks with a recurrent block, followed by a residual block that incorporates the local MQA attention block.\u003c/p\u003e\u003cp data-block-key=\"pj86\"\u003eThe \u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf\"\u003ecore parameters\u003c/a\u003e of the architecture are summarized in the table below.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-21_at_4.29.54PM.original.png\" alt=\"Core parameters of the architecture of 2B and 9B models\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"r6w58\"\u003e\u003cb\u003eNon-Embedding params and Embedding params\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"8842q\"\u003eNon-embedding parameters are distributed throughout the hidden layers of the model, in components like attention mechanisms and feedforward networks.\u003c/p\u003e\u003cp data-block-key=\"43ten\"\u003e\u003cb\u003eNote:\u003c/b\u003e The naming of the model “2B” comes from this parameter\u003c/p\u003e\u003cp data-block-key=\"df4vc\"\u003eEmbedding Parameters are usually found in the dedicated layer called an embedding layer. This layer is responsible for mapping discrete tokens (like words or characters) into continuous vector representations (embeddings).\u003c/p\u003e\u003cp data-block-key=\"cuuh8\"\u003e\u003cb\u003eNote:\u003c/b\u003e 0.7B can be calculated as 256k (vocabulary size) x 2560 (model width)\u003c/p\u003e\u003ch3 data-block-key=\"cs0rh\"\u003e\u003cbr/\u003e\u003cb\u003eModel width and RNN width\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"2ofr5\"\u003eModel width refers to the size of the hidden layers in the model, determining the model’s capacity to represent complex patterns, just like the base Gemma Models.\u003c/p\u003e\u003cp data-block-key=\"cf89t\"\u003eRecurrent neural network (RNN) width is the size of the hidden state maintained by the Real-Gated Linear Recurrent Unit (RG-LRU). Unlike traditional Transformers, the recurrent block maintains a fixed-size internal state, regardless of the input length. This allows RecurrentGemma to process longer sequences with less memory, making it more efficient for tasks like generating long articles or code.\u003c/p\u003e\u003ch3 data-block-key=\"2i2j6\"\u003e\u003cbr/\u003e\u003cb\u003eMLP expansion factor\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"67jrt\"\u003eIt’s the same as feedforward hidden dimensions in the base Gemma model. For simplicity, we applied an expansion factor of 3 in the Recurrent Gemma model, resulting in an MLP dimension of 7680 (calculated as 2560 x 3).\u003c/p\u003e\u003ch3 data-block-key=\"13kl5\"\u003e\u003cbr/\u003e\u003cb\u003eLocal attention window size\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9fi60\"\u003eThe state maintained by RecurrentGemma has a finite size and does not grow with sequences longer than the local attention window of 2k tokens. This means that while the maximum length of samples generated autoregressively by Gemma is limited by the host system\u0026#39;s memory capacity, RecurrentGemma can generate sequences of arbitrary length, overcoming this constraint.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eRecurrentGemmaForCausalLM\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaModel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembed_tokens\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e256000\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epadding_idx\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e2\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eRecurrentGemmaDecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRecurrentBlock\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elinear_y\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elinear_x\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elinear_out\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003econv_1d\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eConv1d\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ekernel_size\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003estride\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003epadding\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e3\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003egroups\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erg_lru\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRglru\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echannel_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaMlp\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e2\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaDecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaSdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echannel_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaMlp\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n\n      \u003cspan\u003e:\u003c/span\u003e\n\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e23\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaDecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaSdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echannel_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaMlp\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e24\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e25\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e2\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eRecurrentGemmaDecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003etemporal_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRecurrentBlock\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elinear_y\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elinear_x\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elinear_out\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003econv_1d\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eConv1d\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ekernel_size\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e4\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003estride\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003epadding\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e3\u003c/span\u003e\u003cspan\u003e,),\u003c/span\u003e \u003cspan\u003egroups\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erg_lru\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRglru\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003echannel_pre_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp_block\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaMlp\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e7680\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003efinal_norm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eRecurrentGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elm_head\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2560\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256000\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"r6w58\"\u003e\u003cb\u003eembed_tokens (Embedding Layer)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9n44b\"\u003eTakes the input text as a sequence of tokens and maps each token to a continuous vector representation of size 2560. It has a vocabulary size of 256000 which is the same with base Gemma models.\u003c/p\u003e\u003ch3 data-block-key=\"5nlve\"\u003e\u003cbr/\u003e\u003cb\u003elayers\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"18an9\"\u003eThere are 26 decoder layers in total, grouped into repeating patterns.\u003c/p\u003e\u003cp data-block-key=\"ag23m\"\u003eThe model begins with two residual blocks with a recurrent block (0-1). This sequence is then followed by a residual block (2) and a series of continuous blocks that alternate until the end of the layer (25).\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image4_7W5j3q2.original.png\" alt=\"Recurrent block architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch4 data-block-key=\"r6w58\"\u003e\u003cb\u003eResidual block with a recurrent block\u003c/b\u003e\u003c/h4\u003e\u003cp data-block-key=\"57vjo\"\u003eIn the recurrent block (Temporal mixing block), the model takes the input of dimension (Model width) 2560 and applies two linear layers with output dimension (RNN width) 2560 in parallel, creating two branches.\u003c/p\u003e\u003cp data-block-key=\"flato\"\u003eOn the first branch (right side), it applies a small separable Conv1D layer with a temporal filter dimension of 4. And the RG-LRU(Real-Gated Linear Recurrent Unit) layer follows.\u003c/p\u003e\u003cp data-block-key=\"6g1rb\"\u003eOn the second branch (left side), it applies a GeLU nonlinearity.\u003c/p\u003e\u003cp data-block-key=\"79afp\"\u003eAnd then merge the branches by element-wise multiplication, apply a final linear layer with output dimension (Model width) 2560.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/RecurrentGemma-Residual-block.original.png\" alt=\"RecurrentGemma-Residual-block\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"r6w58\"\u003eAfter applying RMSNorm, the MLP block follows.\u003c/p\u003e\u003ch4 data-block-key=\"7dvgt\"\u003e\u003cb\u003e\u003cbr/\u003eResidual Block with a local MQA\u003c/b\u003e\u003c/h4\u003e\u003cp data-block-key=\"80b7f\"\u003eAfter having two residual blocks with a recurrent block (0-1), a residual block with a local MQA (2) follows. One of the key disadvantages of using global attention is that its computational complexity grows quadratically in the sequence length. To address this, RecurrentGemma uses a local sliding window attention. It allows each position to attend only to a fixed number of tokens in the past.\u003c/p\u003e\u003cp data-block-key=\"5nuga\"\u003eIn the local MQA block (Temporal mixing block), the model takes the input of dimension (Model width) 2560. It uses linear projections (\u003ci\u003eq_proj\u003c/i\u003e, \u003ci\u003ek_proj\u003c/i\u003e, \u003ci\u003ev_proj\u003c/i\u003e, \u003ci\u003eo_proj\u003c/i\u003e) to create query, key, value, and output representations. Note that \u003ci\u003eout_features\u003c/i\u003e for \u003ci\u003ek_proj\u003c/i\u003e and \u003ci\u003ev_proj\u003c/i\u003e is 256 as they share the same head with a size of 256, while \u003ci\u003eq_proj\u003c/i\u003e and \u003ci\u003eo_proj\u003c/i\u003e have 10 heads (256 x 10 = 2560) in parallel.\u003c/p\u003e\u003cp data-block-key=\"e54fg\"\u003eIt incorporates \u003ci\u003erotary_emb\u003c/i\u003e (RecurrentGemmaRotaryEmbedding) for rotary positional embeddings (RoPE) just like the base Gemma models.\u003c/p\u003e\u003cp data-block-key=\"16eut\"\u003eApplying RMSNorm and the MLP block is the same with the previous residual block.\u003c/p\u003e\u003ch2 data-block-key=\"e68p4\"\u003e\u003cbr/\u003e\u003cb\u003eWhat’s Next?\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"1k5nh\"\u003eIn this article, you learned about RecurrentGemma.\u003c/p\u003e\u003cp data-block-key=\"74p63\"\u003eIn the next post, you will explore PaliGemma which is a lightweight open vision-language model (VLM).\u003c/p\u003e\u003cp data-block-key=\"8fevt\"\u003eStay tuned and thank you for reading!\u003c/p\u003e\u003chr/\u003e\u003ch2 data-block-key=\"emo75\"\u003eReferences\u003c/h2\u003e\u003ch3 data-block-key=\"bm035\"\u003e\u003cb\u003ePapers\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"d8o80\"\u003e\u003ca href=\"https://arxiv.org/abs/2402.19427\"\u003eGriffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models\u003c/a\u003e\u003cbr/\u003e\u003c/li\u003e\u003cli data-block-key=\"6in86\"\u003e\u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf\"\u003eRecurrentGemma: Moving Past Transformers for Efficient Open Language Models\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"cvjum\"\u003e\u003cbr/\u003e\u003cb\u003eCode Examples\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"chfum\"\u003e\u003ca href=\"https://colab.sandbox.google.com/github/google-deepmind/recurrentgemma/blob/main/colabs/sampling_tutorial_jax.ipynb\"\u003eGetting started with RecurrentGemma\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"b8s4o\"\u003e\u003cbr/\u003e📋 The complete Gemma architecture series\u003c/h2\u003e\u003cul\u003e\u003cli data-block-key=\"9cl0s\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures\"\u003eGemma explained: An overview of Gemma model family architectures\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"2mm17\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-new-in-gemma-2\"\u003eGemma explained: What’s new in Gemma 2\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"ld9d\"\u003eGemma explained: RecurrentGemma architecture\u003c/li\u003e\u003cli data-block-key=\"d2jnh\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/\"\u003eGemma explained: PaliGemma architecture\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2024-08-29T00:00:00Z",
  "modifiedTime": null
}
