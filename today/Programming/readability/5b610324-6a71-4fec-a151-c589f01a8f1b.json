{
  "id": "5b610324-6a71-4fec-a151-c589f01a8f1b",
  "title": "Noisy Neighbor Detection with eBPF",
  "link": "https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd?source=rss----2615bd06b42e---4",
  "description": "",
  "author": "Netflix Technology Blog",
  "published": "Tue, 10 Sep 2024 18:00:21 GMT",
  "source": "https://netflixtechblog.com/feed",
  "categories": [
    "observability",
    "linux",
    "ebpf",
    "performance",
    "containers"
  ],
  "byline": "Netflix Technology Blog",
  "length": 13335,
  "excerpt": "The Compute and Performance Engineering teams at Netflix regularly investigate performance issues in our multi-tenant environment. The first step is determining whether the problem originates from…",
  "siteName": "Netflix TechBlog",
  "favicon": "https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png",
  "text": "By Jose Fernandez, Sebastien Dabdoub, Jason Koch, Artem TkachukThe Compute and Performance Engineering teams at Netflix regularly investigate performance issues in our multi-tenant environment. The first step is determining whether the problem originates from the application or the underlying infrastructure. One issue that often complicates this process is the \"noisy neighbor\" problem. On Titus, our multi-tenant compute platform, a \"noisy neighbor\" refers to a container or system service that heavily utilizes the server's resources, causing performance degradation in adjacent containers. We usually focus on CPU utilization because it is our workloads’ most frequent source of noisy neighbor issues.Detecting the effects of noisy neighbors is complex. Traditional performance analysis tools such as perf can introduce significant overhead, risking further performance degradation. Additionally, these tools are typically deployed after the fact, which is too late for effective investigation. Another challenge is that debugging noisy neighbor issues requires significant low-level expertise and specialized tooling. In this blog post, we'll reveal how we leveraged eBPF to achieve continuous, low-overhead instrumentation of the Linux scheduler, enabling effective self-serve monitoring of noisy neighbor issues. You’ll learn how Linux kernel instrumentation can improve your infrastructure observability with deeper insights and enhanced monitoring.Continuous Instrumentation of the Linux SchedulerTo ensure the reliability of our workloads that depend on low latency responses, we instrumented the run queue latency for each container, which measures the time processes spend in the scheduling queue before being dispatched to the CPU. Extended waiting in this queue can be a telltale of performance issues, especially when containers are not utilizing their total CPU allocation. Continuous instrumentation is critical to catching such matters as they emerge, and eBPF, with its hooks into the Linux scheduler with minimal overhead, enabled us to monitor run queue latency efficiently.To emit a run queue latency metric, we leveraged three eBPF hooks: sched_wakeup, sched_wakeup_new, and sched_switch.Diagram of how run queue latency is measured and instrumentedThe sched_wakeup and sched_wakeup_new hooks are invoked when a process changes state from 'sleeping' to 'runnable.' They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.struct { __uint(type, BPF_MAP_TYPE_HASH); __uint(max_entries, MAX_TASK_ENTRIES); __uint(key_size, sizeof(u32)); __uint(value_size, sizeof(u64));} runq_enqueued SEC(\".maps\");SEC(\"tp_btf/sched_wakeup\")int tp_sched_wakeup(u64 *ctx){ struct task_struct *task = (void *)ctx[0]; u32 pid = task-\u003epid; u64 ts = bpf_ktime_get_ns(); bpf_map_update_elem(\u0026runq_enqueued, \u0026pid, \u0026ts, BPF_NOEXIST); return 0;}Conversely, the sched_switch hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task's process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.SEC(\"tp_btf/sched_switch\")int tp_sched_switch(u64 *ctx){ struct task_struct *prev = (struct task_struct *)ctx[1]; struct task_struct *next = (struct task_struct *)ctx[2]; u32 prev_pid = prev-\u003epid; u32 next_pid = next-\u003epid; // fetch timestamp of when the next task was enqueued u64 *tsp = bpf_map_lookup_elem(\u0026runq_enqueued, \u0026next_pid); if (tsp == NULL) { return 0; // missed enqueue } // calculate runq latency before deleting the stored timestamp u64 now = bpf_ktime_get_ns(); u64 runq_lat = now - *tsp; // delete pid from enqueued map bpf_map_delete_elem(\u0026runq_enqueued, \u0026next_pid); ....One of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process's cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an RCU (Read Copy Update) lock.To safely access this RCU-protected information, we can leverage kfuncs in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.void bpf_rcu_read_lock(void) __ksym;void bpf_rcu_read_unlock(void) __ksym;u64 get_task_cgroup_id(struct task_struct *task){ struct css_set *cgroups; u64 cgroup_id; bpf_rcu_read_lock(); cgroups = task-\u003ecgroups; cgroup_id = cgroups-\u003edfl_cgrp-\u003ekn-\u003eid; bpf_rcu_read_unlock(); return cgroup_id;}Once the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF ring buffer. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.struct { __uint(type, BPF_MAP_TYPE_RINGBUF); __uint(max_entries, RINGBUF_SIZE_BYTES);} events SEC(\".maps\");struct { __uint(type, BPF_MAP_TYPE_PERCPU_HASH); __uint(max_entries, MAX_TASK_ENTRIES); __uint(key_size, sizeof(u64)); __uint(value_size, sizeof(u64));} cgroup_id_to_last_event_ts SEC(\".maps\");struct runq_event { u64 prev_cgroup_id; u64 cgroup_id; u64 runq_lat; u64 ts;};SEC(\"tp_btf/sched_switch\")int tp_sched_switch(u64 *ctx){ // .... // The previous code // .... u64 prev_cgroup_id = get_task_cgroup_id(prev); u64 cgroup_id = get_task_cgroup_id(next); // per-cgroup-id-per-CPU rate-limiting // to balance observability with performance overhead u64 *last_ts = bpf_map_lookup_elem(\u0026cgroup_id_to_last_event_ts, \u0026cgroup_id); u64 last_ts_val = last_ts == NULL ? 0 : *last_ts; // check the rate limit for the cgroup_id in consideration // before doing more work if (now - last_ts_val \u003c RATE_LIMIT_NS) { // Rate limit exceeded, drop the event return 0; } struct runq_event *event; event = bpf_ringbuf_reserve(\u0026events, sizeof(*event), 0); if (event) { event-\u003eprev_cgroup_id = prev_cgroup_id; event-\u003ecgroup_id = cgroup_id; event-\u003erunq_lat = runq_lat; event-\u003ets = now; bpf_ringbuf_submit(event, 0); // Update the last event timestamp for the current cgroup_id bpf_map_update_elem(\u0026cgroup_id_to_last_event_ts, \u0026cgroup_id, \u0026now, BPF_ANY); } return 0;}Our userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, Atlas. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric (runq.latency) for that container. We also increment a counter metric (sched.switch.out) to monitor preemptions occurring for the container's processes. Access to the prev_cgroup_id of the preempted process allows us to tag the metric with the cause of the preemption, whether it's due to a process within the same container (or cgroup), a process in another container, or a system service.It's important to highlight that both the runq.latency metric and the sched.switch.out metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve — relying solely on the runq.latency metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it's actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.A Noisy Neighbor StoryBelow is the runq.latency metric for a server running a single container with ample CPU capacity. The 99th percentile averages 83.4µs (microseconds), serving as our baseline. Although there are some spikes reaching 400µs, the latency remains within acceptable parameters.container1’s 99th percentile runq.latency averages 83µs (microseconds), with spikes up to 400µs, without adjacent containers. This serves as our baseline for a container not contending for CPU on a host.At 10:35, launching container2, which fully utilized all CPUs on the host, caused a significant 131-millisecond spike (131,000 microseconds) in container1's P99 run queue latency. This spike would be noticeable in the userspace application if it were serving HTTP traffic. If userspace app owners reported an unexplained latency spike, we could quickly identify the noisy neighbor issue through run queue latency metrics.Launching container2 at 10:35, which maxes out all CPUs on the host, caused a 131-millisecond spike in container1’s P99 run queue latency due to increased preemptions by system processes. This indicates a noisy neighbor issue, where system services compete for CPU time with containers.The sched.switch.out metric indicates that the spike was due to increased preemptions by system processes, highlighting a noisy neighbor issue where system services compete with containers for CPU time. Our metrics show that the noisy neighbors were actually system processes, likely triggered by container2 consuming all available CPU capacity.Optimizing eBPF CodeWe developed an open-source eBPF process monitoring tool called bpftop to measure the overhead of eBPF code in this kernel hot path. Our profiling with bpftop shows that the instrumentation adds less than 600 nanoseconds to each sched_* hook. We conducted a performance analysis on a Java service running in a container, and the instrumentation did not introduce significant overhead. The performance variance with the run queue profiling code active versus inactive was not measurable in milliseconds.During our research on how eBPF statistics are measured in the kernel, we identified an opportunity to improve the calculation. We submitted this patch, which was included in the Linux kernel 6.10 release.Through trial and error and using bpftop, we identified several optimizations that helped maintain low overhead for our eBPF code:We found that BPF_MAP_TYPE_HASH was the most performant for storing enqueued timestamps. Using BPF_MAP_TYPE_TASK_STORAGE resulted in nearly a twofold performance decline. BPF_MAP_TYPE_PERCPU_HASH was slightly less performant than BPF_MAP_TYPE_HASH, which was unexpected and requires further investigation.BPF_MAP_TYPE_LRU_HASH maps are 40–50 nanoseconds slower per operation than regular hash maps. Due to space concerns from PID churn, we initially used them for enqueued timestamps. Ultimately, we settled on BPF_MAP_TYPE_HASH with an increased size to mitigate this risk.The BPF_CORE_READ helper adds 20–30 nanoseconds per invocation. In the case of raw tracepoints, specifically those that are \"BTF-enabled\" (tp_btf/*), it is safe and more efficient to access the task struct members directly. Andrii Nakryiko recommends this approach in this blog post.The sched_switch, sched_wakeup, and sched_wakeup_new are all triggered for kernel tasks, which are identifiable by their PID of 0. We found monitoring these tasks unnecessary, so we implemented several early exit conditions and conditional logic to prevent executing costly operations, such as accessing BPF maps, when dealing with a kernel task. Notably, kernel tasks operate through the scheduler queue like any regular process.ConclusionOur findings highlight the value of low-overhead continuous instrumentation of the Linux kernel with eBPF. We have integrated these metrics into customer dashboards, enabling actionable insights and guiding multitenancy performance discussions. We can also now use these metrics to refine CPU isolation strategies to minimize the impact of noisy neighbors. Additionally, thanks to these metrics, we've gained deeper insights into the Linux scheduler.This work has also deepened our understanding of eBPF technology and underscored the importance of tools like bpftop for optimizing eBPF code. As eBPF adoption increases, we foresee more infrastructure observability and business logic shifting to it. One promising project in this space is sched_ext, which has the potential to revolutionize how scheduling decisions are made and tailored to specific workload needs.",
  "image": "https://miro.medium.com/v2/resize:fill:1200:632/g:fp:0.48:0.5/1*6bapyclfXZPsUIaXFM-xaQ.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003ca href=\"https://netflixtechblog.medium.com/?source=post_page-----64b1f4b3bbdd--------------------------------\" rel=\"noopener follow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix Technology Blog\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg\" width=\"44\" height=\"44\" loading=\"lazy\" data-testid=\"authorPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003ca href=\"https://netflixtechblog.com/?source=post_page-----64b1f4b3bbdd--------------------------------\" rel=\"noopener  ugc nofollow\"\u003e\u003cdiv aria-hidden=\"false\"\u003e\u003cp\u003e\u003cimg alt=\"Netflix TechBlog\" src=\"https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png\" width=\"24\" height=\"24\" loading=\"lazy\" data-testid=\"publicationPhoto\"/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/a\u003e\u003c/div\u003e\u003cp id=\"3108\"\u003e\u003cem\u003eBy \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/josefernandezmn/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eJose Fernandez\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/sebastien-dabdoub-2a5a0958/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eSebastien Dabdoub\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/jason-koch-5692172/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eJason Koch\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, \u003c/em\u003e\u003ca href=\"https://www.linkedin.com/in/artemtkachuk/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem\u003eArtem Tkachuk\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\u003cp id=\"8709\"\u003eThe Compute and Performance Engineering teams at Netflix regularly investigate performance issues in our multi-tenant environment. The first step is determining whether the problem originates from the application or the underlying infrastructure. One issue that often complicates this process is the \u0026#34;noisy neighbor\u0026#34; problem. On \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436\"\u003eTitus\u003c/a\u003e, our multi-tenant compute platform, a \u0026#34;noisy neighbor\u0026#34; refers to a container or system service that heavily utilizes the server\u0026#39;s resources, causing performance degradation in adjacent containers. We usually focus on CPU utilization because it is our workloads’ most frequent source of noisy neighbor issues.\u003c/p\u003e\u003cp id=\"f191\"\u003eDetecting the effects of noisy neighbors is complex. Traditional performance analysis tools such as \u003ca href=\"https://www.brendangregg.com/perf.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eperf\u003c/a\u003e can introduce significant overhead, risking further performance degradation. Additionally, these tools are typically deployed after the fact, which is too late for effective investigation.\u003cem\u003e \u003c/em\u003eAnother challenge is that debugging noisy neighbor issues requires significant low-level expertise and specialized tooling\u003cem\u003e. \u003c/em\u003eIn this blog post, we\u0026#39;ll reveal how we leveraged \u003ca href=\"https://ebpf.io/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eeBPF\u003c/a\u003e to achieve continuous, low-overhead instrumentation of the Linux scheduler, enabling effective self-serve monitoring of noisy neighbor issues. You’ll learn how Linux kernel instrumentation can improve your infrastructure observability with deeper insights and enhanced monitoring.\u003c/p\u003e\u003ch2 id=\"1e46\"\u003eContinuous Instrumentation of the Linux Scheduler\u003c/h2\u003e\u003cp id=\"792f\"\u003eTo ensure the reliability of our workloads that depend on low latency responses, we instrumented the \u003ca href=\"https://en.wikipedia.org/wiki/Run_queue\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003erun queue\u003c/a\u003e latency for each container, which measures the time processes spend in the scheduling queue before being dispatched to the CPU. Extended waiting in this queue can be a telltale of performance issues, especially when containers are not utilizing their total CPU allocation. Continuous instrumentation is critical to catching such matters as they emerge, and eBPF, with its hooks into the Linux scheduler with minimal overhead, enabled us to monitor run queue latency efficiently.\u003c/p\u003e\u003cp id=\"048b\"\u003eTo emit a run queue latency metric, we leveraged three eBPF hooks: \u003ccode\u003esched_wakeup\u003c/code\u003e\u003cstrong\u003e, \u003c/strong\u003e\u003ccode\u003esched_wakeup_new\u003c/code\u003e\u003cstrong\u003e,\u003c/strong\u003e and \u003ccode\u003esched_switch\u003c/code\u003e.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eDiagram of how run queue latency is measured and instrumented\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"184b\"\u003eThe \u003ccode\u003esched_wakeup\u003c/code\u003e\u003cstrong\u003e \u003c/strong\u003eand \u003ccode\u003esched_wakeup_new\u003c/code\u003e hooks are invoked when a process changes state from \u0026#39;sleeping\u0026#39; to \u0026#39;runnable.\u0026#39; They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"07d3\"\u003estruct {\u003cbr/\u003e    __uint(type, BPF_MAP_TYPE_HASH);\u003cbr/\u003e    __uint(max_entries, MAX_TASK_ENTRIES);\u003cbr/\u003e    __uint(key_size, sizeof(u32));\u003cbr/\u003e    __uint(value_size, sizeof(u64));\u003cbr/\u003e} runq_enqueued SEC(\u0026#34;.maps\u0026#34;);\u003cp\u003eSEC(\u0026#34;tp_btf/sched_wakeup\u0026#34;)\u003cbr/\u003eint tp_sched_wakeup(u64 *ctx)\u003cbr/\u003e{\u003cbr/\u003e    struct task_struct *task = (void *)ctx[0];\u003cbr/\u003e    u32 pid = task-\u0026gt;pid;\u003cbr/\u003e    u64 ts = bpf_ktime_get_ns();\u003c/p\u003e\u003cp\u003e    bpf_map_update_elem(\u0026amp;runq_enqueued, \u0026amp;pid, \u0026amp;ts, BPF_NOEXIST);\u003cbr/\u003e    return 0;\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"5852\"\u003eConversely, the \u003ccode\u003esched_switch\u003c/code\u003e hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task\u0026#39;s process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"012a\"\u003eSEC(\u0026#34;tp_btf/sched_switch\u0026#34;)\u003cbr/\u003eint tp_sched_switch(u64 *ctx)\u003cbr/\u003e{\u003cbr/\u003e    struct task_struct *prev = (struct task_struct *)ctx[1];\u003cbr/\u003e    struct task_struct *next = (struct task_struct *)ctx[2];\u003cbr/\u003e    u32 prev_pid = prev-\u0026gt;pid;\u003cbr/\u003e    u32 next_pid = next-\u0026gt;pid;\u003cp\u003e     // fetch timestamp of when the next task was enqueued\u003cbr/\u003e    u64 *tsp = bpf_map_lookup_elem(\u0026amp;runq_enqueued, \u0026amp;next_pid);\u003cbr/\u003e    if (tsp == NULL) {\u003cbr/\u003e        return 0; // missed enqueue\u003cbr/\u003e    }\u003c/p\u003e\u003cp\u003e    // calculate runq latency before deleting the stored timestamp\u003cbr/\u003e    u64 now = bpf_ktime_get_ns();\u003cbr/\u003e    u64 runq_lat = now - *tsp;\u003c/p\u003e\u003cp\u003e    // delete pid from enqueued map\u003cbr/\u003e    bpf_map_delete_elem(\u0026amp;runq_enqueued, \u0026amp;next_pid);\u003cbr/\u003e    ....\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"58f0\"\u003eOne of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process\u0026#39;s cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an\u003ca href=\"https://elixir.bootlin.com/linux/v6.6.16/source/include/linux/sched.h#L1225\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e RCU (Read Copy Update) lock\u003c/a\u003e.\u003c/p\u003e\u003cp id=\"0a15\"\u003eTo safely access this RCU-protected information, we can leverage \u003ca href=\"https://docs.kernel.org/bpf/kfuncs.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ekfuncs\u003c/a\u003e in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"769f\"\u003evoid bpf_rcu_read_lock(void) __ksym;\u003cbr/\u003evoid bpf_rcu_read_unlock(void) __ksym;\u003cp\u003eu64 get_task_cgroup_id(struct task_struct *task)\u003cbr/\u003e{\u003cbr/\u003e    struct css_set *cgroups;\u003cbr/\u003e    u64 cgroup_id;\u003cbr/\u003e    bpf_rcu_read_lock();\u003cbr/\u003e    cgroups = task-\u0026gt;cgroups;\u003cbr/\u003e    cgroup_id = cgroups-\u0026gt;dfl_cgrp-\u0026gt;kn-\u0026gt;id;\u003cbr/\u003e    bpf_rcu_read_unlock();\u003cbr/\u003e    return cgroup_id;\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"ba4a\"\u003eOnce the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF \u003ca href=\"https://nakryiko.com/posts/bpf-ringbuf/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ering buffer\u003c/a\u003e. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5a42\"\u003estruct {\u003cbr/\u003e    __uint(type, BPF_MAP_TYPE_RINGBUF);\u003cbr/\u003e    __uint(max_entries, RINGBUF_SIZE_BYTES);\u003cbr/\u003e} events SEC(\u0026#34;.maps\u0026#34;);\u003cp\u003estruct {\u003cbr/\u003e    __uint(type, BPF_MAP_TYPE_PERCPU_HASH);\u003cbr/\u003e    __uint(max_entries, MAX_TASK_ENTRIES);\u003cbr/\u003e    __uint(key_size, sizeof(u64));\u003cbr/\u003e    __uint(value_size, sizeof(u64));\u003cbr/\u003e} cgroup_id_to_last_event_ts SEC(\u0026#34;.maps\u0026#34;);\u003c/p\u003e\u003cp\u003estruct runq_event {\u003cbr/\u003e    u64 prev_cgroup_id;\u003cbr/\u003e    u64 cgroup_id;\u003cbr/\u003e    u64 runq_lat;\u003cbr/\u003e    u64 ts;\u003cbr/\u003e};\u003c/p\u003e\u003cp\u003eSEC(\u0026#34;tp_btf/sched_switch\u0026#34;)\u003cbr/\u003eint tp_sched_switch(u64 *ctx)\u003cbr/\u003e{\u003cbr/\u003e    // ....\u003cbr/\u003e    // The previous code\u003cbr/\u003e    // ....\u003c/p\u003e\u003cp\u003e     u64 prev_cgroup_id = get_task_cgroup_id(prev);\u003cbr/\u003e    u64 cgroup_id = get_task_cgroup_id(next);\u003c/p\u003e\u003cp\u003e     // per-cgroup-id-per-CPU rate-limiting \u003cbr/\u003e    // to balance observability with performance overhead\u003cbr/\u003e    u64 *last_ts = \u003cbr/\u003e        bpf_map_lookup_elem(\u0026amp;cgroup_id_to_last_event_ts, \u0026amp;cgroup_id);\u003cbr/\u003e    u64 last_ts_val = last_ts == NULL ? 0 : *last_ts;\u003c/p\u003e\u003cp\u003e    // check the rate limit for the cgroup_id in consideration\u003cbr/\u003e    // before doing more work\u003cbr/\u003e    if (now - last_ts_val \u0026lt; RATE_LIMIT_NS) {\u003cbr/\u003e        // Rate limit exceeded, drop the event\u003cbr/\u003e        return 0;\u003cbr/\u003e    }\u003c/p\u003e\u003cp\u003e    struct runq_event *event;\u003cbr/\u003e    event = bpf_ringbuf_reserve(\u0026amp;events, sizeof(*event), 0);\u003c/p\u003e\u003cp\u003e      if (event) {\u003cbr/\u003e        event-\u0026gt;prev_cgroup_id = prev_cgroup_id;\u003cbr/\u003e        event-\u0026gt;cgroup_id = cgroup_id;\u003cbr/\u003e        event-\u0026gt;runq_lat = runq_lat;\u003cbr/\u003e        event-\u0026gt;ts = now;\u003cbr/\u003e        bpf_ringbuf_submit(event, 0);\u003cbr/\u003e        // Update the last event timestamp for the current cgroup_id\u003cbr/\u003e        bpf_map_update_elem(\u0026amp;cgroup_id_to_last_event_ts, \u0026amp;cgroup_id,\u003cbr/\u003e            \u0026amp;now, BPF_ANY);\u003c/p\u003e\u003cp\u003e    }\u003c/p\u003e\u003cp\u003e    return 0;\u003cbr/\u003e}\u003c/p\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp id=\"cd03\"\u003eOur userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a\"\u003eAtlas\u003c/a\u003e. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric (\u003ccode\u003erunq.latency\u003c/code\u003e) for that container. We also increment a counter metric (\u003ccode\u003esched.switch.out\u003c/code\u003e) to monitor preemptions occurring for the container\u0026#39;s processes. Access to the \u003ccode\u003eprev_cgroup_id\u003c/code\u003e of the preempted process allows us to tag the metric with the cause of the preemption, whether it\u0026#39;s due to a process within the same container (or cgroup), a process in another container, or a system service.\u003c/p\u003e\u003cp id=\"1bb4\"\u003eIt\u0026#39;s important to highlight that both the \u003ccode\u003erunq.latency\u003c/code\u003e metric and the \u003ccode\u003esched.switch.out\u003c/code\u003e metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve — relying solely on the \u003ccode\u003erunq.latency \u003c/code\u003emetric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it\u0026#39;s actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.\u003c/p\u003e\u003ch2 id=\"b5da\"\u003eA Noisy Neighbor Story\u003c/h2\u003e\u003cp id=\"8be9\"\u003eBelow is the \u003ccode\u003erunq.latency\u003c/code\u003e metric for a server running a single container with ample CPU capacity. The 99th percentile averages 83.4µs (microseconds), serving as our baseline. Although there are some spikes reaching 400µs, the latency remains within acceptable parameters.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003econtainer1’s 99th percentile runq.latency averages 83µs (microseconds), with spikes up to 400µs, without adjacent containers. This serves as our baseline for a container not contending for CPU on a host.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"d6c6\"\u003eAt 10:35, launching \u003ccode\u003econtainer2\u003c/code\u003e, which fully utilized all CPUs on the host, caused a significant 131-millisecond spike (131,000 microseconds) in \u003ccode\u003econtainer1\u003c/code\u003e\u0026#39;s P99 run queue latency. This spike would be noticeable in the userspace application if it were serving HTTP traffic. If userspace app owners reported an unexplained latency spike, we could quickly identify the noisy neighbor issue through run queue latency metrics.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cfigure\u003e\u003cfigcaption\u003eLaunching container2 at 10:35, which maxes out all CPUs on the host, \u003cstrong\u003ecaused a 131-millisecond spike in container1’s P99 run queue latency\u003c/strong\u003e due to increased preemptions by system processes. This indicates a noisy neighbor issue, where system services compete for CPU time with containers.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"1ce4\"\u003eThe \u003ccode\u003esched.switch.out\u003c/code\u003e metric indicates that the spike was due to increased preemptions by system processes, highlighting a noisy neighbor issue where system services compete with containers for CPU time. Our metrics show that the noisy neighbors were actually system processes, likely triggered by \u003ccode\u003econtainer2\u003c/code\u003e consuming all available CPU capacity.\u003c/p\u003e\u003ch2 id=\"37af\"\u003eOptimizing eBPF Code\u003c/h2\u003e\u003cp id=\"5964\"\u003eWe developed an open-source eBPF process monitoring tool called \u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5\"\u003ebpftop\u003c/a\u003e to measure the overhead of eBPF code in this kernel hot path. Our profiling with \u003ccode\u003ebpftop\u003c/code\u003e shows that the instrumentation adds less than 600 nanoseconds to each \u003ccode\u003esched_*\u003c/code\u003e hook. We conducted a performance analysis on a Java service running in a container, and the instrumentation did not introduce significant overhead. The performance variance with the run queue profiling code active versus inactive was not measurable in milliseconds.\u003c/p\u003e\u003cp id=\"2b6f\"\u003eDuring our research on how eBPF statistics are measured in the kernel, we identified an opportunity to improve the calculation. We submitted this \u003ca href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ce09cbdd988887662546a1175bcfdfc6c8fdd150\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003epatch\u003c/a\u003e, which was included in the Linux kernel 6.10 release.\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp id=\"3e54\"\u003eThrough trial and error and using \u003ccode\u003ebpftop\u003c/code\u003e, we identified several optimizations that helped maintain low overhead for our eBPF code:\u003c/p\u003e\u003cul\u003e\u003cli id=\"bdd6\"\u003eWe found that \u003ccode\u003eBPF_MAP_TYPE_HASH\u003c/code\u003e was the most performant for storing enqueued timestamps. Using \u003ccode\u003eBPF_MAP_TYPE_TASK_STORAGE\u003c/code\u003e resulted in nearly a twofold performance decline. \u003ccode\u003eBPF_MAP_TYPE_PERCPU_HASH\u003c/code\u003e was slightly less performant than \u003ccode\u003eBPF_MAP_TYPE_HASH\u003c/code\u003e, which was unexpected and requires further investigation.\u003c/li\u003e\u003cli id=\"6517\"\u003e\u003ccode\u003eBPF_MAP_TYPE_LRU_HASH\u003c/code\u003e maps are 40–50 nanoseconds slower per operation than regular hash maps. Due to space concerns from PID churn, we initially used them for enqueued timestamps. Ultimately, we settled on \u003ccode\u003eBPF_MAP_TYPE_HASH\u003c/code\u003e with an increased size to mitigate this risk.\u003c/li\u003e\u003cli id=\"0f38\"\u003eThe \u003ccode\u003eBPF_CORE_READ\u003c/code\u003e helper adds 20–30 nanoseconds per invocation. In the case of raw tracepoints, specifically those that are \u0026#34;BTF-enabled\u0026#34; (\u003ccode\u003etp_btf/*\u003c/code\u003e), it is safe and more efficient to access the task struct members directly. Andrii Nakryiko recommends this approach in this \u003ca href=\"https://nakryiko.com/posts/bpf-core-reference-guide/#btf-enabled-bpf-program-types-with-direct-memory-reads\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eblog post\u003c/a\u003e.\u003c/li\u003e\u003cli id=\"c6da\"\u003eThe \u003ccode\u003esched_switch\u003c/code\u003e, \u003ccode\u003esched_wakeup\u003c/code\u003e, and \u003ccode\u003esched_wakeup_new\u003c/code\u003e are all triggered for kernel tasks, which are identifiable by their PID of 0. We found monitoring these tasks unnecessary, so we implemented several early exit conditions and conditional logic to prevent executing costly operations, such as accessing BPF maps, when dealing with a kernel task. Notably, kernel tasks operate through the scheduler queue like any regular process.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"d4d7\"\u003eConclusion\u003c/h2\u003e\u003cp id=\"6194\"\u003eOur findings highlight the value of low-overhead continuous instrumentation of the Linux kernel with eBPF. We have integrated these metrics into customer dashboards, enabling actionable insights and guiding multitenancy performance discussions. We can also now use these metrics to refine CPU isolation strategies to minimize the impact of noisy neighbors. Additionally, thanks to these metrics, we\u0026#39;ve gained deeper insights into the Linux scheduler.\u003c/p\u003e\u003cp id=\"4565\"\u003eThis work has also deepened our understanding of eBPF technology and underscored the importance of tools like \u003ccode\u003ebpftop\u003c/code\u003e for optimizing eBPF code. As eBPF adoption increases, we foresee more infrastructure observability and business logic shifting to it. One promising project in this space is \u003ca href=\"https://github.com/sched-ext/scx\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esched_ext\u003c/a\u003e, which has the potential to revolutionize how scheduling decisions are made and tailored to specific workload needs.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2024-09-10T17:57:48.745Z",
  "modifiedTime": null
}
