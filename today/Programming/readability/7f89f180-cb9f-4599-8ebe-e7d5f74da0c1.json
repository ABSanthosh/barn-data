{
  "id": "7f89f180-cb9f-4599-8ebe-e7d5f74da0c1",
  "title": "MiniMax Releases M1: A 456B Hybrid-Attention Model for Long-Context Reasoning and Software Tasks",
  "link": "https://www.infoq.com/news/2025/06/minimax-m1/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "MiniMax has introduced MiniMax-M1, a new open-weight reasoning model built to handle extended contexts and complex problem-solving with high efficiency. Built on top of the earlier MiniMax-Text-01, M1 features a hybrid Mixture-of-Experts (MoE) architecture and a novel “lightning attention” mechanism. By Robert Krzaczyński",
  "author": "Robert Krzaczyński",
  "published": "Tue, 24 Jun 2025 18:55:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Benchmark",
    "Large language models",
    "Hugging Face",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Robert Krzaczyński",
  "length": 2713,
  "excerpt": "MiniMax has introduced MiniMax-M1, a new open-weight reasoning model built to handle extended contexts and complex problem-solving with high efficiency. Built on top of the earlier MiniMax-Text-01, M1",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20250605075544/apple-touch-icon.png",
  "text": "MiniMax has introduced MiniMax-M1, an open-weight language model designed for long-context reasoning and tool use. Based on the earlier MiniMax-Text-01, M1 uses a hybrid Mixture-of-Experts (MoE) architecture and a new “lightning attention” mechanism. The model has a total capacity of 456 billion parameters, with 45.9 billion active per token, and supports context lengths of up to 1 million tokens. M1 distinguishes itself through its efficient use of compute and support for long-context reasoning. Its lightning attention mechanism reduces test-time computation, requiring only 25% of the FLOPs used by DeepSeek R1 for sequences of 100K tokens. The model was trained using large-scale reinforcement learning across a range of domains, including mathematical problem-solving and software engineering environments. Two versions of the model are available. The models are evaluated using a custom RL scaling approach. Notably, MiniMax introduces CISPO, a novel RL algorithm that clips importance sampling weights rather than token updates—reportedly improving stability and performance over traditional variants. Across benchmarks, MiniMax-M1-80K consistently ranks at or near the top among open-weight models, with strong results in: Long-context tasks (OpenAI-MRCR 128K: 73.4%, LongBench-v2: 61.5%) Software engineering (SWE-bench Verified: 56.0%) Tool use (TAU-bench airline: 62.0%, retail: 63.5%) Reasoning-heavy math benchmarks (AIME 2024: 86.0%) One Reddit user commented on its standout capabilities: This looks pretty great. Especially for function calling (Tau-bench) and long context, this seems like SOTA for open-weights. The latter by some big margin, which I don't even find unbelievable because their old non-reasoning model was also great for this. However, others pointed to limitations in practice. For example, dubesor86 shared: It's unusable, though. I had it play chess matches (usually takes a few minutes), and I had to have it run all night, and it still wasn't done by the time I woke up. All the scores in the world mean nothing if the usability is zero. MiniMax-M1 also supports structured function calling, making it suitable for agent frameworks. The model is available in two versions (40K and 80K) via HuggingFace. For deployment, the team recommends vLLM, offering optimized serving, memory management, and batching performance. Developers can also experiment via the MiniMax MCP Server, which bundles API access and capabilities such as video and image generation, speech synthesis, and voice cloning. About the Author Robert Krzaczyński",
  "image": "https://res.infoq.com/news/2025/06/minimax-m1/en/headerimage/generatedHeaderImage-1750790684754.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cdiv\u003e\u003cp\u003eMiniMax has introduced \u003ca href=\"https://huggingface.co/MiniMaxAI/MiniMax-M1-40k\"\u003eMiniMax-M1\u003c/a\u003e, an open-weight language model designed for long-context reasoning and tool use. Based on the earlier \u003ca href=\"https://huggingface.co/MiniMaxAI/MiniMax-Text-01\"\u003eMiniMax-Text-01\u003c/a\u003e, M1 uses a hybrid Mixture-of-Experts (MoE) architecture and a new “lightning attention” mechanism. The model has a total capacity of 456 billion parameters, with 45.9 billion active per token, and supports context lengths of up to 1 million tokens.\u003c/p\u003e\u003cp\u003e\n\nM1 distinguishes itself through its efficient use of compute and support for long-context reasoning. Its lightning attention mechanism reduces test-time computation, requiring only 25% of the \u003ca href=\"https://pl.wikipedia.org/wiki/FLOPS\"\u003eFLOPs\u003c/a\u003e used by \u003ca href=\"https://www.infoq.com/news/2025/02/deepseek-r1-release/\"\u003eDeepSeek R1\u003c/a\u003e for sequences of 100K tokens. The model was trained using large-scale reinforcement learning across a range of domains, including mathematical problem-solving and software engineering environments.\u003c/p\u003e\u003cp\u003e\n\nTwo versions of the model are available. The models are evaluated using a custom RL scaling approach. Notably, MiniMax introduces CISPO, a novel RL algorithm that clips importance sampling weights rather than token updates—reportedly improving stability and performance over traditional variants.\u003c/p\u003e\u003cp\u003e\n\nAcross benchmarks, MiniMax-M1-80K consistently ranks at or near the top among open-weight models, with strong results in:\u003c/p\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\t\u003cli\u003eLong-context tasks (OpenAI-MRCR 128K: 73.4%, LongBench-v2: 61.5%)\u003c/li\u003e\n\t\u003cli\u003eSoftware engineering (SWE-bench Verified: 56.0%)\u003c/li\u003e\n\t\u003cli\u003eTool use (TAU-bench airline: 62.0%, retail: 63.5%)\u003c/li\u003e\n\t\u003cli\u003eReasoning-heavy math benchmarks (AIME 2024: 86.0%)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOne Reddit user \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lcuglb/comment/my38g60/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003ecommented\u003c/a\u003e on its standout capabilities:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis looks pretty great. Especially for function calling (Tau-bench) and long context, this seems like SOTA for open-weights. The latter by some big margin, which I don\u0026#39;t even find unbelievable because their old non-reasoning model was also great for this.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eHowever, others pointed to limitations in practice. For example, dubesor86 \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lcuglb/comment/my38g60/?utm_source=share\u0026amp;utm_medium=web3x\u0026amp;utm_name=web3xcss\u0026amp;utm_term=1\u0026amp;utm_content=share_button\"\u003eshared\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eIt\u0026#39;s unusable, though. I had it play chess matches (usually takes a few minutes), and I had to have it run all night, and it still wasn\u0026#39;t done by the time I woke up. All the scores in the world mean nothing if the usability is zero.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eMiniMax-M1 also supports structured function calling, making it suitable for agent frameworks. The model is available in two versions (40K and 80K) via \u003ca href=\"https://huggingface.co/MiniMaxAI\"\u003eHuggingFace\u003c/a\u003e. For deployment, the team recommends \u003ca href=\"https://docs.vllm.ai/en/latest/\"\u003evLLM\u003c/a\u003e, offering optimized serving, memory management, and batching performance. Developers can also experiment via the \u003ca href=\"https://github.com/MiniMax-AI/MiniMax-MCP\"\u003eMiniMax MCP Server\u003c/a\u003e, which bundles API access and capabilities such as video and image generation, speech synthesis, and voice cloning.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Robert-Krzaczyński\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eRobert Krzaczyński\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": "2025-06-24T00:00:00Z",
  "modifiedTime": null
}
