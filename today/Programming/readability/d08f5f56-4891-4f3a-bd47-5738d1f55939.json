{
  "id": "d08f5f56-4891-4f3a-bd47-5738d1f55939",
  "title": "Enhance your prompts with Vertex AI Prompt Optimizer",
  "link": "https://developers.googleblog.com/en/enhance-your-prompts-with-vertex-ai-prompt-optimizer/",
  "description": "Vertex AI Prompt Optimizer, a new managed automated prompt engineering service, can help save time and effort in prompt engineering while ensuring performing prompts ready for your generative AI applications.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ivan Nardini, George Lee",
  "length": 13189,
  "excerpt": "Vertex AI Prompt Optimizer, a new managed automated prompt engineering service, can help save time and effort in prompt engineering while ensuring performing prompts ready for your generative AI applications.",
  "siteName": "",
  "favicon": "",
  "text": "George Lee Product Manager Cloud AI Research Prompt design and engineering stands out as one of the most approachable methods to drive meaningful output from a Large Language Model (LLM). ​​However, prompting large language models can feel like navigating a complex maze.Designing a prompt is a relatively new discipline with several techniques that need to be explored. To get an idea, check the prompt engineering guide. In addition, to obtain the best results from an LLM, you must experiment with various combinations of instructions and examples to achieve the desired output. Moreover, even if you find the ideal prompt template, there is no guarantee that the prompt will continue to achieve the task for a different LLM. As a result, you end up spending more time migrating or translating a prompt template from one model to another.To mitigate \"prompt fatigue\" one might experience while building LLM-based applications, we are announcing Vertex AI Prompt Optimizer in Public Preview. In this blog, you will learn how to get started with Vertex AI Prompt Optimizer using the Vertex AI SDK for Python. By the end of this article, you will have a better understanding of Vertex AI Prompt Optimizer and how it helps save you time and effort in prompt engineering while ensuring high-performing prompts ready for your GenAI applications.Vertex AI Prompt Optimizer: From research to productionVertex AI Prompt Optimizer is a prompt optimization service that helps users find the best prompt (instruction and demonstrations) for any preferred model on Vertex AI, where Instructions include the system instruction, context, and task of your prompt template and Demonstrations are the few-shot examples you provide in your prompt to elicit a specific style or tone from the model response. Vertex AI Prompt Optimizer is based on Google Research’s paper on automatic prompt optimization (APO) methods (accepted by NeurIPS 2024).Imagine that you want to solve this math problem as the one below. You need clear instructions and examples to help solve it. The instructions tell us the rules for solving the problems (e.g. how to handle negative numbers). The examples demonstrate how to apply the rules. That’s the idea behind Vertex AI Prompt Optimizer.To find best instructions and examples, Vertex AI Prompt Optimizer employs an iterative LLM-based optimization algorithm where the optimizer model and evaluator model work together to generate and evaluate candidate prompts and subsequently selects the best instructions and demonstrations based on the evaluation metrics the user wants to optimize against. Below you can see an illustration of how Vertex AI Prompt Optimizer works. Sorry, your browser doesn't support playback for this video With just a few labeled examples (input and ground truth output pair) and optimization set-up, Vertex AI Prompt Optimizer finds the best prompt (instruction and demonstrations) for the target model, significantly saving time and effort for users. Ultimately, the product streamlines the process of prompt design and prompt engineering and enhances overall quality of LLM-based applications. Users can now craft a new prompt for a particular task or translate a prompt from one model to another model on Vertex AI with ease.Now that you have a better understanding of how Vertex AI Prompt Optimizer works, let’s see how to enhance a prompt to use it with a Google model on Vertex AI.Get started with Vertex AI Prompt OptimizerImagine that you build a simple AI cooking assistant that provides suggestions on how to cook healthier dishes. For example, you ask “How do you create healthy desserts that are still delicious and satisfying, while minimizing added sugars and unhealthy fats?”. And the AI cooking assistant answers: “Here are some tips on how to achieve this balance in your recipe, minimizing added sugars and unhealthy fats: …”. Below you have an example of a generated answer. The initial version of the AI cooking assistant uses an LLM with the following simple prompt template:Given a question with some context, provide the correct answer to the question. \\nQuestion: {{question}}\\nContext:{{context}}\\nAnswer: {{target}}Based on the Q\u0026A evaluation dataset you collected and the Q\u0026A evaluation metrics calculated using Vertex AI GenAI Evaluation, the initial version of your AI cooking assistant can generate high-quality and contextually relevant answers. Here's a summary of the evaluation metrics report. Not bad. But there is room for improvement in the quality of generated answers with respect to associated questions. Let’s imagine that you want to use Gemini 1.5 Flash as more efficient LLMs for your assistant, but you don’t have previous experience with the Gemini model family to find a more performing prompt template to complete the task with Gemini 1.5 Flash. This is where Vertex AI Prompt Optimizer comes into play.To use Vertex AI Prompt Optimizer for enhancing your prompt template, you follow these steps:Prepare the prompt templateUpload labeled samples to a Cloud Storage bucketConfigure the optimization settingsRun the optimization jobGet the optimized prompt and evaluate the optimization1. Prepare the prompt templateTo start, prepare the prompt template you want to optimize. Vertex AI Prompt Optimizer expects a prompt with both the instruction template which is a fixed part of the prompt template shared across all queries for a given task and context and task template which is the dynamic part of the prompt template that changes based on the task. Below you can see the original template you prepare to use with Vertex AI Prompt Optimizer in a Q\u0026A task. INSTRUCTION_TEMPLATE = \"\"\" Given a question with some context, provide the correct answer to the question. \"\"\" CONTEXT_TASK_TEMPLATE = \"\"\" Question: {{question}} Answer: {{target}} \"\"\" 2. Upload labeled samples to a Cloud Storage bucketNext, Vertex AI Prompt optimizer requires a CSV or JSONL file containing labeled samples (input, ground truth output pairs) they are going to be used during the optimization process. In this use case, it is recommended to label examples from the source models that the target model struggles with. This would help identify areas of improvement. Below you can find an example of the labeled sample you upload to Google Cloud bucket. {\"target\":\"Here\\'s how to tackle those delicious red meats and pork while keeping things healthy:\\\\n\\\\n**Prioritize Low and Slow:**\\\\n\\\\n* **Braising and Stewing:** These techniques involve gently simmering meat in liquid over low heat for an extended period. This breaks down tough collagen, resulting in incredibly tender and flavorful meat. Plus, since the cooking temperature is lower, it minimizes the formation of potentially harmful compounds associated with high-heat cooking. \\\\n\\\\n* **Sous Vide:** This method involves sealing meat in a vacuum bag and immersing it in a precisely temperature-controlled water bath...\",\"question\":\"What are some techniques for cooking red meat and pork that maximize flavor and tenderness while minimizing the formation of unhealthy compounds? \\\\n\\\\nnContext:\\\\nRed meat and pork should be cooked to an internal temperature of 145\\\\u00b0F (63\\\\u00b0C) to ensure safety. \\\\nMarinating meat in acidic ingredients like lemon juice or vinegar can help tenderize it by breaking down tough muscle fibers. \\\\nHigh-heat cooking methods like grilling and pan-searing can create delicious browning and caramelization, but it\\'s important to avoid charring, which can produce harmful compounds. \\\\n\"} 3. Configure the optimization settingsTo run the prompt optimization job, Vertex AI prompt optimizer also requires configuring the optimization settings. Vertex AI Prompt Optimizer job runs as Vertex AI Training Custom Job. It supports any Google models supported by the Vertex LLM API and a wide range of evaluation metrics, computation based, LLM based or even the ones defined by the users. This is because Vertex AI Prompt Optimizer is integrated with Vertex Rapid Evaluation Service. In order to pass these configurations, Vertex AI Prompt Optimizer accepts either a list of arguments or the Google Cloud Bucket file path of a JSON configuration file. Here are some examples of basic configurations in Vertex AI Prompt Optimizer. params = { 'num_steps': OPTIMIZATION_STEPS, 'system_instruction': SYSTEM_INSTRUCTION, 'prompt_template': PROMPT_TEMPLATE, 'target_model': TARGET_MODEL, 'eval_metrics_types': EVALUATION_METRICS, 'optimization_mode': OPTIMIZATION_MODE, 'num_template_eval_per_step': OPTIMIZATION_PROMPT_PER_STEPS, 'num_demo_set_candidates': DEMO_OPTIMIZATION_STEPS, 'demo_set_size': DEMO_OPTIMIZATION_PROMPT_PER_STEPS, 'input_data_path': INPUT_DATA_FILE_URI, 'output_data_path': OUTPUT_DATA_FILE_URI, } Vertex AI Prompt Optimizer allows you to optimize prompts by optimizing instructions only, demonstration only, or both (optimization_mode), and after you set the system instruction, prompt templates that will be optimized (system_instruction, prompt_template), and the model you want to optimize for (target_model), it allows to condition the optimization process by setting evaluation metrics, number of iterations used to improve the prompt and more. Check out the documentation to know more about supported optimization parameters.Once you have both your samples and your configuration, you upload them on Google Cloud bucket as shown below. from etils import epath # upload configuration with epath.Path(CONFIG_FILE_URI).open('w') as config_file: json.dump(args, config_file) config_file.close() # upload prompt opt dataset prepared_prompt_df.to_json(INPUT_DATA_FILE_URI, orient=\"records\", lines=True) 4. Run the optimization jobAt this point, everything is ready to run your first Vertex AI Prompt optimizer job using the Vertex AI SDK for Python. WORKER_POOL_SPECS = [{ 'machine_spec': { 'machine_type': 'n1-standard-4', }, 'replica_count': 1, 'container_spec': { 'image_uri' : APD_CONTAINER_URI, 'args': [\"--config=\" + CONFIG_FILE_URI] }}] custom_job = aiplatform.CustomJob( display_name=PROMPT_OPTIMIZATION_JOB, worker_pool_specs=WORKER_POOL_SPECS, ) custom_job.run() Notice how the Vertex AI Prompt Optimizer runs as a Vertex AI Training Custom job using the Vertex AI Prompt Optimizer container. The fact that this service leverages both Vertex AI Training and Vertex AI GenAI Evaluation is a proof of how Vertex AI provides a platform to run GenAI, even those who come directly from research as in this case.After submitting the Vertex AI Prompt optimizer job, you can monitor it from the Vertex AI Training custom jobs view as shown here. 5. Get the optimized prompt and evaluate the optimizationAfter the optimization job successfully runs, you can find either optimized instructions or demonstrations or both as json files in the output Cloud Storage bucket. Thanks to some helper functions, you can get the following output indicating the optimization step when you get the best instruction according to the metrics you define. Same result you get for the optimized demonstrations.Finally, you can generate the new responses with the optimized output. Below you can see an example of a generated response using the optimized system instructions template. And if you use them to run a new round of evaluation with Vertex AI GenAI Evaluation, you might get an output like the one below where the optimized prompt overperforms the previous model with the previous prompt template respective to the evaluation metrics you selected. ConclusionPrompt engineering is one of the most important yet challenging steps of the process to operationalize LLM-based applications. To help craft your prompt template, Vertex AI Prompt Optimizer finds the best prompt (instruction and demonstrations) for any preferred model on Vertex AI.This article showed one example of how you can use Vertex AI Prompt Optimizer to enhance your prompt template for a Gemini model using the Vertex AI SDK for Python. You can also use Vertex AI Prompt Optimizer via the UI notebook here.In summary, Vertex AI Prompt Optimizer can save you time and effort in prompt engineering while ensuring you have high-performing prompts for your GenAI applications.Thanks for reading!What’s nextDo you want to learn more about Vertex AI Prompt Optimizer and how to use it? Check out the following resources:DocumentationOptimize prompts with Vertex AI Prompt OptimizerGithub samplesGetting started with Vertex AI Prompt Optimizer UIGetting started with Vertex AI Prompt Optimizer SDKGoogle Cloud blogAnnouncing Public Preview of Vertex AI Prompt Optimizer",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Vertex-AI-Social.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=George+Lee\"\u003eGeorge Lee\u003c/a\u003e\n            \n              \u003cspan\u003eProduct Manager\u003c/span\u003e\n            \n            \n              \u003cspan\u003eCloud AI Research\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"i1swl\"\u003ePrompt design and engineering stands out as one of the most approachable methods to drive meaningful output from a Large Language Model (LLM). ​​However, prompting large language models can feel like navigating a complex maze.\u003c/p\u003e\u003cp data-block-key=\"b5dh6\"\u003eDesigning a prompt is a relatively new discipline with several techniques that need to be explored. To get an idea, check the \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design\"\u003eprompt engineering guide\u003c/a\u003e. In addition, to obtain the best results from an LLM, you must experiment with various combinations of instructions and examples to achieve the desired output. Moreover, even if you find the ideal prompt template, there is no guarantee that the prompt will continue to achieve the task for a different LLM. As a result, you end up spending more time migrating or translating a prompt template from one model to another.\u003c/p\u003e\u003cp data-block-key=\"dogu1\"\u003eTo mitigate \u0026#34;prompt fatigue\u0026#34; one might experience while building LLM-based applications, we are announcing \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer\"\u003eVertex AI Prompt Optimizer\u003c/a\u003e in Public Preview. In this blog, you will learn how to get started with Vertex AI Prompt Optimizer using the Vertex AI SDK for Python. By the end of this article, you will have a better understanding of Vertex AI Prompt Optimizer and how it helps save you time and effort in prompt engineering while ensuring high-performing prompts ready for your GenAI applications.\u003c/p\u003e\u003ch2 data-block-key=\"28l6v\"\u003e\u003cbr/\u003eVertex AI Prompt Optimizer: From research to production\u003c/h2\u003e\u003cp data-block-key=\"8qjvc\"\u003eVertex AI Prompt Optimizer is a prompt optimization service that helps users find the best prompt (instruction and demonstrations) for any preferred model on Vertex AI, where \u003cb\u003eInstructions\u003c/b\u003e include the \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#system-instructions\"\u003esystem instruction\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#contextual-information\"\u003econtext\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#task\"\u003etask\u003c/a\u003e of your prompt template and \u003cb\u003eDemonstrations\u003c/b\u003e are the \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#few-shot-examples\"\u003efew-shot examples\u003c/a\u003e you provide in your prompt to elicit a specific style or tone from the model response. Vertex AI Prompt Optimizer is based on Google Research’s \u003ca href=\"https://arxiv.org/pdf/2406.15708\"\u003epaper\u003c/a\u003e on automatic prompt optimization (APO) methods (accepted by NeurIPS 2024).\u003c/p\u003e\u003cp data-block-key=\"3neh7\"\u003eImagine that you want to solve this math problem as the one below. You need clear instructions and examples to help solve it. The \u003ci\u003einstructions\u003c/i\u003e tell us the rules for solving the problems (e.g. how to handle negative numbers). The \u003ci\u003eexamples\u003c/i\u003e demonstrate how to apply the rules. That’s the idea behind Vertex AI Prompt Optimizer.\u003c/p\u003e\u003cp data-block-key=\"ednpk\"\u003eTo find best instructions and examples, Vertex AI Prompt Optimizer employs an iterative LLM-based optimization algorithm where the optimizer model and evaluator model work together to generate and evaluate candidate prompts and subsequently selects the best instructions and demonstrations based on the evaluation metrics the user wants to optimize against. Below you can see an illustration of how Vertex AI Prompt Optimizer works.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \n        \u003cvideo autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" poster=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/wagtailvideo-xr4ouyvb_thumb.jpg\"\u003e\n\u003csource src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/original_videos/Automated_prompt_design_dataflow.mp4\" type=\"video/mp4\"/\u003e\n\u003cp\u003eSorry, your browser doesn\u0026#39;t support playback for this video\u003c/p\u003e\n\n\u003c/video\u003e\n    \n    \n\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"i1swl\"\u003eWith just a few labeled examples (input and ground truth output pair) and optimization set-up, Vertex AI Prompt Optimizer finds the best prompt (instruction and demonstrations) for the target model, significantly saving time and effort for users. Ultimately, the product streamlines the process of prompt design and prompt engineering and enhances overall quality of LLM-based applications. Users can now craft a new prompt for a particular task or translate a prompt from one model to another model on Vertex AI with ease.\u003c/p\u003e\u003cp data-block-key=\"b5cis\"\u003eNow that you have a better understanding of how Vertex AI Prompt Optimizer works, let’s see how to enhance a prompt to use it with a Google model on Vertex AI.\u003c/p\u003e\u003ch2 data-block-key=\"3241c\"\u003e\u003cbr/\u003e\u003cb\u003eGet started with Vertex AI Prompt Optimizer\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"s023\"\u003eImagine that you build a simple AI cooking assistant that provides suggestions on how to cook healthier dishes. For example, you ask “How do you create healthy desserts that are still delicious and satisfying, while minimizing added sugars and unhealthy fats?”. And the AI cooking assistant answers: “Here are some tips on how to achieve this balance in your recipe, minimizing added sugars and unhealthy fats: …”. Below you have an example of a generated answer.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_BIxmPRj.original.png\" alt=\"Example of user question, context, prompt, answer\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"i1swl\"\u003eThe initial version of the AI cooking assistant uses an LLM with the following simple prompt template:\u003c/p\u003e\u003cp data-block-key=\"31pgj\"\u003e\u003ci\u003eGiven a question with some context, provide the correct answer to the question. \\nQuestion: {{question}}\\nContext:{{context}}\\nAnswer: {{target}}\u003c/i\u003e\u003c/p\u003e\u003cp data-block-key=\"bamfe\"\u003eBased on the Q\u0026amp;A evaluation dataset you collected and the Q\u0026amp;A evaluation metrics calculated using Vertex AI GenAI Evaluation, the initial version of your AI cooking assistant can generate high-quality and contextually relevant answers. Here\u0026#39;s a summary of the evaluation metrics report.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image7_DUjlhKK.original.png\" alt=\"Graph of evaluation metrics, showing question_answering_quality/mean is 4 times as high in mean value as groundedness/mean\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"i1swl\"\u003eNot bad. But there is room for improvement in the quality of generated answers with respect to associated questions. Let’s imagine that you want to use Gemini 1.5 Flash as more efficient LLMs for your assistant, but you don’t have previous experience with the Gemini model family to find a more performing prompt template to complete the task with Gemini 1.5 Flash. This is where Vertex AI Prompt Optimizer comes into play.\u003c/p\u003e\u003cp data-block-key=\"5fhhl\"\u003eTo use Vertex AI Prompt Optimizer for enhancing your prompt template, you follow these steps:\u003c/p\u003e\u003col\u003e\u003cli data-block-key=\"4o0mq\"\u003ePrepare the prompt template\u003c/li\u003e\u003cli data-block-key=\"9e4l3\"\u003eUpload labeled samples to a Cloud Storage bucket\u003c/li\u003e\u003cli data-block-key=\"d38d6\"\u003eConfigure the optimization settings\u003c/li\u003e\u003cli data-block-key=\"dlhhp\"\u003eRun the optimization job\u003c/li\u003e\u003cli data-block-key=\"3vk2q\"\u003eGet the optimized prompt and evaluate the optimization\u003c/li\u003e\u003c/ol\u003e\u003ch3 data-block-key=\"9m5i2\"\u003e\u003cbr/\u003e\u003cb\u003e1. Prepare the prompt template\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"fugi2\"\u003eTo start, prepare the prompt template you want to optimize. Vertex AI Prompt Optimizer expects a prompt with both the instruction template which is a fixed part of the prompt template shared across all queries for a given task and context and task template which is the dynamic part of the prompt template that changes based on the task. Below you can see the original template you prepare to use with Vertex AI Prompt Optimizer in a Q\u0026amp;A task.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eINSTRUCTION_TEMPLATE\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003cspan\u003eGiven a question with some context, provide the correct answer to the question.\u003c/span\u003e\n\u003cspan\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\n\u003cspan\u003eCONTEXT_TASK_TEMPLATE\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003cspan\u003eQuestion: {{question}}\u003c/span\u003e\n\u003cspan\u003eAnswer: {{target}}\u003c/span\u003e\n\u003cspan\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"i1swl\"\u003e\u003cb\u003e2. Upload labeled samples to a Cloud Storage bucket\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"1l6ik\"\u003eNext, Vertex AI Prompt optimizer requires a CSV or JSONL file containing labeled samples (input, ground truth output pairs) they are going to be used during the optimization process. In this use case, it is recommended to label examples from the source models that the target model struggles with. This would help identify areas of improvement. Below you can find an example of the labeled sample you upload to Google Cloud bucket.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e{\u0026#34;target\u0026#34;:\u0026#34;Here\\\u0026#39;s how to tackle those delicious red meats and pork while keeping things healthy:\\\\n\\\\n**Prioritize Low and Slow:**\\\\n\\\\n* **Braising and Stewing:** These techniques involve gently simmering meat in liquid over low heat for an extended period. This breaks down tough collagen, resulting in incredibly tender and flavorful meat. Plus, since the cooking temperature is lower, it minimizes the formation of potentially harmful compounds associated with high-heat cooking. \\\\n\\\\n* **Sous Vide:** This method involves sealing meat in a vacuum bag and immersing it in a precisely temperature-controlled water bath...\u0026#34;,\u0026#34;question\u0026#34;:\u0026#34;What are some techniques for cooking red meat and pork that maximize flavor and tenderness while minimizing the formation of unhealthy compounds? \\\\n\\\\nnContext:\\\\nRed meat and pork should be cooked to an internal temperature of 145\\\\u00b0F (63\\\\u00b0C) to ensure safety. \\\\nMarinating meat in acidic ingredients like lemon juice or vinegar can help tenderize it by breaking down tough muscle fibers. \\\\nHigh-heat cooking methods like grilling and pan-searing can create delicious browning and caramelization, but it\\\u0026#39;s important to avoid charring, which can produce harmful compounds. \\\\n\u0026#34;}\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"i1swl\"\u003e\u003cb\u003e3. Configure the optimization settings\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"haam\"\u003eTo run the prompt optimization job, Vertex AI prompt optimizer also requires configuring the optimization settings. Vertex AI Prompt Optimizer job runs as Vertex AI Training Custom Job. It supports any Google models supported by the Vertex LLM API and a wide range of evaluation metrics, computation based, LLM based or even the ones defined by the users. This is because Vertex AI Prompt Optimizer is integrated with Vertex Rapid Evaluation Service. In order to pass these configurations, Vertex AI Prompt Optimizer accepts either a list of arguments or the Google Cloud Bucket file path of a JSON configuration file. Here are some examples of basic configurations in Vertex AI Prompt Optimizer.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eparams\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;num_steps\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eOPTIMIZATION_STEPS\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;system_instruction\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eSYSTEM_INSTRUCTION\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;prompt_template\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003ePROMPT_TEMPLATE\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;target_model\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eTARGET_MODEL\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;eval_metrics_types\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eEVALUATION_METRICS\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;optimization_mode\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eOPTIMIZATION_MODE\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;num_template_eval_per_step\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eOPTIMIZATION_PROMPT_PER_STEPS\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;num_demo_set_candidates\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eDEMO_OPTIMIZATION_STEPS\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;demo_set_size\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eDEMO_OPTIMIZATION_PROMPT_PER_STEPS\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;input_data_path\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eINPUT_DATA_FILE_URI\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;output_data_path\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eOUTPUT_DATA_FILE_URI\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n\n\u003cspan\u003e}\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"i1swl\"\u003eVertex AI Prompt Optimizer allows you to optimize prompts by optimizing instructions only, demonstration only, or both (\u003ccode\u003eoptimization_mode\u003c/code\u003e), and after you set the system instruction, prompt templates that will be optimized (\u003ccode\u003esystem_instruction\u003c/code\u003e, \u003ccode\u003eprompt_template\u003c/code\u003e), and the model you want to optimize for (\u003ccode\u003etarget_model\u003c/code\u003e), it allows to condition the optimization process by setting evaluation metrics, number of iterations used to improve the prompt and more. Check out the \u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer\"\u003edocumentation\u003c/a\u003e to know more about supported optimization parameters.\u003c/p\u003e\u003cp data-block-key=\"bfmq6\"\u003eOnce you have both your samples and your configuration, you upload them on Google Cloud bucket as shown below.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003eetils\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eepath\u003c/span\u003e\n\n\u003cspan\u003e# upload configuration\u003c/span\u003e\n\u003cspan\u003ewith\u003c/span\u003e \u003cspan\u003eepath\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003ePath\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eCONFIG_FILE_URI\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eopen\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#39;w\u0026#39;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003eas\u003c/span\u003e \u003cspan\u003econfig_file\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\n      \u003cspan\u003ejson\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003edump\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eargs\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003econfig_file\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003econfig_file\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eclose\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n\n\u003cspan\u003e# upload prompt opt dataset\u003c/span\u003e\n\u003cspan\u003eprepared_prompt_df\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eto_json\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eINPUT_DATA_FILE_URI\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eorient\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e\u0026#34;records\u0026#34;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003elines\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eTrue\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"15c7g\"\u003e\u003cb\u003e4. Run the optimization job\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"f25ig\"\u003eAt this point, everything is ready to run your first Vertex AI Prompt optimizer job using the Vertex AI SDK for Python.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eWORKER_POOL_SPECS\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003e[{\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;machine_spec\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n        \u003cspan\u003e\u0026#39;machine_type\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e\u0026#39;n1-standard-4\u0026#39;\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e},\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;replica_count\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e1\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n    \u003cspan\u003e\u0026#39;container_spec\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e{\u003c/span\u003e\n        \u003cspan\u003e\u0026#39;image_uri\u0026#39;\u003c/span\u003e \u003cspan\u003e:\u003c/span\u003e \u003cspan\u003eAPD_CONTAINER_URI\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n        \u003cspan\u003e\u0026#39;args\u0026#39;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e \u003cspan\u003e[\u003c/span\u003e\u003cspan\u003e\u0026#34;--config=\u0026#34;\u003c/span\u003e \u003cspan\u003e+\u003c/span\u003e \u003cspan\u003eCONFIG_FILE_URI\u003c/span\u003e\u003cspan\u003e]\u003c/span\u003e\n\u003cspan\u003e}}]\u003c/span\u003e\n\n\u003cspan\u003ecustom_job\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eaiplatform\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eCustomJob\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003edisplay_name\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003ePROMPT_OPTIMIZATION_JOB\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n      \u003cspan\u003eworker_pool_specs\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eWORKER_POOL_SPECS\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n\n\u003cspan\u003ecustom_job\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003erun\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"i1swl\"\u003eNotice how the Vertex AI Prompt Optimizer runs as a Vertex AI Training Custom job using the Vertex AI Prompt Optimizer container. The fact that this service leverages both Vertex AI Training and Vertex AI GenAI Evaluation is a proof of how Vertex AI provides a platform to run GenAI, even those who come directly from research as in this case.\u003c/p\u003e\u003cp data-block-key=\"5t9ja\"\u003eAfter submitting the Vertex AI Prompt optimizer job, you can monitor it from the Vertex AI Training custom jobs view as shown here.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_kgsx0vt.original.png\" alt=\"Training custom jobs view in Vertex AI\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"i1swl\"\u003e\u003cb\u003e5. Get the optimized prompt and evaluate the optimization\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"12lir\"\u003eAfter the optimization job successfully runs, you can find either optimized instructions or demonstrations or both as json files in the output Cloud Storage bucket. Thanks to some helper functions, you can get the following output indicating the optimization step when you get the best instruction according to the metrics you define.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_QoCZ7Xd.original.png\" alt=\"Optimized instruction report in Vertex AI\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"i1swl\"\u003eSame result you get for the optimized demonstrations.\u003c/p\u003e\u003cp data-block-key=\"4rsve\"\u003eFinally, you can generate the new responses with the optimized output. Below you can see an example of a generated response using the optimized system instructions template.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_OZPJzN4.original.png\" alt=\"Gemini_answer_with_apd example\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"i1swl\"\u003eAnd if you use them to run a new round of evaluation with Vertex AI GenAI Evaluation, you might get an output like the one below where the optimized prompt overperforms the previous model with the previous prompt template respective to the evaluation metrics you selected.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_3Hu2Ir5.original.png\" alt=\"Graph showing increased question_answering_quality/mean\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"i1swl\"\u003e\u003cb\u003eConclusion\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"5jjd\"\u003ePrompt engineering is one of the most important yet challenging steps of the process to operationalize LLM-based applications. To help craft your prompt template, Vertex AI Prompt Optimizer finds the best prompt (instruction and demonstrations) for any preferred model on Vertex AI.\u003c/p\u003e\u003cp data-block-key=\"37r7n\"\u003eThis article showed one example of how you can use Vertex AI Prompt Optimizer to enhance your prompt template for a Gemini model using the Vertex AI SDK for Python. You can also use Vertex AI Prompt Optimizer via the UI notebook \u003ca href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp data-block-key=\"3b9ns\"\u003eIn summary, Vertex AI Prompt Optimizer can save you time and effort in prompt engineering while ensuring you have high-performing prompts for your GenAI applications.\u003c/p\u003e\u003cp data-block-key=\"c2947\"\u003eThanks for reading!\u003c/p\u003e\u003ch2 data-block-key=\"2dp76\"\u003e\u003cbr/\u003eWhat’s next\u003c/h2\u003e\u003cp data-block-key=\"66do\"\u003eDo you want to learn more about Vertex AI Prompt Optimizer and how to use it? Check out the following resources:\u003c/p\u003e\u003ch3 data-block-key=\"7q0al\"\u003e\u003cb\u003eDocumentation\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"3m6me\"\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer\"\u003eOptimize prompts with Vertex AI Prompt Optimizer\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"b1bp3\"\u003e\u003cb\u003eGithub samples\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"3sf7f\"\u003e\u003ca href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\"\u003eGetting started with Vertex AI Prompt Optimizer UI\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"f31fq\"\u003e\u003ca href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk.ipynb\"\u003eGetting started with Vertex AI Prompt Optimizer SDK\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"888r6\"\u003e\u003cb\u003eGoogle Cloud blog\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"deu29\"\u003e\u003ca href=\"https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer\"\u003eAnnouncing Public Preview of Vertex AI Prompt Optimizer\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "14 min read",
  "publishedTime": "2024-09-26T00:00:00Z",
  "modifiedTime": null
}
