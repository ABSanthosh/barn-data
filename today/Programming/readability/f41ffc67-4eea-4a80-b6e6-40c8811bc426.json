{
  "id": "f41ffc67-4eea-4a80-b6e6-40c8811bc426",
  "title": "Streamlining LLM Inference at the Edge with TFLite",
  "link": "https://developers.googleblog.com/en/streamlining-llm-inference-at-the-edge-with-tflite/",
  "description": "XNNPack, the default TensorFlow Lite CPU inference engine, has been updated to improve performance and memory management, allow cross-process collaboration, and simplify the user-facing API.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Quentin Khan, Linkun Chen",
  "length": 9211,
  "excerpt": "XNNPack, the default TensorFlow Lite CPU inference engine, has been updated to improve performance and memory management, allow cross-process collaboration, and simplify the user-facing API.",
  "siteName": "",
  "favicon": "",
  "text": "Products More Solutions Events Learn Community Developer Program Blog Optimizing Time to First Token and Peak Memory Usage with a Smarter Cache for XNNPackXNNPack is the default TensorFlow Lite CPU inference engine for all models. It delivers game changing speedups across mobile, desktop, and Web platforms. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors’ pipelines.The inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. Previous efforts have been made to reduce that cost by adding an in-memorycache to XNNPack. This cache allows sharing the packed weights between independent TFLite interpreters that would run the same model independently.TFLite XNNPack delegate implementation has been improved to address some of the shortcomings of the existing cache.1. The cache lives in anonymous memory, which incurs swapping to disk in case of memory pressure, leading to poor performance.2. It requires repacking the initial weights every time a process is started.3. Because repacking reads the original TFLite weights and writes to a new buffer, this leads to a high peak memory usage during the packing.4. It requires tedious steps and careful lifecycle management to properly enable caching through XNNPack delegate.5. It doesn’t allow sharing the weights across processes. . The New XNNPack Cache Provider InterfaceXNNPack has been updated and provides an interface that lets you implement a weight cache provider. A weight cache provider behaves as a dictionary that XNNPack will fill and query in order to access packed buffers. Here are its main functions.look_up looks up a packed buffer key and returns a unique identifier (or a special identifier reserved for NotFound) that may be later used to retrieve the buffer address.reserve_space reserves a buffer that may be used to store information of a given size. That buffer then needs to be committed using look_up_or_insert.look_up_or_insert checks if a buffer matching the given key exists in the cache provider. If not, the given data is committed to the cache provider. This function also returns the identifier that may be used to retrieve the buffer address.offset_to_addr returns the buffer address from the identifier returned by look_up and look_up_or_insert.The interactions between XNNPack and the weight cache provider are illustrated in the following diagram. . Loading the Cache From Disk with MMAP in the TFLite DelegateThe TFLite Delegate now uses this new interface and has its own weight cache provider. This provider is capable of saving and loading the packed weights directly to / from disk. TFLite has been leveraging flatbuffer and file-backed memory mapping for a long time. We are filling the gap here by leveraging the same technique, for the following advantages.It eliminates the repacking overhead.Persisting packed weights on disk bypasses the costly repacking process each time a model is loaded. This translates to a significant reduction in both startup latency and peak memory usage. Even for the initial building, this offers packed data deduplication and further improves packing performance by avoiding repacking the same data again.It improves memory management.mmap leverages the operating system's virtual memory management allowing it to optimize overall system memory usage and performance. In our case, this is especially advantageous for random access bulky read-only file access, like a neural network’s operation’s constant weights for instance.With packed data stored on disk, the XNNPack cache no longer relies on anonymous memory which can be prone to performance issues under memory pressure. Instead, it leverages the operating system's virtual memory management for smoother operation.By eliminating the need to copy data between the file system and memory, mmap significantly reduces overhead and speeds up access times.You can find more information about file mappings and memory usage directly from mmap’s man page and other interesting reads.It allows cross-process collaboration.mmap-based file loading opens the door for seamless weight sharing between multiple processes as each process’ virtual address space maps to the same physical memory pages. This not only reduces the overall memory footprint as multiple processes share the same memory but also accelerates model loading across the board. . It simplifies the user facing API.Instead of requiring the user to setup and manage the cache object throughout the application lifetime, they can simply provide a path to the cache file. std::unique_ptr\u003ctflite::Interpreter\u003e interpreter; // Setup the options for the XNNPack delegate. TfLiteXNNPackDelegateOptions xnnpack_options = TfLiteXNNPackDelegateOptionsDefault(); xnnpack_options.weight_cache_file_path = \"/tmp/cache_file.xnn_cache\"; // Create and apply the XNNPack delegate to a TFLite interpreter. // Static weights will be packed and written into weights_cache on the first run. // They will be automatically loaded for all other runs. TfLiteDelegate* delegate = TfLiteXNNPackDelegateCreate(\u0026xnnpack_options); interpreter-\u003eModifyGraphWithDelegate(delegate); Maintaining Cache IntegrityTo guarantee accurate and efficient inference, it's crucial to invalidate the XNNPack cache under specific conditions:Model Evolution: if your model's weights or structure change, the cached data becomes outdated and must be invalidated. This means removing the file at the provided cache path.XNNPack Upgrades: updates to XNNPack's internal packing algorithm may result in incompatible cached weights, requiring the cache to be recomputed. Fortunately XNNPack is capable of detecting this and will replace the existing cache automatically.In essence, any modification that could impact the way weights are packed or utilized by XNNPack should trigger a cache invalidation.BenchmarksThe session initialisation is dominated by the weight packing. For LLMs several subgraphs are reusing the same weights. Building the cache is faster because the deduplication functionality avoids packing those same weights multiple times. For more standard models, like stable diffusion, there is no deduplication and the slightly higher initialisation time is due to saving the cache to disk. Reloading the cache (from the 2nd run on) brings the initialisation down to a fraction of the previous time in all the cases.The session initialisation improvement naturally affects the time to the first token for LLMs, roughly dividing it by 2 in the benchmarks.The memory gains brought by the cache implementation can also be seen. The peak Resident Set Size is lowered for LLMs thanks to the deduplication. For other models that don’t benefit from the deduplication, there is no change. Reloading the cache brings the peak RSS even further down because the TFLite original models aren’t read anymore and therefore never get pulled into memory.Gemma 2B on a Pixel 8 Pro . Phi2 on a Pixel 8 Pro . Stable Diffusion on a Pixel 8 Pro . Future WorkCurrently the cache is tied to using the file system. We want to be able to take advantage of the data deduplication mechanism independently for use cases that do not want to trade traditional allocated memory with file-backed mappings. mmap allows making anonymous mappings which will allow reusing most of the implementation.",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/TF-Wagtail-Feature.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\n        \n        \n        \n\n        \n\n\t\t\t\t\n        \n\n\n\n\n\u003cdiv top-level-nav=\"\"\u003e\n  \u003cnav aria-label=\"Side menu\"\u003e\n    \n    \u003cdiv\u003e\n        \u003cul\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/products\" data-label=\"Tab: Products\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Products\n             \u003c/span\u003e\n            \u003c/a\u003e\n            \u003cul\u003e\n              \u003cli\u003e\n                \u003cspan tabindex=\"0\" data-label=\"More Products\"\u003e\n                  \u003cspan menu=\"Products\"\u003e\n                    More\n                  \u003c/span\u003e\n                  \u003cspan menu=\"Products\"\u003e\n                    \n                  \u003c/span\u003e\n                \u003c/span\u003e\n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/solutions/catalog\" data-label=\"Tab: Solutions\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Solutions\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/events\" data-label=\"Tab: Events\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Events\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/learn\" data-label=\"Tab: Learn\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Learn\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/community\" data-label=\"Tab: Community\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Community\n             \u003c/span\u003e\n            \u003c/a\u003e\n            \u003cul\u003e\n              \u003cli\u003e\n                \n              \u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.google.com/profile/u/me\" data-label=\"Tab: Developer Program\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Developer Program\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\n            \u003ca href=\"https://developers.googleblog.com/\" data-label=\"Tab: Blog\"\u003e\n              \u003cspan tooltip=\"\"\u003e\n                Blog\n             \u003c/span\u003e\n            \u003c/a\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/div\u003e\n  \u003c/nav\u003e\n  \u003c/div\u003e\n\n\n\n        \n  \u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003ch2 data-block-key=\"71o51\"\u003e\u003cb\u003eOptimizing Time to First Token and Peak Memory Usage with a Smarter Cache for XNNPack\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"63t9t\"\u003e\u003cbr/\u003e\u003ca href=\"https://github.com/google/XNNPACK\"\u003eXNNPack\u003c/a\u003e is the default TensorFlow Lite CPU inference engine for all models. It \u003ca href=\"https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html\"\u003edelivers game changing speedups across mobile, desktop, and Web platforms\u003c/a\u003e. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors’ pipelines.\u003c/p\u003e\u003cp data-block-key=\"7m2pt\"\u003eThe inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. Previous efforts have been made to \u003ca href=\"https://blog.tensorflow.org/2022/06/memory-efficient-inference-with-xnnpack.html\"\u003ereduce that cost by adding an in-memorycache to XNNPack\u003c/a\u003e. This cache allows sharing the packed weights between independent TFLite interpreters that would run the same model independently.\u003c/p\u003e\u003cp data-block-key=\"fvu4s\"\u003eTFLite XNNPack delegate implementation has been improved to address some of the shortcomings of the existing cache.\u003c/p\u003e\u003cp data-block-key=\"9vkch\"\u003e\u003cbr/\u003e1. The cache lives in \u003ca href=\"https://unix.stackexchange.com/a/677020\"\u003eanonymous memory\u003c/a\u003e, which incurs swapping to disk in case of memory pressure, leading to poor performance.\u003c/p\u003e\u003cp data-block-key=\"af6u9\"\u003e2. It requires repacking the initial weights every time a process is started.\u003c/p\u003e\u003cp data-block-key=\"3dje8\"\u003e3. Because repacking reads the original TFLite weights and writes to a new buffer, this leads to a high peak memory usage during the packing.\u003c/p\u003e\u003cp data-block-key=\"8pbfq\"\u003e4. It requires tedious steps and careful lifecycle management to properly enable caching through XNNPack delegate.\u003c/p\u003e\u003cp data-block-key=\"ee6u3\"\u003e5. It doesn’t allow sharing the weights across processes.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_6fxx2k0.original.png\" alt=\"TFLite XNNPack delegate architecture\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    .\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"71o51\"\u003eThe New XNNPack Cache Provider Interface\u003c/h2\u003e\u003cp data-block-key=\"ev3pc\"\u003eXNNPack has been updated and provides \u003ca href=\"https://github.com/google/XNNPACK/blob/5d5715cf33c93dc7e2153f6c1f1d194690ff9ab0/include/xnnpack.h#L1865-L1903\"\u003ean interface that lets you implement a weight cache provider\u003c/a\u003e. A weight cache provider behaves as a dictionary that XNNPack will fill and query in order to access packed buffers. Here are its main functions.\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"89rs1\"\u003e\u003ccode\u003elook_up\u003c/code\u003e looks up a packed buffer key and returns a unique identifier (or a special identifier reserved for NotFound) that may be later used to retrieve the buffer address.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"3josu\"\u003e\u003ccode\u003ereserve_space\u003c/code\u003e reserves a buffer that may be used to store information of a given size. That buffer then needs to be committed using \u003ccode\u003elook_up_or_insert\u003c/code\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1bddn\"\u003e\u003ccode\u003elook_up_or_insert\u003c/code\u003e checks if a buffer matching the given key exists in the cache provider. If not, the given data is committed to the cache provider. This function also returns the identifier that may be used to retrieve the buffer address.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"dr7if\"\u003e\u003ccode\u003eoffset_to_addr\u003c/code\u003e returns the buffer address from the identifier returned by look_up and \u003ccode\u003elook_up_or_insert\u003c/code\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"38d7n\"\u003eThe interactions between XNNPack and the weight cache provider are illustrated in the following diagram.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_A32yfjm.original.png\" alt=\"The interactions between XNNPack and the weight cache provider\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    .\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"71o51\"\u003eLoading the Cache From Disk with MMAP in the TFLite Delegate\u003c/h2\u003e\u003cp data-block-key=\"c8f0\"\u003eThe TFLite Delegate now uses this new interface and has its own weight cache provider. This provider is capable of saving and loading the packed weights directly to / from disk. TFLite has been leveraging flatbuffer and file-backed memory mapping for a long time. We are filling the gap here by leveraging the same technique, for the following advantages.\u003c/p\u003e\u003ch4 data-block-key=\"bs64b\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIt eliminates the repacking overhead.\u003c/h4\u003e\u003cp data-block-key=\"adnq5\"\u003ePersisting packed weights on disk bypasses the costly repacking process each time a model is loaded. This translates to a significant reduction in both startup latency and peak memory usage. Even for the initial building, this offers packed data deduplication and further improves packing performance by avoiding repacking the same data again.\u003c/p\u003e\u003ch4 data-block-key=\"42dsc\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIt improves memory management.\u003c/h4\u003e\u003cp data-block-key=\"94dlh\"\u003e\u003ccode\u003emmap\u003c/code\u003e leverages the operating system\u0026#39;s virtual memory management allowing it to optimize overall system memory usage and performance. In our case, this is especially advantageous for random access bulky read-only file access, like a neural network’s operation’s constant weights for instance.\u003c/p\u003e\u003cp data-block-key=\"26i7a\"\u003eWith packed data stored on disk, the XNNPack cache no longer relies on anonymous memory which can be prone to performance issues under memory pressure. Instead, it leverages the operating system\u0026#39;s virtual memory management for smoother operation.\u003c/p\u003e\u003cp data-block-key=\"emvc4\"\u003eBy eliminating the need to copy data between the file system and memory, mmap significantly reduces overhead and speeds up access times.\u003c/p\u003e\u003cp data-block-key=\"8o3ai\"\u003eYou can find more information about file mappings and memory usage directly from mmap’s \u003ca href=\"https://man7.org/linux/man-pages/man2/mmap.2.html\"\u003eman page\u003c/a\u003e and other \u003ca href=\"https://landley.net/writing/memory-faq.txt\"\u003einteresting\u003c/a\u003e \u003ca href=\"https://biriukov.dev/docs/page-cache/7-how-much-memory-my-program-uses-or-the-tale-of-working-set-size/\"\u003ereads\u003c/a\u003e.\u003c/p\u003e\u003ch4 data-block-key=\"8lnv2\"\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIt allows cross-process collaboration.\u003c/h4\u003e\u003cp data-block-key=\"95cih\"\u003e\u003ccode\u003emmap\u003c/code\u003e-based file loading opens the door for seamless weight sharing between multiple processes as each process’ virtual address space maps to the same physical memory pages. This not only reduces the overall memory footprint as multiple processes share the same memory but also accelerates model loading across the board.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_v4g101B.original.png\" alt=\"mmap-based file loading architecture\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    .\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch4 data-block-key=\"71o51\"\u003eIt simplifies the user facing API.\u003c/h4\u003e\u003cp data-block-key=\"21vvp\"\u003eInstead of requiring the user to setup and manage the cache object throughout the application lifetime, they can simply provide a path to the cache file.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003estd\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003eunique_ptr\u003c/span\u003e\u003cspan\u003e\u0026lt;\u003c/span\u003e\u003cspan\u003etflite\u003c/span\u003e\u003cspan\u003e::\u003c/span\u003e\u003cspan\u003eInterpreter\u003c/span\u003e\u003cspan\u003e\u0026gt;\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003einterpreter\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\u003cspan\u003e// Setup the options for the XNNPack delegate.\u003c/span\u003e\n\u003cspan\u003eTfLiteXNNPackDelegateOptions\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003exnnpack_options\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eTfLiteXNNPackDelegateOptionsDefault\u003c/span\u003e\u003cspan\u003e();\u003c/span\u003e\n\u003cspan\u003exnnpack_options\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eweight_cache_file_path\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;/tmp/cache_file.xnn_cache\u0026#34;\u003c/span\u003e\u003cspan\u003e;\u003c/span\u003e\n\u003cspan\u003e// Create and apply the XNNPack delegate to a TFLite interpreter.\u003c/span\u003e\n\u003cspan\u003e// Static weights will be packed and written into weights_cache on the first run.\u003c/span\u003e\n\u003cspan\u003e// They will be automatically loaded for all other runs.\u003c/span\u003e\n\u003cspan\u003eTfLiteDelegate\u003c/span\u003e\u003cspan\u003e*\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003edelegate\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003eTfLiteXNNPackDelegateCreate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026amp;\u003c/span\u003e\u003cspan\u003exnnpack_options\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\n\u003cspan\u003einterpreter\u003c/span\u003e\u003cspan\u003e-\u0026gt;\u003c/span\u003e\u003cspan\u003eModifyGraphWithDelegate\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edelegate\u003c/span\u003e\u003cspan\u003e);\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"71o51\"\u003eMaintaining Cache Integrity\u003c/h2\u003e\u003cp data-block-key=\"8dhdc\"\u003eTo guarantee accurate and efficient inference, it\u0026#39;s crucial to invalidate the XNNPack cache under specific conditions:\u003c/p\u003e\u003cp data-block-key=\"bvcco\"\u003e\u003cb\u003eModel Evolution\u003c/b\u003e: if your model\u0026#39;s weights or structure change, the cached data becomes outdated and must be invalidated. This means removing the file at the provided cache path.\u003c/p\u003e\u003cp data-block-key=\"8jg16\"\u003e\u003cb\u003eXNNPack Upgrades\u003c/b\u003e: updates to XNNPack\u0026#39;s internal packing algorithm may result in incompatible cached weights, requiring the cache to be recomputed. Fortunately XNNPack is capable of detecting this and will replace the existing cache automatically.\u003c/p\u003e\u003cp data-block-key=\"mn71\"\u003eIn essence, any modification that could impact the way weights are packed or utilized by XNNPack should trigger a cache invalidation.\u003c/p\u003e\u003ch2 data-block-key=\"ek9a0\"\u003e\u003cbr/\u003eBenchmarks\u003c/h2\u003e\u003cp data-block-key=\"encs1\"\u003eThe session initialisation is dominated by the weight packing. For LLMs several subgraphs are reusing the same weights. Building the cache is faster because the deduplication functionality avoids packing those same weights multiple times. For more standard models, like stable diffusion, there is no deduplication and the slightly higher initialisation time is due to saving the cache to disk. Reloading the cache (from the 2nd run on) brings the initialisation down to a fraction of the previous time in all the cases.\u003c/p\u003e\u003cp data-block-key=\"kpmf\"\u003eThe session initialisation improvement naturally affects the time to the first token for LLMs, roughly dividing it by 2 in the benchmarks.\u003c/p\u003e\u003cp data-block-key=\"3n4s2\"\u003eThe memory gains brought by the cache implementation can also be seen. The peak Resident Set Size is lowered for LLMs thanks to the deduplication. For other models that don’t benefit from the deduplication, there is no change. Reloading the cache brings the peak RSS even further down because the TFLite original models aren’t read anymore and therefore never get pulled into memory.\u003c/p\u003e\u003ch4 data-block-key=\"22r27\"\u003e\u003cbr/\u003e\u003cb\u003eGemma 2B on a Pixel 8 Pro\u003c/b\u003e\u003c/h4\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-06_at_5.09.18PM.original.png\" alt=\"Benchmarks - Gemma 2B on a Pixel 8 Pro\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    .\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cp\u003e\n    \u003ch4 data-block-key=\"71o51\"\u003e\u003cb\u003ePhi2 on a Pixel 8 Pro\u003c/b\u003e\u003c/h4\u003e\n\u003c/p\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-06_at_5.09.30PM.original.png\" alt=\"Benchmarks - Phi2 on a Pixel 8 Pro\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    .\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cp\u003e\n    \u003ch4 data-block-key=\"71o51\"\u003e\u003cb\u003eStable Diffusion on a Pixel 8 Pro\u003c/b\u003e\u003c/h4\u003e\n\u003c/p\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-06_at_5.09.56PM.original.png\" alt=\"Stable Diffusion on a Pixel 8 Pro\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    .\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch2 data-block-key=\"71o51\"\u003eFuture Work\u003c/h2\u003e\u003cp data-block-key=\"db4g3\"\u003eCurrently the cache is tied to using the file system. We want to be able to take advantage of the data deduplication mechanism independently for use cases that do not want to trade traditional allocated memory with file-backed mappings. mmap allows making anonymous mappings which will allow reusing most of the implementation.\u003c/p\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\n\n\n\t\t\t\t\n\t\t\t\t\n\n\n\n\n\n        \n\t\t\t\t\n\n        \n        \n        \n        \n\n        \n\n        \n  \n\n    \n\n\u003c/div\u003e",
  "readingTime": "10 min read",
  "publishedTime": "2024-08-13T00:00:00Z",
  "modifiedTime": null
}
