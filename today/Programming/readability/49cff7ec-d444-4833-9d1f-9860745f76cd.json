{
  "id": "49cff7ec-d444-4833-9d1f-9860745f76cd",
  "title": "PyTorch 2.5 Release Includes Support for Intel GPUs",
  "link": "https://www.infoq.com/news/2024/10/pytorch-25-release/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "The PyTorch Foundation recently released PyTorch version 2.5, which contains support for Intel GPUs. The release also includes several performance enhancements, such as the FlexAttention API, TorchInductor CPU backend optimizations, and a regional compilation feature which reduces compilation time. Overall, the release contains 4095 commits since PyTorch 2.4. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 29 Oct 2024 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Deep Learning",
    "PyTorch",
    "Neural Networks",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3564,
  "excerpt": "The PyTorch Foundation recently released PyTorch version 2.5, which contains support for Intel GPUs. The release also includes several performance enhancements, such as the FlexAttention API, TorchInd",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241029101913/apple-touch-icon.png",
  "text": "The PyTorch Foundation recently released PyTorch version 2.5, which contains support for Intel GPUs. The release also includes several performance enhancements, such as the FlexAttention API, TorchInductor CPU backend optimizations, and a regional compilation feature which reduces compilation time. Overall, the release contains 4095 commits since PyTorch 2.4. The Intel GPU support was previewed at the recent PyTorch conference. Intel engineers Eikan Wang and Min Jean Cho described the PyTorch changes made to support the hardware. This included generalizing the PyTorch runtime and device layers which makes it easier to integrate new hardware backends. Intel specific backends were also implemented for torch.compile and torch.distributed. According to Kismat Singh, Intel's VP of engineering for AI frameworks: We have added support for Intel client GPUs in PyTorch 2.5 and that basically means that you'll be able to run PyTorch on the Intel laptops and desktops that are built using the latest Intel processors. We think it's going to unlock 40 million laptops and desktops for PyTorch users this year and we expect the number to go to around 100 million by the end of next year. The release includes a new FlexAttention API which makes it easier for PyTorch users to experiment with different attention mechanisms in their models. Typically, researchers who want to try a new attention variant need to hand-code it directly from PyTorch operators. However, this could result in \"slow runtime and CUDA OOMs.\" The new API supports writing these instead with \"a few lines of idiomatic PyTorch code.\" The compiler then converts these to an optimized kernel \"that doesn’t materialize any extra memory and has performance competitive with handwritten ones.\" Several performance improvements have been released in beta status. A new backend Fused Flash Attention provides \"up to 75% speed-up over FlashAttentionV2\" for NVIDIA H100 GPUs. A regional compilation feature for torch.compile reduces the need for full model compilation; instead, repeated nn.Modules, such as Transformer layers, are compiled. This can reduce compilation latency while incurring only a few percent performance degradation. There are also several optimizations to the TorchInductor CPU backend. Flight Recorder, a new debugging tool for stuck jobs, was also included in the release. Stuck jobs can occur during distributed training, and could have many root causes, including data starvation, network issues, or software bugs. Flight Recorder uses an in-memory circular buffer to capture diagnostic info. When it detects a stuck job, it dumps the diagnostics to a file; the data can then be analyzed using a script of heuristics to identify the root cause. In discussions about the release on Reddit, many users were glad to see support for Intel GPUs, calling it a \"game changer.\" Another user wrote: Excited to see the improvements in torch.compile, especially the ability to reuse repeated modules to speed up compilation. That could be a game-changer for large models with lots of similar components. The FlexAttention API also looks really promising - being able to implement various attention mechanisms with just a few lines of code and get near-handwritten performance is huge. Kudos to the PyTorch team and contributors for another solid release! The PyTorch 2.5 code and release notes are available on GitHub. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2024/10/pytorch-25-release/en/headerimage/generatedHeaderImage-1729436355246.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eThe \u003ca href=\"https://pytorch.org/\"\u003ePyTorch Foundation\u003c/a\u003e recently released \u003ca href=\"https://pytorch.org/blog/pytorch2-5/\"\u003ePyTorch version 2.5\u003c/a\u003e, which contains support for Intel GPUs. The release also includes several performance enhancements, such as the \u003ca href=\"https://pytorch.org/blog/flexattention/\"\u003eFlexAttention\u003c/a\u003e API, TorchInductor CPU backend optimizations, and a \u003ca href=\"https://pytorch.org/tutorials/recipes/regional_compilation.html\"\u003eregional compilation feature\u003c/a\u003e which reduces compilation time. Overall, the release contains 4095 commits since PyTorch 2.4.\u003c/p\u003e\n\n\u003cp\u003eThe Intel GPU support was previewed at the recent \u003ca href=\"https://pytorch.org/blog/pytorch-conference-2024-recap/\"\u003ePyTorch conference\u003c/a\u003e. Intel engineers Eikan Wang and Min Jean Cho \u003ca href=\"https://www.youtube.com/watch?v=HRdda_kVEh4\"\u003edescribed the PyTorch changes\u003c/a\u003e made to support the hardware. This included generalizing the PyTorch runtime and device layers which makes it easier to integrate new hardware backends. Intel specific backends were also implemented for torch.compile and torch.distributed. \u003ca href=\"https://www.youtube.com/watch?v=tGEtpXaoXuk\"\u003eAccording to Kismat Singh\u003c/a\u003e, Intel\u0026#39;s VP of engineering for AI frameworks:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWe have added support for Intel client GPUs in PyTorch 2.5 and that basically means that you\u0026#39;ll be able to run PyTorch on the Intel laptops and desktops that are built using the latest Intel processors. We think it\u0026#39;s going to unlock 40 million laptops and desktops for PyTorch users this year and we expect the number to go to around 100 million by the end of next year.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe release includes a new FlexAttention API which makes it easier for PyTorch users to experiment with different attention mechanisms in their models. Typically, researchers who want to try a new attention variant need to hand-code it directly from PyTorch operators. However, this could result in \u0026#34;slow runtime and CUDA OOMs.\u0026#34; The new API supports writing these instead with \u0026#34;a few lines of idiomatic PyTorch code.\u0026#34; The compiler then converts these to an optimized kernel \u0026#34;that doesn’t materialize any extra memory and has performance competitive with handwritten ones.\u0026#34;\u003c/p\u003e\n\n\u003cp\u003eSeveral performance improvements have been released in beta status. A new backend Fused Flash Attention provides \u0026#34;up to 75% speed-up over FlashAttentionV2\u0026#34; for NVIDIA H100 GPUs. A regional compilation feature for torch.compile reduces the need for full model compilation; instead, repeated nn.Modules, such as Transformer layers, are compiled. This can reduce compilation latency while incurring only a few percent performance degradation. There are also several optimizations to the TorchInductor CPU backend.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://pytorch.org/tutorials/prototype/flight_recorder_tutorial.html\"\u003eFlight Recorder\u003c/a\u003e, a new debugging tool for stuck jobs, was also included in the release. Stuck jobs can occur during distributed training, and could have many root causes, including data starvation, network issues, or software bugs. Flight Recorder uses an in-memory circular buffer to capture diagnostic info. When it detects a stuck job, it dumps the diagnostics to a file; the data can then be analyzed using a script of heuristics to identify the root cause.\u003c/p\u003e\n\n\u003cp\u003eIn discussions about the release on Reddit, many users were \u003ca href=\"https://www.reddit.com/r/IntelArc/comments/1g6qxs4/pytorch_250_has_been_released_theyve_finally/\"\u003eglad to see support for Intel GPUs\u003c/a\u003e, calling it a \u0026#34;game changer.\u0026#34; \u003ca href=\"https://www.reddit.com/r/MachineLearning/comments/1g62vyh/d_pytorch_250_released/\"\u003eAnother user wrote\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eExcited to see the improvements in torch.compile, especially the ability to reuse repeated modules to speed up compilation. That could be a game-changer for large models with lots of similar components. The FlexAttention API also looks really promising - being able to implement various attention mechanisms with just a few lines of code and get near-handwritten performance is huge. Kudos to the PyTorch team and contributors for another solid release!\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe PyTorch 2.5 \u003ca href=\"https://github.com/pytorch/pytorch/releases/tag/v2.5.0\"\u003ecode and release notes\u003c/a\u003e are available on GitHub.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-10-29T00:00:00Z",
  "modifiedTime": null
}
