{
  "id": "291ef552-e810-41f0-9872-80cb0202726d",
  "title": "From MCP to multi-agents: The top 10 new open source AI projects on GitHub right now and why they matter",
  "link": "https://github.blog/open-source/maintainers/from-mcp-to-multi-agents-the-top-10-open-source-ai-projects-on-github-right-now-and-why-they-matter/",
  "description": "Get insights on the latest trends from GitHub experts while catching up on these exciting new projects. The post From MCP to multi-agents: The top 10 new open source AI projects on GitHub right now and why they matter appeared first on The GitHub Blog.",
  "author": "Jeimy Ruiz",
  "published": "Wed, 30 Apr 2025 16:00:49 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "Maintainers",
    "News \u0026 insights",
    "Open Source",
    "Research",
    "AI agents",
    "generative AI",
    "MCP",
    "Octoverse",
    "open source",
    "research"
  ],
  "byline": "Jeimy Ruiz",
  "length": 10682,
  "excerpt": "Get insights on the latest trends from GitHub experts while catching up on these exciting new projects.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "Every day, new public and open source repositories appear on GitHub, and navigating the sheer amount of activity can be a challenge for the best of us. Luckily, weâ€™ve done the heavy lifting for you. Together with our panel of GitHub expertsâ€”who have experience across open source and developer relationsâ€”we analyzed every open source project created in the last 99 days (as of March 29, 2025), and ranked them in consideration of a number of factors including stars-per-day, forks, traffic spikes, and contributor velocity. Our GitHub panel includes: Abigail Cabunoc Mayes, aka @abbycabs, who works on open source maintainer programs and serves as a Director on the OpenJS foundation. Kara Sowles, aka @karasowles, who works with maintainers and the open source community. Kevin Crosby, aka @kevincrosby, who runs GitHubâ€™s open source funding program. Jeff Luszcz, aka @jeffrey-luszcz, who helps manage GitHubâ€™s open source program office (OSPO). Below, weâ€™ll give you a rundown of these projectsâ€”and also discuss why we believe theyâ€™re the ones developers keep coming back to. Letâ€™s dive in. ğŸ“œ MIT license First up is a proxy server that turns MCP tools into OpenAPI-compatible HTTP servers. Developers building AI-powered apps are using the server to easily connect MCP-based tools with anything that uses standard RESTful OpenAPI interfaces. 2. Unbody: the â€œSupabase of AIâ€ ğŸ§© ğŸ“œ Apache 2.0 license Think Supabase, but for AI: thatâ€™s Unbody in a nutshell. Itâ€™s a modular backend that lets you build AI-native software that actually understands and reasons about knowledge, instead of just shuffling data around. The project breaks things down into four layers that you can mix and match: Perception: Ingests, parses, enhances, and vectorizes raw data. Memory: Stores structured knowledge in vector databases and persistent storage. Reasoning: Generates content, calls functions, and plans actions. Action: Exposes knowledge via APIs, SDKs, and triggers. 3. OWL: multi-agent collaboration in action ğŸ¦‰ ğŸ“œ Apache 2.0 license When one AI agent isnâ€™t enough, OWL enters the chat. Built on the CAMEL-AI frameworkâ€”best known for popularizing multi-agent role-play and releasing a trove of synthetic â€œtask + dataâ€ bundlesâ€”OWL lets several specialized agents cooperate through browsers, terminals, function calls, and MCP tools. It even tops the open-source leaderboard on the GAIA benchmark (58.18). ğŸ“œ MIT license CLI fans: Hereâ€™s a command-line interface for working with MCP servers thatâ€™ll make you feel right at home. Built by GitHub Star Fatih Kadir Akin (who also shipped the GitHub Copilot prompts feature), it lets you discover and call tools, access resources, and manage prompts from any MCP-compatible server. MCP Tools supports input/output over stdin/stdout or HTTP, and spits out results in JSON or table views. It even lets you create mock servers for testing or proxy MCP requests to shell scripts. 5. Nutlope/self.so: Build your personal site with AI in seconds âš¡ ğŸ“œ MIT license If putting together a personal website isnâ€™t your idea of fun, Nutlope/self.so can lend a hand. Upload your rÃ©sumÃ© or LinkedIn profile, and the tool will pull together a straightforward site for you, using AI to handle the layout so you can skip the CSS headaches. The tech stack includes Together.ai for language modeling, Vercelâ€™s AI SDK, Clerk for auth, Next.js for the framework, Helicone for observability, S3 for storage, Upstash Redis for the database, and Vercel for hosting. 6. VoiceStar: precise control for text-to-speech applications ğŸ™ï¸ ğŸ“œ MIT code license, CC-BY-4.0 model license If your project needs speech that lands within a specific time window, VoiceStarâ€™s duration-controllable synthesis can help. It lets developers set target lengths so voice output fits time-sensitive use casesâ€”like fixed-length prompts or narrationâ€”without extra audio editing. The project includes both CLI and Gradio interfaces for inference, plus pre-trained models you can use right away. As voice interfaces become more important in apps, having open source models with this level of control is a helpful step forward 7. Create your digital twin with Second-Me ğŸ¤– ğŸ“œ Apache 2.0 license Interested in experimenting with an AI stand-in? Second-Me lets you try a basic â€œdigital twinâ€â€”an agent that aims to reflect some of your knowledge, communication style, and preferences. The possibilities range from personal assistants that actually understand how you think to innovative ways to share your expertise with others. One example of Second-Me in action: Have your digital twin manage your LinkedIn or Airbnb account, playing the role of the professional or host. 8. SesameAILabs/csm: reimagining speech synthesis ğŸ”Š ğŸ“œ Apache 2.0 code license (model has restrictions on Abuse) The Conversational Speech Model (CSM) brings a fresh approach to speech generation. It converts text and audio inputs into Residual Vector Quantization (RVQ) audio codes using a Llama-based architecture. Its dedicated audio decoder produces Mimi audio codes that result in surprisingly natural-sounding speech. Whatâ€™s interesting here is how CSM merges language model architecture with specialized audio decodingâ€”giving you an open alternative to the proprietary text-to-speech options that dominate the market. 9. Letta: a universal standard for portable AI agents ğŸ“¦ ğŸ“œ Apache 2.0 license Letta introduces an open file format (.af) for packaging up AI agents with their memory and behavior intact. Think of it as a portable container for agents. You can share, checkpoint, and version control them across different frameworks. For developers juggling multiple agent frameworks, this could be a time-saver. Want to move an agent from one system to another without rebuilding it from scratch? Thatâ€™s the problem Letta is solving. Notably, the Letta project is an offshoot of the cpacker/memgpt projectâ€”it spun out the serialization layer that MemGPT originally used to snapshot its â€œvirtual-contextâ€ agents. The team carved that code into a clean, framework-agnostic spec (agent-file) so any stackâ€”MemGPT/Letta, LangGraph, CrewAI, you name itâ€”can import or export a fully stateful agent with a single .af archive. 10. Blender meets Claude: bridging 3D creation and AI ğŸ¨ ğŸ“œ MIT license Blender artists, this oneâ€™s for you: a third party tool that connects the popular open source 3D creation suite Blender with Claude AI through the MCP. With Blender-MCP, developers can control Blender operations with natural languageâ€”or add AI assistance to their 3D workflow. Blender-MCP shows how the MCP can act as a universal â€œtool portâ€ for LLM agents: today itâ€™s Blender; tomorrow it could be Unity, Unreal, or any complex desktop app. For 3D artists and prototypers, that means faster scene blocking, easy style experiments, and a brand-new way to teach beginners. Just describe what you want and watch the software build it. Interested? Installing Blender-MCP is as simple as running a bash command. What these projects tell us about AIâ€™s evolution in open source These patterns not only reflect the current state of AI in open source, but also hint at the challenges and opportunities that lie ahead. Hereâ€™s what GitHub experts have to say about the rapidly evolving space: Integration in AI via MCP is the new frontier ğŸ”— The prominence of MCP across multiple projects highlights the growing importance of standardized integration patterns in AI development. â€œA big pattern that I saw is the pain point around AI and integration,â€ notes Abigail. â€œMore standards like MCP will help with this.â€ Multi-agent collaboration emerges ğŸ‘¥ Projects like OWL point to a future where multiple specialized AI agents work together to solve complex problems. â€œYou have to think about it in the construct of person to person, agent to agent, and then having multiple agents working in tandem,â€ explains Kevin, highlighting the complexity and potential of this approach. Speech generation is advancing ğŸ—£ï¸ Speech tech certainly isnâ€™t newâ€”but large language models are reshaping both text-to-speech (TTS) and speech-to-text (STT) so dramatically that a fresh wave of possibilities is opening up. The bigger story is what this means downstream with implications across media, customer support, and product UX. The evolving landscape of open source participation ğŸŒ± AI has drawn a fresh wave of maintainers and contributors to open source, bringing new energy and approaches. Kara Deloss, senior program manager of developer relations at GitHub, notes: â€œWeâ€™re seeing a new generation, or a new type of maintainerâ€ in the AI space. Kevin adds that â€œif you have a big community on day one, thatâ€™s valuable,â€ highlighting how the ecosystem is evolving. This blend of established and emerging development practices is creating exciting opportunities for collaboration across the community. The importance of OSI-approved licenses ğŸ“„ Every top project in our list uses OSI-approved licenses (mostly MIT and Apache 2.0), and thatâ€™s no accident. â€œIn general, you wonâ€™t get a lot of positive sentiment from the community if you call yourself open source but arenâ€™t using an OSI-approved license,â€ Jeff points out. â€œThese licenses matter because they provide clear guarantees around usage, modification, and redistribution rights that build trust in the community.â€ Jeff also notes an emerging challenge: â€œAs AI powered services and tools become more powerful, we are seeing a trend where some projects attach Abuse and Fraud related restrictions on model or service use. This may not make them completely open source under an OSI approved license, and the projects and the community will continue to have an intense conversation about these conditions.â€ He continues: â€œItâ€™s important to understand and document any restrictions in place before using a model or service,â€ which is a timely reminder as the open source AI community navigates these evolving licensing questions. The projects weâ€™ve highlighted here are just the tip of the iceberg. As AI keeps evolving, the open source ecosystem is where many of the most exciting standards, tools, and techniques are popping up first. Hereâ€™s how to get involved: Check out these projects to see how they might fit into your workflow Join in and contribute to projects that spark your interest Keep an eye on MCP and other emerging standards Tags: AI agents generative AI MCP Octoverse open source research Written by",
  "image": "https://github.blog/wp-content/uploads/2025/04/wallpaper_github_generic_1.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003cp\u003eEvery day, new public and open source repositories appear on GitHub, and navigating the sheer amount of activity can be a challenge for the best of us. Luckily, weâ€™ve done the heavy lifting for you.\u003c/p\u003e\n\u003cp\u003eTogether with our panel of GitHub expertsâ€”who have experience across open source and developer relationsâ€”we analyzed every open source project created in the last 99 days (as of March 29, 2025), and ranked them in consideration of a number of factors  including stars-per-day, forks, traffic spikes, and contributor velocity.\u003c/p\u003e\n\u003cp\u003eOur GitHub panel includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/abbycabs\"\u003eAbigail Cabunoc Mayes\u003c/a\u003e, aka @abbycabs, who works on open source maintainer programs and serves as a Director on the OpenJS foundation.   \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/karasowles\"\u003eKara Sowles\u003c/a\u003e, aka @karasowles, who works with maintainers and the open source community.  \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kevincrosby\"\u003eKevin Crosby\u003c/a\u003e, aka @kevincrosby, who runs GitHubâ€™s open source funding program.   \u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Jeffrey-Luszcz\"\u003eJeff Luszcz\u003c/a\u003e, aka @jeffrey-luszcz, who helps manage GitHubâ€™s open source program office (OSPO).   \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBelow, weâ€™ll give you a rundown of these projectsâ€”and also discuss why we believe theyâ€™re the ones developers keep coming back to. Letâ€™s dive in.\u003c/p\u003e\n\n\n\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ MIT license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFirst up is a proxy server that turns MCP tools into OpenAPI-compatible HTTP servers. Developers building AI-powered apps are using the server to easily connect MCP-based tools with anything that uses standard RESTful OpenAPI interfaces.\u003c/p\u003e\n\n\u003ch2 id=\"2-unbody-the-supabase-of-ai-%f0%9f%a7%a9\" id=\"2-unbody-the-supabase-of-ai-%f0%9f%a7%a9\"\u003e2. Unbody: the â€œSupabase of AIâ€ ğŸ§©\u003ca href=\"#2-unbody-the-supabase-of-ai-%f0%9f%a7%a9\" aria-label=\"2. Unbody: the â€œSupabase of AIâ€ ğŸ§©\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cdiv\u003e \t\t\t\n\u003cp\u003e\u003ca href=\"https://github.blog/wp-content/uploads/2025/04/chatbot-mobile-website-smartwatch.png\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/04/chatbot-mobile-website-smartwatch.png?resize=761%2C488\" alt=\"A diagram showing chatbot, mobile, website, and smartwatch feeding into API, which then feeds into data.\" width=\"761\" height=\"488\" srcset=\"https://github.blog/wp-content/uploads/2025/04/chatbot-mobile-website-smartwatch.png?w=761 761w, https://github.blog/wp-content/uploads/2025/04/chatbot-mobile-website-smartwatch.png?w=300 300w\" sizes=\"auto, (max-width: 761px) 100vw, 761px\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ Apache 2.0 license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThink \u003ca href=\"https://supabase.com/\"\u003eSupabase\u003c/a\u003e, but for AI: thatâ€™s Unbody in a nutshell. Itâ€™s a modular backend that lets you build AI-native software that actually \u003cem\u003eunderstands\u003c/em\u003e and \u003cem\u003ereasons\u003c/em\u003e about knowledge, instead of just shuffling data around.\u003c/p\u003e\n\u003cp\u003eThe project breaks things down into four layers that you can mix and match:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePerception\u003c/strong\u003e: Ingests, parses, enhances, and vectorizes raw data.  \u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory\u003c/strong\u003e: Stores structured knowledge in vector databases and persistent storage.  \u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReasoning\u003c/strong\u003e: Generates content, calls functions, and plans actions.  \u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAction\u003c/strong\u003e: Exposes knowledge via APIs, SDKs, and triggers.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"3-owl-multi-agent-collaboration-in-action-%f0%9f%a6%89\" id=\"3-owl-multi-agent-collaboration-in-action-%f0%9f%a6%89\"\u003e3. OWL: multi-agent collaboration in action ğŸ¦‰\u003ca href=\"#3-owl-multi-agent-collaboration-in-action-%f0%9f%a6%89\" aria-label=\"3. OWL: multi-agent collaboration in action ğŸ¦‰\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cdiv\u003e \t\t\t\n\u003cp\u003e\u003ca href=\"https://github.blog/wp-content/uploads/2025/04/owl_architecture.png\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?resize=1024%2C576\" alt=\"An image showing an OWL System Architecture.\" width=\"1024\" height=\"576\" srcset=\"https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?w=3200 3200w, https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?w=300 300w, https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?w=768 768w, https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?w=1024 1024w, https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?w=1536 1536w, https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?w=2048 2048w, https://github.blog/wp-content/uploads/2025/04/owl_architecture.png?w=3000 3000w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ Apache 2.0 license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhen one AI agent isnâ€™t enough, OWL enters the chat. Built on the \u003ca href=\"https://www.camel-ai.org\"\u003eCAMEL-AI\u003c/a\u003e frameworkâ€”best known for popularizing multi-agent role-play and releasing a trove of synthetic â€œtask + dataâ€ bundlesâ€”\u003cstrong\u003eOWL lets several specialized agents cooperate through browsers, terminals, function calls, and MCP tools\u003c/strong\u003e. It even tops the open-source leaderboard on the GAIA benchmark (58.18).\u003c/p\u003e\n\n\n\u003cdiv\u003e \t\t\t\n\u003cp\u003e\u003ca href=\"https://github.blog/wp-content/uploads/2025/04/mcp-tools.png\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/04/mcp-tools.png?resize=1024%2C659\" alt=\"A screenshot showing MCP Tools Shell.\" width=\"1024\" height=\"659\" srcset=\"https://github.blog/wp-content/uploads/2025/04/mcp-tools.png?w=1944 1944w, https://github.blog/wp-content/uploads/2025/04/mcp-tools.png?w=300 300w, https://github.blog/wp-content/uploads/2025/04/mcp-tools.png?w=768 768w, https://github.blog/wp-content/uploads/2025/04/mcp-tools.png?w=1024 1024w, https://github.blog/wp-content/uploads/2025/04/mcp-tools.png?w=1536 1536w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ MIT license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCLI fans: Hereâ€™s a command-line interface for working with MCP servers thatâ€™ll make you feel right at home. Built by \u003ca href=\"https://stars.github.com/profiles/f/\"\u003eGitHub Star Fatih Kadir Akin\u003c/a\u003e (who also shipped the \u003ca href=\"https://docs.github.com/en/copilot/using-github-copilot/copilot-chat/prompt-engineering-for-copilot-chat\"\u003eGitHub Copilot prompts feature\u003c/a\u003e), it lets you discover and call tools, access resources, and manage prompts from any MCP-compatible server.\u003c/p\u003e\n\u003cp\u003eMCP Tools supports input/output over stdin/stdout or HTTP, and spits out results in JSON or table views. It even lets you create mock servers for testing or proxy MCP requests to shell scripts.\u003c/p\u003e\n\n\u003ch2 id=\"5-nutlope-self-so-build-your-personal-site-with-ai-in-seconds-%e2%9a%a1\" id=\"5-nutlope-self-so-build-your-personal-site-with-ai-in-seconds-%e2%9a%a1\"\u003e5. Nutlope/self.so: Build your personal site with AI in seconds âš¡\u003ca href=\"#5-nutlope-self-so-build-your-personal-site-with-ai-in-seconds-%e2%9a%a1\" aria-label=\"5. Nutlope/self.so: Build your personal site with AI in seconds âš¡\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cdiv\u003e \t\t\t\n\u003cp\u003e\u003ca href=\"https://github.blog/wp-content/uploads/2025/04/LinkedIn-to-Website.png\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/04/LinkedIn-to-Website.png?resize=1024%2C538\" alt=\"A screenshot of Self.so turning a LinkedIn profile into a website.\" width=\"1024\" height=\"538\" srcset=\"https://github.blog/wp-content/uploads/2025/04/LinkedIn-to-Website.png?w=1200 1200w, https://github.blog/wp-content/uploads/2025/04/LinkedIn-to-Website.png?w=300 300w, https://github.blog/wp-content/uploads/2025/04/LinkedIn-to-Website.png?w=768 768w, https://github.blog/wp-content/uploads/2025/04/LinkedIn-to-Website.png?w=1024 1024w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ MIT license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIf putting together a personal website isnâ€™t your idea of fun, Nutlope/self.so can lend a hand. Upload your rÃ©sumÃ© or LinkedIn profile, and the tool will pull together a straightforward site for you, using AI to handle the layout so you can skip the CSS headaches.\u003c/p\u003e\n\u003cp\u003eThe tech stack includes Together.ai for language modeling, Vercelâ€™s AI SDK, Clerk for auth, Next.js for the framework, Helicone for observability, S3 for storage, Upstash Redis for the database, and Vercel for hosting.\u003c/p\u003e\n\n\u003ch2 id=\"6-voicestar-precise-control-for-text-to-speech-applications-%f0%9f%8e%99%ef%b8%8f\" id=\"6-voicestar-precise-control-for-text-to-speech-applications-%f0%9f%8e%99%ef%b8%8f\"\u003e6. VoiceStar: precise control for text-to-speech applications ğŸ™ï¸\u003ca href=\"#6-voicestar-precise-control-for-text-to-speech-applications-%f0%9f%8e%99%ef%b8%8f\" aria-label=\"6. VoiceStar: precise control for text-to-speech applications ğŸ™ï¸\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ MIT code license, CC-BY-4.0 model license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIf your project needs speech that lands within a specific time window, VoiceStarâ€™s duration-controllable synthesis can help. It lets developers set target lengths so voice output fits time-sensitive use casesâ€”like fixed-length prompts or narrationâ€”without extra audio editing.\u003c/p\u003e\n\u003cp\u003eThe project includes both CLI and Gradio interfaces for inference, plus pre-trained models you can use right away. As voice interfaces become more important in apps, having open source models with this level of control is a helpful step forward\u003c/p\u003e\n\n\u003ch2 id=\"7-create-your-digital-twin-with-second-me-%f0%9f%a4%96\" id=\"7-create-your-digital-twin-with-second-me-%f0%9f%a4%96\"\u003e7. Create your digital twin with Second-Me ğŸ¤–\u003ca href=\"#7-create-your-digital-twin-with-second-me-%f0%9f%a4%96\" aria-label=\"7. Create your digital twin with Second-Me ğŸ¤–\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cdiv\u003e \t\t\t\n\u003cp\u003e\u003ca href=\"https://github.blog/wp-content/uploads/2025/04/secondme.png\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" loading=\"lazy\" src=\"https://github.blog/wp-content/uploads/2025/04/secondme.png?resize=1024%2C500\" alt=\"A screenshot of Second Me.\" width=\"1024\" height=\"500\" srcset=\"https://github.blog/wp-content/uploads/2025/04/secondme.png?w=2940 2940w, https://github.blog/wp-content/uploads/2025/04/secondme.png?w=300 300w, https://github.blog/wp-content/uploads/2025/04/secondme.png?w=768 768w, https://github.blog/wp-content/uploads/2025/04/secondme.png?w=1024 1024w, https://github.blog/wp-content/uploads/2025/04/secondme.png?w=1536 1536w, https://github.blog/wp-content/uploads/2025/04/secondme.png?w=2048 2048w\" sizes=\"auto, (max-width: 1000px) 100vw, 1000px\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ Apache 2.0 license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eInterested in experimenting with an AI stand-in? Second-Me lets you try a basic â€œdigital twinâ€â€”an agent that aims to reflect some of your knowledge, communication style, and preferences.\u003c/p\u003e\n\u003cp\u003eThe possibilities range from personal assistants that actually understand how you think to innovative ways to share your expertise with others. One example of Second-Me in action: Have \u003ca href=\"https://secondme.gitbook.io/secondme/getting-started#second-x-apps\"\u003eyour digital twin manage your LinkedIn or Airbnb account\u003c/a\u003e, playing the role of the professional or host.\u003c/p\u003e\n\n\u003ch2 id=\"8-sesameailabs-csm-reimagining-speech-synthesis-%f0%9f%94%8a\" id=\"8-sesameailabs-csm-reimagining-speech-synthesis-%f0%9f%94%8a\"\u003e8. SesameAILabs/csm: reimagining speech synthesis ğŸ”Š\u003ca href=\"#8-sesameailabs-csm-reimagining-speech-synthesis-%f0%9f%94%8a\" aria-label=\"8. SesameAILabs/csm: reimagining speech synthesis ğŸ”Š\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ Apache 2.0 code license (model has \u003ca href=\"https://github.com/SesameAILabs/csm?tab=readme-ov-file#misuse-and-abuse-%EF%B8%8F\"\u003erestrictions\u003c/a\u003e on Abuse)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe Conversational Speech Model (CSM) brings a fresh approach to speech generation. It converts text and audio inputs into Residual Vector Quantization (RVQ) audio codes using a Llama-based architecture. Its dedicated audio decoder produces Mimi audio codes that result in surprisingly natural-sounding speech.\u003c/p\u003e\n\u003cp\u003eWhatâ€™s interesting here is how CSM merges language model architecture with specialized audio decodingâ€”giving you an open alternative to the proprietary text-to-speech options that dominate the market.\u003c/p\u003e\n\n\u003ch2 id=\"9-letta-a-universal-standard-for-portable-ai-agents-%f0%9f%93%a6\" id=\"9-letta-a-universal-standard-for-portable-ai-agents-%f0%9f%93%a6\"\u003e9. Letta: a universal standard for portable AI agents ğŸ“¦\u003ca href=\"#9-letta-a-universal-standard-for-portable-ai-agents-%f0%9f%93%a6\" aria-label=\"9. Letta: a universal standard for portable AI agents ğŸ“¦\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ Apache 2.0 license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eLetta introduces an open file format (.af) for packaging up AI agents with their memory and behavior intact. Think of it as a portable container for agents. You can share, checkpoint, and version control them across different frameworks.\u003c/p\u003e\n\u003cp\u003eFor developers juggling multiple agent frameworks, this could be a time-saver. Want to move an agent from one system to another without rebuilding it from scratch? Thatâ€™s the problem Letta is solving.\u003c/p\u003e\n\u003cp\u003eNotably, the Letta project is an offshoot of \u003ca href=\"https://github.com/cpacker/memgpt\"\u003ethe cpacker/memgpt project\u003c/a\u003eâ€”it spun out the serialization layer that MemGPT originally used to snapshot its â€œvirtual-contextâ€ agents. The team carved that code into a clean, framework-agnostic spec (agent-file) so any stackâ€”MemGPT/Letta, LangGraph, CrewAI, you name itâ€”can import or export a fully stateful agent with a single .af archive.\u003c/p\u003e\n\n\u003ch2 id=\"10-blender-meets-claude-bridging-3d-creation-and-ai-%f0%9f%8e%a8\" id=\"10-blender-meets-claude-bridging-3d-creation-and-ai-%f0%9f%8e%a8\"\u003e10. Blender meets Claude: bridging 3D creation and AI ğŸ¨\u003ca href=\"#10-blender-meets-claude-bridging-3d-creation-and-ai-%f0%9f%8e%a8\" aria-label=\"10. Blender meets Claude: bridging 3D creation and AI ğŸ¨\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\n\n\u003cp\u003e\u003cstrong\u003eğŸ“œ MIT license\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBlender artists, this oneâ€™s for you: a third party tool that connects the popular open source 3D creation suite Blender with Claude AI through the MCP. With Blender-MCP, developers can control Blender operations with natural languageâ€”or add AI assistance to their 3D workflow.\u003c/p\u003e\n\u003cp\u003eBlender-MCP shows how the \u003cstrong\u003eMCP can act as a universal â€œtool portâ€\u003c/strong\u003e for LLM agents: today itâ€™s Blender; tomorrow it could be Unity, Unreal, or any complex desktop app. For 3D artists and prototypers, that means faster scene blocking, easy style experiments, and a brand-new way to teach beginners. Just describe what you want and watch the software build it.\u003c/p\u003e\n\u003cp\u003eInterested? Installing Blender-MCP is as simple as running a bash command.\u003c/p\u003e\n\n\u003ch2 id=\"what-these-projects-tell-us-about-ais-evolution-in-open-source\" id=\"what-these-projects-tell-us-about-ais-evolution-in-open-source\"\u003eWhat these projects tell us about AIâ€™s evolution in open source\u003ca href=\"#what-these-projects-tell-us-about-ais-evolution-in-open-source\" aria-label=\"What these projects tell us about AIâ€™s evolution in open source\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThese patterns not only reflect the current state of AI in open source, but also hint at the challenges and opportunities that lie ahead. Hereâ€™s what GitHub experts have to say about the rapidly evolving space:\u003c/p\u003e\n\u003ch3 id=\"integration-in-ai-via-mcp-is-the-new-frontier-%f0%9f%94%97\" id=\"integration-in-ai-via-mcp-is-the-new-frontier-%f0%9f%94%97\"\u003eIntegration in AI via MCP is the new frontier ğŸ”—\u003ca href=\"#integration-in-ai-via-mcp-is-the-new-frontier-%f0%9f%94%97\" aria-label=\"Integration in AI via MCP is the new frontier ğŸ”—\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe prominence of MCP across multiple projects highlights the growing importance of standardized integration patterns in AI development.\u003c/p\u003e\n\u003cp\u003eâ€œA big pattern that I saw is the pain point around AI and integration,â€ notes Abigail. â€œMore standards like MCP will help with this.â€\u003c/p\u003e\n\u003ch3 id=\"multi-agent-collaboration-emerges-%f0%9f%91%a5\" id=\"multi-agent-collaboration-emerges-%f0%9f%91%a5\"\u003eMulti-agent collaboration emerges ğŸ‘¥\u003ca href=\"#multi-agent-collaboration-emerges-%f0%9f%91%a5\" aria-label=\"Multi-agent collaboration emerges ğŸ‘¥\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eProjects like OWL point to a future where multiple specialized AI agents work together to solve complex problems.\u003c/p\u003e\n\u003cp\u003eâ€œYou have to think about it in the construct of person to person, agent to agent, and then having multiple agents working in tandem,â€ explains Kevin, highlighting the complexity and potential of this approach.\u003c/p\u003e\n\u003ch3 id=\"speech-generation-is-advancing-%f0%9f%97%a3%ef%b8%8f\" id=\"speech-generation-is-advancing-%f0%9f%97%a3%ef%b8%8f\"\u003eSpeech generation is advancing ğŸ—£ï¸\u003ca href=\"#speech-generation-is-advancing-%f0%9f%97%a3%ef%b8%8f\" aria-label=\"Speech generation is advancing ğŸ—£ï¸\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSpeech tech certainly isnâ€™t newâ€”but large language models are reshaping both \u003cstrong\u003etext-to-speech (TTS)\u003c/strong\u003e and \u003cstrong\u003espeech-to-text (STT)\u003c/strong\u003e so dramatically that a fresh wave of possibilities is opening up.\u003c/p\u003e\n\u003cp\u003eThe bigger story is what this means downstream with implications across media, customer support, and product UX.\u003c/p\u003e\n\u003ch3 id=\"the-evolving-landscape-of-open-source-participation-%f0%9f%8c%b1\" id=\"the-evolving-landscape-of-open-source-participation-%f0%9f%8c%b1\"\u003eThe evolving landscape of open source participation ğŸŒ±\u003ca href=\"#the-evolving-landscape-of-open-source-participation-%f0%9f%8c%b1\" aria-label=\"The evolving landscape of open source participation ğŸŒ±\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAI has drawn a fresh wave of maintainers and contributors to open source, bringing new energy and approaches. Kara Deloss, senior program manager of developer relations at GitHub, notes: â€œWeâ€™re seeing a new generation, or a new type of maintainerâ€ in the AI space. Kevin adds that â€œif you have a big community on day one, thatâ€™s valuable,â€ highlighting how the ecosystem is evolving.\u003c/p\u003e\n\u003cp\u003eThis blend of established and emerging development practices is creating exciting opportunities for collaboration across the community.\u003c/p\u003e\n\u003ch3 id=\"the-importance-of-osi-approved-licenses-%f0%9f%93%84\" id=\"the-importance-of-osi-approved-licenses-%f0%9f%93%84\"\u003eThe importance of OSI-approved licenses ğŸ“„\u003ca href=\"#the-importance-of-osi-approved-licenses-%f0%9f%93%84\" aria-label=\"The importance of OSI-approved licenses ğŸ“„\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eEvery top project in our list uses OSI-approved licenses (mostly MIT and Apache 2.0), and thatâ€™s no accident. â€œIn general, you wonâ€™t get a lot of positive sentiment from the community if you call yourself open source but arenâ€™t using an OSI-approved license,â€ Jeff points out. â€œThese licenses matter because they provide clear guarantees around usage, modification, and redistribution rights that build trust in the community.â€\u003c/p\u003e\n\u003cp\u003eJeff also notes an emerging challenge: â€œAs AI powered services and tools become more powerful, we are seeing a trend where some projects attach Abuse and Fraud related restrictions on model or service use. This may not make them completely open source under an OSI approved license, and the projects and the community will continue to have an intense conversation about these conditions.â€\u003c/p\u003e\n\u003cp\u003eHe continues: â€œItâ€™s important to understand and document any restrictions in place before using a model or service,â€  which is a timely reminder as the open source AI community navigates these evolving licensing questions.\u003c/p\u003e\n\n\u003cp\u003eThe projects weâ€™ve highlighted here are just the tip of the iceberg. As AI keeps evolving, the open source ecosystem is where many of the most exciting standards, tools, and techniques are popping up first.\u003c/p\u003e\n\u003cp\u003eHereâ€™s how to get involved:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCheck out these projects to see how they might fit into your workflow  \u003c/li\u003e\n\u003cli\u003eJoin in and contribute to projects that spark your interest  \u003c/li\u003e\n\u003cli\u003eKeep an eye on MCP and other emerging standards\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\t\n\u003csection\u003e\n\t\u003chr/\u003e\n\t\u003cdiv\u003e\n\t\t\u003ch2\u003eTags:\u003c/h2\u003e\n\t\t\u003cul\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/ai-agents/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tAI agents\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/generative-ai/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tgenerative AI\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/mcp/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tMCP\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/octoverse/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tOctoverse\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/open-source/\" rel=\"tag\"\u003e\n\t\t\t\t\t\topen source\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\t\t\u003cli\u003e\n\t\t\t\t\t\u003ca href=\"https://github.blog/tag/research/\" rel=\"tag\"\u003e\n\t\t\t\t\t\tresearch\t\t\t\t\t\u003c/a\u003e\n\t\t\t\t\u003c/li\u003e\n\t\t\t\t\t\u003c/ul\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/94011710?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/94011710?v=4\u0026amp;s=200\" alt=\"Jeimy Ruiz\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003c/div\u003e",
  "readingTime": "12 min read",
  "publishedTime": "2025-04-30T16:00:49Z",
  "modifiedTime": "2025-05-01T00:25:34Z"
}
