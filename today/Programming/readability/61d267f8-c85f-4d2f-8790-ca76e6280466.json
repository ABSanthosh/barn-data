{
  "id": "61d267f8-c85f-4d2f-8790-ca76e6280466",
  "title": "University of Chinese Academy of Sciences Open-Sources Multimodal LLM LLaMA-Omni",
  "link": "https://www.infoq.com/news/2024/10/llama-omni/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Researchers at the University of Chinese Academy of Sciences (UCAS) recently open-sourced LLaMA-Omni, an LLM that can operate on both speech and text data. LLaMA-Omni is based on Meta's Llama-3.1-8B-Instruct LLM and outperforms similar baseline models while requiring less training data and compute. By Anthony Alford",
  "author": "Anthony Alford",
  "published": "Tue, 08 Oct 2024 13:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Generative AI",
    "Large language models",
    "AI, ML \u0026 Data Engineering",
    "news"
  ],
  "byline": "Anthony Alford",
  "length": 3562,
  "excerpt": "Researchers at the University of Chinese Academy of Sciences (UCAS) recently open-sourced LLaMA-Omni, an LLM that can operate on both speech and text data. LLaMA-Omni is based on Meta's Llama-3.1-8B-I",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s1_20241001113528/apple-touch-icon.png",
  "text": "Researchers at the University of Chinese Academy of Sciences (UCAS) recently open-sourced LLaMA-Omni, an LLM that can operate on both speech and text data. LLaMA-Omni is based on Meta's Llama-3.1-8B-Instruct LLM and outperforms similar baseline models while requiring less training data and compute. The LLaMa-Omni architecture extends Llama-3 by including a speech encoder at the input and a speech decoder at the output. Compared to other schemes where standalone speech recognition (SR) and text-to-speech (TTS) modules are used in series with an LLM, this architecture reduces the latency between an input speech prompt and output speech generation. The model is fine-tuned on InstructS2S-200K, a custom dataset created by the UCAS team, which has 200 thousand speech prompts and their expected speech replies. According to the researchers: Experimental results show that, compared to [baseline] speech-language models, LLaMA-Omni delivers superior responses in both content and style, with a response latency as low as 226ms. Moreover, training LLaMA-Omni requires less than 3 days on 4 GPUs, enabling rapid development of speech interaction models based on the latest LLMs. In the future, we plan to explore enhancing the expressiveness of generated speech responses and improving real-time interaction capabilities. The research team evaluated LLaMa-Omni's performance on two tasks: speech-to-text instruction-following (S2TIF) and speech-to-speech instruction-following (S2SIF), and compared it to other baseline models, including Qwen2-Audio. The evaluation dataset was a subset of Alpaca-Eval, with a total of 199 prompts; the team also fed the prompts into a TTS system to generate speech-based prompts. The team used GPT-4o to automatically score each model's output, judging it on content (whether the output achieves the user's instruction) and style (whether the output is suited for speech interaction). On the S2TIF task, LLaMA-Omni outperformed the baselines on style, and on the S2SIF task, it outperformed on both content and style. In a discussion about LLaMa-Omni on Hacker News, one user pointed out the benefits of an end-to-end model for speech and text, vs a cascaded system of standalone components: Essentially, there's data loss from audio -\u003e text. Sometimes that loss is unimportant, but sometimes it meaningfully improves output quality. However, there are some other potential fringe benefits here: improving the latency of replies, improving speaker diarization, and reacting to pauses better for conversations. Users on Reddit also commented on the model, especially its use of OpenAI's Whisper model for speech encoding: [T]heir input approach is similar to how LLaVA added image understanding by training a glue layer for Llama and CLIP. LLaMA-Omni takes whisper as their encoder like LLaVA takes CLIP. Then the embeddings are projected into the feature space of their underlying Llama model. I didn't immediately understand their voice output architecture so I can't comment on that. The integration of speech I/O into LLMs is a growing trend. Earlier this year, InfoQ covered the release of OpenAI's GPT-4 omni, which is a version of GTP-4 that is trained end-to-end to handle speech data. InfoQ also covered Alibaba's open-weight Qwen2-Audio, which can handle speech input but only outputs text. The LLaMa-Omni model files are available on Huggingface. About the Author Anthony Alford",
  "image": "https://res.infoq.com/news/2024/10/llama-omni/en/headerimage/generatedHeaderImage-1727531829012.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eResearchers at the \u003ca href=\"https://english.ucas.ac.cn/\"\u003eUniversity of Chinese Academy of Sciences\u003c/a\u003e (UCAS) recently open-sourced \u003ca href=\"https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni\"\u003eLLaMA-Omni\u003c/a\u003e, an LLM that can operate on both speech and text data. LLaMA-Omni is based on Meta\u0026#39;s \u003ca href=\"https://www.infoq.com/news/2024/05/meta-llama-3/\"\u003eLlama-3.1-8B-Instruct\u003c/a\u003e LLM and outperforms similar baseline models while requiring less training data and compute.\u003c/p\u003e\n\n\u003cp\u003eThe LLaMa-Omni architecture extends Llama-3 by including a speech encoder at the input and a speech decoder at the output. Compared to other schemes where standalone speech recognition (SR) and text-to-speech (TTS) modules are used in series with an LLM, this architecture reduces the latency between an input speech prompt and output speech generation. The model is fine-tuned on InstructS2S-200K, a custom dataset created by the UCAS team, which has 200 thousand speech prompts and their expected speech replies. According to the researchers:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eExperimental results show that, compared to [baseline] speech-language models, LLaMA-Omni delivers superior responses in both content and style, with a response latency as low as 226ms. Moreover, training LLaMA-Omni requires less than 3 days on 4 GPUs, enabling rapid development of speech interaction models based on the latest LLMs. In the future, we plan to explore enhancing the expressiveness of generated speech responses and improving real-time interaction capabilities.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe research team evaluated LLaMa-Omni\u0026#39;s performance on two tasks: speech-to-text instruction-following (S2TIF) and speech-to-speech instruction-following (S2SIF), and compared it to other baseline models, including \u003ca href=\"https://www.infoq.com/news/2024/09/alibaba-qwen2-models/\"\u003eQwen2-Audio\u003c/a\u003e. The evaluation dataset was a subset of \u003ca href=\"https://github.com/tatsu-lab/alpaca_eval\"\u003eAlpaca-Eval\u003c/a\u003e, with a total of 199 prompts; the team also fed the prompts into a TTS system to generate speech-based prompts.\u003c/p\u003e\n\n\u003cp\u003eThe team used GPT-4o to automatically score each model\u0026#39;s output, judging it on content (whether the output achieves the user\u0026#39;s instruction) and style (whether the output is suited for speech interaction). On the S2TIF task, LLaMA-Omni outperformed the baselines on style, and on the S2SIF task, it outperformed on both content and style.\u003c/p\u003e\n\n\u003cp\u003eIn a \u003ca href=\"https://news.ycombinator.com/item?id=41582180\"\u003ediscussion about LLaMa-Omni\u003c/a\u003e on Hacker News, one user pointed out the benefits of an end-to-end model for speech and text, vs a cascaded system of standalone components:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eEssentially, there\u0026#39;s data loss from audio -\u0026gt; text. Sometimes that loss is unimportant, but sometimes it meaningfully improves output quality. However, there are some other potential fringe benefits here: improving the latency of replies, improving speaker diarization, and reacting to pauses better for conversations.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eUsers on Reddit also \u003ca href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fe7owt/llamaomni_seamless_speech_interaction_with_large/\"\u003ecommented on the model\u003c/a\u003e, especially its use of \u003ca href=\"https://www.infoq.com/news/2022/10/openai-whisper-speech/\"\u003eOpenAI\u0026#39;s Whisper\u003c/a\u003e model for speech encoding:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e[T]heir input approach is similar to how \u003ca href=\"https://github.com/haotian-liu/LLaVA\"\u003eLLaVA\u003c/a\u003e added image understanding by training a glue layer for Llama and \u003ca href=\"https://github.com/openai/CLIP\"\u003eCLIP\u003c/a\u003e. LLaMA-Omni takes whisper as their encoder like LLaVA takes CLIP. Then the embeddings are projected into the feature space of their underlying Llama model. I didn\u0026#39;t immediately understand their voice output architecture so I can\u0026#39;t comment on that.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe integration of speech I/O into LLMs is a growing trend. Earlier this year, InfoQ covered the release of OpenAI\u0026#39;s \u003ca href=\"https://www.infoq.com/news/2024/05/openai-gpt4o/\"\u003eGPT-4 omni\u003c/a\u003e, which is a version of GTP-4 that is trained end-to-end to handle speech data. InfoQ also covered Alibaba\u0026#39;s open-weight \u003ca href=\"https://www.infoq.com/news/2024/09/alibaba-qwen2-models/\"\u003eQwen2-Audio\u003c/a\u003e, which can handle speech input but only outputs text.\u003c/p\u003e\n\n\u003cp\u003eThe LLaMa-Omni model files are available on Huggingface.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Anthony-Alford\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eAnthony Alford\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-10-08T00:00:00Z",
  "modifiedTime": null
}
