{
  "id": "12a5b776-8310-40d5-be12-6ba4235a97df",
  "title": "Breaking down CPU speed: How utilization impacts performance",
  "link": "https://github.blog/engineering/architecture-optimization/breaking-down-cpu-speed-how-utilization-impacts-performance/",
  "description": "The Performance Engineering team at GitHub assessed how CPU performance degrades as utilization increases and how this relates to capacity. The post Breaking down CPU speed: How utilization impacts performance appeared first on The GitHub Blog.",
  "author": "Andreas Strikos",
  "published": "Mon, 25 Nov 2024 17:00:20 +0000",
  "source": "https://github.blog/feed/",
  "categories": [
    "Architecture \u0026 optimization",
    "Engineering",
    "performance engineering"
  ],
  "byline": "Andreas Strikos",
  "length": 11358,
  "excerpt": "The Performance Engineering team at GitHub assessed how CPU performance degrades as utilization increases and how this relates to capacity.",
  "siteName": "The GitHub Blog",
  "favicon": "https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=192%2C192",
  "text": "Introduction ‚õµ The GitHub Performance Engineering team regularly conducts experiments to observe how our systems perform under varying load conditions. A consistent pattern in these experiments is the significant impact of CPU utilization on system performance. We‚Äôve observed that as CPU utilization rises, it can lead to increased latency, which provides an opportunity to optimize system efficiency. Addressing this challenge allows us to maintain performance levels while reducing the need for additional machines, ultimately preventing inefficiencies. Although we recognized the correlation between higher CPU utilization and increased latency, we saw an opportunity to explore the specific thresholds and impacts at various stages in greater detail. With a diverse set of instance types powered by different CPU families, we focused on understanding the unique performance characteristics of each CPU model. This deeper insight empowered us to make smarter, data-driven decisions, enabling us to provision our infrastructure with greater efficiency and confidence. With these goals in mind, we embarked on a new journey of exploration and experimentation to uncover these insights. Experiment setup üß∞ Collecting accurate data for this type of experiment was no easy feat. We needed to gather data from workloads that were as close to our production as possible, while also capturing how the system behaves under different phases of load. Since CPU usage patterns vary across workloads, we focused primarily on our flagship workloads. However, increasing the load could introduce small performance discrepancies, so our goal was to minimize disruption for our users. Fortunately, a year ago, the Performance Engineering team developed an environment designed to meet these requirements, codenamed Large Unicorn Collider (LUC). This environment operates within a small portion of our Kubernetes clusters, mirroring the same architecture and configuration as our flagship workloads. It also has the flexibility to be hosted on dedicated machines, preventing interference from or with other workloads. Typically, the LUC environment remains idle, but when needed, we can direct a small, adjustable amount of traffic towards it. Activating or deactivating this traffic takes only seconds, allowing us to react quickly if performance concerns arise. To accurately assess the impact of CPU utilization, we first established a baseline by sending moderate production traffic to a LUC Kubernetes pod hosted on one of its dedicated machines. This provided us with a benchmark for comparison. Importantly, the number of requests handled by the LUC pods remained constant throughout the experiment, ensuring consistent CPU load over time. Once the baseline was set, we gradually increased CPU utilization using a tool called ‚Äústress,‚Äù which artificially occupies a specified number of CPU cores by running random processing tasks. Each instance type has a different number of CPU cores, so we adjusted the steps accordingly. However, the common factor across all instances was the total CPU utilization. Note: It‚Äôs important to recognize that this is not a direct 1:1 comparison to the load generated by actual production workloads. The stress tool continuously runs mathematical operations, while our production workloads involve I/O operations and interrupts, which place different demands on system resources. Nevertheless, this approach still offers valuable insights into how our CPUs perform under load. With the environment set up and our plan in place, we proceeded to collect as much data as possible to analyze the impact. Results üìÉ With our experiment setup finalized, let‚Äôs examine the data we gathered. As previously mentioned, we repeated the process across different instance types. Each instance type showed unique behavior and varying thresholds where performance started to decline. As anticipated, CPU time increased for all instance types as CPU utilization rose. The graph below illustrates the CPU time per request as CPU utilization increases. CPU time per request vs CPU utilization The latency differences between instance types are expected due to the variations in CPU models. Focusing on the percentage increase in latency may provide more meaningful insights. Latency percentage increase vs CPU utilization In both graphs, one line stands out by deviating more than the others. We‚Äôll examine this case in detail shortly. Turbo Boost effect An interesting observation is how CPU frequency changes as utilization increases, which can be attributed to Intel‚Äôs Turbo Boost Technology. Since all the instances we used are equipped with Intel CPUs, the impact of Turbo Boost is noticeable across all of them. In the graph below, you can see how the CPU frequency decreases as the CPU utilization increases. The red arrows are showing the CPU utilization level. CPU Cores Frequency When CPU utilization remains at lower levels (around 30% or below), we benefit from increased core frequencies, leading to faster CPU times and, consequently, lower overall latency. However, as the demand for more CPU cores rises and utilization increases, we are likely to reach the CPU‚Äôs thermal and power limits, causing frequencies to decrease. In essence, lower CPU utilization results in better performance, while higher utilization leads to a decline in performance. For instance, a workload running on a specific node with approximately 30% CPU utilization will report faster response times compared to the same workload on the same VM when CPU utilization exceeds 50%. Hyper-Threading Variations in CPU frequency are not the only factors influencing performance changes. All our nodes have Hyper-Threading enabled, an Intel technology that allows a single physical CPU core to operate as two virtual cores. Although there is only one physical core, the Linux kernel recognizes it as two virtual CPU cores. The kernel attempts to distribute the CPU load across these cores, aiming to keep only one hardware thread (virtual core) busy per physical core. This approach is effective until we reach a certain level of CPU utilization. Beyond this threshold, we cannot fully utilize both virtual CPU cores, resulting in reduced performance compared to normal operation. Finding the ‚ÄúGolden Ratio‚Äù of CPU utilization Underutilized nodes lead to wasted resources, power, and space in our data centers, while nodes that are excessively utilized also create inefficiencies. As noted, higher CPU utilization results in decreased performance, which can give a misleading impression that additional resources are necessary, resulting in a cycle of over-provisioning. This issue is particularly pronounced with blocking workloads that do not follow an asynchronous model. As CPU performance deteriorates, each process can manage fewer tasks per second, making existing capacity inadequate. To achieve the optimal balance‚Äîthe ‚ÄúGolden Ratio‚Äù of CPU utilization‚Äîwe must identify a threshold where CPU utilization is sufficiently high to ensure efficiency without significantly impairing performance. Striving to keep our nodes near this threshold will enable us to utilize our current hardware more effectively alongside our existing software. Since we already have experimental data demonstrating how CPU time increases with rising utilization, we can develop a mathematical model to identify this threshold. First, we need to determine what percentage of CPU time degradation is acceptable for our specific use case. This may depend on user expectations or performance Service Level Agreements (SLAs). Once we establish this threshold, it will help us select a level of CPU utilization that remains within acceptable limits. We can plot the CPU utilization vs. CPU time (latency) and find the point where: CPU utilization is high enough to avoid resource underutilization. CPU time degradation does not exceed your acceptable limit. A specific example derived from the data above can be illustrated in the following graph. Percentage Increase in P50 Latency vs CPU Utilization In this example, we aim to achieve less than 40% CPU time degradation, which would correspond to a CPU utilization of 61% on the specific instance. Outlier case As previously mentioned, there was a specific instance that displayed some outlying data points. Our experiment confirmed an already recognized issue where certain instances were not achieving their advertised maximum Turbo Boost CPU frequency. Instead, we observed steady CPU frequencies that fell below the maximum advertised value under low CPU utilization. In the example below, you can see an instance from a CPU family that advertises Turbo Boost frequencies above 3 GHz, but it is only reporting a maximum CPU frequency of 2.8 GHz. CPU cores frequency This issue turned out to be caused by a disabled CPU C-state, which prevented the CPU cores from halting even when they were not in use. As a result, these cores were perceived as ‚Äúbusy‚Äù by the turbo driver, limiting our ability to take advantage of Turbo Boost benefits with higher CPU frequencies. By enabling the C-state and allowing for optimization and power reduction during idle mode, we observed the expected Turbo Boost behavior. This change had an immediate impact on the CPU time spent by our test workloads. The images below illustrate the prompt changes in CPU frequencies and latency reported following the C-state adjustment. CPU cores frequency P50 CPU time on a request Upon re-evaluating the percentage change in CPU time, we now observe similar behavior across all instances. Percentage Increase in P50 Latency vs CPU Utilization Wrap-up As we anticipated many of these insights, our objective was to validate our theories using data from our complex system. While we confirmed that performance lowers as CPU utilization increases across different CPU families, by identifying optimal CPU utilization thresholds, we can achieve a better balance between performance and efficiency, ensuring that our infrastructure remains both cost-effective and high performing. Going forward, these insights will inform us of our resource provisioning strategies and help us maximize the effectiveness of our hardware investments. Thank you for sticking with us until the end!! A special shout-out to @adrmike, @schlubbi, @terrorobe, the @github/compute-platform and finally the @github/performance-engineering team for their invaluable assistance throughout these experiments, data analysis, and for reviewing the content for accuracy and consistency. ‚ù§Ô∏è Written by Andreas Strikos is a Senior Software Engineer at GitHub working on the Performance Engineering team. Explore more from GitHub Docs Everything you need to master GitHub, all in one place. Go to Docs GitHub Build what‚Äôs next on GitHub, the place for anyone from anywhere to build anything. Start building Customer stories Meet the companies and engineering teams that build with GitHub. Learn more GitHub Universe 2024 Get tickets to the 10th anniversary of our global developer event on AI, DevEx, and security. Get tickets",
  "image": "https://github.blog/wp-content/uploads/2024/01/Productivity-DarkMode-3.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\n\u003ch2 id=\"introduction-%e2%9b%b5\" id=\"introduction-%e2%9b%b5\"\u003eIntroduction ‚õµ\u003ca href=\"#introduction-%e2%9b%b5\" aria-label=\"Introduction ‚õµ\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe GitHub Performance Engineering team regularly conducts experiments to observe how our systems perform under varying load conditions. A consistent pattern in these experiments is the significant impact of CPU utilization on system performance. We‚Äôve observed that as CPU utilization rises, it can lead to increased latency, which provides an opportunity to optimize system efficiency. Addressing this challenge allows us to maintain performance levels while reducing the need for additional machines, ultimately preventing inefficiencies.\u003c/p\u003e\n\u003cp\u003eAlthough we recognized the correlation between higher CPU utilization and increased latency, we saw an opportunity to explore the specific thresholds and impacts at various stages in greater detail. With a diverse set of instance types powered by different CPU families, we focused on understanding the unique performance characteristics of each CPU model. This deeper insight empowered us to make smarter, data-driven decisions, enabling us to provision our infrastructure with greater efficiency and confidence.\u003c/p\u003e\n\u003cp\u003eWith these goals in mind, we embarked on a new journey of exploration and experimentation to uncover these insights.\u003c/p\u003e\n\u003ch2 id=\"experiment-setup-%f0%9f%a7%b0\" id=\"experiment-setup-%f0%9f%a7%b0\"\u003eExperiment setup üß∞\u003ca href=\"#experiment-setup-%f0%9f%a7%b0\" aria-label=\"Experiment setup üß∞\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eCollecting accurate data for this type of experiment was no easy feat. We needed to gather data from workloads that were as close to our production as possible, while also capturing how the system behaves under different phases of load. Since CPU usage patterns vary across workloads, we focused primarily on our flagship workloads. However, increasing the load could introduce small performance discrepancies, so our goal was to minimize disruption for our users.\u003c/p\u003e\n\u003cp\u003eFortunately, a year ago, the Performance Engineering team developed an environment designed to meet these requirements, codenamed Large Unicorn Collider (LUC). This environment operates within a small portion of our Kubernetes clusters, mirroring the same architecture and configuration as our flagship workloads. It also has the flexibility to be hosted on dedicated machines, preventing interference from or with other workloads. Typically, the LUC environment remains idle, but when needed, we can direct a small, adjustable amount of traffic towards it. Activating or deactivating this traffic takes only seconds, allowing us to react quickly if performance concerns arise.\u003c/p\u003e\n\u003cp\u003eTo accurately assess the impact of CPU utilization, we first established a baseline by sending moderate production traffic to a LUC Kubernetes pod hosted on one of its dedicated machines. This provided us with a benchmark for comparison. Importantly, the number of requests handled by the LUC pods remained constant throughout the experiment, ensuring consistent CPU load over time.\u003c/p\u003e\n\u003cp\u003eOnce the baseline was set, we gradually increased CPU utilization using a tool called ‚Äú\u003ca href=\"https://linux.die.net/man/1/stress\"\u003estress\u003c/a\u003e,‚Äù which artificially occupies a specified number of CPU cores by running random processing tasks. Each instance type has a different number of CPU cores, so we adjusted the steps accordingly. However, the common factor across all instances was the total CPU utilization.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote: It‚Äôs important to recognize that this is not a direct 1:1 comparison to the load generated by actual production workloads. The stress tool continuously runs mathematical operations, while our production workloads involve I/O operations and interrupts, which place different demands on system resources. Nevertheless, this approach still offers valuable insights into how our CPUs perform under load.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWith the environment set up and our plan in place, we proceeded to collect as much data as possible to analyze the impact.\u003c/p\u003e\n\u003ch2 id=\"results-%f0%9f%93%83\" id=\"results-%f0%9f%93%83\"\u003eResults üìÉ\u003ca href=\"#results-%f0%9f%93%83\" aria-label=\"Results üìÉ\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWith our experiment setup finalized, let‚Äôs examine the data we gathered. As previously mentioned, we repeated the process across different instance types. Each instance type showed unique behavior and varying thresholds where performance started to decline.\u003c/p\u003e\n\u003cp\u003eAs anticipated, CPU time increased for all instance types as CPU utilization rose. The graph below illustrates the CPU time per request as CPU utilization increases.\u003c/p\u003e\n\u003cfigure id=\"attachment_81410\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"969\" src=\"https://github.blog/wp-content/uploads/2024/11/p50-CPI-time-vs-CPU-otpimization.png?w=1024\u0026amp;resize=1600%2C969\" alt=\"CPU time per request vs CPU utilization\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/p50-CPI-time-vs-CPU-otpimization.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/p50-CPI-time-vs-CPU-otpimization.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/p50-CPI-time-vs-CPU-otpimization.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/p50-CPI-time-vs-CPU-otpimization.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/p50-CPI-time-vs-CPU-otpimization.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eCPU time per request vs CPU utilization\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eThe latency differences between instance types are expected due to the variations in CPU models. Focusing on the percentage increase in latency may provide more meaningful insights.\u003c/p\u003e\n\u003cfigure id=\"attachment_81411\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"976\" src=\"https://github.blog/wp-content/uploads/2024/11/percent-increase.png?w=1024\u0026amp;resize=1600%2C976\" alt=\"Latency percentage increase vs CPU utilization\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/percent-increase.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/percent-increase.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/percent-increase.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/percent-increase.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/percent-increase.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eLatency percentage increase vs CPU utilization\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eIn both graphs, one line stands out by deviating more than the others. We‚Äôll examine this case in detail shortly.\u003c/p\u003e\n\u003ch3 id=\"turbo-boost-effect\" id=\"turbo-boost-effect\"\u003eTurbo Boost effect\u003ca href=\"#turbo-boost-effect\" aria-label=\"Turbo Boost effect\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAn interesting observation is how CPU frequency changes as utilization increases, which can be attributed to Intel‚Äôs Turbo Boost Technology. Since all the instances we used are equipped with Intel CPUs, the impact of Turbo Boost is noticeable across all of them. In the graph below, you can see how the CPU frequency decreases as the CPU utilization increases. The red arrows are showing the CPU utilization level.\u003c/p\u003e\n\u003cfigure id=\"attachment_81412\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"647\" src=\"https://github.blog/wp-content/uploads/2024/11/instance-CPU-frequency.png?w=1024\u0026amp;resize=1600%2C647\" alt=\"CPU Cores Frequency \" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/instance-CPU-frequency.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/instance-CPU-frequency.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/instance-CPU-frequency.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/instance-CPU-frequency.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/instance-CPU-frequency.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eCPU Cores Frequency\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eWhen CPU utilization remains at lower levels (around 30% or below), we benefit from increased core frequencies, leading to faster CPU times and, consequently, lower overall latency. However, as the demand for more CPU cores rises and utilization increases, we are likely to reach the CPU‚Äôs thermal and power limits, causing frequencies to decrease. In essence, lower CPU utilization results in better performance, while higher utilization leads to a decline in performance. For instance, a workload running on a specific node with approximately 30% CPU utilization will report faster response times compared to the same workload on the same VM when CPU utilization exceeds 50%.\u003c/p\u003e\n\u003ch3 id=\"hyper-threading\" id=\"hyper-threading\"\u003eHyper-Threading\u003ca href=\"#hyper-threading\" aria-label=\"Hyper-Threading\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eVariations in CPU frequency are not the only factors influencing performance changes. All our nodes have Hyper-Threading enabled, an Intel technology that allows a single physical CPU core to operate as two virtual cores. Although there is only one physical core, the Linux kernel recognizes it as two virtual CPU cores. The kernel attempts to distribute the CPU load across these cores, aiming to keep only one hardware thread (virtual core) busy per physical core. This approach is effective until we reach a certain level of CPU utilization. Beyond this threshold, we cannot fully utilize both virtual CPU cores, resulting in reduced performance compared to normal operation.\u003c/p\u003e\n\u003ch3 id=\"finding-the-golden-ratio-of-cpu-utilization\" id=\"finding-the-golden-ratio-of-cpu-utilization\"\u003eFinding the ‚ÄúGolden Ratio‚Äù of CPU utilization\u003ca href=\"#finding-the-golden-ratio-of-cpu-utilization\" aria-label=\"Finding the ‚ÄúGolden Ratio‚Äù of CPU utilization\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eUnderutilized nodes lead to wasted resources, power, and space in our data centers, while nodes that are excessively utilized also create inefficiencies. As noted, higher CPU utilization results in decreased performance, which can give a misleading impression that additional resources are necessary, resulting in a cycle of over-provisioning. This issue is particularly pronounced with blocking workloads that do not follow an asynchronous model. As CPU performance deteriorates, each process can manage fewer tasks per second, making existing capacity inadequate. To achieve the optimal balance‚Äîthe ‚ÄúGolden Ratio‚Äù of CPU utilization‚Äîwe must identify a threshold where CPU utilization is sufficiently high to ensure efficiency without significantly impairing performance. Striving to keep our nodes near this threshold will enable us to utilize our current hardware more effectively alongside our existing software.\u003c/p\u003e\n\u003cp\u003eSince we already have experimental data demonstrating how CPU time increases with rising utilization, we can develop a mathematical model to identify this threshold. First, we need to determine what percentage of CPU time degradation is acceptable for our specific use case. This may depend on user expectations or performance Service Level Agreements (SLAs). Once we establish this threshold, it will help us select a level of CPU utilization that remains within acceptable limits.\u003c/p\u003e\n\u003cp\u003eWe can plot the CPU utilization vs. CPU time (latency) and find the point where:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU utilization is high enough to avoid resource underutilization.\u003c/li\u003e\n\u003cli\u003eCPU time degradation does not exceed your acceptable limit.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA specific example derived from the data above can be illustrated in the following graph.\u003c/p\u003e\n\u003cfigure id=\"attachment_81413\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"1127\" src=\"https://github.blog/wp-content/uploads/2024/11/percent-increase-2.png?w=1024\u0026amp;resize=1600%2C1127\" alt=\"Percentage Increase in P50 Latency vs CPU Utilization\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/percent-increase-2.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/percent-increase-2.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/percent-increase-2.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/percent-increase-2.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/percent-increase-2.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003ePercentage Increase in P50 Latency vs CPU Utilization\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eIn this example, we aim to achieve less than 40% CPU time degradation, which would correspond to a CPU utilization of 61% on the specific instance.\u003c/p\u003e\n\u003ch3 id=\"outlier-case\" id=\"outlier-case\"\u003eOutlier case\u003ca href=\"#outlier-case\" aria-label=\"Outlier case\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAs previously mentioned, there was a specific instance that displayed some outlying data points. Our experiment confirmed an already recognized issue where certain instances were not achieving their advertised maximum Turbo Boost CPU frequency. Instead, we observed steady CPU frequencies that fell below the maximum advertised value under low CPU utilization. In the example below, you can see an instance from a CPU family that advertises Turbo Boost frequencies above 3 GHz, but it is only reporting a maximum CPU frequency of 2.8 GHz.\u003c/p\u003e\n\u003cfigure id=\"attachment_81414\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"588\" src=\"https://github.blog/wp-content/uploads/2024/11/CPU-cores-frequency.png?w=1024\u0026amp;resize=1600%2C588\" alt=\"CPU cores frequency\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/CPU-cores-frequency.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/CPU-cores-frequency.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/CPU-cores-frequency.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/CPU-cores-frequency.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/CPU-cores-frequency.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eCPU cores frequency\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eThis issue turned out to be caused by a disabled CPU C-state, which prevented the CPU cores from halting even when they were not in use. As a result, these cores were perceived as ‚Äúbusy‚Äù by the turbo driver, limiting our ability to take advantage of Turbo Boost benefits with higher CPU frequencies. By enabling the C-state and allowing for optimization and power reduction during idle mode, we observed the expected Turbo Boost behavior. This change had an immediate impact on the CPU time spent by our test workloads. The images below illustrate the prompt changes in CPU frequencies and latency reported following the C-state adjustment.\u003c/p\u003e\n\u003cfigure id=\"attachment_81415\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"617\" src=\"https://github.blog/wp-content/uploads/2024/11/cpu-cores-frequency-21.png?w=1024\u0026amp;resize=1600%2C617\" alt=\"CPU cores frequency\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/cpu-cores-frequency-21.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/cpu-cores-frequency-21.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/cpu-cores-frequency-21.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/cpu-cores-frequency-21.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/cpu-cores-frequency-21.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eCPU cores frequency\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cfigure id=\"attachment_81416\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"770\" src=\"https://github.blog/wp-content/uploads/2024/11/p50-CPU-time.png?w=1024\u0026amp;resize=1600%2C770\" alt=\"P50 CPU time on a request\" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/p50-CPU-time.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/p50-CPU-time.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/p50-CPU-time.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/p50-CPU-time.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/p50-CPU-time.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003eP50 CPU time on a request\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003eUpon re-evaluating the percentage change in CPU time, we now observe similar behavior across all instances.\u003c/p\u003e\n\u003cfigure id=\"attachment_81417\"\u003e\u003cimg data-recalc-dims=\"1\" decoding=\"async\" width=\"1600\" height=\"974\" src=\"https://github.blog/wp-content/uploads/2024/11/percent-increase-3.png?w=1024\u0026amp;resize=1600%2C974\" alt=\"Percentage Increase in P50 Latency vs CPU Utilization \" loading=\"lazy\" srcset=\"https://github.blog/wp-content/uploads/2024/11/percent-increase-3.png?w=1600 1600w, https://github.blog/wp-content/uploads/2024/11/percent-increase-3.png?w=300 300w, https://github.blog/wp-content/uploads/2024/11/percent-increase-3.png?w=768 768w, https://github.blog/wp-content/uploads/2024/11/percent-increase-3.png?w=1024 1024w, https://github.blog/wp-content/uploads/2024/11/percent-increase-3.png?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\u003cfigcaption\u003ePercentage Increase in P50 Latency vs CPU Utilization\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2 id=\"wrap-up\" id=\"wrap-up\"\u003eWrap-up\u003ca href=\"#wrap-up\" aria-label=\"Wrap-up\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAs we anticipated many of these insights, our objective was to validate our theories using data from our complex system. While we confirmed that performance lowers as CPU utilization increases across different CPU families, by identifying optimal CPU utilization thresholds, we can achieve a better balance between performance and efficiency, ensuring that our infrastructure remains both cost-effective and high performing. Going forward, these insights will inform us of our resource provisioning strategies and help us maximize the effectiveness of our hardware investments.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003eThank you for sticking with us until the end!! A special shout-out to \u003ccode\u003e@adrmike\u003c/code\u003e, \u003ccode\u003e@schlubbi\u003c/code\u003e, \u003ccode\u003e@terrorobe\u003c/code\u003e, the \u003ccode\u003e@github/compute-platform\u003c/code\u003e and finally the \u003ccode\u003e@github/performance-engineering\u003c/code\u003e team for their invaluable assistance throughout these experiments, data analysis, and for reviewing the content for accuracy and consistency. ‚ù§Ô∏è\u003c/p\u003e\n\n\t\n\n\t\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tWritten by\t\u003c/h2\u003e\n\t\n\t\t\t\u003carticle\u003e\n\t\u003cdiv\u003e\n\t\t\t\t\t\u003cdiv\u003e\n\t\t\t\t\u003cpicture\u003e\n\t\t\t\t\t\u003csource srcset=\"https://avatars.githubusercontent.com/u/1355590?v=4\u0026amp;s=200\" width=\"120\" height=\"120\" media=\"(min-width: 768px)\"/\u003e\n\t\t\t\t\t\u003cimg src=\"https://avatars.githubusercontent.com/u/1355590?v=4\u0026amp;s=200\" alt=\"Andreas Strikos\" width=\"80\" height=\"80\" loading=\"lazy\" decoding=\"async\"/\u003e\n\t\t\t\t\u003c/picture\u003e\n\t\t\t\u003c/div\u003e\n\t\t\t\t\n\t\t\t\t\t\u003cp\u003eAndreas Strikos is a Senior Software Engineer at GitHub working on the Performance Engineering team.\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003c/article\u003e\n\t\u003c/div\u003e\n\u003c/section\u003e\u003cdiv\u003e\n\t\u003ch2\u003e\n\t\tExplore more from GitHub\t\u003c/h2\u003e\n\t\u003cdiv\u003e\n\t\t\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg\" width=\"44\" height=\"44\" alt=\"Docs\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tDocs\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eEverything you need to master GitHub, all in one place.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;\" href=\"https://docs.github.com/\" target=\"_blank\" aria-label=\"Go to Docs\"\u003e\n\t\t\t\t\tGo to Docs\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg\" width=\"44\" height=\"44\" alt=\"GitHub\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eBuild what‚Äôs next on GitHub, the place for anyone from anywhere to build anything.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Start building; ref_location:bottom recirculation;\" href=\"https://github.blog/developer-skills/github/\" target=\"_blank\" aria-label=\"Start building\"\u003e\n\t\t\t\t\tStart building\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" fill=\"none\"\u003e\u003cpath fill=\"currentColor\" d=\"M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z\"\u003e\u003c/path\u003e\u003cpath stroke=\"currentColor\" d=\"M1.75 8H11\" stroke-width=\"1.5\" stroke-linecap=\"round\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg\" width=\"44\" height=\"44\" alt=\"Customer stories\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tCustomer stories\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eMeet the companies and engineering teams that build with GitHub.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Learn more; ref_location:bottom recirculation;\" href=\"https://github.com/customer-stories\" target=\"_blank\" aria-label=\"Learn more\"\u003e\n\t\t\t\t\tLearn more\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\u003cdiv\u003e\n\t\t\u003cp\u003e\u003cimg src=\"https://github.blog/wp-content/uploads/2024/04/Icon.svg\" width=\"44\" height=\"44\" alt=\"GitHub Universe 2024\"/\u003e\u003c/p\u003e\u003ch3\u003e\n\t\t\tGitHub Universe 2024\t\t\u003c/h3\u003e\n\t\t\u003cp\u003eGet tickets to the 10th anniversary of our global developer event on AI, DevEx, and security.\u003c/p\u003e\n\t\t\t\t\t\u003cp\u003e\n\t\t\t\t\u003ca data-analytics-click=\"Blog, click on module, text: Get tickets; ref_location:bottom recirculation;\" href=\"https://githubuniverse.com/?utm_source=Blog\u0026amp;utm_medium=GitHub\u0026amp;utm_campaign=blog-module\" target=\"_blank\" aria-label=\"Get tickets\"\u003e\n\t\t\t\t\tGet tickets\t\t\t\t\t\t\t\t\t\t\t\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"\u003e\u003cpath fill-rule=\"evenodd\" d=\"M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z\"\u003e\u003c/path\u003e\u003c/svg\u003e\n\t\t\t\t\t\t\t\t\t\u003c/a\u003e\n\t\t\t\u003c/p\u003e\n\t\t\t\u003c/div\u003e\n\t\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "13 min read",
  "publishedTime": "2024-11-25T17:00:20Z",
  "modifiedTime": "2024-11-25T22:59:42Z"
}
