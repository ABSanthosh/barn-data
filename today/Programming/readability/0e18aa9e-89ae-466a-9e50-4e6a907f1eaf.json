{
  "id": "0e18aa9e-89ae-466a-9e50-4e6a907f1eaf",
  "title": "Google Cloud Adds Scalable Vector Search to Memorystore for Valkey \u0026 Redis Cluster",
  "link": "https://www.infoq.com/news/2024/10/vector-search-memorystore/?utm_campaign=infoq_content\u0026utm_source=infoq\u0026utm_medium=feed\u0026utm_term=global",
  "description": "Google Cloud has introduced scalable vector-search capabilities to its Memorystore for Valkey and Redis Cluster. This update allows developers to perform vector searches at ultra-low latencies over billions of vectors. By Mohit Palriwal",
  "author": "Mohit Palriwal",
  "published": "Sat, 19 Oct 2024 15:00:00 GMT",
  "source": "https://feed.infoq.com",
  "categories": [
    "Valkey",
    "Google Cloud",
    "Redis",
    "DevOps",
    "Architecture \u0026 Design",
    "news"
  ],
  "byline": "Mohit Palriwal",
  "length": 4136,
  "excerpt": "Google Cloud has introduced scalable vector-search capabilities to its Memorystore for Valkey and Redis Cluster. This update allows developers to perform vector searches at ultra-low latencies over bi",
  "siteName": "InfoQ",
  "favicon": "https://cdn.infoq.com/statics_s2_20241001113644/apple-touch-icon.png",
  "text": "Google Cloud has introduced scalable vector-search capabilities to its Memorystore for Valkey and Redis Cluster. This update allows developers to perform vector searches at ultra-low latencies over billions of vectors. This enhancement is particularly beneficial for applications that rely on generative AI, such as retrieval-augmented generation (RAG), recommendation systems, and semantic search. The update leverages the ability to partition vector indices across nodes in a cluster. Each node contains a partition of the index that corresponds to its portion of the keyspace, enabling the cluster to handle billions of vectors while maintaining single-digit milliseconds latency and over 99% recall. This architecture not only accelerates index build times linearly as nodes are added but also optimizes search performance – logarithmically for hierarchical navigable small-world (HNSW) searches and linearly for brute-force searches. Developers can use these new abilities to scale out their clusters to 250 shards, storing billions of vectors in a single instance. This scalability is essential for enterprise applications that need to perform semantic searches over extensive datasets. In addition to scalability, the update introduces support for hybrid queries. Hybrid queries enable developers to combine vector searches with filters on numeric and tag fields. This functionality is particularly useful for fine-tuning search results based on specific criteria. For example, an online clothing retailer can use hybrid search to recommend similar items while filtering results based on clothing type and price range. To implement a hybrid query, developers can create a new vector index with additional fields for filtering: FT.CREATE inventory_index SCHEMA embedding VECTOR HNSW 6 DIM 128 TYPE FLOAT32 DISTANCE_METRIC L2 clothing_type TAG clothing_price_usd NUMERIC This creates an index ‘inventory_index’ with a vector field `embedding` for the semantic embedding of the clothing item, a tag field `clothing_type` for the type of article of clothing (e.g., “dress” or “hat”), and a numeric field `clothing_price_usd` for the price of the article of clothing. To perform a hybrid query on ‘inventory_index’: FT.SEARCH inventory_index “(@clothing_type:{dress} @clothing_price_usd:[100-200])=\u003e[KNN 10 @embedding $query_vector]“ PARAMS 2 query_vector “...” DIALECT 2 This query retrieved 10 results filtered by clothing type “dress” and a price range of 100-200, combined with a vector similarity search. Some community members have cautioned about adopting Redis’s vector search if this technology is not already deployed within an organization. For example, marr75 on Reddit, stated: The better advice is probably to stick with your dominant data persistence and query technology, though. If that's RediSearch, stick with it. If it's not, don't pick it up for its vector search support which is fine but not best in class or state of the art. Google Cloud is also contributing to the open-source community by donating its vector search capabilities to the Valkey key-value datastore. This initiative aims to enable Valkey developers to leverage vector search to create advanced generative AI applications. In a recent Google announcement blog, Sanjeev Mohan, principal analyst at SanjMo and former Gartner VP, shared his perspective on Google’s contributions: Valkey is important for continuing to advance community-led efforts to provide feature-rich, open-source database alternatives. The launch of Valkey support in Memorystore is yet another example of Google’s dedication to providing truly open and accessible solutions for users. Their contributions to Valkey not only benefit developers seeking flexibility, but also strengthens the broader open-source ecosystem. Rapid and precise vector searches are relevant in industries such as e-commerce, where understanding customer preference and delivering tailored recommendations can be beneficial. About the Author Mohit Palriwal",
  "image": "https://res.infoq.com/news/2024/10/vector-search-memorystore/en/headerimage/generatedHeaderImage-1729344154281.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\t\t\t\u003cp\u003eGoogle Cloud has introduced \u003ca href=\"https://cloud.google.com/blog/products/databases/vector-search-for-memorystore-for-valkey-and-redis-cluster\"\u003escalable vector-search capabilities to its Memorystore for Valkey and Redis Cluster\u003c/a\u003e. This update allows developers to perform vector searches at ultra-low latencies over billions of vectors.\u003c/p\u003e\n\n\u003cp\u003eThis enhancement is particularly beneficial for applications that rely on generative AI, such as \u003ca href=\"https://www.infoq.com/presentations/rag-patterns/\"\u003eretrieval-augmented generation (RAG)\u003c/a\u003e, recommendation systems, and semantic search.\u003c/p\u003e\n\n\u003cp\u003eThe update leverages the ability to partition vector indices across nodes in a cluster. Each node contains a partition of the index that corresponds to its portion of the keyspace, enabling the cluster to handle billions of vectors while maintaining single-digit milliseconds latency and over 99% recall. This architecture not only accelerates index build times linearly as nodes are added but also optimizes search performance – logarithmically for \u003ca href=\"https://arxiv.org/abs/1603.09320\"\u003ehierarchical navigable small-world (HNSW)\u003c/a\u003e searches and linearly for \u003ca href=\"https://en.wikipedia.org/wiki/Brute-force_search\"\u003ebrute-force searches\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eDevelopers can use these new abilities to scale out their clusters to 250 shards, storing billions of vectors in a single instance. This scalability is essential for enterprise applications that need to perform semantic searches over extensive datasets.\u003c/p\u003e\n\n\u003cp\u003eIn addition to scalability, the update introduces support for hybrid queries. Hybrid queries enable developers to combine vector searches with filters on numeric and tag fields. This functionality is particularly useful for fine-tuning search results based on specific criteria. For example, an online clothing retailer can use hybrid search to recommend similar items while filtering results based on clothing type and price range.\u003c/p\u003e\n\n\u003cp\u003eTo implement a hybrid query, developers can create a new vector index with additional fields for filtering:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003eFT.CREATE inventory_index SCHEMA embedding VECTOR HNSW 6 DIM 128 TYPE FLOAT32 DISTANCE_METRIC L2 clothing_type TAG clothing_price_usd NUMERIC\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThis creates an index \u003cem\u003e‘inventory_index’\u003c/em\u003e with a vector field `\u003cem\u003eembedding\u003c/em\u003e` for the semantic embedding of the clothing item, a tag field `\u003cem\u003eclothing_type\u003c/em\u003e` for the type of article of clothing (e.g., “dress” or “hat”), and a numeric field `\u003cem\u003eclothing_price_usd\u003c/em\u003e` for the price of the article of clothing.\u003c/p\u003e\n\n\u003cp\u003eTo perform a hybrid query on \u003cem\u003e‘inventory_index’:\u003c/em\u003e\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003eFT.SEARCH inventory_index “(@clothing_type:{dress} @clothing_price_usd:[100-200])=\u0026gt;[KNN 10 @embedding $query_vector]“ PARAMS 2 query_vector “...” DIALECT 2\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThis query retrieved 10 results filtered by clothing type “dress” and a price range of 100-200, combined with a vector similarity search.\u003c/p\u003e\n\n\u003cp\u003eSome community members have cautioned about adopting Redis’s vector search if this technology is not already deployed within an organization. For example, \u003ca href=\"https://www.reddit.com/r/MachineLearning/comments/1e6rhjx/d_redis_as_vector_database_any_personal/#t1_ldv9hj6-comment-rtjson-content\"\u003emarr75 on Reddit\u003c/a\u003e, stated:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe better advice is probably to stick with your dominant data persistence and query technology, though. If that\u0026#39;s RediSearch, stick with it. If it\u0026#39;s not, don\u0026#39;t pick it up for its vector search support which is fine but not best in class or state of the art.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eGoogle Cloud is also contributing to the open-source community by donating its vector search capabilities to the Valkey key-value datastore. This initiative aims to enable Valkey developers to leverage vector search to create advanced generative AI applications.\u003c/p\u003e\n\n\u003cp\u003eIn a recent Google announcement blog, \u003ca href=\"https://www.linkedin.com/in/sanjmo\"\u003eSanjeev Mohan\u003c/a\u003e, principal analyst at SanjMo and former Gartner VP, \u003ca href=\"https://cloud.google.com/blog/products/databases/announcing-memorystore-for-valkey\"\u003eshared his perspective\u003c/a\u003e on Google’s contributions:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eValkey is important for continuing to advance community-led efforts to provide feature-rich, open-source database alternatives. The launch of Valkey support in Memorystore is yet another example of Google’s dedication to providing truly open and accessible solutions for users. Their contributions to Valkey not only benefit developers seeking flexibility, but also strengthens the broader open-source ecosystem.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eRapid and precise vector searches are relevant in industries such as e-commerce, where understanding customer preference and delivering tailored recommendations can be beneficial.\u003c/p\u003e\n\n\t\t\t\t\t\t\t\t\n\n\n\n\n\n\n\n\n\n  \n    \u003cdiv\u003e \n        \u003ch2\u003eAbout the Author\u003c/h2\u003e \n\n        \n            \n                \n            \n            \u003cdiv data-id=\"author-Mohit-Palriwal\"\u003e\n                    \u003ch4\u003e\u003cstrong\u003eMohit Palriwal\u003c/strong\u003e\u003c/h4\u003e\n                    \n                \u003c/div\u003e\n        \n    \u003c/div\u003e\n\n\t\t\t\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2024-10-19T00:00:00Z",
  "modifiedTime": null
}
