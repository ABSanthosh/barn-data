{
  "id": "1f61421a-fb4a-4761-b720-ef9db1c138af",
  "title": "Gemma explained: An overview of Gemma model family architectures",
  "link": "https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/",
  "description": "Learn more about the different variations of Gemma models, how they are designed for different use cases, and the core parameters of their architecture.",
  "author": "",
  "published": "",
  "source": "http://feeds.feedburner.com/GDBcode",
  "categories": null,
  "byline": "Ju-yeong Ji, Ravin Kumar",
  "length": 13562,
  "excerpt": "Learn more about the different variations of Gemma models, how they are designed for different use cases, and the core parameters of their architecture.",
  "siteName": "",
  "favicon": "",
  "text": "Ravin Kumar Google Data Scientist Language Applications Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models.Different variations of Gemma are designed for different use cases and modalities, such as:Single modality (Text in, Text out)Specialization for coding use casesMulti modality (Text and Image in, Text out)Varying sizes for different hardware types, inference needs, and other constraints.“Novel” architecturesBecause all these models share a similar DNA, the Gemma family presents a unique way to learn about the architectures and design choices that are available in modern LLM systems. We hope this contributes to a rich ecosystem of open models and promotes a greater understanding of how LLM systems work.This series will cover:Gemma 1 (2B, 7B) - Transformer based text-to-text models.CodeGemma (2B and 7B) - A fine-tuned version of Gemma, optimized for code completion and generation.Gemma 2 (2B, 9B, 27B) - Updated text-to-text models trained with newer architecture with the 2B and 9B versions trained through distillation from larger models.RecurrentGemma (2B, 9B) - A model built on the novel Griffin architecture. This architecture uses a mixture of local attention and linear recurrences to achieve fast inference when generating long sequences.PaliGemma (3B) - A vision-language model that can take in text and images and provide a text output.How to use this guideIn this series, we willCollate the specific architectures of various modelsExplain how these parameters affect model generations (e.g. num embeddings, Multi Query vs Multi Head vs Grouped Query)Provide code examples of the models for further explorationTo provide information about the model, we use Hugging Face Transformers print module, like the simple code below. from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\") print(model) You can explore inside the model with torchinfo or summary() in the Keras Model class API as well.What this guide is notThis guide is not an introduction to AI. It assumes working knowledge of neural networks, Transformers and associated terms like tokens. If you need a refresher on these concepts here are some resources to get you started:A hands on neural network learning tool that works in browserhttp://playground.tensorflow.org/An introduction to transformersTransformers, explained: Understand the model behind GPT, BERT, and T5https://github.com/google/sentencepieceGemmaGemma is an open weight LLM. It comes in both instruction-tuned and raw, pretrained variants at various parameter sizes. It is based on the LLM architecture introduced by Google Research in the Attention Is All You Need paper. Its primary function is to generate text tokenword by tokenword, based on a prompt provided by a user. In tasks like translation, Gemma takes a sentence from one language as input and outputs its equivalent in another language.As you’ll soon see Gemma is both a great model by itself, but also lends itself to custom extensions to meet different user needs.Gemma ArchitectureFirst, let’s see the transformer decoder that Gemma models are based on. Unlike the original encoder-decoder transformer model architecture introduced in “Attention Is All You Need”, Gemma is solely a “decoder-only” model.The core parameters of the architecture are summarized in the table below. Models are trained on a context length of 8192 tokens. This means they can process up to approximately 6144 words (using the rule of thumb of 100 tokens ~= 75 words) at a time.It's worth noting that the practical input limit can vary based on the task and usage. This is because text generation consumes tokens within the context window, effectively reducing space for new input. Although the technical input limit remains constant, generated output becomes part of the subsequent input, influencing further generations.d_model (2B: 2048, 7B: 3072)d_model represents the size of the embeddings (vector representations of words or subwords a.k.a tokens) used as input to the decoder. It also determines the size of the internal representation within the decoder layers. “d_model x Num heads x Head size” defines the parameter number in self_attn A larger d_model value means the model has more “space” to represent the nuances of different words and their relationships. This can lead to better performance, especially for complex language tasks. However, increasing d_model also makes the model larger and more computationally expensive to train and use.Layers (2B: 18, 7B: 28)Transformers consist of multiple stacked layers. Deeper models have more layers, and therefore more parameters and can learn more intricate patterns. However these additional parameters mean they are also more prone to overfitting and require more computational resources.This augmented representational capacity might result in the model learning noise or specific training data patterns that lack the ability to generalize to novel examples.Furthermore, deeper models often necessitate more training data to avert overfitting. In cases where available data is limited, the model might lack sufficient examples to learn a generalizable representation, leading to the memorization of training data instead.Feedforward hidden dims (2B: 32768, 7B: 49152)Each Transformer layer includes a feedforward network after the attention mechanism. This network has its own dimensionality, often larger than the d_model size to increase the model’s expressive power.It is implemented as a multi-layer perceptron (MLP), a kind of neural network, to further transform the embeddings and extract more intricate patterns. In Gemma, the standard ReLU non-linearity is replaced by the GeGLU activation function, a variation of GLU (Gate Linear Unit). GeGLU divides the activation into two parts: a sigmoidal part and a linear projection. The output of the sigmoidal part is element-wise multiplied with the linear projection, resulting in a non-linear activation function. Num heads (2B: 8, 7B: 16)Each Transformer layer contains multiple attention mechanisms working in parallel. These “heads” allow the model to focus on different aspects of the input sequence simultaneously. Increasing the number of heads can enhance the model's ability to capture diverse relationships in the data.Num KV heads (2B: 1, 7B: 16)The 7B model uses multi-head attention(MHA), while the 2B model uses multi-query attention(MQA). MQA shares the same key and value projections, which means each head focuses on the same underlying representation but with different query projections.The original MHA offers richer representation learning but comes with higher computational costs. MQA provides an efficient alternative that has been shown to be effective.Head size (2B: 256, 7B: 256)It refers to the dimensionality of each attention head within the multi-head attention mechanism. It is calculated by dividing the embedding dimension by the number of heads. For example, if the embedding dimension is 2048 and there are 8 heads, then each head would have a size of 256.Vocab size (2B: 256128, 7B: 256128)It defines the number of unique tokens (words, sub words or characters) that the model understands and can process. Gemma tokenizer is based on SentencePiece. The size of the vocabulary is predetermined before training. SentencePiece then learns the optimal subword segmentation based on the chosen vocabulary size and the training data. Gemma’s large 256k vocabulary allows it to handle diverse text inputs and potentially improve performance on various tasks, e.g. handling multilingual text inputs.Gemma 7B GemmaForCausalLM( (model): GemmaModel( (embed_tokens): Embedding(256000, 3072, padding_idx=0) (layers): ModuleList( (0-27): 28 x GemmaDecoderLayer( (self_attn): GemmaSdpaAttention( (q_proj): Linear(in_features=3072, out_features=4096, bias=False) (k_proj): Linear(in_features=3072, out_features=4096, bias=False) (v_proj): Linear(in_features=3072, out_features=4096, bias=False) (o_proj): Linear(in_features=4096, out_features=3072, bias=False) (rotary_emb): GemmaRotaryEmbedding() ) (mlp): GemmaMLP( (gate_proj): Linear(in_features=3072, out_features=24576, bias=False) (up_proj): Linear(in_features=3072, out_features=24576, bias=False) (down_proj): Linear(in_features=24576, out_features=3072, bias=False) (act_fn): PytorchGELUTanh() ) (input_layernorm): GemmaRMSNorm() (post_attention_layernorm): GemmaRMSNorm() ) ) (norm): GemmaRMSNorm() ) (lm_head): Linear(in_features=3072, out_features=256000, bias=False) ) embed_tokens (Embedding Layer)This layer converts the input tokens (words or subwords) into dense numerical representations (embeddings) that the model can process. It has a vocabulary size of 256,000 and creates embeddings of dimension 3072.layersThis is the heart of the model, consisting of 28 stacked GemmaDecoderLayer blocks. Each of these layers refines the token embeddings to capture complex relationships between words and their context.self_attnIn the self-attention mechanism, the model assigns different weights to the words in the input when creating the next word. Leveraging a scaled dot-product attention mechanism, the model employs linear projections (q_proj, k_proj, v_proj, and o_proj) to generate query, key, value, and output representations.All out_features values are the same 4096 for q_proj, k_proj and v_proj as this model uses Multi Head Attention (MHA). They have 16 heads with a size of 256 in parallel, totaling 4096 (256 x 16).Furthermore, the model leverages positional information more effectively by employing rotary_emb (GemmaRotaryEmbedding) for positional encoding (a.k.a RoPE).Finally, o_proj layer projects the attention output back to the original dimension (3072).Note that the Gemma 2B model uses Multi Query Attention (MQA). k_proj and v_proj share the same head with a size of 256, resulting in out_features of 256. In contrast, q_proj and o_proj have 8 heads (256 x 8 = 2048) in parallel. (self_attn): GemmaSdpaAttention( (q_proj): Linear(in_features=2048, out_features=2048, bias=False) (k_proj): Linear(in_features=2048, out_features=256, bias=False) (v_proj): Linear(in_features=2048, out_features=256, bias=False) (o_proj): Linear(in_features=2048, out_features=2048, bias=False) (rotary_emb): GemmaRotaryEmbedding() ) mlpIt utilizes gate_proj and up_proj for a gating mechanism, followed by down_proj to reduce the dimension back to 3072.input_layernorm, post_attention_layernorm and normThese normalization layers stabilize training and improve the model’s ability to learn effectively.lm_headThis final layer maps the refined embeddings (3072) back to a probability distribution for the next token over the vocabulary space (256000).CodeGemma (2B and 7B)CodeGemma models are fine-tuned Gemma models that are optimized for code completion and coding chat assistance. CodeGemma models are trained on more than 500 billion tokens of primarily code. In addition CodeGemma adds fill-in-the- middle capability, allowing completions that occur between two pieces of existing text.CodeGemma highlights the finetunability of the Gemma checkpoints. Through additional training the models become specialized at a certain task, learning a more complex completion than pure suffix completion.Code Gemma UsageYou can use 4 user-defined tokens - 3 for FIM and a \"\u003c|file_separator|\u003e\" token for multi-file context support. BEFORE_CURSOR = \"\u003c|fim_prefix|\u003e\" AFTER_CURSOR = \"\u003c|fim_suffix|\u003e\" AT_CURSOR = \"\u003c|fim_middle|\u003e\" FILE_SEPARATOR = \"\u003c|file_separator|\u003e\" Imagine that you are trying to complete the code like the screen below. And the input prompt should look like this \u003c|fim_prefix|\u003eimport \u003c|fim_suffix|\u003eif __name__ == \"__main__\":\\n sys.exit(0)\u003c|fim_middle|\u003e The model will provide \"sys\" as the suggested code completion.You can explore more about CodeGemma on CodeGemma / Quickstart.What’s Next?This article discussed the Gemma architecture.In our next series of posts, you will explore the latest model, Gemma 2. With substantial enhancements in safety measures, this model surpasses its predecessor in terms of performance and efficiency during inference.Stay tuned and thanks for reading!ReferencesPapersAttention Is All You NeedGemma: Open Models Based on Gemini Research and TechnologyGLU Variants Improve TransformerFast Transformer Decoding: One Write-Head is All You Need Code ExamplesGemmaGet started with Gemma using KerasNLPCodeGemmaQuickstartAI Assisted coding with CodeGemma📋 The complete Gemma architecture seriesGemma explained: An overview of Gemma model family architecturesGemma explained: What’s new in Gemma 2Gemma explained: RecurrentGemma architectureGemma explained: PaliGemma architecture",
  "image": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma-social.2e16d0ba.fill-1200x600.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n    \n      \n    \n\n    \n\n    \n\n    \u003csection\u003e\n      \n        \n          \n        \n          \u003cp\u003e\u003ca href=\"https://developers.googleblog.com/en/search/?author=Ravin+Kumar\"\u003eRavin Kumar\u003c/a\u003e\n            \n              \u003cspan\u003eGoogle Data Scientist\u003c/span\u003e\n            \n            \n              \u003cspan\u003eLanguage Applications\u003c/span\u003e\n            \n          \u003c/p\u003e\n        \n\n      \n      \u003c/section\u003e\n\n    \n    \u003cdiv\u003e\n          \n\n\u003cdiv\u003e\n    \u003cp data-block-key=\"jy293\"\u003e\u003ca href=\"https://ai.google.dev/gemma\"\u003eGemma\u003c/a\u003e is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models.\u003c/p\u003e\u003cp data-block-key=\"9mr9j\"\u003eDifferent variations of Gemma are designed for different use cases and modalities, such as:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"5c52n\"\u003eSingle modality (Text in, Text out)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"ab5dq\"\u003eSpecialization for coding use cases\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"1fltp\"\u003eMulti modality (Text and Image in, Text out)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4asln\"\u003eVarying sizes for different hardware types, inference needs, and other constraints.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"2q018\"\u003e“Novel” architectures\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"3a444\"\u003eBecause all these models share a similar DNA, the Gemma family presents a unique way to learn about the architectures and design choices that are available in modern LLM systems. We hope this contributes to a rich ecosystem of open models and promotes a greater understanding of how LLM systems work.\u003c/p\u003e\u003cp data-block-key=\"62o40\"\u003eThis series will cover:\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"ffinb\"\u003e\u003cb\u003eGemma 1 (2B, 7B)\u003c/b\u003e - Transformer based text-to-text models.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"2odrf\"\u003e\u003cb\u003eCodeGemma (2B and 7B)\u003c/b\u003e - A fine-tuned version of Gemma, optimized for code completion and generation.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7k6bv\"\u003e\u003cb\u003eGemma 2 (2B, 9B, 27B)\u003c/b\u003e - Updated text-to-text models trained with newer architecture with the 2B and 9B versions trained through distillation from larger models.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"4dcc2\"\u003e\u003cb\u003eRecurrentGemma (2B, 9B)\u003c/b\u003e - A model built on the novel Griffin architecture. This architecture uses a mixture of local attention and linear recurrences to achieve fast inference when generating long sequences.\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"aajrl\"\u003e\u003cb\u003ePaliGemma (3B)\u003c/b\u003e - A vision-language model that can take in text and images and provide a text output.\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"9jk2g\"\u003e\u003cbr/\u003eHow to use this guide\u003c/h2\u003e\u003cp data-block-key=\"e4i9f\"\u003eIn this series, we will\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"vfl8\"\u003eCollate the specific architectures of various models\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7k4vh\"\u003eExplain how these parameters affect model generations (e.g. num embeddings, Multi Query vs Multi Head vs Grouped Query)\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli data-block-key=\"7q3ui\"\u003eProvide code examples of the models for further exploration\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"1cv80\"\u003eTo provide information about the model, we use Hugging Face Transformers print module, like the simple code below.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003efrom\u003c/span\u003e \u003cspan\u003etransformers\u003c/span\u003e \u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003eAutoModelForCausalLM\u003c/span\u003e\n\u003cspan\u003emodel\u003c/span\u003e \u003cspan\u003e=\u003c/span\u003e \u003cspan\u003eAutoModelForCausalLM\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003efrom_pretrained\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e\u0026#34;google/gemma-7b\u0026#34;\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003eprint\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"jy293\"\u003eYou can explore inside the model with \u003ca href=\"https://github.com/TylerYep/torchinfo\"\u003etorchinfo\u003c/a\u003e or \u003ca href=\"https://keras.io/api/models/model/#summary-method\"\u003esummary()\u003c/a\u003e in the Keras Model class API as well.\u003c/p\u003e\u003ch2 data-block-key=\"2upob\"\u003e\u003cbr/\u003eWhat this guide is not\u003c/h2\u003e\u003cp data-block-key=\"37rk3\"\u003eThis guide is not an introduction to AI. It assumes working knowledge of neural networks, Transformers and associated terms like tokens. If you need a refresher on these concepts here are some resources to get you started:\u003c/p\u003e\u003cp data-block-key=\"a2n73\"\u003eA hands on neural network learning tool that works in browser\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"22qdr\"\u003e\u003ca href=\"http://playground.tensorflow.org/\"\u003ehttp://playground.tensorflow.org/\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"bs1qo\"\u003eAn introduction to transformers\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"1b89l\"\u003e\u003ca href=\"https://www.youtube.com/watch?v=SZorAJ4I-sA\u0026amp;t=26s\"\u003eTransformers, explained: Understand the model behind GPT, BERT, and T5\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"aotd8\"\u003e\u003ca href=\"https://github.com/google/sentencepiece\"\u003ehttps://github.com/google/sentencepiece\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"7bd85\"\u003e\u003cbr/\u003e\u003cb\u003eGemma\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"acak6\"\u003eGemma is an open weight LLM. It comes in both instruction-tuned and raw, pretrained variants at various parameter sizes. It is based on the LLM architecture introduced by Google Research in the \u003ca href=\"https://arxiv.org/abs/1706.03762v7\"\u003eAttention Is All You Need\u003c/a\u003e paper. Its primary function is to generate text tokenword by tokenword, based on a prompt provided by a user. In tasks like translation, Gemma takes a sentence from one language as input and outputs its equivalent in another language.\u003c/p\u003e\u003cp data-block-key=\"5l4fi\"\u003eAs you’ll soon see Gemma is both a great model by itself, but also lends itself to custom extensions to meet different user needs.\u003c/p\u003e\u003ch3 data-block-key=\"3v9cm\"\u003e\u003cbr/\u003e\u003cb\u003eGemma Architecture\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"toie\"\u003eFirst, let’s see \u003ca href=\"https://arxiv.org/abs/1706.03762v7\"\u003ethe transformer decoder\u003c/a\u003e that Gemma models are based on.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image8_h40ZNWW.original.png\" alt=\"Transformer decoder architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"jy293\"\u003eUnlike the original encoder-decoder transformer model architecture introduced in “Attention Is All You Need”, Gemma is solely a “decoder-only” model.\u003c/p\u003e\u003cp data-block-key=\"at9et\"\u003eThe \u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf\"\u003ecore parameters\u003c/a\u003e of the architecture are summarized in the table below.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-08_at_2.16.18PM.original.png\" alt=\"Core parameters of the architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"jy293\"\u003eModels are trained on a context length of 8192 tokens. This means they can process up to approximately 6144 words (using the rule of thumb of 100 tokens ~= 75 words) at a time.\u003c/p\u003e\u003cp data-block-key=\"dv1i3\"\u003eIt\u0026#39;s worth noting that the practical input limit can vary based on the task and usage. This is because text generation consumes tokens within the context window, effectively reducing space for new input. Although the technical input limit remains constant, generated output becomes part of the subsequent input, influencing further generations.\u003c/p\u003e\u003ch3 data-block-key=\"9i2ni\"\u003e\u003cb\u003e\u003cbr/\u003ed_model (2B: 2048, 7B: 3072)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"c0n44\"\u003e\u003ci\u003ed_model\u003c/i\u003e represents the size of the embeddings (vector representations of words or subwords a.k.a tokens) used as input to the decoder. It also determines the size of the internal representation within the decoder layers.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n        \n            \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image7_PhVB9TE.original.png\" alt=\"d_model x Num heads x Head size\"/\u003e\u003c/p\u003e\u003cp\u003e\n                    “d_model x Num heads x Head size” defines the parameter number in self_attn\n                \u003c/p\u003e\n            \n        \n    \u003c/div\u003e\n  \u003cdiv\u003e\n    \u003cp data-block-key=\"jy293\"\u003eA larger \u003ci\u003ed_model\u003c/i\u003e value means the model has more “space” to represent the nuances of different words and their relationships. This can lead to better performance, especially for complex language tasks. However, increasing \u003ci\u003ed_model\u003c/i\u003e also makes the model larger and more computationally expensive to train and use.\u003c/p\u003e\u003ch3 data-block-key=\"9mf73\"\u003e\u003cbr/\u003e\u003cb\u003eLayers (2B: 18, 7B: 28)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"393aq\"\u003eTransformers consist of multiple stacked layers. Deeper models have more layers, and therefore more parameters and can learn more intricate patterns. However these additional parameters mean they are also more prone to overfitting and require more computational resources.\u003c/p\u003e\u003cp data-block-key=\"8p9cn\"\u003eThis augmented representational capacity might result in the model learning noise or specific training data patterns that lack the ability to generalize to novel examples.\u003c/p\u003e\u003cp data-block-key=\"51i2r\"\u003eFurthermore, deeper models often necessitate more training data to avert overfitting. In cases where available data is limited, the model might lack sufficient examples to learn a generalizable representation, leading to the memorization of training data instead.\u003c/p\u003e\u003ch3 data-block-key=\"4vmil\"\u003e\u003cbr/\u003e\u003cb\u003eFeedforward hidden dims (2B: 32768, 7B: 49152)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"9b6sl\"\u003eEach Transformer layer includes a feedforward network after the attention mechanism. This network has its own dimensionality, often larger than the d_model size to increase the model’s expressive power.\u003c/p\u003e\u003cp data-block-key=\"6ki07\"\u003eIt is implemented as a multi-layer perceptron (MLP), a kind of neural network, to further transform the embeddings and extract more intricate patterns.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_l7UnOuC.original.png\" alt=\"multi-layer perceptron (MLP) neural network achitecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"jy293\"\u003eIn Gemma, the standard ReLU non-linearity is replaced by the \u003ca href=\"https://arxiv.org/abs/2002.05202\"\u003eGeGLU activation function\u003c/a\u003e, a variation of GLU (Gate Linear Unit). GeGLU divides the activation into two parts: a sigmoidal part and a linear projection. The output of the sigmoidal part is element-wise multiplied with the linear projection, resulting in a non-linear activation function.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2024-08-09_at_10.23.20AM.original.png\" alt=\"GeGLU activation function example\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"jy293\"\u003e\u003cb\u003eNum heads (2B: 8, 7B: 16)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"5i978\"\u003eEach Transformer layer contains multiple attention mechanisms working in parallel. These “heads” allow the model to focus on different aspects of the input sequence simultaneously. Increasing the number of heads can enhance the model\u0026#39;s ability to capture diverse relationships in the data.\u003c/p\u003e\u003ch3 data-block-key=\"aump7\"\u003e\u003cbr/\u003e\u003cb\u003eNum KV heads (2B: 1, 7B: 16)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"6tk04\"\u003eThe 7B model uses multi-head attention(MHA), while the 2B model uses multi-query attention(MQA). MQA shares the same key and value projections, which means each head focuses on the same underlying representation but with different query projections.\u003c/p\u003e\u003cp data-block-key=\"5kide\"\u003eThe original MHA offers richer representation learning but comes with higher computational costs. MQA provides an efficient alternative that has been \u003ca href=\"https://arxiv.org/abs/1911.02150\"\u003eshown to be effective\u003c/a\u003e.\u003c/p\u003e\u003ch3 data-block-key=\"d3p0f\"\u003e\u003cb\u003e\u003cbr/\u003eHead size (2B: 256, 7B: 256)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"ci170\"\u003eIt refers to the dimensionality of each attention head within the multi-head attention mechanism. It is calculated by dividing the embedding dimension by the number of heads. For example, if the embedding dimension is 2048 and there are 8 heads, then each head would have a size of 256.\u003c/p\u003e\u003ch3 data-block-key=\"7rfmm\"\u003e\u003cb\u003e\u003cbr/\u003eVocab size (2B: 256128, 7B: 256128)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"f2u1a\"\u003eIt defines the number of unique tokens (words, sub words or characters) that the model understands and can process. Gemma tokenizer is based on \u003ca href=\"https://github.com/google/sentencepiece\"\u003eSentencePiece\u003c/a\u003e. The size of the vocabulary is predetermined before training. SentencePiece then learns the optimal subword segmentation based on the chosen vocabulary size and the training data. Gemma’s large 256k vocabulary allows it to handle diverse text inputs and potentially improve performance on various tasks, e.g. handling multilingual text inputs.\u003c/p\u003e\u003ch2 data-block-key=\"2r3r7\"\u003e\u003cb\u003e\u003cbr/\u003eGemma 7B\u003c/b\u003e\u003c/h2\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eGemmaForCausalLM\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emodel\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaModel\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eembed_tokens\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eEmbedding\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e256000\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003epadding_idx\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elayers\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eModuleList\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n      \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e-\u003c/span\u003e\u003cspan\u003e27\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003e28\u003c/span\u003e \u003cspan\u003ex\u003c/span\u003e \u003cspan\u003eGemmaDecoderLayer\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaSdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e4096\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003emlp\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaMLP\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003egate_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e24576\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eup_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e24576\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003edown_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e24576\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eact_fn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003ePytorchGELUTanh\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003einput_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003epost_attention_layernorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n      \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e)\u003c/span\u003e\n    \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003enorm\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRMSNorm\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n  \u003cspan\u003e)\u003c/span\u003e\n  \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003elm_head\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e3072\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256000\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n\u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_iinFugc.original.png\" alt=\"Gemma 7B architecture\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cdiv\u003e\n    \u003ch3 data-block-key=\"jy293\"\u003e\u003cb\u003eembed_tokens (Embedding Layer)\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"2uknd\"\u003eThis layer converts the input tokens (words or subwords) into dense numerical representations (embeddings) that the model can process. It has a vocabulary size of 256,000 and creates embeddings of dimension 3072.\u003c/p\u003e\u003ch3 data-block-key=\"1v900\"\u003e\u003cbr/\u003e\u003cb\u003elayers\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"d17nl\"\u003eThis is the heart of the model, consisting of 28 stacked GemmaDecoderLayer blocks. Each of these layers refines the token embeddings to capture complex relationships between words and their context.\u003c/p\u003e\u003ch3 data-block-key=\"2d8qk\"\u003e\u003cbr/\u003e\u003cb\u003eself_attn\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"evedh\"\u003eIn the self-attention mechanism, the model assigns different weights to the words in the input when creating the next word. Leveraging a scaled dot-product attention mechanism, the model employs linear projections (\u003ci\u003eq_proj\u003c/i\u003e, \u003ci\u003ek_proj\u003c/i\u003e, \u003ci\u003ev_proj\u003c/i\u003e, and \u003ci\u003eo_proj\u003c/i\u003e) to generate query, key, value, and output representations.\u003c/p\u003e\u003cp data-block-key=\"8ih4v\"\u003eAll out_features values are the same 4096 for \u003ci\u003eq_proj\u003c/i\u003e, \u003ci\u003ek_proj\u003c/i\u003e and \u003ci\u003ev_proj\u003c/i\u003e as this model uses \u003cb\u003eMulti Head Attention (MHA)\u003c/b\u003e. They have 16 heads with a size of 256 in parallel, totaling 4096 (256 x 16).\u003c/p\u003e\u003cp data-block-key=\"28bc8\"\u003eFurthermore, the model leverages positional information more effectively by employing \u003ci\u003erotary_emb\u003c/i\u003e (GemmaRotaryEmbedding) for positional encoding (a.k.a RoPE).\u003c/p\u003e\u003cp data-block-key=\"7vn05\"\u003eFinally, \u003ci\u003eo_proj\u003c/i\u003e layer projects the attention output back to the original dimension (3072).\u003c/p\u003e\u003chr/\u003e\u003cp data-block-key=\"aakv8\"\u003eNote that the \u003cb\u003eGemma 2B\u003c/b\u003e model uses \u003cb\u003eMulti Query Attention (MQA)\u003c/b\u003e.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_3kHryqa.original.png\" alt=\"Multi-Query Attention (MQA) architecture used in Gemma 2B model\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"jy293\"\u003e\u003ci\u003ek_proj\u003c/i\u003e and \u003ci\u003ev_proj\u003c/i\u003e share the same head with a size of 256, resulting in \u003ci\u003eout_features\u003c/i\u003e of 256. In contrast, \u003ci\u003eq_proj\u003c/i\u003e and \u003ci\u003eo_proj\u003c/i\u003e have 8 heads (256 x 8 = 2048) in parallel.\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eself_attn\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaSdpaAttention\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eq_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ek_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ev_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e256\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003eo_proj\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eLinear\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003ein_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003eout_features\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e2048\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ebias\u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003eFalse\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\n          \u003cspan\u003e(\u003c/span\u003e\u003cspan\u003erotary_emb\u003c/span\u003e\u003cspan\u003e):\u003c/span\u003e \u003cspan\u003eGemmaRotaryEmbedding\u003c/span\u003e\u003cspan\u003e()\u003c/span\u003e\n        \u003cspan\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003chr/\u003e\u003ch3 data-block-key=\"5ccpk\"\u003e\u003cb\u003emlp\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"c8t7b\"\u003eIt utilizes \u003ci\u003egate_proj\u003c/i\u003e and \u003ci\u003eup_proj\u003c/i\u003e for a gating mechanism, followed by \u003ci\u003edown_proj\u003c/i\u003e to reduce the dimension back to 3072.\u003c/p\u003e\u003ch3 data-block-key=\"4a0ss\"\u003e\u003cbr/\u003e\u003cb\u003einput_layernorm, post_attention_layernorm and norm\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"e2lse\"\u003eThese normalization layers stabilize training and improve the model’s ability to learn effectively.\u003c/p\u003e\u003ch3 data-block-key=\"56cmo\"\u003e\u003cbr/\u003e\u003cb\u003elm_head\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"41sr6\"\u003eThis final layer maps the refined embeddings (3072) back to a probability distribution for the next token over the vocabulary space (256000).\u003c/p\u003e\u003ch2 data-block-key=\"4nvb\"\u003e\u003cbr/\u003e\u003cb\u003eCodeGemma (2B and 7B)\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"498la\"\u003eCodeGemma models are fine-tuned Gemma models that are optimized for code completion and coding chat assistance. CodeGemma models are trained on more than 500 billion tokens of primarily code. In addition CodeGemma adds fill-in-the- middle capability, allowing completions that occur between two pieces of existing text.\u003c/p\u003e\u003cp data-block-key=\"f8of3\"\u003eCodeGemma highlights the finetunability of the Gemma checkpoints. Through additional training the models become specialized at a certain task, learning a more complex completion than pure suffix completion.\u003c/p\u003e\u003ch3 data-block-key=\"10ok3\"\u003e\u003cbr/\u003e\u003cb\u003eCode Gemma Usage\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"7bb4p\"\u003eYou can use 4 user-defined tokens - 3 for FIM and a \u0026#34;\u0026lt;|file_separator|\u0026gt;\u0026#34; token for multi-file context support.\u003c/p\u003e\n\u003c/div\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003eBEFORE_CURSOR\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;|fim_prefix|\u0026gt;\u0026#34;\u003c/span\u003e\n\u003cspan\u003eAFTER_CURSOR\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;|fim_suffix|\u0026gt;\u0026#34;\u003c/span\u003e\n\u003cspan\u003eAT_CURSOR\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;|fim_middle|\u0026gt;\u0026#34;\u003c/span\u003e\n\u003cspan\u003eFILE_SEPARATOR\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e=\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003cspan\u003e\u0026#34;\u0026lt;|file_separator|\u0026gt;\u0026#34;\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cp data-block-key=\"jy293\"\u003eImagine that you are trying to complete the code like the screen below.\u003c/p\u003e   \n\n\u003cdiv\u003e\n    \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image4_QvGp009.original.png\" alt=\"Code snippet example - CodeGemma (2B and 7B)\"/\u003e\n        \n        \n    \u003c/p\u003e\n\u003c/div\u003e\n  \u003cp data-block-key=\"jy293\"\u003eAnd the input prompt should look like this\u003c/p\u003e   \n\n\u003cdiv\u003e\u003cpre\u003e\u003cspan\u003e\u003c/span\u003e\u003cspan\u003e\u0026lt;|\u003c/span\u003e\u003cspan\u003efim_prefix\u003c/span\u003e\u003cspan\u003e|\u0026gt;\u003c/span\u003e\u003cspan\u003eimport\u003c/span\u003e \u003cspan\u003e\u0026lt;|\u003c/span\u003e\u003cspan\u003efim_suffix\u003c/span\u003e\u003cspan\u003e|\u0026gt;\u003c/span\u003e\u003cspan\u003eif\u003c/span\u003e \u003cspan\u003e__name__\u003c/span\u003e \u003cspan\u003e==\u003c/span\u003e \u003cspan\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\\\u003cspan\u003en\u003c/span\u003e    \u003cspan\u003esys\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003cspan\u003eexit\u003c/span\u003e\u003cspan\u003e(\u003c/span\u003e\u003cspan\u003e0\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003cspan\u003e\u0026lt;|\u003c/span\u003e\u003cspan\u003efim_middle\u003c/span\u003e\u003cspan\u003e|\u0026gt;\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e  \u003cdiv\u003e\n    \u003cp data-block-key=\"jy293\"\u003eThe model will provide \u0026#34;sys\u0026#34; as the suggested code completion.\u003c/p\u003e\u003cp data-block-key=\"aouoi\"\u003eYou can explore more about CodeGemma on \u003ca href=\"https://ai.google.dev/gemma/docs/codegemma/keras_quickstart\"\u003eCodeGemma / Quickstart\u003c/a\u003e.\u003c/p\u003e\u003ch2 data-block-key=\"99lt0\"\u003e\u003cbr/\u003e\u003cb\u003eWhat’s Next?\u003c/b\u003e\u003c/h2\u003e\u003cp data-block-key=\"2esk0\"\u003eThis article discussed the Gemma architecture.\u003c/p\u003e\u003cp data-block-key=\"6pvvv\"\u003eIn our next series of posts, you will explore the latest model, Gemma 2. With substantial enhancements in safety measures, this model surpasses its predecessor in terms of performance and efficiency during inference.\u003c/p\u003e\u003cp data-block-key=\"7tjuq\"\u003eStay tuned and thanks for reading!\u003c/p\u003e\u003chr/\u003e\u003ch2 data-block-key=\"86nlh\"\u003e\u003cbr/\u003eReferences\u003c/h2\u003e\u003ch3 data-block-key=\"1dro3\"\u003e\u003cb\u003e\u003cbr/\u003ePapers\u003c/b\u003e\u003c/h3\u003e\u003cul\u003e\u003cli data-block-key=\"5uv0c\"\u003e\u003ca href=\"https://arxiv.org/abs/1706.03762v7\"\u003eAttention Is All You Need\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"eoi4f\"\u003e\u003ca href=\"https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf\"\u003eGemma: Open Models Based on Gemini Research and Technology\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"760a3\"\u003e\u003ca href=\"https://arxiv.org/abs/2002.05202\"\u003eGLU Variants Improve Transformer\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"ft3in\"\u003e\u003ca href=\"https://arxiv.org/abs/1911.02150\"\u003eFast Transformer Decoding: One Write-Head is All You Need\u003c/a\u003e \u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3 data-block-key=\"cpgfm\"\u003e\u003cb\u003eCode Examples\u003c/b\u003e\u003c/h3\u003e\u003cp data-block-key=\"5p105\"\u003eGemma\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"ac2v3\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/get_started\"\u003eGet started with Gemma using KerasNLP\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp data-block-key=\"c2dft\"\u003eCodeGemma\u003c/p\u003e\u003cul\u003e\u003cli data-block-key=\"dd181\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/codegemma/keras_quickstart\"\u003eQuickstart\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"esv5o\"\u003e\u003ca href=\"https://ai.google.dev/gemma/docs/codegemma/code_assist_keras\"\u003eAI Assisted coding with CodeGemma\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 data-block-key=\"7usg\"\u003e\u003cbr/\u003e📋 The complete Gemma architecture series\u003c/h2\u003e\u003cul\u003e\u003cli data-block-key=\"b1db0\"\u003eGemma explained: An overview of Gemma model family architectures\u003c/li\u003e\u003cli data-block-key=\"f1v1d\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-new-in-gemma-2/\"\u003eGemma explained: What’s new in Gemma 2\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"b8rcl\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-recurrentgemma-architecture/\"\u003eGemma explained: RecurrentGemma architecture\u003c/a\u003e\u003c/li\u003e\u003cli data-block-key=\"d58np\"\u003e\u003ca href=\"https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/\"\u003eGemma explained: PaliGemma architecture\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\n\u003c/div\u003e \n      \u003c/div\u003e\n    \n\n    \n\n    \n    \n    \n  \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2024-08-15T00:00:00Z",
  "modifiedTime": null
}
