{
  "id": "aab11ba1-2185-428f-bddc-38a744d1b6f9",
  "title": "How to Set Kubernetes Resource Requests and Limits - A Saga to Improve Cluster Stability and Efficiency",
  "link": "https://buffer.com/resources/how-to-set-kubernetes-resource-requests-and-limits-e2-80-8a--e2-80-8aa-saga-to-improve-cluster-stability-and-efficiency/",
  "description": "Learn how to set kubernetes resource requests and limits. This post explains how to set Kubernetes resource requests and limits.",
  "author": "Steven Cheng",
  "published": "Wed, 13 Nov 2019 19:18:46 GMT",
  "source": "https://buffer.com/resources/overflow/rss/",
  "categories": [
    "Workplace of the future",
    "Overflow"
  ],
  "byline": "Tamilore Oladipo",
  "length": 8069,
  "excerpt": "Learn how to set kubernetes resource requests and limits. This post explains how to set Kubernetes resource requests and limits.",
  "siteName": "Buffer: All-you-need social media toolkit for small businesses",
  "favicon": "https://buffer.com/static/icons/apple-touch-icon.png",
  "text": "A mysterySo, it all started on September 1st, right after our cluster upgrade from 1.11 to 1.12. Almost on the next day, we began to see alerts on kubelet reported by Datadog. On some days we would get a few (3 – 5) of them, other days we would get more than 10 in a single day. The alert monitor is based on a Datadog check – kubernetes.kubelet.check, and it’s triggered whenever the kubelet process is down in a node.We know kubelet plays an important role in Kubernetes scheduling. Not having it running properly in a node would directly remove that node from a functional cluster. Having more nodes with problematic kubelet then we get a cluster degradation. Now, Imagine waking up to 16 alerts in the morning. It was absolutely terrifying.What really puzzled us was all the services running on the problematic nodes seemed to be innocuous. In some cases, there were only a handful of running services, and some high CPU usage right before. It was extremely hard to point the finger on anything when the potential offender might have left the scene, thus leaving no trace for us to diagnose further. Funny enough, there weren’t any obvious performance impact such as request latency across our services. This little fact added even more mystery to the whole thing.This phenomenon continued to start around the same time every day (5:30AM PT), and usually stopped before noon, except for the weekends. To a point, I felt I could use these Datadog alerts for my alarm clock. Not super fun, and I certainly got some grey hair with this challenge.Our investigationFrom the start, we knew this was going to be a tough investigation that would require a systematic approach. For brevity, I’m going to just list out some key experiments we attempted and spare you from the details. As much as they are good investigative steps, I don’t believe they are important for this post. Here are what we triedWe upgraded the cluster from 1.12 to 1.13We created some tainted nodes and moved all our cronjobs to themWe created more tainted nodes and moved most CPU consuming workers to themWe scaled up the cluster by almost 20%, from 42 nodes to 50 nodesWe scaled down the cluster again because we didn’t see any improvementsWe recycled (delete and recreate) all the nodes that previously reported kubelet issue, only to see new nodes followed suit on the next dayJust between you and me, I even theorized the Datadog alert might be broken because there wasn’t any obvious service performance impact. But I couldn’t bring myself to close the case knowing the culprit might still be at large.With a stroke of luck and a lot of witch-hunting, this piqued my attentionWe saw 10 buffer-publish pods were scheduled to a single node for around 10 minutes, only to be terminated shortly. At the same time the CPU usage spiked, kubelet cried out, and the pods disappeared from the node in the next few minutes after termination.No wonder we could never find anything after alerts. But what were so special about these pods, I thought? The only fact we had was the high CPU usage. Now, let’s take a look at the resource requests/limitsresources:   limits:     cpu: 1000m     memory: 512Mi   requests:     cpu: 100m     memory: 50MiCPU/Memory requests parameter tells Kubenetes how much resource should be allocated initiallyCPU/Memory limits parameter tells Kubenetes the max resource should be given under all circumstancesHere is a post that does a much better job in explaining this concept. I highly recommend reading it in full. Kudos to the team at kubecost!Now, back to where we are. The CPU requests/limits ratio is 10, and it should be fine, right? We allocate 0.1 CPU to a pod in the beginning and limit the max usage to 1 CPU. In this way, we have a conservative start while still having some kind of, although arbitrary upper boundary. It almost feels like we are following the best practice!Then I thought, this doesn’t make any sense at all. When 10 pods are scheduled in a single node the total CPU this parameter would allow for is 10 CPUs, but there aren’t 10 CPUs in a m4.xlarge node. What would happen during our peak-hours, say 5:30AM PT when America wakes up? Now I can almost visualize a grim picture of these node killing pods taking all CPU, to a point that even kubelet starts to die off, then the whole node just crash and burn.So now, what we can do about it?The remedyObviously the easiest way is to lower the CPU limits so these pods will kill themselves before they kill a node. But this doesn’t feel quite right to me. What if they really need that much CPU for normal operations, so throttling ( more on this) doesn’t lead to low performance.Okay, how about increasing the CPU requests so these pods are more spread out and don’t get scheduled into a single node. That sounds like a better plan, and that was the plan we implemented. Here are the details:Figure out how much you typically needI used the Datadog metric kubernetes.cpu.usage.total over the past week on the max reported value to give me some point of referenceYou could see in general it stays below 200m (0.2 CPU). This tells me it’s hard to go wrong with this value for CPU requests.Put a limit on itNow, this was the tricky part, and like most tricky things in life, there isn’t a simple solution. In my experiences, a good start would be 2x of the requests. In this case, it would be 400m (0.4 CPU). After the change, I spent some time eyeballing the service performance metrics to make sure the performance wasn’t impacted by CPU throttling. Chances are if it were, I would need to up it to a more reasonable number. This is more of an iterative process until you get right.Pay attention to the ratioIt’s key not to have low requests tricking Kubernetes into scheduling all pods into one node, only to exhaust all CPU with incredibly high limits. Ideally, the requests/limits should not be too far away from each other, say within 2x to 5x range. Otherwise, an application is considered to be too spiky, or even has some kind of leaks. If this is the case, it’s prudent to get to the bottom of the application footprints.Review regularlyApplications will undergo changes as long as they are active, so will their footprints. Make sure you have some kind of review process that takes you back to Step 1 (Figure out how much it typically needs). This is the only way to keep things in tip-top shape.ProfitSo, did it work? You bet! There were quite a few services in our cluster with disproportional requests/limits. After I adjusted these heavy-duty services, the cluster runs with more stability Here is how it looks now ?Wait! How about efficiency promised in the title? Please note the band has gotten more constricted after the changes. This shows the CPU resource across the cluster is being utilized more uniformly. This subsequently makes scaling up to have a linear effect, which is a lot more effective.Closing wordsIn contrast with deploying each service on a set of dedicated computing instances, service-oriented architecture allows many services to share a single Kubernetes cluster. Precisely because of this, each service now bears the responsibility of specifying its own resource requirements. And this step is not to be taken lightly. An unstable cluster affects all the residing services, and troubleshooting is often challenging. Admittedly, not all of us are experienced with this kind of new configurations. In the good ol’ days all we needed was to deploy our one thing on some servers, and scale up/down to our liking. I think this might be why I don’t see a lot of discussions around the resource parameters in Kubernetes. Through this post, it’s my hope to help a few people out there who are struggling with this new concept (I know I did). More importantly, perhaps learn from someone who has some other techniques. If you have any thoughts on this, please feel free to hit me up on Twitter.Originally published at http://github.com.Try Buffer for free140,000+ small businesses like yours use Buffer to build their brand on social media every monthGet started now",
  "image": "https://buffer.com/resources/content/images/wp-content/uploads/2019/11/2560px-Kubernetes_logo_without_workmark.svg_.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003ch3 id=\"a-mystery\"\u003eA mystery\u003c/h3\u003e\u003cp\u003eSo, it all started on September 1st, right after our cluster upgrade from 1.11 to 1.12. Almost on the next day, we began to see alerts on \u003ccode\u003ekubelet\u003c/code\u003e reported by Datadog. On some days we would get a few (3 – 5) of them, other days we would get more than 10 in a single day. The alert monitor is based on a Datadog check – \u003ccode\u003ekubernetes.kubelet.check\u003c/code\u003e, and it’s triggered whenever the \u003ccode\u003ekubelet\u003c/code\u003e process is down in a node.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://buffer.com/resources/content/images/wp-content/uploads/2019/11/bdca2c2a8545bf665d0933fdb7e67075_Image2019-11-06at12.48.47PM-945x1024.png\" alt=\"\" loading=\"lazy\" width=\"945\" height=\"1024\" srcset=\"https://buffer.com/resources/content/images/size/w600/wp-content/uploads/2019/11/bdca2c2a8545bf665d0933fdb7e67075_Image2019-11-06at12.48.47PM-945x1024.png 600w, https://buffer.com/resources/content/images/wp-content/uploads/2019/11/bdca2c2a8545bf665d0933fdb7e67075_Image2019-11-06at12.48.47PM-945x1024.png 945w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eWe know \u003ca href=\"https://kubernetes.io/docs/concepts/overview/components/#kubelet\" rel=\"noreferrer noopener\"\u003ekubelet\u003c/a\u003e plays an important role in Kubernetes scheduling. Not having it running properly in a node would directly remove that node from a functional cluster. Having more nodes with problematic \u003ccode\u003ekubelet\u003c/code\u003e then we get a cluster degradation. Now, Imagine waking up to 16 alerts in the morning. It was absolutely terrifying.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://buffer.com/resources/content/images/wp-content/uploads/2019/11/Image-2019-11-04-at-4.25.37-PM-1024x581.png\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"581\" srcset=\"https://buffer.com/resources/content/images/size/w600/wp-content/uploads/2019/11/Image-2019-11-04-at-4.25.37-PM-1024x581.png 600w, https://buffer.com/resources/content/images/size/w1000/wp-content/uploads/2019/11/Image-2019-11-04-at-4.25.37-PM-1024x581.png 1000w, https://buffer.com/resources/content/images/wp-content/uploads/2019/11/Image-2019-11-04-at-4.25.37-PM-1024x581.png 1024w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eWhat really puzzled us was all the services running on the problematic nodes seemed to be innocuous. In some cases, there were only a handful of running services, and some high CPU usage right before. It was extremely hard to point the finger on anything when the potential offender might have left the scene, thus leaving no trace for us to diagnose further. Funny enough, there weren’t any obvious performance impact such as request latency across our services. This little fact added even more mystery to the whole thing.\u003c/p\u003e\u003cp\u003eThis phenomenon continued to start around the same time every day (5:30AM PT), and usually stopped before noon, except for the weekends. To a point, I felt I could use these Datadog alerts for my alarm clock. Not super fun, and I certainly got some grey hair with this challenge.\u003c/p\u003e\u003ch3 id=\"our-investigation\"\u003eOur investigation\u003c/h3\u003e\u003cp\u003eFrom the start, we knew this was going to be a tough investigation that would require a systematic approach. For brevity, I’m going to just list out some key experiments we attempted and spare you from the details. As much as they are good investigative steps, I don’t believe they are important for this post. Here are what we tried\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe upgraded the cluster from 1.12 to 1.13\u003c/li\u003e\u003cli\u003eWe created some tainted nodes and moved all our cronjobs to them\u003c/li\u003e\u003cli\u003eWe created more tainted nodes and moved most CPU consuming workers to them\u003c/li\u003e\u003cli\u003eWe scaled up the cluster by almost 20%, from 42 nodes to 50 nodes\u003c/li\u003e\u003cli\u003eWe scaled down the cluster again because we didn’t see any improvements\u003c/li\u003e\u003cli\u003eWe recycled (delete and recreate) all the nodes that previously reported kubelet issue, only to see new nodes followed suit on the next day\u003c/li\u003e\u003cli\u003eJust between you and me, I even theorized the Datadog alert might be broken because there wasn’t any obvious service performance impact. But I couldn’t bring myself to close the case knowing the culprit might still be at large.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith a stroke of luck and a lot of witch-hunting, this piqued my attention\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://buffer.com/resources/content/images/wp-content/uploads/2019/11/1-1024x395.png\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"395\" srcset=\"https://buffer.com/resources/content/images/size/w600/wp-content/uploads/2019/11/1-1024x395.png 600w, https://buffer.com/resources/content/images/size/w1000/wp-content/uploads/2019/11/1-1024x395.png 1000w, https://buffer.com/resources/content/images/wp-content/uploads/2019/11/1-1024x395.png 1024w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eWe saw 10 \u003ccode\u003ebuffer-publish\u003c/code\u003e pods were scheduled to a single node for around 10 minutes, only to be terminated shortly. At the same time the CPU usage spiked, \u003ccode\u003ekubelet\u003c/code\u003e cried out, and the pods disappeared from the node in the next few minutes after termination.\u003c/p\u003e\u003cp\u003eNo wonder we could never find anything after alerts. But what were so special about these pods, I thought? The only fact we had was the high CPU usage. Now, let’s take a look at the resource requests/limits\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eresources:\n  limits:\n    cpu: 1000m\n    memory: 512Mi\n  requests:\n    cpu: 100m\n    memory: 50Mi\u003c/code\u003e\u003c/pre\u003e\u003cblockquote\u003e\u003cem\u003eCPU/Memory \u003c/em\u003e\u003cstrong\u003e\u003cem\u003erequests\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e parameter tells Kubenetes how much resource should be allocated initially\u003c/em\u003e\u003c/blockquote\u003e\u003cblockquote\u003e\u003cem\u003eCPU/Memory \u003c/em\u003e\u003cstrong\u003e\u003cem\u003elimits\u003c/em\u003e\u003c/strong\u003e\u003cem\u003e parameter tells Kubenetes the max resource should be given under all circumstances\u003c/em\u003e\u003c/blockquote\u003e\u003cp\u003eHere is a \u003ca href=\"http://blog.kubecost.com/blog/requests-and-limits/\" rel=\"noreferrer noopener\"\u003epost\u003c/a\u003e that does a much better job in explaining this concept. I highly recommend reading it in full. Kudos to the team at \u003ca href=\"https://kubecost.com/\" rel=\"noreferrer noopener\"\u003ekubecost\u003c/a\u003e!\u003c/p\u003e\u003cp\u003eNow, back to where we are. The CPU requests/limits ratio is 10, and it should be fine, right? We allocate 0.1 CPU to a pod in the beginning and limit the max usage to 1 CPU. In this way, we have a conservative start while still having some kind of, although arbitrary upper boundary. It almost feels like we are following the best practice!\u003c/p\u003e\u003cp\u003eThen I thought, this doesn’t make any sense at all. When 10 pods are scheduled in a single node the total CPU this parameter would allow for is 10 CPUs, but there aren’t 10 CPUs in a \u003ccode\u003em4.xlarge\u003c/code\u003e node. What would happen during our peak-hours, say 5:30AM PT when America wakes up? Now I can almost visualize a grim picture of these node killing pods taking all CPU, to a point that even \u003ccode\u003ekubelet\u003c/code\u003e starts to die off, then the whole node just crash and burn.\u003c/p\u003e\u003cp\u003eSo now, what we can do about it?\u003c/p\u003e\u003ch3 id=\"the-remedy\"\u003eThe remedy\u003c/h3\u003e\u003cp\u003eObviously the easiest way is to lower the CPU limits so these pods will kill themselves before they kill a node. But this doesn’t feel quite right to me. What if they really need that much CPU for normal operations, so throttling ( \u003ca href=\"https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/\" rel=\"noreferrer noopener\"\u003emore on this\u003c/a\u003e) doesn’t lead to low performance.\u003c/p\u003e\u003cp\u003eOkay, how about increasing the CPU requests so these pods are more spread out and don’t get scheduled into a single node. That sounds like a better plan, and that was the plan we implemented. Here are the details:\u003c/p\u003e\u003ch4 id=\"figure-out-how-much-you-typically-need\"\u003eFigure out how much you typically need\u003c/h4\u003e\u003cp\u003eI used the Datadog metric \u003ccode\u003ekubernetes.cpu.usage.total\u003c/code\u003e over the past week on the max reported value to give me some point of reference\u003c/p\u003e\u003cp\u003eYou could see in general it stays below 200m (0.2 CPU). This tells me it’s hard to go wrong with this value for CPU requests.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://buffer.com/resources/content/images/wp-content/uploads/2019/11/2-1024x478.png\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"478\" srcset=\"https://buffer.com/resources/content/images/size/w600/wp-content/uploads/2019/11/2-1024x478.png 600w, https://buffer.com/resources/content/images/size/w1000/wp-content/uploads/2019/11/2-1024x478.png 1000w, https://buffer.com/resources/content/images/wp-content/uploads/2019/11/2-1024x478.png 1024w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003ch4 id=\"put-a-limit-on-it\"\u003ePut a limit on it\u003c/h4\u003e\u003cp\u003eNow, this was the tricky part, and like most tricky things in life, there isn’t a simple solution. In my experiences, a good start would be 2x of the requests. In this case, it would be 400m (0.4 CPU). After the change, I spent some time eyeballing the service performance metrics to make sure the performance wasn’t impacted by CPU throttling. Chances are if it were, I would need to up it to a more reasonable number. This is more of an iterative process until you get right.\u003c/p\u003e\u003ch4 id=\"pay-attention-to-the-ratio\"\u003ePay attention to the ratio\u003c/h4\u003e\u003cp\u003eIt’s key not to have low requests tricking Kubernetes into scheduling all pods into one node, only to exhaust all CPU with incredibly high limits. Ideally, the requests/limits should not be too far away from each other, say within 2x to 5x range. Otherwise, an application is considered to be too spiky, or even has some kind of leaks. If this is the case, it’s prudent to get to the bottom of the application footprints.\u003c/p\u003e\u003ch4 id=\"review-regularly\"\u003eReview regularly\u003c/h4\u003e\u003cp\u003eApplications will undergo changes as long as they are active, so will their footprints. Make sure you have some kind of review process that takes you back to Step 1 (Figure out how much it typically needs). This is the only way to keep things in tip-top shape.\u003c/p\u003e\u003ch3 id=\"profit\"\u003eProfit\u003c/h3\u003e\u003cp\u003eSo, did it work? You bet! There were quite a few services in our cluster with disproportional requests/limits. After I adjusted these heavy-duty services, the cluster runs with more stability Here is how it looks now ?\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://buffer.com/resources/content/images/wp-content/uploads/2019/11/3-1024x353.png\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"353\" srcset=\"https://buffer.com/resources/content/images/size/w600/wp-content/uploads/2019/11/3-1024x353.png 600w, https://buffer.com/resources/content/images/size/w1000/wp-content/uploads/2019/11/3-1024x353.png 1000w, https://buffer.com/resources/content/images/wp-content/uploads/2019/11/3-1024x353.png 1024w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eWait! How about efficiency promised in the title? Please note the band has gotten more constricted after the changes. This shows the CPU resource across the cluster is being utilized more uniformly. This subsequently makes scaling up to have a linear effect, which is a lot more effective.\u003c/p\u003e\u003ch3 id=\"closing-words\"\u003eClosing words\u003c/h3\u003e\u003cp\u003eIn contrast with deploying each service on a set of dedicated computing instances, service-oriented architecture allows many services to share a single Kubernetes cluster. Precisely because of this, each service now bears the responsibility of specifying its own resource requirements. And this step is not to be taken lightly. An unstable cluster affects all the residing services, and troubleshooting is often challenging. Admittedly, not all of us are experienced with this kind of new configurations. In the good ol’ days all we needed was to deploy our one thing on some servers, and scale up/down to our liking. I think this might be why I don’t see a lot of discussions around the resource parameters in Kubernetes. Through this post, it’s my hope to help a few people out there who are struggling with this new concept (I know I did). More importantly, perhaps learn from someone who has some other techniques. If you have any thoughts on this, please feel free to hit me up on \u003ca href=\"https://twitter.com/stevenc81\" rel=\"noreferrer noopener\"\u003eTwitter\u003c/a\u003e.\u003c/p\u003e\u003chr/\u003e\u003cp\u003e\u003cem\u003eOriginally published at \u003c/em\u003e\u003ca href=\"https://gist.github.com/stevenc81/086d5ed7435ee66d4ea697e6d4461ca2\" rel=\"noreferrer noopener\"\u003e\u003cem\u003ehttp://github.com\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"midpost-cta\"\u003e\u003ch3\u003eTry Buffer for free\u003c/h3\u003e\u003cp\u003e140,000+ small businesses like yours use Buffer to build their brand on social media every month\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://login.buffer.com/signup?product=buffer\u0026amp;plan=free\u0026amp;cycle=year\u0026amp;cta=bufferBlogLibrary-post-midCTA-signup-1\" role=\"button\"\u003eGet started now\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "9 min read",
  "publishedTime": null,
  "modifiedTime": null
}
