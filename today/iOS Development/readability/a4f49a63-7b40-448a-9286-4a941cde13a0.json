{
  "id": "a4f49a63-7b40-448a-9286-4a941cde13a0",
  "title": "Scalable Continuous Integration for iOS",
  "link": "https://albertodebortoli.com/2024/01/03/scalable-continuous-integration-for-ios/",
  "description": "How Just Eat Takeaway.com leverage AWS, Packer, Terraform and GitHub Actions to manage a CI stack of macOS runners.",
  "author": "Alberto De Bortoli",
  "published": "Wed, 03 Jan 2024 22:26:50 GMT",
  "source": "https://albertodebortoli.com/rss/",
  "categories": [
    "CI",
    "mobile",
    "iOS",
    "AWS",
    "macOS"
  ],
  "byline": "Alberto De Bortoli",
  "length": 22974,
  "excerpt": "How Just Eat Takeaway.com leverage AWS, Packer, Terraform and GitHub Actions to manage a CI stack of macOS runners.",
  "siteName": "Alberto De Bortoli",
  "favicon": "",
  "text": "Originally published on the Just Eat Takeaway Engineering Blog.How Just Eat Takeaway.com leverage AWS, Packer, Terraform and GitHub Actions to manage a CI stack of macOS runners.ProblemAt Just Eat Takeaway.com (JET), our journey through continuous integration (CI) reflects a landscape of innovation and adaptation. Historically, JET’s multiple iOS teams operated independently, each employing their distinct CI solutions.The original Just Eat iOS and Android teams had pioneered an in-house CI solution anchored in Jenkins. This setup, detailed in our 2021 article, served as the backbone of our CI practices up until 2020. It was during this period that the iOS team initiated a pivotal migration: moving from in-house Mac Pros and Mac Minis to AWS EC2 macOS instances.Fast forward to 2023, a significant transition occurred within our Continuous Delivery Engineering (CDE) Platform Engineering team. The decision to adopt GitHub Actions company-wide has marked the end of our reliance on Jenkins while other teams are in the process of migrating away from solutions such as CircleCI and GitLab CI. This transition represented a fundamental shift in our CI philosophy. By moving away from Jenkins, we eliminated the need to maintain an instance for the Jenkins server and the complexities of managing how agents connected to it. Our focus then shifted to transforming our Jenkins pipelines into GitHub Actions workflows.This transformation extended beyond mere tool adoption. Our primary goal was to ensure that our macOS instances were not only scalable but also configured in code. We therefore enhanced our global CI practices and set standards across the entire company.Desired state of CIAs we embarked on our journey to refine and elevate our CI process, we envisioned a state-of-the-art CI system. Our goals were ambitious yet clear, focusing on scalability, automation, and efficiency. At the time of implementing the system, no other player in the industry seemed to have implemented the complete solution we envisioned.Below is a summary of our desired CI state:Instance setup in code: One primary objective was to enable the definition of the setup of the instances entirely in code. This includes specifying macOS version, Xcode version, Ruby version, and other crucial configurations. For this purpose, the HashiCorp tool Packer, emerged once again as an ideal solution, offering the flexibility and precision we required.IaC (Infrastructure as Code) for macOS instances: To define the infrastructure of our fleet of macOS instances, we leaned towards Terraform, another HashiCorp tool. Terraform provided us with the capability to not only deploy but also to scale and migrate our infrastructure seamlessly, crucially maintaining its state.Auto and Manual Scaling: We wanted the ability to dynamically create CI runners based on demand, ensuring that resources were optimally utilized and available when needed. To optimize resource utilization, especially during off-peak hours, we desired an autoscaling feature. Scaling down our CI runners on weekends when developer activity is minimal was critical to be cost-effective.Automated Connection to GitHub Actions: We aimed for the instances to automatically connect to GitHub Actions as runners upon deployment. This automation was crucial in eliminating manual interventions via SSH or VNC.Multi-Team Use: Our vision included CI runners that could be easily used by multiple teams across different time zones. This would not only maximize the utility of our infrastructure but also encourage reuse and standardization.Centralized Management via GitHub Actions: To further streamline our CI processes, we intended to run all tasks through GitHub Actions workflows. This approach would allow the teams to self-serve and alleviate the need for developers to use Docker or maintain local environments.Getting to the desired state was a journey that presented multiple challenges and constant adjustments to make sure we could migrate smoothly to a new system.Instance setup in codeWe implemented the desired configuration with Packer leveraging a number of Shell Provisioners and variables to configure the instance. Here are some of the configuration steps:Set user password (to allow remote desktop access)Resize the partition to use all the space available on the EBS volumeStart the Apple Remote Desktop agent and enable remote desktop accessUpdate Brew \u0026 Install Brew packagesInstall CloudWatch agentInstall rbenv/Ruby/bundlerInstall Xcode versionsInstall GitHub Actions actions-runnerCopy scripts to connect to GitHub Actions as a runnerCopy daemon to start the GitHub Actions self-hosted runner as a serviceSet macos-init modules to perform provisioning of the first launchWhile the steps above are naturally configuration steps to perform when creating the AMI, the macos-init modules include steps to perform once the instance becomes available.The create_ami workflow accepts inputs that are eventually passed to Packer to generate the AMI.packer build \\ --var ami_name_prefix=${{ env.AMI_NAME_PREFIX }} \\ --var region=${{ env.REGION }} \\ --var subnet_id=${{ env.SUBNET_ID }} \\ --var vpc_id=${{ env.VPC_ID }} \\ --var root_volume_size_gb=${{ env.ROOT_VOLUME_SIZE_GB }} \\ --var macos_version=${{ inputs.macos-version}} \\ --var ruby_version=${{ inputs.ruby-version }} \\ --var xcode_versions='${{ steps.parse-xcode-versions.outputs.list }}' \\ --var gha_version=${{ inputs.gha-version}} \\ bare-metal-runner.pkr.hclDifferent teams often use different versions of software, like Xcode. To accommodate this, we permit multiple versions to be installed on the same instance. The choice of which version to use is then determined within the GitHub Actions workflows.The seamless generation of AMIs has proven to be a significant enabler. For example, when Xcode 15.1 was released, we executed this workflow the same evening. In just over two hours, we had an AMI ready to deploy all the runners (it usually takes 70–100 minutes for a macOS AMI with 400GB of EBS volume to become ready after creation). This efficiency enabled our teams to use the new Xcode version just a few hours after its release.IaC (Infrastructure as Code) for macOS instancesInitially, we used distinct Terraform modules for each instance to facilitate the deployment and decommissioning of each one. Given the high cost of EC2 Mac instances, we managed this process with caution, carefully balancing host usage while also being mindful of the 24-hour minimum allocation time.We ultimately ended up using Terraform to define a single infrastructure (i.e. a single Terraform module) defining resources such as:aws_key_pair, aws_instance, aws_amiaws_security_group, aws_security_group_ruleaws_secretsmanager_secretaws_vpc, aws_subnetaws_cloudwatch_metric_alarmaws_sns_topic, aws_sns_topic_subscriptionaws_iam_role, aws_iam_policy, aws_iam_role_policy_attachment, aws_iam_instance_profileA crucial part was to use count in aws_instance, setting the value of a variable passed in from deploy_infra workflow. Terraform performs the necessary scaling upon changing the value.We have implemented a workflow to perform Terraform apply and destroy commands for the infrastructure. Only the AMI and the number of instances are required as inputs.terraform ${{ inputs.command }} \\ --var ami_name=${{ inputs.ami-name }} \\ --var fleet_size=${{ inputs.fleet-size }} \\ --auto-approveUsing the name of the AMI instead of the ID allows us to use the most recent one that was generated, useful in case of name clashes.variable \"ami_name\" { type = string } variable \"fleet_size\" { type = number } data \"aws_ami\" \"bare_metal_gha_runner\" { most_recent = true filter { name = \"name\" values = [\"${var.ami_name}\"] } ... } resource \"aws_instance\" \"bare_metal\" { count = var.fleet_size ami = data.aws_ami.bare_metal_gha_runner.id instance_type = \"mac2.metal\" tenancy = \"host\" key_name = aws_key_pair.bare_metal.key_name ... }Instead of maintaining multiple CI instances with varying software configurations, we concluded that it’s simpler and more efficient to have a single, standardised setup. While teams still have the option to create and deploy their unique setups, a smaller, unified system allows for easier support by a single global configuration.Auto and Manual ScalingThe deploy_infra workflow allows us to scale on demand but it doesn’t release the underlying dedicated hosts which are the resources that are ultimately billed.The autoscaling solution provided by AWS is great for VMs but gets sensibly more complex when actioned on dedicated hosts. Auto Scaling groups on macOS instances would require a Custom Managed License, a Host Resource Group and, of course, a Launch Template. Using only AWS services appears to be a lot of work to pull things together and the result wouldn’t allow for automatic release of the dedicated hosts.AirBnb mention in their Flexible Continuous Integration for iOS article that an internal scaling service was implemented:An internal scaling service manages the desired capacity of each environment’s Auto Scaling group.Some articles explain how to set up Auto Scaling groups for mac instances (see 1 and 2) but after careful consideration, we agreed that implementing a simple scaling service via GitHub Actions (GHA) was the easiest and most maintainable solution.We implemented 2 GHA workflows to fully automate the weekend autoscaling:Upscaling workflow to n, triggered at a specific time at the beginning of the working weekDownscaling workflow to 1, triggered at a specific time at the beginning of the weekendWe retain the capability for manual scaling, which is essential for situations where we need to scale down, such as on bank holidays, or scale up, like on release cut days, when activity typically exceeds the usual levels.Additionally, we have implemented a workflow that runs multiple times a day and tries to release all available hosts without an instance attached. This step lifts us from the burden of having to remember to release the hosts. Dedicated hosts take up to 110 minutes to move from the Pending to the Available state due to the scrubbing workflow performed by AWS.Manual scaling can be executed between the times the autoscaling workflows are triggered and they must be resilient to unexpected statuses of the infrastructure, which thankfully Terraform takes care of.Both down and upscaling are covered in the following flowchart:The autoscaling values are defined as configuration variables in the repo settings:It usually takes ~8 minutes for an EC2 mac2.metal instance to become reachable after creation, meaning that we can redeploy the entire infrastructure very quickly.Automated Connection to GitHub ActionsWe provide some user data when deploying the instances.resource \"aws_instance\" \"bare_metal\" { ami = data.aws_ami.bare_metal_gha_runner.id count = var.fleet_size ... user_data = \u003c\u003cEOF { \"github_enterprise\": \"\u003cGHE_ENTERPRISE_NAME\u003e\", \"github_pat_secret_manager_arn\": ${data.aws_secretsmanager_secret_version.ghe_pat.arn}, \"github_url\": \"\u003cGHE_ENTERPRISE_URL\u003e\", \"runner_group\": \"CI-MobileTeams\", \"runner_name\": \"bare-metal-runner-${count.index + 1}\" } EOFThe user data is stored in a specific folder by macos-init and we implement a module to copy the content to ~/actions-runner-config.json.### Group 10 ### [[Module]] Name = \"Create actions-runner-config.json from userdata\" PriorityGroup = 10 RunPerInstance = true FatalOnError = false [Module.Command] Cmd = [\"/bin/zsh\", \"-c\", 'instanceId=\"$(curl http://169.254.169.254/latest/meta-data/instance-id)\"; if [[ ! -z $instanceId ]]; then cp /usr/local/aws/ec2-macos-init/instances/$instanceId/userdata ~/actions-runner-config.json; fi'] RunAsUser = \"ec2-user\"which is in turn used by the configure_runner.sh script to configure the GitHub Actions runner.#!/bin/bash GITHUB_ENTERPRISE=$(cat $HOME/actions-runner-config.json | jq -r .github_enterprise) GITHUB_PAT_SECRET_MANAGER_ARN=$(cat $HOME/actions-runner-config.json | jq -r .github_pat_secret_manager_arn) GITHUB_PAT=$(aws secretsmanager get-secret-value --secret-id $GITHUB_PAT_SECRET_MANAGER_ARN | jq -r .SecretString) GITHUB_URL=$(cat $HOME/actions-runner-config.json | jq -r .github_url) RUNNER_GROUP=$(cat $HOME/actions-runner-config.json | jq -r .runner_group) RUNNER_NAME=$(cat $HOME/actions-runner-config.json | jq -r .runner_name) RUNNER_JOIN_TOKEN=` curl -L \\ -X POST \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer $GITHUB_PAT\"\\ $GITHUB_URL/api/v3/enterprises/$GITHUB_ENTERPRISE/actions/runners/registration-token | jq -r '.token'` MACOS_VERSION=`sw_vers -productVersion` XCODE_VERSIONS=`find /Applications -type d -name \"Xcode-*\" -maxdepth 1 \\ -exec basename {} \\; \\ | tr '\\n' ',' \\ | sed 's/,$/\\n/' \\ | sed 's/.app//g'` $HOME/actions-runner/config.sh \\ --unattended \\ --url $GITHUB_URL/enterprises/$GITHUB_ENTERPRISE \\ --token $RUNNER_JOIN_TOKEN \\ --runnergroup $RUNNER_GROUP \\ --labels ec2,bare-metal,$RUNNER_NAME,macOS-$MACOS_VERSION,$XCODE_VERSIONS \\ --name $RUNNER_NAME \\ --replaceThe above script is run by a macos-init module.### Group 11 ### [[Module]] Name = \"Configure the GHA runner\" PriorityGroup = 11 RunPerInstance = true FatalOnError = false [Module.Command] Cmd = [\"/bin/zsh\", \"-c\", \"/Users/ec2-user/configure_runner.sh\"] RunAsUser = \"ec2-user\"The GitHub documentation states that it’s possible to create a customized service starting from a provided template. It took some research and various attempts to find the right configuration that allows the connection without having to log in in the UI (over VNC) which would represent a blocker for a complete automation of the deployment. We believe that the single person who managed to get this right is Sébastien Stormacq who provided the correct solution.The connection to GHA can be completed with 2 more modules that install the runner as a service and load the custom daemon.### Group 12 ### [[Module]] Name = \"Run the self-hosted runner application as a service\" PriorityGroup = 12 RunPerInstance = true FatalOnError = false [Module.Command] Cmd = [\"/bin/zsh\", \"-c\", \"cd /Users/ec2-user/actions-runner \u0026\u0026 ./svc.sh install\"] RunAsUser = \"ec2-user\" ### Group 13 ### [[Module]] Name = \"Launch actions runner daemon\" PriorityGroup = 13 RunPerInstance = true FatalOnError = false [Module.Command] Cmd = [\"sudo\", \"/bin/launchctl\", \"load\", \"/Library/LaunchDaemons/com.justeattakeaway.actions-runner-service.plist\"] RunAsUser = \"ec2-user\"Using a daemon instead of an agent (see Creating Launch Daemons and Agents), doesn’t require us to set up any auto-login which on macOS is a bit of a tricky procedure and is best avoided also for security reasons. The following is the content of the daemon for completeness.\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e \u003cplist version=\"1.0\"\u003e \u003cdict\u003e \u003ckey\u003eLabel\u003c/key\u003e \u003cstring\u003ecom.justeattakeaway.actions-runner-service\u003c/string\u003e \u003ckey\u003eProgramArguments\u003c/key\u003e \u003carray\u003e \u003cstring\u003e/Users/ec2-user/actions-runner/runsvc.sh\u003c/string\u003e \u003c/array\u003e \u003ckey\u003eUserName\u003c/key\u003e \u003cstring\u003eec2-user\u003c/string\u003e \u003ckey\u003eGroupName\u003c/key\u003e \u003cstring\u003estaff\u003c/string\u003e \u003ckey\u003eWorkingDirectory\u003c/key\u003e \u003cstring\u003e/Users/ec2-user/actions-runner\u003c/string\u003e \u003ckey\u003eRunAtLoad\u003c/key\u003e \u003ctrue/\u003e \u003ckey\u003eStandardOutPath\u003c/key\u003e \u003cstring\u003e/Users/ec2-user/Library/Logs/com.justeattakeaway.actions-runner-service/stdout.log\u003c/string\u003e \u003ckey\u003eStandardErrorPath\u003c/key\u003e \u003cstring\u003e/Users/ec2-user/Library/Logs/com.justeattakeaway.actions-runner-service/stderr.log\u003c/string\u003e \u003ckey\u003eEnvironmentVariables\u003c/key\u003e \u003cdict\u003e \u003ckey\u003eACTIONS_RUNNER_SVC\u003c/key\u003e \u003cstring\u003e1\u003c/string\u003e \u003c/dict\u003e \u003ckey\u003eProcessType\u003c/key\u003e \u003cstring\u003eInteractive\u003c/string\u003e \u003ckey\u003eSessionCreate\u003c/key\u003e \u003ctrue/\u003e \u003c/dict\u003e \u003c/plist\u003eNot long after the deployment, all the steps above are executed and we can appreciate the runners appearing as connected.Multi-Team UseWe start the downscaling at 11:59 PM on Fridays and start the upscaling at 6:00 AM on Mondays. These times have been chosen in a way that guarantees a level of service to teams in the UK, the Netherlands (GMT+1) and Canada (Winnipeg is on GMT-6) accounting for BST (British Summer Time) and DST (Daylight Saving Time) too. Times are defined in UTC in the GHA workflow triggers and the local time of the runner is not taken into account.Since the instances are used to build multiple projects and tools owned by different teams, one problem we faced was that instances could get compromised if workflows included unsafe steps (e.g. modifications to global configurations).GitHub Actions has a documentation page about Hardening self-hosted runners specifically stating:Self-hosted runners for GitHub do not have guarantees around running in ephemeral clean virtual machines, and can be persistently compromised by untrusted code in a workflow.We try to combat such potential problems by educating people on how to craft workflows and rely on the quick redeployment of the stack should the instances break.We also run scripts before and after each job to ensure that instances can be reused as much as possible. This includes actions like deleting the simulators’ content, derived data, caches and archives.Centralized Management via GitHub ActionsThe macOS runners stack is defined in a dedicated macOS-runners repository. We implemented GHA workflows to cover the use cases that allow teams to self-serve:create macOS AMIdeploy CIdownscale for the weekend*upscale for the working week*release unused hosts** run without inputs and on a scheduled triggerThe runners running the jobs in this repo are small t2.micro Linux instances and come with the AWSCLI installed. An IAM instance role with the correct policies is used to make sure that aws ec2 commands allocate-hosts, describe-hosts and release-hosts could execute and we used jq to parse the API responses.A note on VM runnersIn this article, we discussed how we’ve used bare metal instances as runners. We have spent a considerable amount of time investigating how we could leverage the Virtualization framework provided by Apple to create virtual machines via Tart.If you’ve grasped the complexity of crafting a CI system of runners on bare metal instances, you can understand that introducing VMs makes the setup sensibly more convoluted which would be best discussed in a separate article.While a setup with Tart VMs has been implemented, we realised that it’s not performant enough to be put to use. Using VMs, the number of runners would double but we preferred to have native performance as the slowdown is over 40% compared to bare metal. Moreover, when it comes to running heavy UI test suites like ours, tests became too flaky.Testing the VMs, we also realised that the standard values of Throughput and IOPS on the EBS volume didn’t seem to be enough and caused disk congestion resulting in an unacceptable slowdown in performance.Here is a quick summary of the setup and the challenges we have faced.Virtual runners require 2 images: one for the VMs (tart) and one for the host (AMI).We use Packer to create VM images (Vanilla, Base, IDE, Tools) with the software required based on the templates provided by Tart and we store the OCI-compliant images on ECR. We create these images on CI with dedicated workflows similar to the one described earlier for bare metal but, in this case, macOS runners (instead of Linux) are required as publishing to ECR is done with tart which runs on macOS. Extra policies are required on the instance role to allow the runner to push to ECR (using temporary_iam_instance_profile_policy_document in Packer’s Amazon EBS).Apple set a limit to the number of VMs that can be run on an instance to 2, which would allow to double the size of the fleet of runners. Creating AMIs hosting 2 VMs is done with Packer and steps include cloning the image from ECR and configuring macos-init modules to run daemons to run the VMs via Tart.Deploying a virtual CI infrastructure is identical to what has already been described for bare metal.Connecting to and interfacing with the VMs happens from within the host. Opening SSH and especially VNC sessions from within the bare metal instances can be very confusing.The version of macOS on the host and the one on the VMs could differ. The version used on the host must be provided with an AMI by AWS, while the version used on the VMs is provided by Apple in IPSW files (see ipsw.me).The GHA runners run on the VMs meaning that the host won’t require Xcode installed nor any other software used by the workflows.VMs don’t allow for provisioning meaning we have to share configurations with the VMs via shared folders on the host with the — dir flag which causes extra setup complexity.VMs can’t easily run the GHA runner as a service. The svc script requires the runner to be configured first, an operation that cannot be done during the provisioning of the host. We therefore need to implement an agent ourselves to configure and connect the runner in a single script.To have UI access (a-la VNC) to the VMs, it’s first required to stop the VMs and then run them without the --no-graphics flag. At the time of writing, copy-pasting won’t work even if using the --vnc or --vnc-experimental flags.Tartelet is a macOS app on top of Tart that allows to manage multiple GitHub Actions runners in ephemeral environments on a single host machine. We didn’t consider it to avoid relying on too much third-party software and because it doesn’t have yet GitHub Enterprise support.Worth noting that the Tart team worked on an orchestration solution named Orchard that seems to be in its initial stage.ConclusionIn 2023 we have revamped and globalised our approach to CI. We have migrated from Jenkins to GitHub Actions as the CI/CD solution of choice for the whole group and have profoundly optimised and improved our pipelines introducing a greater level of job parallelisation.We have implemented a brand new scalable setup for bare metal macOS runners leveraging the HashiCorp tools Packer and Terraform. We have also implemented a setup based on Tart virtual machines.We have increased the size of our iOS team over the past few years, now including more than 40 developers, and still managed to be successful with only 5 bare metal instances on average, which is a clear statement of how performant and optimised our setup is.We have extended the capabilities of our Internal Developer Platform with a globalised approach to provide macOS runners; we feel that this setup will stand the test of time and serve well various teams across JET for years to come.",
  "image": "https://albertodebortoli.com/content/images/size/w1200/2024/01/1_SaGE67XSSeflkTKknVOHbg.webp",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\t\t\t\u003cp\u003e\u003cem\u003eOriginally published on the \u003c/em\u003e\u003ca href=\"https://medium.com/justeattakeaway-tech/scalable-continuous-integration-for-ios-15ff33435992?ref=albertodebortoli.com\" rel=\"noreferrer\"\u003e\u003cem\u003eJust Eat Takeaway Engineering Blog\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eHow Just Eat Takeaway.com leverage AWS, Packer, Terraform and GitHub Actions to manage a CI stack of macOS runners.\u003c/p\u003e\u003ch3 id=\"problem\"\u003eProblem\u003c/h3\u003e\u003cp\u003eAt Just Eat Takeaway.com (JET), our journey through continuous integration (CI) reflects a landscape of innovation and adaptation. Historically, JET’s multiple iOS teams operated independently, each employing their distinct CI solutions.\u003c/p\u003e\u003cp\u003eThe original Just Eat iOS and Android teams had pioneered an in-house CI solution anchored in Jenkins. This setup, detailed in our 2021 \u003ca href=\"https://medium.com/justeattakeaway-tech/the-continuous-integration-system-used-by-the-mobile-teams-28ba057ef628?ref=albertodebortoli.com\" rel=\"noopener\"\u003earticle\u003c/a\u003e, served as the backbone of our CI practices up until 2020. It was during this period that the iOS team initiated a pivotal migration: moving from in-house Mac Pros and Mac Minis to AWS EC2 macOS instances.\u003c/p\u003e\u003cp\u003eFast forward to 2023, a significant transition occurred within our Continuous Delivery Engineering (CDE) Platform Engineering team. The decision to adopt GitHub Actions company-wide has marked the end of our reliance on Jenkins while other teams are in the process of migrating away from solutions such as CircleCI and GitLab CI. This transition represented a fundamental shift in our CI philosophy. By moving away from Jenkins, we eliminated the need to maintain an instance for the Jenkins server and the complexities of managing how agents connected to it. Our focus then shifted to transforming our Jenkins pipelines into GitHub Actions workflows.\u003c/p\u003e\u003cp\u003eThis transformation extended beyond mere tool adoption. Our primary goal was to ensure that our macOS instances were not only scalable but also configured in code. We therefore enhanced our global CI practices and set standards across the entire company.\u003c/p\u003e\u003ch3 id=\"desired-state-of-ci\"\u003eDesired state of CI\u003c/h3\u003e\u003cp\u003eAs we embarked on our journey to refine and elevate our CI process, we envisioned a state-of-the-art CI system. Our goals were ambitious yet clear, focusing on scalability, automation, and efficiency. At the time of implementing the system, no other player in the industry seemed to have implemented the complete solution we envisioned.\u003c/p\u003e\u003cp\u003eBelow is a summary of our desired CI state:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInstance setup in code\u003c/strong\u003e: One primary objective was to enable the definition of the setup of the instances entirely in code. This includes specifying macOS version, Xcode version, Ruby version, and other crucial configurations. For this purpose, the HashiCorp tool Packer, emerged once again as an ideal solution, offering the flexibility and precision we required.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIaC (Infrastructure as Code) for macOS instances\u003c/strong\u003e: To define the infrastructure of our fleet of macOS instances, we leaned towards Terraform, another HashiCorp tool. Terraform provided us with the capability to not only deploy but also to scale and migrate our infrastructure seamlessly, crucially maintaining its state.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAuto and Manual Scaling\u003c/strong\u003e: We wanted the ability to dynamically create CI runners based on demand, ensuring that resources were optimally utilized and available when needed. To optimize resource utilization, especially during off-peak hours, we desired an autoscaling feature. Scaling down our CI runners on weekends when developer activity is minimal was critical to be cost-effective.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAutomated Connection to GitHub Actions\u003c/strong\u003e: We aimed for the instances to automatically connect to GitHub Actions as runners upon deployment. This automation was crucial in eliminating manual interventions via SSH or VNC.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMulti-Team Use\u003c/strong\u003e: Our vision included CI runners that could be easily used by multiple teams across different time zones. This would not only maximize the utility of our infrastructure but also encourage reuse and standardization.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCentralized Management via GitHub Actions\u003c/strong\u003e: To further streamline our CI processes, we intended to run all tasks through GitHub Actions workflows. This approach would allow the teams to self-serve and alleviate the need for developers to use Docker or maintain local environments.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eGetting to the desired state was a journey that presented multiple challenges and constant adjustments to make sure we could migrate smoothly to a new system.\u003c/p\u003e\u003ch3 id=\"instance-setup-in-code\"\u003e\u003cstrong\u003eInstance setup in code\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe implemented the desired configuration \u003ca href=\"https://aws.amazon.com/blogs/compute/building-amazon-machine-images-amis-for-ec2-mac-instances-with-packer/?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003ewith Packer\u003c/a\u003e leveraging a number of \u003ca href=\"https://developer.hashicorp.com/packer/docs/provisioners/shell?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eShell Provisioners\u003c/a\u003e and variables to configure the instance. Here are some of the configuration steps:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSet user password (to allow remote desktop access)\u003c/li\u003e\u003cli\u003eResize the partition to use all the space available on the EBS volume\u003c/li\u003e\u003cli\u003eStart the Apple Remote Desktop agent and enable remote desktop access\u003c/li\u003e\u003cli\u003eUpdate Brew \u0026amp; Install Brew packages\u003c/li\u003e\u003cli\u003eInstall CloudWatch agent\u003c/li\u003e\u003cli\u003eInstall rbenv/Ruby/bundler\u003c/li\u003e\u003cli\u003eInstall Xcode versions\u003c/li\u003e\u003cli\u003eInstall GitHub Actions actions-runner\u003c/li\u003e\u003cli\u003eCopy scripts to connect to GitHub Actions as a runner\u003c/li\u003e\u003cli\u003eCopy daemon to start the GitHub Actions self-hosted runner \u003ca href=\"https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/configuring-the-self-hosted-runner-application-as-a-service?platform=mac\u0026amp;ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eas a service\u003c/a\u003e\u003c/li\u003e\u003cli\u003eSet \u003ca href=\"https://github.com/aws/ec2-macos-init?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003emacos-init\u003c/a\u003e modules to perform provisioning of the first launch\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile the steps above are naturally configuration steps to perform when creating the AMI, the macos-init modules include steps to perform once the instance becomes available.\u003c/p\u003e\u003cp\u003eThe \u003ccode\u003ecreate_ami\u003c/code\u003e workflow accepts inputs that are eventually passed to Packer to generate the AMI.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2024/01/1_UTehZkRYTyeGW-xHO2uGEw.webp\" alt=\"\" loading=\"lazy\" width=\"788\" height=\"878\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2024/01/1_UTehZkRYTyeGW-xHO2uGEw.webp 600w, https://albertodebortoli.com/content/images/2024/01/1_UTehZkRYTyeGW-xHO2uGEw.webp 788w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cpre\u003e\u003ccode\u003epacker build \\\n  --var ami_name_prefix=${{ env.AMI_NAME_PREFIX }} \\\n  --var region=${{ env.REGION }} \\\n  --var subnet_id=${{ env.SUBNET_ID }} \\\n  --var vpc_id=${{ env.VPC_ID }} \\\n  --var root_volume_size_gb=${{ env.ROOT_VOLUME_SIZE_GB }} \\\n  --var macos_version=${{ inputs.macos-version}} \\\n  --var ruby_version=${{ inputs.ruby-version }} \\\n  --var xcode_versions=\u0026#39;${{ steps.parse-xcode-versions.outputs.list }}\u0026#39; \\\n  --var gha_version=${{ inputs.gha-version}} \\\n  bare-metal-runner.pkr.hcl\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eDifferent teams often use different versions of software, like Xcode. To accommodate this, we permit multiple versions to be installed on the same instance. The choice of which version to use is then determined within the GitHub Actions workflows.\u003c/p\u003e\u003cp\u003eThe seamless generation of AMIs has proven to be a significant enabler. For example, when Xcode 15.1 was released, we executed this workflow the same evening. In just over two hours, we had an AMI ready to deploy all the runners (it usually takes 70–100 minutes for a macOS AMI with 400GB of EBS volume to become ready after creation). This efficiency enabled our teams to use the new Xcode version just a few hours after its release.\u003c/p\u003e\u003ch3 id=\"iac-infrastructure-as-code-for-macos-instances\"\u003e\u003cstrong\u003eIaC \u003c/strong\u003e(Infrastructure as Code) \u003cstrong\u003efor macOS instances\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eInitially, we used distinct Terraform modules for each instance to facilitate the deployment and decommissioning of each one. Given the high cost of EC2 Mac instances, we managed this process with caution, carefully balancing host usage while also being mindful of the 24-hour minimum allocation time.\u003c/p\u003e\u003cp\u003eWe ultimately ended up using Terraform to define a single infrastructure (i.e. a single Terraform module) defining resources such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode\u003eaws_key_pair\u003c/code\u003e, \u003ccode\u003eaws_instance\u003c/code\u003e, \u003ccode\u003eaws_ami\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eaws_security_group\u003c/code\u003e, \u003ccode\u003eaws_security_group_rule\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eaws_secretsmanager_secret\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eaws_vpc\u003c/code\u003e, \u003ccode\u003eaws_subnet\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eaws_cloudwatch_metric_alarm\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eaws_sns_topic\u003c/code\u003e, \u003ccode\u003eaws_sns_topic_subscription\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eaws_iam_role\u003c/code\u003e, \u003ccode\u003eaws_iam_policy\u003c/code\u003e, \u003ccode\u003eaws_iam_role_policy_attachment\u003c/code\u003e, \u003ccode\u003eaws_iam_instance_profile\u003c/code\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eA crucial part was to use \u003ccode\u003ecount\u003c/code\u003e in \u003ccode\u003eaws_instance\u003c/code\u003e, setting the value of a variable passed in from \u003ccode\u003edeploy_infra\u003c/code\u003e workflow. Terraform performs the necessary scaling upon changing the value.\u003c/p\u003e\u003cp\u003eWe have implemented a workflow to perform Terraform \u003ccode\u003eapply\u003c/code\u003e and \u003ccode\u003edestroy\u003c/code\u003e commands for the infrastructure. Only the AMI and the number of instances are required as inputs.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2024/01/1_DBUpUssi6rj8DqpIL6VXkA.webp\" alt=\"\" loading=\"lazy\" width=\"762\" height=\"680\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2024/01/1_DBUpUssi6rj8DqpIL6VXkA.webp 600w, https://albertodebortoli.com/content/images/2024/01/1_DBUpUssi6rj8DqpIL6VXkA.webp 762w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cpre\u003e\u003ccode\u003eterraform ${{ inputs.command }} \\\n  --var ami_name=${{ inputs.ami-name }} \\\n  --var fleet_size=${{ inputs.fleet-size }} \\\n  --auto-approve\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eUsing the name of the AMI instead of the ID allows us to use the most recent one that was generated, useful in case of name clashes.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003evariable \u0026#34;ami_name\u0026#34; {\n  type = string\n}\n\nvariable \u0026#34;fleet_size\u0026#34; {\n  type = number\n}\n\ndata \u0026#34;aws_ami\u0026#34; \u0026#34;bare_metal_gha_runner\u0026#34; {\n  most_recent = true\n\n  filter {\n    name   = \u0026#34;name\u0026#34;\n    values = [\u0026#34;${var.ami_name}\u0026#34;]\n  }\n  \n  ...\n}\n\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;bare_metal\u0026#34; {\n  count         = var.fleet_size\n  ami           = data.aws_ami.bare_metal_gha_runner.id\n  instance_type = \u0026#34;mac2.metal\u0026#34;\n  tenancy       = \u0026#34;host\u0026#34;\n  key_name      = aws_key_pair.bare_metal.key_name\n  ...\n}\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eInstead of maintaining multiple CI instances with varying software configurations, we concluded that it’s simpler and more efficient to have a single, standardised setup. While teams still have the option to create and deploy their unique setups, a smaller, unified system allows for easier support by a single global configuration.\u003c/p\u003e\u003ch3 id=\"auto-and-manual-scaling\"\u003e\u003cstrong\u003eAuto and Manual Scaling\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe \u003ccode\u003edeploy_infra\u003c/code\u003e workflow allows us to scale on demand but it doesn’t release the underlying dedicated hosts which are the resources that are ultimately billed.\u003c/p\u003e\u003cp\u003eThe autoscaling solution provided by AWS is great for VMs but gets sensibly more complex when actioned on dedicated hosts. \u003ca href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eAuto Scaling groups\u003c/a\u003e on macOS instances would require a \u003ca href=\"https://docs.aws.amazon.com/license-manager/latest/userguide/create-license-configuration.html?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eCustom Managed License\u003c/a\u003e, a \u003ca href=\"https://docs.aws.amazon.com/license-manager/latest/userguide/host-resource-groups.html?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eHost Resource Group\u003c/a\u003e and, of course, a \u003ca href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eLaunch Template\u003c/a\u003e. Using only AWS services appears to be a lot of work to pull things together and the result wouldn’t allow for automatic release of the dedicated hosts.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2024/01/1_RF0qy26XVnZldt3SpFkc8w.webp\" alt=\"\" loading=\"lazy\" width=\"1950\" height=\"224\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2024/01/1_RF0qy26XVnZldt3SpFkc8w.webp 600w, https://albertodebortoli.com/content/images/size/w1000/2024/01/1_RF0qy26XVnZldt3SpFkc8w.webp 1000w, https://albertodebortoli.com/content/images/size/w1600/2024/01/1_RF0qy26XVnZldt3SpFkc8w.webp 1600w, https://albertodebortoli.com/content/images/2024/01/1_RF0qy26XVnZldt3SpFkc8w.webp 1950w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eAirBnb mention in their \u003ca href=\"https://medium.com/airbnb-engineering/flexible-continuous-integration-for-ios-4ab33ea4072f?ref=albertodebortoli.com\" rel=\"noopener\"\u003eFlexible Continuous Integration for iOS\u003c/a\u003e article that an internal scaling service was implemented:\u003c/p\u003e\u003cblockquote\u003eAn internal scaling service manages the desired capacity of each environment’s Auto Scaling group.\u003c/blockquote\u003e\u003cp\u003eSome articles explain how to set up Auto Scaling groups for mac instances (see \u003ca href=\"https://aws.amazon.com/blogs/compute/implementing-autoscaling-for-ec2-mac-instances/?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003e1\u003c/a\u003e and \u003ca href=\"https://devdosvid.blog/2021/10/24/auto-scaling-group-for-your-macos-ec2-instances-fleet/?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003e2\u003c/a\u003e) but after careful consideration, we agreed that implementing a simple scaling service via GitHub Actions (GHA) was the easiest and most maintainable solution.\u003c/p\u003e\u003cp\u003eWe implemented 2 GHA workflows to fully automate the weekend autoscaling:\u003c/p\u003e\u003cul\u003e\u003cli\u003eUpscaling workflow to \u003ccode\u003en\u003c/code\u003e, triggered at a specific time at the beginning of the working week\u003c/li\u003e\u003cli\u003eDownscaling workflow to \u003ccode\u003e1\u003c/code\u003e, triggered at a specific time at the beginning of the weekend\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe retain the capability for manual scaling, which is essential for situations where we need to scale down, such as on bank holidays, or scale up, like on release cut days, when activity typically exceeds the usual levels.\u003c/p\u003e\u003cp\u003eAdditionally, we have implemented a workflow that runs multiple times a day and tries to release all available hosts without an instance attached. This step lifts us from the burden of having to remember to release the hosts. Dedicated hosts take up to 110 minutes to move from the Pending to the Available state due to the \u003ca href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html?ref=albertodebortoli.com#mac-instance-stop\" rel=\"noopener ugc nofollow\"\u003escrubbing workflow\u003c/a\u003e performed by AWS.\u003c/p\u003e\u003cp\u003eManual scaling can be executed between the times the autoscaling workflows are triggered and they must be resilient to unexpected statuses of the infrastructure, which thankfully Terraform takes care of.\u003c/p\u003e\u003cp\u003eBoth down and upscaling are covered in the following flowchart:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2024/01/1_uT28oUGw_PZWKCs7_IUlag.webp\" alt=\"\" loading=\"lazy\" width=\"1400\" height=\"1348\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2024/01/1_uT28oUGw_PZWKCs7_IUlag.webp 600w, https://albertodebortoli.com/content/images/size/w1000/2024/01/1_uT28oUGw_PZWKCs7_IUlag.webp 1000w, https://albertodebortoli.com/content/images/2024/01/1_uT28oUGw_PZWKCs7_IUlag.webp 1400w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eThe autoscaling values are defined as configuration variables in the repo settings:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2024/01/1_In73VTcwiWA7iMoaTEClKA.webp\" alt=\"\" loading=\"lazy\" width=\"1722\" height=\"674\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2024/01/1_In73VTcwiWA7iMoaTEClKA.webp 600w, https://albertodebortoli.com/content/images/size/w1000/2024/01/1_In73VTcwiWA7iMoaTEClKA.webp 1000w, https://albertodebortoli.com/content/images/size/w1600/2024/01/1_In73VTcwiWA7iMoaTEClKA.webp 1600w, https://albertodebortoli.com/content/images/2024/01/1_In73VTcwiWA7iMoaTEClKA.webp 1722w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eIt usually takes ~8 minutes for an EC2 mac2.metal instance to become reachable after creation, meaning that we can redeploy the entire infrastructure very quickly.\u003c/p\u003e\u003ch3 id=\"automated-connection-to-github-actions\"\u003e\u003cstrong\u003eAutomated Connection to GitHub Actions\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe provide some user data when deploying the instances.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eresource \u0026#34;aws_instance\u0026#34; \u0026#34;bare_metal\u0026#34; {\n  ami       = data.aws_ami.bare_metal_gha_runner.id\n  count     = var.fleet_size\n  ...\n  user_data = \u0026lt;\u0026lt;EOF\n{\n    \u0026#34;github_enterprise\u0026#34;: \u0026#34;\u0026lt;GHE_ENTERPRISE_NAME\u0026gt;\u0026#34;,\n    \u0026#34;github_pat_secret_manager_arn\u0026#34;: ${data.aws_secretsmanager_secret_version.ghe_pat.arn},\n    \u0026#34;github_url\u0026#34;: \u0026#34;\u0026lt;GHE_ENTERPRISE_URL\u0026gt;\u0026#34;,\n    \u0026#34;runner_group\u0026#34;: \u0026#34;CI-MobileTeams\u0026#34;,\n    \u0026#34;runner_name\u0026#34;: \u0026#34;bare-metal-runner-${count.index + 1}\u0026#34;\n}\n  EOF\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe user data is stored in a specific folder by \u003ca href=\"https://github.com/aws/ec2-macos-init?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003emacos-init\u003c/a\u003e and we implement a module to copy the content to \u003ccode\u003e~/actions-runner-config.json\u003c/code\u003e.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e### Group 10 ###\n[[Module]]\n    Name = \u0026#34;Create actions-runner-config.json from userdata\u0026#34;\n    PriorityGroup = 10\n    RunPerInstance = true\n    FatalOnError = false\n    [Module.Command]\n        Cmd = [\u0026#34;/bin/zsh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#39;instanceId=\u0026#34;$(curl http://169.254.169.254/latest/meta-data/instance-id)\u0026#34;; if [[ ! -z $instanceId ]]; then cp /usr/local/aws/ec2-macos-init/instances/$instanceId/userdata ~/actions-runner-config.json; fi\u0026#39;]\n        RunAsUser = \u0026#34;ec2-user\u0026#34;\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ewhich is in turn used by the \u003ccode\u003econfigure_runner.sh\u003c/code\u003e script to configure the GitHub Actions runner.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e#!/bin/bash\n\nGITHUB_ENTERPRISE=$(cat $HOME/actions-runner-config.json | jq -r .github_enterprise)\nGITHUB_PAT_SECRET_MANAGER_ARN=$(cat $HOME/actions-runner-config.json | jq -r .github_pat_secret_manager_arn)\nGITHUB_PAT=$(aws secretsmanager get-secret-value --secret-id $GITHUB_PAT_SECRET_MANAGER_ARN | jq -r .SecretString)\nGITHUB_URL=$(cat $HOME/actions-runner-config.json | jq -r .github_url)\nRUNNER_GROUP=$(cat $HOME/actions-runner-config.json | jq -r .runner_group)\nRUNNER_NAME=$(cat $HOME/actions-runner-config.json | jq -r .runner_name)\n\nRUNNER_JOIN_TOKEN=` curl -L \\\n  -X POST \\\n  -H \u0026#34;Accept: application/vnd.github+json\u0026#34; \\\n  -H \u0026#34;Authorization: Bearer $GITHUB_PAT\u0026#34;\\\n  $GITHUB_URL/api/v3/enterprises/$GITHUB_ENTERPRISE/actions/runners/registration-token | jq -r \u0026#39;.token\u0026#39;`\n\nMACOS_VERSION=`sw_vers -productVersion`\n\nXCODE_VERSIONS=`find /Applications -type d -name \u0026#34;Xcode-*\u0026#34; -maxdepth 1 \\\n  -exec basename {} \\; \\\n  | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; \\\n  | sed \u0026#39;s/,$/\\n/\u0026#39; \\\n  | sed \u0026#39;s/.app//g\u0026#39;`\n\n$HOME/actions-runner/config.sh \\\n  --unattended \\\n  --url $GITHUB_URL/enterprises/$GITHUB_ENTERPRISE \\\n  --token $RUNNER_JOIN_TOKEN \\\n  --runnergroup $RUNNER_GROUP \\\n  --labels ec2,bare-metal,$RUNNER_NAME,macOS-$MACOS_VERSION,$XCODE_VERSIONS \\\n  --name $RUNNER_NAME \\\n  --replace\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe above script is run by a macos-init module.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e### Group 11 ###\n[[Module]]\n    Name = \u0026#34;Configure the GHA runner\u0026#34;\n    PriorityGroup = 11\n    RunPerInstance = true\n    FatalOnError = false\n    [Module.Command]\n        Cmd = [\u0026#34;/bin/zsh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/Users/ec2-user/configure_runner.sh\u0026#34;]\n        RunAsUser = \u0026#34;ec2-user\u0026#34;\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe \u003ca href=\"https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/configuring-the-self-hosted-runner-application-as-a-service?platform=mac\u0026amp;ref=albertodebortoli.com#customizing-the-self-hosted-runner-service-1\" rel=\"noopener ugc nofollow\"\u003eGitHub documentation\u003c/a\u003e states that it’s possible to create a customized service starting from a provided template. It took some research and various attempts to find the right configuration that allows the connection without having to log in in the UI (over VNC) which would represent a blocker for a complete automation of the deployment. We believe that the single person who managed to get this right is \u003ca href=\"https://github.com/sebsto?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eSébastien Stormacq\u003c/a\u003e who provided the \u003ca href=\"https://github.com/actions/runner/issues/1056?ref=albertodebortoli.com#issuecomment-1237426462\" rel=\"noopener ugc nofollow\"\u003ecorrect solution\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe connection to GHA can be completed with 2 more modules that install the runner as a service and load the custom daemon.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e### Group 12 ###\n[[Module]]\n    Name = \u0026#34;Run the self-hosted runner application as a service\u0026#34;\n    PriorityGroup = 12\n    RunPerInstance = true\n    FatalOnError = false\n    [Module.Command]\n        Cmd = [\u0026#34;/bin/zsh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;cd /Users/ec2-user/actions-runner \u0026amp;\u0026amp; ./svc.sh install\u0026#34;]\n        RunAsUser = \u0026#34;ec2-user\u0026#34;\n\n### Group 13 ###\n[[Module]]\n    Name = \u0026#34;Launch actions runner daemon\u0026#34;\n    PriorityGroup = 13\n    RunPerInstance = true\n    FatalOnError = false\n    [Module.Command]\n        Cmd = [\u0026#34;sudo\u0026#34;, \u0026#34;/bin/launchctl\u0026#34;, \u0026#34;load\u0026#34;, \u0026#34;/Library/LaunchDaemons/com.justeattakeaway.actions-runner-service.plist\u0026#34;]\n        RunAsUser = \u0026#34;ec2-user\u0026#34;\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eUsing a daemon instead of an agent (see \u003ca href=\"https://developer.apple.com/library/archive/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/CreatingLaunchdJobs.html?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eCreating Launch Daemons and Agents\u003c/a\u003e), doesn’t require us to set up any auto-login which on macOS is a bit of a tricky procedure and is best avoided also for security reasons. The following is the content of the daemon for completeness.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\n\u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt;\n\u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt;\n  \u0026lt;dict\u0026gt;\n    \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt;\n    \u0026lt;string\u0026gt;com.justeattakeaway.actions-runner-service\u0026lt;/string\u0026gt;\n    \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt;\n    \u0026lt;array\u0026gt;\n      \u0026lt;string\u0026gt;/Users/ec2-user/actions-runner/runsvc.sh\u0026lt;/string\u0026gt;\n    \u0026lt;/array\u0026gt;\n    \u0026lt;key\u0026gt;UserName\u0026lt;/key\u0026gt;\n    \u0026lt;string\u0026gt;ec2-user\u0026lt;/string\u0026gt;\n    \u0026lt;key\u0026gt;GroupName\u0026lt;/key\u0026gt;\n    \u0026lt;string\u0026gt;staff\u0026lt;/string\u0026gt;\n    \u0026lt;key\u0026gt;WorkingDirectory\u0026lt;/key\u0026gt;\n    \u0026lt;string\u0026gt;/Users/ec2-user/actions-runner\u0026lt;/string\u0026gt;\n    \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt;\n    \u0026lt;true/\u0026gt;    \n    \u0026lt;key\u0026gt;StandardOutPath\u0026lt;/key\u0026gt;\n    \u0026lt;string\u0026gt;/Users/ec2-user/Library/Logs/com.justeattakeaway.actions-runner-service/stdout.log\u0026lt;/string\u0026gt;\n    \u0026lt;key\u0026gt;StandardErrorPath\u0026lt;/key\u0026gt;\n    \u0026lt;string\u0026gt;/Users/ec2-user/Library/Logs/com.justeattakeaway.actions-runner-service/stderr.log\u0026lt;/string\u0026gt;\n    \u0026lt;key\u0026gt;EnvironmentVariables\u0026lt;/key\u0026gt;\n    \u0026lt;dict\u0026gt; \n      \u0026lt;key\u0026gt;ACTIONS_RUNNER_SVC\u0026lt;/key\u0026gt;\n      \u0026lt;string\u0026gt;1\u0026lt;/string\u0026gt;\n    \u0026lt;/dict\u0026gt;\n    \u0026lt;key\u0026gt;ProcessType\u0026lt;/key\u0026gt;\n    \u0026lt;string\u0026gt;Interactive\u0026lt;/string\u0026gt;\n    \u0026lt;key\u0026gt;SessionCreate\u0026lt;/key\u0026gt;\n    \u0026lt;true/\u0026gt;\n  \u0026lt;/dict\u0026gt;\n\u0026lt;/plist\u0026gt;\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNot long after the deployment, all the steps above are executed and we can appreciate the runners appearing as connected.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2024/01/1_SRzNVowEas-7_g5J1nxzTw.webp\" alt=\"\" loading=\"lazy\" width=\"1712\" height=\"726\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2024/01/1_SRzNVowEas-7_g5J1nxzTw.webp 600w, https://albertodebortoli.com/content/images/size/w1000/2024/01/1_SRzNVowEas-7_g5J1nxzTw.webp 1000w, https://albertodebortoli.com/content/images/size/w1600/2024/01/1_SRzNVowEas-7_g5J1nxzTw.webp 1600w, https://albertodebortoli.com/content/images/2024/01/1_SRzNVowEas-7_g5J1nxzTw.webp 1712w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003ch3 id=\"multi-team-use\"\u003eMulti-Team Use\u003c/h3\u003e\u003cp\u003eWe start the downscaling at 11:59 PM on Fridays and start the upscaling at 6:00 AM on Mondays. These times have been chosen in a way that guarantees a level of service to teams in the UK, the Netherlands (GMT+1) and Canada (Winnipeg is on GMT-6) accounting for BST (British Summer Time) and DST (Daylight Saving Time) too. Times are defined in UTC in the GHA workflow triggers and the local time of the runner is not taken into account.\u003c/p\u003e\u003cp\u003eSince the instances are used to build multiple projects and tools owned by different teams, one problem we faced was that instances could get compromised if workflows included unsafe steps (e.g. modifications to global configurations).\u003c/p\u003e\u003cp\u003eGitHub Actions has a documentation page about \u003ca href=\"https://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions?ref=albertodebortoli.com#hardening-for-self-hosted-runners\" rel=\"noopener ugc nofollow\"\u003eHardening self-hosted runners\u003c/a\u003e specifically stating:\u003c/p\u003e\u003cblockquote\u003eSelf-hosted runners for GitHub do not have guarantees around running in ephemeral clean virtual machines, and can be persistently compromised by untrusted code in a workflow.\u003c/blockquote\u003e\u003cp\u003eWe try to combat such potential problems by educating people on how to craft workflows and rely on the quick redeployment of the stack should the instances break.\u003c/p\u003e\u003cp\u003eWe also run \u003ca href=\"https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/running-scripts-before-or-after-a-job?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003escripts before and after each job\u003c/a\u003e to ensure that instances can be reused as much as possible. This includes actions like deleting the simulators’ content, derived data, caches and archives.\u003c/p\u003e\u003ch3 id=\"centralized-management-via-github-actions\"\u003eCentralized Management via GitHub Actions\u003c/h3\u003e\u003cp\u003eThe macOS runners stack is defined in a dedicated \u003ccode\u003emacOS-runners\u003c/code\u003e repository. We implemented GHA workflows to cover the use cases that allow teams to self-serve:\u003c/p\u003e\u003cul\u003e\u003cli\u003ecreate macOS AMI\u003c/li\u003e\u003cli\u003edeploy CI\u003c/li\u003e\u003cli\u003edownscale for the weekend*\u003c/li\u003e\u003cli\u003eupscale for the working week*\u003c/li\u003e\u003cli\u003erelease unused hosts*\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cem\u003e* run without inputs and on a scheduled trigger\u003c/em\u003e\u003c/p\u003e\u003cp\u003eThe runners running the jobs in this repo are small t2.micro Linux instances and come with the \u003ca href=\"https://aws.amazon.com/cli/?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eAWSCLI\u003c/a\u003e installed. An IAM instance role with the correct policies is used to make sure that \u003ccode\u003eaws ec2\u003c/code\u003e commands \u003ccode\u003eallocate-hosts\u003c/code\u003e, \u003ccode\u003edescribe-hosts\u003c/code\u003e and \u003ccode\u003erelease-hosts\u003c/code\u003e could execute and we used \u003ccode\u003ejq\u003c/code\u003e to parse the API responses.\u003c/p\u003e\u003ch3 id=\"a-note-on-vm-runners\"\u003eA note on VM runners\u003c/h3\u003e\u003cp\u003eIn this article, we discussed how we’ve used bare metal instances as runners. We have spent a considerable amount of time investigating how we could leverage the \u003ca href=\"https://developer.apple.com/documentation/virtualization?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eVirtualization framework\u003c/a\u003e provided by Apple to create virtual machines via \u003ca href=\"https://tart.run/?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eTart\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIf you’ve grasped the complexity of crafting a CI system of runners on bare metal instances, you can understand that introducing VMs makes the setup sensibly more convoluted which would be best discussed in a separate article.\u003c/p\u003e\u003cp\u003eWhile a setup with Tart VMs has been implemented, we realised that it’s not performant enough to be put to use. Using VMs, the number of runners would double but we preferred to have native performance as the slowdown is over 40% compared to bare metal. Moreover, when it comes to running heavy UI test suites like ours, tests became too flaky.\u003c/p\u003e\u003cp\u003eTesting the VMs, we also realised that the standard values of Throughput and IOPS on the EBS volume didn’t seem to be enough and caused disk congestion resulting in an unacceptable slowdown in performance.\u003c/p\u003e\u003cp\u003eHere is a quick summary of the setup and the challenges we have faced.\u003c/p\u003e\u003cul\u003e\u003cli\u003eVirtual runners require 2 images: one for the VMs (tart) and one for the host (AMI).\u003c/li\u003e\u003cli\u003eWe use Packer to create VM images (Vanilla, Base, IDE, Tools) with the software required based on the \u003ca href=\"https://github.com/cirruslabs/macos-image-templates?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003etemplates\u003c/a\u003e provided by Tart and we store the OCI-compliant images on ECR. We create these images on CI with dedicated workflows similar to the one described earlier for bare metal but, in this case, macOS runners (instead of Linux) are required as publishing to ECR is done with tart which runs on macOS. Extra policies are required on the instance role to allow the runner to push to ECR (using \u003ccode\u003etemporary_iam_instance_profile_policy_document\u003c/code\u003e in Packer’s \u003ca href=\"https://developer.hashicorp.com/packer/integrations/hashicorp/amazon/latest/components/builder/ebs?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eAmazon EBS\u003c/a\u003e).\u003c/li\u003e\u003cli\u003eApple set a limit to the number of VMs that can be run on an instance to 2, which would allow to double the size of the fleet of runners. Creating AMIs hosting 2 VMs is done with Packer and steps include cloning the image from ECR and configuring macos-init modules to run daemons to run the VMs via Tart.\u003c/li\u003e\u003cli\u003eDeploying a virtual CI infrastructure is identical to what has already been described for bare metal.\u003c/li\u003e\u003cli\u003eConnecting to and interfacing with the VMs happens from within the host. Opening SSH and especially VNC sessions from within the bare metal instances can be very confusing.\u003c/li\u003e\u003cli\u003eThe version of macOS on the host and the one on the VMs could differ. The version used on the host must be provided with an AMI by AWS, while the version used on the VMs is provided by Apple in IPSW files (see \u003ca href=\"https://ipsw.me/?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eipsw.me\u003c/a\u003e).\u003c/li\u003e\u003cli\u003eThe GHA runners run on the VMs meaning that the host won’t require Xcode installed nor any other software used by the workflows.\u003c/li\u003e\u003cli\u003eVMs don’t allow for provisioning meaning we have to share configurations with the VMs via shared folders on the host with the \u003ccode\u003e— dir\u003c/code\u003e flag which causes extra setup complexity.\u003c/li\u003e\u003cli\u003eVMs can’t easily run the GHA runner as a service. The \u003ccode\u003esvc\u003c/code\u003e script requires the runner to be configured first, an operation that cannot be done during the provisioning of the host. We therefore need to implement an agent ourselves to configure and connect the runner in a single script.\u003c/li\u003e\u003cli\u003eTo have UI access (a-la VNC) to the VMs, it’s first required to stop the VMs and then run them without the \u003ccode\u003e--no-graphics\u003c/code\u003e flag. At the time of writing, copy-pasting won’t work even if using the \u003ccode\u003e--vnc\u003c/code\u003e or \u003ccode\u003e--vnc-experimental\u003c/code\u003e flags.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/shapehq/tartelet?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eTartelet\u003c/a\u003e is a macOS app on top of Tart that allows to manage multiple GitHub Actions runners in ephemeral environments on a single host machine. We didn’t consider it to avoid relying on too much third-party software and because it \u003ca href=\"https://github.com/shapehq/tartelet/issues/27?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003edoesn’t have yet\u003c/a\u003e GitHub Enterprise support.\u003c/li\u003e\u003cli\u003eWorth noting that the Tart team worked on an orchestration solution named \u003ca href=\"https://github.com/cirruslabs/orchard?ref=albertodebortoli.com\" rel=\"noopener ugc nofollow\"\u003eOrchard\u003c/a\u003e that seems to be in its initial stage.\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\u003cp\u003eIn 2023 we have revamped and globalised our approach to CI. We have migrated from Jenkins to GitHub Actions as the CI/CD solution of choice for the whole group and have profoundly optimised and improved our pipelines introducing a greater level of job parallelisation.\u003c/p\u003e\u003cp\u003eWe have implemented a brand new scalable setup for bare metal macOS runners leveraging the HashiCorp tools Packer and Terraform. We have also implemented a setup based on Tart virtual machines.\u003c/p\u003e\u003cp\u003eWe have increased the size of our iOS team over the past few years, now including more than 40 developers, and still managed to be successful with only 5 bare metal instances on average, which is a clear statement of how performant and optimised our setup is.\u003c/p\u003e\u003cp\u003eWe have extended the capabilities of our Internal Developer Platform with a globalised approach to provide macOS runners; we feel that this setup will stand the test of time and serve well various teams across JET for years to come.\u003c/p\u003e\n\t\t\t\u003c/section\u003e\u003c/div\u003e",
  "readingTime": "24 min read",
  "publishedTime": "2024-01-03T22:26:50Z",
  "modifiedTime": "2024-01-04T07:37:10Z"
}
