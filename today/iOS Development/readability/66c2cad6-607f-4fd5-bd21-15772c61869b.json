{
  "id": "66c2cad6-607f-4fd5-bd21-15772c61869b",
  "title": "The Continuous Integration system used by the mobile teams",
  "link": "https://albertodebortoli.com/2021/07/23/the-continuous-integration-system-used-by-the-mobile-teams/",
  "description": "In this article, we’ll discuss the way our mobile teams have evolved the Continuous Integration (CI) stack over the recent years.",
  "author": "Alberto De Bortoli",
  "published": "Fri, 23 Jul 2021 09:22:24 GMT",
  "source": "https://albertodebortoli.com/rss/",
  "categories": [
    "iOS",
    "Continuous Integration",
    "Jenkins",
    "DevOps"
  ],
  "byline": "Alberto De Bortoli",
  "length": 14202,
  "excerpt": "In this article, we’ll discuss the way our mobile teams have evolved the Continuous Integration (CI) stack over the recent years.",
  "siteName": "Alberto De Bortoli",
  "favicon": "",
  "text": "Originally published on the Just Eat Takeaway Engineering Blog.OverviewIn this article, we’ll discuss the way our mobile teams have evolved the Continuous Integration (CI) stack over the recent years. We don’t have DevOps engineers in our team and, until recently, we had adopted a singular approach in which CI belongs to the whole team and everyone should be able to maintain it. This has proven to be difficult and extremely time-consuming.The Just Eat side of our newly merged entity has a dedicated team providing continuous integration and deployment tools to their teams but they are heavily backend-centric and there has been little interest in implementing solutions tailored for mobile teams. As is often the case in tech companies, there is a missing link between mobile and DevOps teams.The iOS team is the author and first consumer of the solution described but, as you can see, we have ported the same stack to Android as well. We will mainly focus on the iOS implementation in this article, with references to Android as appropriate.2016–2020Historically speaking, the iOS UK app was running on Bitrise because it was decided not to invest time in implementing a CI solution, while the Bristol team was using a Jenkins version installed by a different team. This required manual configuration with custom scripts and it had custom in-house hardware. These are two quite different approaches indeed and, at this stage, things were not great but somehow good enough. It’s fair to say we were still young on the DevOps front.When we merged the teams, it was clear that we wanted to unify the CI solution and the obvious choice for a company of our size was to not use a third-party service, bringing us to invest more and more in Jenkins. Only one team member had good knowledge of Jenkins but the rest of the team showed little interest in learning how to configure and maintain it, causing the stack to eventually become a dumping ground of poorly configured jobs.It was during this time that we introduced Fastlane (making the common tasks portable), migrated the UK app from Bitrise to Jenkins, started running the UI tests on Pull Requests, and other small yet sensible improvements.2020–2021Starting in mid-2020 the iOS team has significantly revamped its CI stack and given it new life. The main goals we wanted to achieve (and did by early 2021) were:Revisit the pipelinesClear Jenkins configuration and deployment strategyMake use of AWS Mac instancesReduce the pool size of our mac hardwareShare our knowledge across teams betterSince the start of the pandemic, we have implemented the pipelines in code (bidding farewell to custom bash scripts), we moved to a monorepo which was a massive step ahead and began using SonarQube even more aggressively.We added Slack reporting and PR Assigner, an internal tool implemented by Andrea Antonioni. We also automated the common release tasks such as cutting and completing a release and uploading the dSYMS to Firebase.We surely invested a lot in optimizing various aspects such as running the UI tests in parallel, making use of shallow repo cloning, We also moved to not checking in the pods within the repo. This, eventually, allowed us to reduce the number of agents for easier infrastructure maintenance.Automating the infrastructure deployment of Jenkins was a fundamental shift compared to the previous setup and we have introduced AWS Mac instances replacing part of the fleet of our in-house hardware.CI system setupLet’s take a look at our stack. Before we start, we’d like to thank Isham Araia for having provided a proof of concept for the configuration and deployment of Jenkins. He talked about it at https://ish-ar.io/jenkins-at-scale/ and it represented a fundamental starting point, saving us several days of researching.Triggering flowStarting from the left, we have our repositories (plural, as some shared dependencies don’t live in the monorepo). The repositories contain the pipelines in the form of Jenkinsfiles and they call into Fastlane lanes. Pretty much every action is a lane, from running the tests to archiving for the App Store to creating the release branches.Changes are raised through pull requests that trigger Jenkins. There are other ways to trigger Jenkins: by user interaction (for things such as completing a release or archiving and uploading the app to App Store Connect) and cron triggers (for things such as building the nightly build, running the tests on the develop branch every 12 hours, or uploading the PACT contract to the broker).Once Jenkins has received the information, it will then schedule the jobs to one of the agents in our pool, which is now made up of 5 agents, 2 in the cloud and 3 in-house mac pros.Reporting flowNow that we’ve talked about the first part of the flow, let’s talk about the flow of information coming back at us.Every PR triggers PR Assigner, a tool that works out a list of reviewers to assign to pull requests and notifies engineers via dedicated Slack channels. The pipelines post on Slack, providing info about all the jobs that are being executed so we can read the history without having to log into Jenkins. We have in place the standard notification flow from Jenkins to GitHub to set the status checks and Jenkins also notifies SonarQube to verify that any change meets the quality standards (namely code coverage percentage and coding rules).We also have a smart lambda named SonarQubeStatusProcessor that reports to GitHub, written by Alan Nichols. This is due to a current limitation of SonarQube, which only allows reporting the status of one SQ project to one GitHub repo. Since we have a monorepo structure we had to come up with this neat customization to report the SQ status for all the modules that have changed as part of the PR.ConfigurationLet’s see what the new interesting parts of Jenkins are. Other than Jenkins itself and several plugins, it’s important to point out JCasC and Job DSL.JCasC stands for Jenkins Configuration as Code, and it allows you to configure Jenkins via a yaml file.The point here is that nobody should ever touch the Jenkins settings directly from the configuration page, in the same way, one ideally shouldn’t apply configuration changes manually in any dashboard. The CasC file is where we define the Slack integration, the user roles, SSO configuration, the number of agents and so on.We could also define the jobs in CasC but we go a step further than that.We use the Job DSL plugin that allows you to configure the jobs in groovy and in much more detail. One job we configure in the CasC file though is the seed job. This is a simple freestyle job that will go pick the jobs defined with Job DSL and create them in Jenkins.DeploymentLet’s now discuss how we can get a configured Jenkins instance on EC2. In other words, how do we deploy Jenkins?We use a combination of tools that are bread and butter for DevOps people.The commands on the left spawn a Docker container that calls into the tools on the right.We start with Packer which allows us to create the AMI (Amazon Machine Image) together with Ansible, allowing us to configure an environment easily (much more easily than Chef or Puppet).Running the create-image command the script will:1. Create a temporary EC2 instance2. Connect to the instance and execute an ansible playbookOur playbook encompasses a number of steps, here’s a summary:install the Jenkins version for the given Linux distributioninstall Nginxcopy the SSL cert overconfigure nginx w/ SSL termination and reverse proxyinstall the plugins for JenkinsOnce the playbook is executed, Packer will export an AMI in EC2 with all of this in it and destroy the instance that was used.With the AMI ready, we can now proceed to deploy our Jenkins. For the actual deployment, we use Terraform which allows us to define our infrastructure in code.The deploy command runs Terraform under the hood to set up the infrastructure, here’s a summary of the task:create an IAM Role + IAM Policyconfigure security groupscreate the VPC and subnet to use with a specific CIDER block and the subnetcreate any private key pair to connect over SSHdeploy the instance using a static private IP (it has to be static otherwise the A record in Route53 would break)copy the JCasC configuration file over so that when Jenkins starts it picks that up to configure itselfThe destroy command runs a “terraform destroy” and destroys everything that was created with the deploy command. Deploy and destroy balance each other out.Now that we have Jenkins up and running, we need to give it some credentials so our pipelines are able to work properly. A neat way of doing this is by having the secrets (SSH keys, Firebase tokens, App Store Connect API Key and so forth) in AWS Secrets Manager which is based on KMS and use a Jenkins plugin to allow Jenkins to access them.It’s important to note that developers don’t have to install Packer, Ansible, Terraform or even the AWS CLI locally because the commands run a Docker container that does the real work with all the tools installed. As a result, the only thing one should have installed is really Docker.CI agentsEnough said about Jenkins, it’s time to talk about the agents.As you probably already know, in order to run tests, compile and archive iOS apps we need Xcode, which is only available on macOS, so Linux or Windows instances are not going to cut it.We experimented with the recently introduced AWS Mac instances and they are great, ready out-of-the-box with minimal configuration on our end.What we were hoping to get to with this recent work was the ability to leverage the Jenkins Cloud agents. That would have been awesome because it would have allowed us to:let Jenkins manage the agent instancesscale the agent pool according to the load on CISadly we couldn't go that far. Limitations are:the bootstrapping of a mac1.metal takes around 15 minutesreusing the dedicated host after having stopped an instance can take up to 3 hours — during that time we just pay for a host that is not usableWhen you stop or terminate a Mac instance, Amazon EC2 performs a scrubbing workflow on the underlying Dedicated Host to erase the internal SSD, to clear the persistent NVRAM variables, and if needed, to update the bridgeOS software on the underlying Mac mini.This ensures that Mac instances provide the same security and data privacy as other EC2 Nitro instances. It also enables you to run the latest macOS AMIs without manually updating the bridgeOS software. During the scrubbing workflow, the Dedicated Host temporarily enters the pending state. If the bridgeOS software does not need to be updated, the scrubbing workflow takes up to 50 minutes to complete. If the bridgeOS software needs to be updated, the scrubbing workflow can take up to 3 hours to complete.Source: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.htmlIn other words: scaling mac instances is not an option and leaving the instances up 24/7 seems to be the easiest option. This is especially valid if your team is distributed and jobs could potentially run over the weekend as well, saving you the hassle of implementing downscaling ahead of the weekend.There are some pricing and instance allocation considerations to make. Note that On-Demand Mac1 Dedicated Hosts have a minimum host allocation and billing duration of 24 hours.“You can purchase Savings Plans to lower your spend on Dedicated Hosts. Savings Plans is a flexible pricing model that provides savings of up to 72% on your AWS compute usage. This pricing model offers lower prices on Amazon EC2 instances usage, regardless of instance family, size, OS, tenancy or AWS Region.”Source: https://aws.amazon.com/ec2/dedicated-hosts/pricing/The On-Demand rate is $1.207 per hour.I’d like to stress that no CI solution comes for free. I’ve often heard developers indicating that Travis and similar products are cheaper. The truth is that the comparison is not even remotely reasonable: virtual boxes are incredibly slow compared to native Apple hardware and take ridiculous bootstrapping times. Even the smallest projects suffer terribly.One might ask if it’s at least possible to use the same configuration process we used for the Jenkins instance (with Packer and Ansible) but sadly we hit additional limitations:Apple requires 2FA for downloading Xcode via xcode-versionApple requires 2FA for signing into XcodeThe above pretty much causes the configuration flow to fall apart making it impossible to configure an instance via Ansible.Cloud agents for AndroidIt was a different story for Android, in which we could easily configure the agent instance with Ansible and therefore leverage the Cloud configuration to allow automatic agent provisioning.This configuration is defined via CasC as everything else.To better control EC2 usage and costs, a few settings come in handy:minimum number of instances (up at all times)minimum number of spare instances (created to accommodate future jobs)instance cap: the maximum number of instances that can be provisioned at the same timeidle termination time: how long agents should be kept alive after they have completed the jobAll of the above allow for proper scaling and a lot less maintenance compared to the iOS setup. A simple setup with 0 instances up at all times allows saving costs overnight and given that in our case the bootstrapping takes only 2 minutes, we can rely on the idle time setting.ConclusionsSetting up an in-house CI is never a straightforward process and it requires several weeks of dedicated work.After years of waiting, Apple has announced Xcode Cloud which we believe will drastically change the landscape of continuous integration on iOS. The solution will most likely cause havoc for companies such as Bitrise and CircleCI and it’s reasonable to assume the pricing will be competitive compared to AWS, maybe running on custom hardware that only Apple is able to produce.A shift this big will take time to occur, so we foresee our solution to stay in use for quite some time.We hope to have inspired you on how a possible setup for mobile teams could be and informed you on what are the pros \u0026 cons of using EC2 mac instances.",
  "image": "https://albertodebortoli.com/content/images/size/w1200/2021/06/GettyImages-1279041535.jpg",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection\u003e\n\t\t\t\t\u003cp\u003e\u003cem\u003e\u003cem\u003e\u003cem\u003e\u003cem\u003eOriginally published on the \u003c/em\u003e\u003c/em\u003e\u003ca href=\"https://medium.com/takeaway-tech/the-continuous-integration-system-used-by-the-mobile-teams-28ba057ef628?ref=albertodebortoli.com\"\u003e\u003cem\u003e\u003cem\u003eJust Eat \u003c/em\u003e\u003c/em\u003eTakeaway \u003cem\u003e\u003cem\u003eEngineering Blog\u003c/em\u003e\u003c/em\u003e\u003c/a\u003e\u003cem\u003e\u003cem\u003e.\u003c/em\u003e\u003c/em\u003e\u003c/em\u003e\u003c/em\u003e\u003c/p\u003e\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\u003cp\u003eIn this article, we’ll discuss the way our mobile teams have evolved the Continuous Integration (CI) stack over the recent years. We don’t have DevOps engineers in our team and, until recently, we had adopted a singular approach in which CI belongs to the whole team and everyone should be able to maintain it. This has proven to be difficult and extremely time-consuming.\u003c/p\u003e\u003cp\u003eThe Just Eat side of our newly merged entity has a dedicated team providing continuous integration and deployment tools to their teams but they are heavily backend-centric and there has been little interest in implementing solutions tailored for mobile teams. As is often the case in tech companies, there is a missing link between mobile and DevOps teams.\u003c/p\u003e\u003cp\u003eThe iOS team is the author and first consumer of the solution described but, as you can see, we have ported the same stack to Android as well. We will mainly focus on the iOS implementation in this article, with references to Android as appropriate.\u003c/p\u003e\u003ch3 id=\"2016%E2%80%932020\"\u003e2016–2020\u003c/h3\u003e\u003cp\u003eHistorically speaking, the iOS UK app was running on \u003ca href=\"https://www.bitrise.io/customer-stories/just-eat?ref=albertodebortoli.com\" rel=\"noopener\"\u003eBitrise\u003c/a\u003e because it was decided not to invest time in implementing a CI solution, while the Bristol team was using a Jenkins version installed by a different team. This required manual configuration with custom scripts and it had custom in-house hardware. These are two quite different approaches indeed and, at this stage, things were not great but somehow good enough. It’s fair to say we were still young on the DevOps front.\u003c/p\u003e\u003cp\u003eWhen we merged the teams, it was clear that we wanted to unify the CI solution and the obvious choice for a company of our size was to not use a third-party service, bringing us to invest more and more in Jenkins. Only one team member had good knowledge of Jenkins but the rest of the team showed little interest in learning how to configure and maintain it, causing the stack to eventually become a dumping ground of poorly configured jobs.\u003c/p\u003e\u003cp\u003eIt was during this time that we introduced Fastlane (making the common tasks portable), migrated the UK app from Bitrise to Jenkins, started running the UI tests on Pull Requests, and other small yet sensible improvements.\u003c/p\u003e\u003ch3 id=\"2020%E2%80%932021\"\u003e2020–2021\u003c/h3\u003e\u003cp\u003eStarting in mid-2020 the iOS team has significantly revamped its CI stack and given it new life. The main goals we wanted to achieve (and did by early 2021) were:\u003c/p\u003e\u003cul\u003e\u003cli\u003eRevisit the pipelines\u003c/li\u003e\u003cli\u003eClear Jenkins configuration and deployment strategy\u003c/li\u003e\u003cli\u003eMake use of \u003ca href=\"https://aws.amazon.com/ec2/instance-types/mac/?ref=albertodebortoli.com\" rel=\"noopener\"\u003eAWS Mac instances\u003c/a\u003e\u003c/li\u003e\u003cli\u003eReduce the pool size of our mac hardware\u003c/li\u003e\u003cli\u003eShare our knowledge across teams better\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSince the start of the pandemic, we have implemented the pipelines in code (bidding farewell to custom bash scripts), we moved to a monorepo which was a massive step ahead and began using SonarQube even more aggressively.\u003c/p\u003e\u003cp\u003eWe added Slack reporting and \u003ca href=\"https://github.com/justeat/PRAssigner?ref=albertodebortoli.com\" rel=\"noopener\"\u003ePR Assigner\u003c/a\u003e, an internal tool implemented by \u003ca href=\"https://twitter.com/_aantonioni?ref=albertodebortoli.com\" rel=\"noopener\"\u003eAndrea Antonioni\u003c/a\u003e. We also automated the common release tasks such as cutting and completing a release and uploading the dSYMS to Firebase.\u003c/p\u003e\u003cp\u003eWe surely invested a lot in optimizing various aspects such as running the UI tests in parallel, making use of shallow repo cloning, We also moved to not checking in the pods within the repo. This, eventually, allowed us to reduce the number of agents for easier infrastructure maintenance.\u003c/p\u003e\u003cp\u003eAutomating the infrastructure deployment of Jenkins was a fundamental shift compared to the previous setup and we have introduced AWS Mac instances replacing part of the fleet of our in-house hardware.\u003c/p\u003e\u003ch2 id=\"ci-system-setup\"\u003eCI system setup\u003c/h2\u003e\u003cp\u003eLet’s take a look at our stack. Before we start, we’d like to thank \u003ca href=\"https://twitter.com/isham_araia?ref=albertodebortoli.com\" rel=\"noopener\"\u003eIsham Araia\u003c/a\u003e for having provided a proof of concept for the configuration and deployment of Jenkins. He talked about it at \u003ca href=\"https://ish-ar.io/jenkins-at-scale/?ref=albertodebortoli.com\" rel=\"noopener\"\u003ehttps://ish-ar.io/jenkins-at-scale/\u003c/a\u003e and it represented a fundamental starting point, saving us several days of researching.\u003c/p\u003e\u003ch3 id=\"triggering-flow\"\u003eTriggering flow\u003c/h3\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2021/06/0_nShqCMN_dUGfxKpv.png\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"564\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2021/06/0_nShqCMN_dUGfxKpv.png 600w, https://albertodebortoli.com/content/images/2021/06/0_nShqCMN_dUGfxKpv.png 800w\"/\u003e\u003c/figure\u003e\u003cp\u003eStarting from the left, we have our repositories (plural, as some shared dependencies don’t live in the monorepo). The repositories contain the pipelines in the form of Jenkinsfiles and they call into Fastlane lanes. Pretty much every action is a lane, from running the tests to archiving for the App Store to creating the release branches.\u003c/p\u003e\u003cp\u003eChanges are raised through pull requests that trigger Jenkins. There are other ways to trigger Jenkins: by user interaction (for things such as completing a release or archiving and uploading the app to App Store Connect) and cron triggers (for things such as building the nightly build, running the tests on the develop branch every 12 hours, or uploading the \u003ca href=\"https://pact.io/?ref=albertodebortoli.com\" rel=\"noopener\"\u003ePACT\u003c/a\u003e contract to the broker).\u003c/p\u003e\u003cp\u003eOnce Jenkins has received the information, it will then schedule the jobs to one of the agents in our pool, which is now made up of 5 agents, 2 in the cloud and 3 in-house mac pros.\u003c/p\u003e\u003ch3 id=\"reporting-flow\"\u003eReporting flow\u003c/h3\u003e\u003cp\u003eNow that we’ve talked about the first part of the flow, let’s talk about the flow of information coming back at us.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2021/06/0_ZQEEEnodojpGtC50.png\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"570\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2021/06/0_ZQEEEnodojpGtC50.png 600w, https://albertodebortoli.com/content/images/2021/06/0_ZQEEEnodojpGtC50.png 800w\"/\u003e\u003c/figure\u003e\u003cp\u003eEvery PR triggers \u003ca href=\"https://github.com/justeat/PRAssigner?ref=albertodebortoli.com\" rel=\"noopener\"\u003ePR Assigner\u003c/a\u003e, a tool that works out a list of reviewers to assign to pull requests and notifies engineers via dedicated Slack channels. The pipelines post on Slack, providing info about all the jobs that are being executed so we can read the history without having to log into Jenkins. We have in place the standard notification flow from Jenkins to GitHub to set the status checks and Jenkins also notifies SonarQube to verify that any change meets the quality standards (namely code coverage percentage and coding rules).\u003c/p\u003e\u003cp\u003eWe also have a smart lambda named SonarQubeStatusProcessor that reports to GitHub, written by Alan Nichols. This is due to a current limitation of SonarQube, which only allows reporting the status of one SQ project to one GitHub repo. Since we have a monorepo structure we had to come up with this neat customization to report the SQ status for all the modules that have changed as part of the PR.\u003c/p\u003e\u003ch2 id=\"configuration\"\u003eConfiguration\u003c/h2\u003e\u003cp\u003eLet’s see what the new interesting parts of Jenkins are. Other than Jenkins itself and several plugins, it’s important to point out \u003ca href=\"https://github.com/jenkinsci/configuration-as-code-plugin?ref=albertodebortoli.com\" rel=\"noopener\"\u003eJCasC\u003c/a\u003e and \u003ca href=\"https://github.com/jenkinsci/job-dsl-plugin?ref=albertodebortoli.com\" rel=\"noopener\"\u003eJob DSL\u003c/a\u003e.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2021/06/1_DPeIIS4s9GBotWrXJQ0RBw.png\" alt=\"\" loading=\"lazy\" width=\"526\" height=\"408\"/\u003e\u003c/figure\u003e\u003cp\u003eJCasC stands for Jenkins Configuration as Code, and it allows you to configure Jenkins via a yaml file.\u003c/p\u003e\u003cp\u003eThe point here is that nobody should ever touch the Jenkins settings directly from the configuration page, in the same way, one ideally shouldn’t apply configuration changes manually in any dashboard. The CasC file is where we define the Slack integration, the user roles, SSO configuration, the number of agents and so on.\u003c/p\u003e\u003cp\u003eWe could also define the jobs in CasC but we go a step further than that.\u003c/p\u003e\u003cp\u003eWe use the Job DSL plugin that allows you to configure the jobs in groovy and in much more detail. One job we configure in the CasC file though is the seed job. This is a simple freestyle job that will go pick the jobs defined with Job DSL and create them in Jenkins.\u003c/p\u003e\u003ch2 id=\"deployment\"\u003eDeployment\u003c/h2\u003e\u003cp\u003eLet’s now discuss how we can get a configured Jenkins instance on EC2. In other words, how do we deploy Jenkins?\u003c/p\u003e\u003cp\u003eWe use a combination of tools that are bread and butter for DevOps people.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2021/06/0_CAnhTU1F_7EskQ7T.png\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"336\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2021/06/0_CAnhTU1F_7EskQ7T.png 600w, https://albertodebortoli.com/content/images/2021/06/0_CAnhTU1F_7EskQ7T.png 800w\"/\u003e\u003c/figure\u003e\u003cp\u003eThe commands on the left spawn a Docker container that calls into the tools on the right.\u003c/p\u003e\u003cp\u003eWe start with Packer which allows us to create the AMI (Amazon Machine Image) together with Ansible, allowing us to configure an environment easily (much more easily than Chef or Puppet).\u003c/p\u003e\u003cp\u003eRunning the \u003ccode\u003ecreate-image\u003c/code\u003e command the script will:\u003c/p\u003e\u003cp\u003e1. Create a temporary EC2 instance\u003c/p\u003e\u003cp\u003e2. Connect to the instance and execute an ansible playbook\u003c/p\u003e\u003cp\u003eOur playbook encompasses a number of steps, here’s a summary:\u003c/p\u003e\u003cul\u003e\u003cli\u003einstall the Jenkins version for the given Linux distribution\u003c/li\u003e\u003cli\u003einstall Nginx\u003c/li\u003e\u003cli\u003ecopy the SSL cert over\u003c/li\u003e\u003cli\u003econfigure nginx w/ SSL termination and reverse proxy\u003c/li\u003e\u003cli\u003einstall the plugins for Jenkins\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce the playbook is executed, Packer will export an AMI in EC2 with all of this in it and destroy the instance that was used.\u003c/p\u003e\u003cp\u003eWith the AMI ready, we can now proceed to deploy our Jenkins. For the actual deployment, we use Terraform which allows us to define our infrastructure in code.\u003c/p\u003e\u003cp\u003eThe deploy command runs Terraform under the hood to set up the infrastructure, here’s a summary of the task:\u003c/p\u003e\u003cul\u003e\u003cli\u003ecreate an IAM Role + IAM Policy\u003c/li\u003e\u003cli\u003econfigure security groups\u003c/li\u003e\u003cli\u003ecreate the VPC and subnet to use with a specific CIDER block and the subnet\u003c/li\u003e\u003cli\u003ecreate any private key pair to connect over SSH\u003c/li\u003e\u003cli\u003edeploy the instance using a static private IP (it has to be static otherwise the A record in Route53 would break)\u003c/li\u003e\u003cli\u003ecopy the JCasC configuration file over so that when Jenkins starts it picks that up to configure itself\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe destroy command runs a “terraform destroy” and destroys everything that was created with the deploy command. Deploy and destroy balance each other out.\u003c/p\u003e\u003cp\u003eNow that we have Jenkins up and running, we need to give it some credentials so our pipelines are able to work properly. A neat way of doing this is by having the secrets (SSH keys, Firebase tokens, App Store Connect API Key and so forth) in AWS Secrets Manager which is based on KMS and use a Jenkins plugin to allow Jenkins to access them.\u003c/p\u003e\u003cp\u003eIt’s important to note that developers don’t have to install Packer, Ansible, Terraform or even the AWS CLI locally because the commands run a Docker container that does the real work with all the tools installed. As a result, the only thing one should have installed is really Docker.\u003c/p\u003e\u003ch2 id=\"ci-agents\"\u003eCI agents\u003c/h2\u003e\u003cp\u003eEnough said about Jenkins, it’s time to talk about the agents.As you probably already know, in order to run tests, compile and archive iOS apps we need Xcode, which is only available on macOS, so Linux or Windows instances are not going to cut it.\u003c/p\u003e\u003cp\u003eWe experimented with the recently introduced AWS Mac instances and they are great, ready out-of-the-box with minimal configuration on our end.\u003c/p\u003e\u003cp\u003eWhat we were hoping to get to with this recent work was the ability to leverage the Jenkins Cloud agents. That would have been awesome because it would have allowed us to:\u003c/p\u003e\u003cul\u003e\u003cli\u003elet Jenkins manage the agent instances\u003c/li\u003e\u003cli\u003escale the agent pool according to the load on CI\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSadly we couldn\u0026#39;t go that far. Limitations are:\u003c/p\u003e\u003cul\u003e\u003cli\u003ethe bootstrapping of a mac1.metal takes around 15 minutes\u003c/li\u003e\u003cli\u003ereusing the dedicated host after having stopped an instance can take up to 3 hours — during that time we just pay for a host that is not usable\u003c/li\u003e\u003c/ul\u003e\u003cblockquote\u003eWhen you stop or terminate a Mac instance, Amazon EC2 performs a scrubbing workflow on the underlying Dedicated Host to erase the internal SSD, to clear the persistent NVRAM variables, and if needed, to update the bridgeOS software on the underlying Mac mini.\u003c/blockquote\u003e\u003cblockquote\u003eThis ensures that Mac instances provide the same security and data privacy as other EC2 Nitro instances. It also enables you to run the latest macOS AMIs without manually updating the bridgeOS software. During the scrubbing workflow, the Dedicated Host temporarily enters the pending state. If the bridgeOS software does not need to be updated, the scrubbing workflow takes up to 50 minutes to complete. If the bridgeOS software needs to be updated, the scrubbing workflow can take up to 3 hours to complete.\u003c/blockquote\u003e\u003cp\u003eSource: \u003ca href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html?ref=albertodebortoli.com\" rel=\"noopener\"\u003ehttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-mac-instances.html\u003c/a\u003e\u003c/p\u003e\u003cp\u003eIn other words: scaling mac instances is not an option and leaving the instances up 24/7 seems to be the easiest option. This is especially valid if your team is distributed and jobs could potentially run over the weekend as well, saving you the hassle of implementing downscaling ahead of the weekend.\u003c/p\u003e\u003cp\u003eThere are some pricing and instance allocation considerations to make. Note that On-Demand Mac1 Dedicated Hosts have a minimum host allocation and billing duration of 24 hours.\u003c/p\u003e\u003cblockquote\u003e“You can purchase Savings Plans to lower your spend on Dedicated Hosts. Savings Plans is a flexible pricing model that provides savings of up to 72% on your AWS compute usage. This pricing model offers lower prices on Amazon EC2 instances usage, regardless of instance family, size, OS, tenancy or AWS Region.”\u003c/blockquote\u003e\u003cp\u003eSource: \u003ca href=\"https://aws.amazon.com/ec2/dedicated-hosts/pricing/?ref=albertodebortoli.com\" rel=\"nofollow noopener\"\u003ehttps://aws.amazon.com/ec2/dedicated-hosts/pricing/\u003c/a\u003e\u003c/p\u003e\u003cp\u003eThe On-Demand rate is \u003cstrong\u003e$1.207 \u003c/strong\u003eper hour.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2021/06/0_NkSjKzAiR5va1BnX.png\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"299\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2021/06/0_NkSjKzAiR5va1BnX.png 600w, https://albertodebortoli.com/content/images/2021/06/0_NkSjKzAiR5va1BnX.png 800w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eI’d like to stress that no CI solution comes for free. I’ve often heard developers indicating that Travis and similar products are cheaper. The truth is that the comparison is not even remotely reasonable: virtual boxes are incredibly slow compared to native Apple hardware and take ridiculous bootstrapping times. Even the smallest projects suffer terribly.\u003c/p\u003e\u003cp\u003eOne might ask if it’s at least possible to use the same configuration process we used for the Jenkins instance (with Packer and Ansible) but sadly we hit additional limitations:\u003c/p\u003e\u003cul\u003e\u003cli\u003eApple requires 2FA for downloading Xcode via \u003ca href=\"https://github.com/xcpretty/xcode-install?ref=albertodebortoli.com\" rel=\"noopener\"\u003excode-version\u003c/a\u003e\u003c/li\u003e\u003cli\u003eApple requires 2FA for signing into Xcode\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe above pretty much causes the configuration flow to fall apart making it impossible to configure an instance via Ansible.\u003c/p\u003e\u003ch3 id=\"cloud-agents-for-android\"\u003eCloud agents for Android\u003c/h3\u003e\u003cp\u003eIt was a different story for Android, in which we could easily configure the agent instance with Ansible and therefore leverage the Cloud configuration to allow automatic agent provisioning.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://albertodebortoli.com/content/images/2021/06/0_t6i18L8XEJjw1-Z-.png\" alt=\"\" loading=\"lazy\" width=\"800\" height=\"350\" srcset=\"https://albertodebortoli.com/content/images/size/w600/2021/06/0_t6i18L8XEJjw1-Z-.png 600w, https://albertodebortoli.com/content/images/2021/06/0_t6i18L8XEJjw1-Z-.png 800w\" sizes=\"(min-width: 720px) 720px\"/\u003e\u003c/figure\u003e\u003cp\u003eThis configuration is defined via CasC as everything else.\u003c/p\u003e\u003cp\u003eTo better control EC2 usage and costs, a few settings come in handy:\u003c/p\u003e\u003cul\u003e\u003cli\u003eminimum number of instances (up at all times)\u003c/li\u003e\u003cli\u003eminimum number of spare instances (created to accommodate future jobs)\u003c/li\u003e\u003cli\u003einstance cap: the maximum number of instances that can be provisioned at the same time\u003c/li\u003e\u003cli\u003eidle termination time: how long agents should be kept alive after they have completed the job\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAll of the above allow for proper scaling and a lot less maintenance compared to the iOS setup. A simple setup with 0 instances up at all times allows saving costs overnight and given that in our case the bootstrapping takes only 2 minutes, we can rely on the idle time setting.\u003c/p\u003e\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\u003cp\u003eSetting up an in-house CI is never a straightforward process and it requires several weeks of dedicated work.\u003c/p\u003e\u003cp\u003eAfter years of waiting, Apple has announced \u003ca href=\"https://developer.apple.com/documentation/Xcode/Xcode-Cloud?ref=albertodebortoli.com\" rel=\"noopener\"\u003eXcode Cloud\u003c/a\u003e which we believe will drastically change the landscape of continuous integration on iOS. The solution will most likely cause havoc for companies such as Bitrise and CircleCI and it’s reasonable to assume the pricing will be competitive compared to AWS, maybe running on custom hardware that only Apple is able to produce.\u003c/p\u003e\u003cp\u003eA shift this big will take time to occur, so we foresee our solution to stay in use for quite some time.\u003c/p\u003e\u003cp\u003eWe hope to have inspired you on how a possible setup for mobile teams could be and informed you on what are the pros \u0026amp; cons of using EC2 mac instances.\u003c/p\u003e\n\t\t\t\u003c/section\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2021-07-23T09:22:24Z",
  "modifiedTime": "2021-07-23T09:23:57Z"
}
