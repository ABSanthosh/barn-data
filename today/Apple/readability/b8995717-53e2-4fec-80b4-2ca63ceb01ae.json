{
  "id": "b8995717-53e2-4fec-80b4-2ca63ceb01ae",
  "title": "Apple researchers taught an AI model to reason about app interfaces",
  "link": "https://9to5mac.com/2025/07/15/apple-researchers-taught-an-ai-model-to-reason-about-app-interfaces/",
  "description": "A new Apple-backed study, in collaboration with Aalto University in Finland, introduces ILuvUI: a vision-language model trained to understand mobile app interfaces from screenshots and from natural language conversations. Here’s what that means, and how they did it. more…",
  "author": "Marcus Mendes",
  "published": "Tue, 15 Jul 2025 22:45:55 +0000",
  "source": "https://9to5mac.com/feed",
  "categories": [
    "News"
  ],
  "byline": "Marcus Mendes",
  "length": 4226,
  "excerpt": "A new Apple study introduces ILuvUI: a model that understands mobile app interfaces from screenshots and from natural language conversations.",
  "siteName": "9to5Mac",
  "favicon": "https://9to5mac.com/wp-content/uploads/sites/6/2019/10/cropped-cropped-mac1-1.png?w=192",
  "text": "A new Apple-backed study, in collaboration with Aalto University in Finland, introduces ILuvUI: a vision-language model trained to understand mobile app interfaces from screenshots and from natural language conversations. Here’s what that means, and how they did it. ILuvUI: an AI that outperformed the model it was based on In the paper, ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations, the team tackles a long-standing challenge in human-computer interaction, or HCI: teaching AI models to reason about user interfaces like humans do, which in practice means visually, as well as semantically. “Understanding and automating actions on UIs is a challenging task since the UI elements in a screen, such as list items, checkboxes, and text fields, encode many layers of information beyond their affordances for interactivity alone. (….) LLMs in particular have demonstrated remarkable abilities to comprehend task instructions in natural language in many domains, however using text descriptions of UIs alone with LLMs leaves out the rich visual information of the UI. “ Currently, as the researchers explain, most vision-language models are trained on natural images, like dogs or street signs, so they don’t perform as well when asked to interpret more structured environments, like app UIs: “Fusing visual with textual information is important to understanding UIs as it mirrors how many humans engage with the world. One approach that has sought to bridge this gap when applied to natural images are Vision-Language Models (VLMs), which accept multimodal inputs of both images and text, typically output only text, and allow for general-purpose question answering, visual reasoning, scene descriptions, and conversations with image inputs. However, the performance of these models on UI tasks fall short compared to natural images because of the lack of UI examples in their training data.” With that in mind, the researchers fine-tuned the open-source VLM LLaVA, and they also adapted its training method to specialize in the UI domain. They trained it on text-image pairs that were synthetically generated following a few “golden examples”. The final dataset included Q\u0026A-style interactions, detailed screen descriptions, predicted action outcomes, and even multi-step plans (like “how to listen to the latest episode of a podcast,” or “how to change brightness settings.”) Once trained on this dataset, the resulting model, ILuvUI, was able to outperform the original LLaVA in both machine benchmarks and human preference tests. What’s more, it doesn’t require a user to specify a region of interest in the interface. Instead, the model understands the entire screen contextually from a simple prompt: ILuvUI (…) does not require a region of interest, and accepts a text prompt as input in addition to the UI image, which enables it to provide answers for use cases such as visual question answering. How will users benefit from this? Apple’s researchers say that their approach might prove useful for accessibility, as well as for automated UI testing. They also note that while ILuvUI is still based on open components, future work could involve larger image encoders, better resolution handling, and output formats that work seamlessly with existing UI frameworks, like JSON. And if you’ve been keeping up to date with Apple’s AI research papers, you might be thinking of a recent investigation of whether AI models could not just understand, but also anticipate the consequences of in-app actions. Put the two together, and things start to get… interesting, especially if you rely on accessibility to navigate your devices, or just wish the OS could autonomously handle the more fiddly parts of your in-app workflows. External drive deals on Amazon Seagate Portable 2TB HDD, USB 3.0: $79.99 SanDisk 2TB Extreme Portable SSD, USB-C: $134.99 (was $209.99) Samsung T7 1TB Portable SSD, USB 3.2 Gen 2: $89.99 (was $129.99) WD 5TB Elements Portable External HDD, USB 3.2 Gen 1: $123.99 (was $139.99) Add 9to5Mac to your Google News feed.  FTC: We use income earning auto affiliate links. More.",
  "image": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/04/There-are-two-big-issues-with-Apples-reported-AI-doctor-plan.jpg?resize=1200%2C628\u0026quality=82\u0026strip=all\u0026ssl=1",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\t\t\t\t\t\n\u003cfigure\u003e\n\t\u003cimg width=\"1600\" height=\"800\" src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/04/There-are-two-big-issues-with-Apples-reported-AI-doctor-plan.jpg?quality=82\u0026amp;strip=all\u0026amp;w=1600\" alt=\"There are two big issues with Apple\u0026#39;s reported \u0026#39;AI doctor\u0026#39; plan | Friendly-looking white robot\" srcset=\"https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/04/There-are-two-big-issues-with-Apples-reported-AI-doctor-plan.jpg?w=320\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/04/There-are-two-big-issues-with-Apples-reported-AI-doctor-plan.jpg?w=640\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/04/There-are-two-big-issues-with-Apples-reported-AI-doctor-plan.jpg?w=1024\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/04/There-are-two-big-issues-with-Apples-reported-AI-doctor-plan.jpg?w=1500\u0026amp;quality=82\u0026amp;strip=all\u0026amp;ssl=1 1500w\" decoding=\"async\" fetchpriority=\"high\"/\u003e\u003c/figure\u003e\n\n\u003cp\u003eA new Apple-backed study, in collaboration with Aalto University in Finland, introduces ILuvUI: a vision-language model trained to understand mobile app interfaces from screenshots and from natural language conversations. Here’s what that means, and how they did it.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"h-iluvui-an-ai-that-outperformed-the-model-it-was-based-on\"\u003eILuvUI:  an AI that outperformed the model it was based on\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the paper, \u003ca href=\"https://arxiv.org/abs/2310.04869\"\u003eILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations\u003c/a\u003e, the team tackles a long-standing challenge in human-computer interaction, or HCI: teaching AI models to reason about user interfaces like humans do, which in practice means visually, as well as semantically.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e“Understanding and automating actions on UIs is a challenging task since the UI elements in a screen, such as list items, checkboxes, and text fields, encode many layers of information beyond their affordances for interactivity alone. (….) LLMs in particular have demonstrated remarkable abilities to comprehend task instructions in natural language in many domains, however using text descriptions of UIs alone with LLMs leaves out the rich visual information of the UI. “\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eCurrently, as the researchers explain, most vision-language models are trained on natural images, like dogs or street signs, so they don’t perform as well when asked to interpret more structured environments, like app UIs:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e“Fusing visual with textual information is important to understanding UIs as it mirrors how many humans engage with the world. One approach that has sought to bridge this gap when applied to natural images are Vision-Language Models (VLMs), which accept multimodal inputs of both images and text, typically output only text, and allow for general-purpose question answering, visual reasoning, scene descriptions, and conversations with image inputs. However, the performance of these models on UI tasks fall short compared to natural images because of the lack of UI examples in their training data.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWith that in mind, the researchers fine-tuned the open-source VLM \u003ca href=\"https://llava-vl.github.io\"\u003eLLaVA\u003c/a\u003e, and they also adapted its training method to specialize in the UI domain.\u003c/p\u003e\n\n\n\n\u003cp\u003eThey trained it on text-image pairs that were \u003ca href=\"https://9to5mac.com/2025/05/20/apple-intelligence-fake-data-is-good-news/\"\u003esynthetically generated\u003c/a\u003e following a few “golden examples”. The final dataset included Q\u0026amp;A-style interactions, detailed screen descriptions, predicted action outcomes, and even multi-step plans (like “how to listen to the latest episode of a podcast,” or “how to change brightness settings.”)\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce trained on this dataset, the resulting model, ILuvUI, was able to outperform the original LLaVA in both machine benchmarks and human preference tests.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg decoding=\"async\" height=\"192\" width=\"1024\" src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?quality=82\u0026amp;strip=all\u0026amp;w=1024\" alt=\"\" srcset=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg 1200w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?resize=155,29 155w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?resize=655,123 655w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?resize=768,144 768w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?resize=1024,192 1024w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?resize=350,66 350w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?resize=140,26 140w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui-2.jpg?resize=150,28 150w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat’s more, it doesn’t require a user to specify a region of interest in the interface. Instead, the model understands the entire screen contextually from a simple prompt:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003eILuvUI (…) does not require a region of interest, and accepts a text prompt as input in addition to the UI image, which enables it to provide answers for use cases such as visual question answering.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" decoding=\"async\" height=\"1024\" width=\"904\" src=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?quality=82\u0026amp;strip=all\u0026amp;w=904\" alt=\"\" srcset=\"https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg 1200w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=115,130 115w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=618,700 618w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=768,870 768w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=904,1024 904w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=309,350 309w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=140,159 140w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=883,1000 883w, https://9to5mac.com/wp-content/uploads/sites/6/2025/07/iluvui.jpg?resize=150,170 150w\" sizes=\"auto, (max-width: 904px) 100vw, 904px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2 id=\"h-how-will-users-benefit-from-this\"\u003eHow will users benefit from this?\u003c/h2\u003e\n\n\n\n\u003cp\u003eApple’s researchers say that their approach might prove useful for accessibility, as well as for automated UI testing. They also note that while ILuvUI is still based on open components, future work could involve larger image encoders, better resolution handling, and output formats that work seamlessly with existing UI frameworks, like JSON.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd if you’ve been keeping up to date with Apple’s AI research papers, you might be thinking of a \u003ca href=\"https://9to5mac.com/2025/06/27/apple-tests-ai-agents-anticipate-consequences-app-use/\"\u003erecent investigation\u003c/a\u003e of whether AI models could not just understand, but also anticipate the consequences of in-app actions.\u003c/p\u003e\n\n\n\n\u003cp\u003ePut the two together, and things start to get… interesting, especially if you rely on accessibility to navigate your devices, or just wish the OS could autonomously handle the more fiddly parts of your in-app workflows.\u003c/p\u003e\n\n\n\n\u003ch4\u003eExternal drive deals on Amazon\u003c/h4\u003e\n\n\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.amazon.com/Seagate-Portable-External-Hard-Drive/dp/B07CRG94G3?tag=marcmendes-20\"\u003eSeagate Portable 2TB HDD, USB 3.0\u003c/a\u003e: \u003cstrong\u003e$79.99\u003c/strong\u003e\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003ca href=\"https://www.amazon.com/SanDisk-2TB-Extreme-Portable-SDSSDE61-2T00-G25/dp/B08HN37XC1?tag=marcmendes-20\"\u003eSanDisk 2TB Extreme Portable SSD, USB-C\u003c/a\u003e: \u003cstrong\u003e$134.99\u003c/strong\u003e (was $209.99)\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003ca href=\"https://www.amazon.com/SAMSUNG-Portable-SSD-1TB-MU-PC1T0T/dp/B0874XN4D8?tag=marcmendes-20\u0026amp;th=1\"\u003eSamsung T7 1TB Portable SSD, USB 3.2 Gen 2\u003c/a\u003e: \u003cstrong\u003e$89.99\u003c/strong\u003e (was $129.99)\u003c/li\u003e\n\n\n\n\u003cli\u003e\u003ca href=\"https://www.amazon.com/Elements-Portable-External-Drive-WDBU6Y0050BBK-WESN/dp/B07X41PWTY?tag=marcmendes-20\"\u003eWD 5TB Elements Portable External HDD, USB 3.2 Gen 1\u003c/a\u003e: \u003cstrong\u003e$123.99\u003c/strong\u003e (was $139.99)\u003c/li\u003e\n\u003c/ul\u003e\n\t\u003cp\u003e\n\t\t\u003ca target=\"_blank\" rel=\"nofollow\" href=\"https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US\u0026amp;gl=US\u0026amp;ceid=US:en\"\u003e\n\t\t\t\u003cem\u003eAdd 9to5Mac to your Google News feed.\u003c/em\u003e \n\t\t\t\t\t\u003c/a\u003e\n\t\u003c/p\u003e\n\t\u003cp\u003e\u003cem\u003eFTC: We use income earning auto affiliate links.\u003c/em\u003e \u003ca href=\"https://9to5mac.com/about/#affiliate\"\u003eMore.\u003c/a\u003e\u003c/p\u003e\t\t\t\t\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-07-15T22:45:55Z",
  "modifiedTime": "2025-07-15T22:45:57Z"
}
