{
  "id": "11583c56-131d-44db-896a-7eb08ab86458",
  "title": "Seven Replies to the Viral Apple Reasoning Paper",
  "link": "https://simonwillison.net/2025/Jun/15/viral-apple-reasoning-paper/",
  "description": "",
  "author": "John Gruber",
  "published": "2025-06-17T19:50:08Z",
  "source": "https://daringfireball.net/feeds/main",
  "categories": null,
  "byline": "Simon Willison",
  "length": 2952,
  "excerpt": "A few weeks ago Apple Research released a new paper The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity. Through extensive …",
  "siteName": "Simon Willison’s Weblog",
  "favicon": "",
  "text": "Seven replies to the viral Apple reasoning paper – and why they fall short (via) A few weeks ago Apple Research released a new paper The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity. Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter-intuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. I skimmed the paper and it struck me as a more thorough example of the many other trick questions that expose failings in LLMs - this time involving puzzles such as the Tower of Hanoi that can have their difficulty level increased to the point that even \"reasoning\" LLMs run out of output tokens and fail to complete them. I thought this paper got way more attention than it warranted - the title \"The Illusion of Thinking\" captured the attention of the \"LLMs are over-hyped junk\" crowd. I saw enough well-reasoned rebuttals that I didn't feel it worth digging into. And now, notable LLM skeptic Gary Marcus has saved me some time by aggregating the best of those rebuttals together in one place! Gary rebuts those rebuttals, but given that his previous headline concerning this paper was a knockout blow for LLMs? it's not surprising that he finds those arguments unconvincing. From that previous piece: The vision of AGI I have always had is one that combines the strengths of humans with the strength of machines, overcoming the weaknesses of humans. I am not interested in a “AGI” that can’t do arithmetic, and I certainly wouldn’t want to entrust global infrastructure or the future of humanity to such a system. Then from his new post: The paper is not news; we already knew these models generalize poorly. True! (I personally have been trying to tell people this for almost thirty years; Subbarao Rao Kambhampati has been trying his best, too). But then why do we think these models are the royal road to AGI? And therein lies my disagreement. I'm not interested in whether or not LLMs are the \"road to AGI\". I continue to care only about whether they have useful applications today, once you've understood their limitations. Reasoning LLMs are a relatively new and interesting twist on the genre. They are demonstrably able to solve a whole bunch of problems that previous LLMs were unable to handle, hence why we've seen a rush of new models from OpenAI and Anthropic and Gemini and DeepSeek and Qwen and Mistral. They get even more interesting when you combine them with tools. They're already useful to me today, whether or not they can reliably solve the Tower of Hanoi or River Crossing puzzles. Update: Gary clarifies that \"the existence of some utility does not mean I can’t also address the rampant but misguided claims of imminent AGI\".",
  "image": "",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple\"\u003eSeven replies to the viral Apple reasoning paper – and why they fall short\u003c/a\u003e\u003c/strong\u003e (\u003ca href=\"https://news.ycombinator.com/item?id=44278403\" title=\"Hacker News\"\u003evia\u003c/a\u003e) A few weeks ago Apple Research released a new paper \u003ca href=\"https://machinelearning.apple.com/research/illusion-of-thinking\"\u003eThe Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThrough extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter-intuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI skimmed the paper and it struck me as a more thorough example of the many other trick questions that expose failings in LLMs - this time involving puzzles such as the Tower of Hanoi that can have their difficulty level increased to the point that even \u0026#34;reasoning\u0026#34; LLMs run out of output tokens and fail to complete them.\u003c/p\u003e\n\u003cp\u003eI thought this paper got \u003cem\u003eway\u003c/em\u003e more attention than it warranted - the title \u0026#34;The Illusion of Thinking\u0026#34; captured the attention of the \u0026#34;LLMs are over-hyped junk\u0026#34; crowd.  I saw enough well-reasoned rebuttals that I didn\u0026#39;t feel it worth digging into.\u003c/p\u003e\n\u003cp\u003eAnd now, notable LLM skeptic Gary Marcus has saved me some time by aggregating the best of those rebuttals \u003ca href=\"https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple\"\u003etogether in one place\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eGary rebuts those rebuttals, but given that his previous headline concerning this paper was \u003ca href=\"https://garymarcus.substack.com/p/a-knockout-blow-for-llms\"\u003ea knockout blow for LLMs?\u003c/a\u003e it\u0026#39;s not surprising that he finds those arguments unconvincing. From that previous piece:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe vision of AGI I have always had is one that \u003cem\u003ecombines\u003c/em\u003e the strengths of humans with the strength of machines, overcoming the weaknesses of humans. I am not interested in a “AGI” that can’t do arithmetic, and I certainly wouldn’t want to entrust global infrastructure or the future of humanity to such a system.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThen from his new post:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThe paper is not news; we already knew these models generalize poorly.\u003c/strong\u003e True! (I personally have been trying to tell people this for almost thirty years; Subbarao Rao Kambhampati has been trying his best, too). But then why do we think these models are the royal road to AGI?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAnd therein lies my disagreement. I\u0026#39;m not interested in whether or not LLMs are the \u0026#34;road to AGI\u0026#34;. I continue to care only about whether they have useful applications today, once you\u0026#39;ve understood their limitations.\u003c/p\u003e\n\u003cp\u003eReasoning LLMs are a relatively new and interesting twist on the genre. They are demonstrably able to solve a whole bunch of problems that previous LLMs were unable to handle, hence why we\u0026#39;ve seen \u003ca href=\"https://simonwillison.net/tags/llm-reasoning/\"\u003ea rush of new models\u003c/a\u003e from OpenAI and Anthropic and Gemini and DeepSeek and Qwen and Mistral.\u003c/p\u003e\n\u003cp\u003eThey get even more interesting when you \u003ca href=\"https://simonwillison.net/2025/Jun/6/six-months-in-llms/#ai-worlds-fair-2025-43.jpeg\"\u003ecombine them with tools\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThey\u0026#39;re already useful to me today, whether or not they can reliably solve the Tower of Hanoi or River Crossing puzzles.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eUpdate:\u003c/strong\u003e Gary \u003ca href=\"https://twitter.com/GaryMarcus/status/1935088453684990204\"\u003eclarifies\u003c/a\u003e that \u0026#34;the existence of some utility does not mean I can’t also address the rampant but misguided claims of imminent AGI\u0026#34;.\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
