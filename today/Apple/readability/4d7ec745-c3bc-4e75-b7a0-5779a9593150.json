{
  "id": "4d7ec745-c3bc-4e75-b7a0-5779a9593150",
  "title": "Apple Researchers Publish Paper on the Limits of Reasoning Models (Showing That They’re Not Really ‘Reasoning’ at All)",
  "link": "https://machinelearning.apple.com/research/illusion-of-thinking",
  "description": "",
  "author": "John Gruber",
  "published": "2025-06-09T01:08:00Z",
  "source": "https://daringfireball.net/feeds/main",
  "categories": null,
  "byline": "",
  "length": 3207,
  "excerpt": "Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes…",
  "siteName": "Apple Machine Learning Research",
  "favicon": "",
  "text": "AuthorsParshin Shojaee*†, Iman Mirzadeh*, Keivan Alizadeh, Maxwell Horton, Samy Bengio, Mehrdad FarajtabarRecent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scal- ing properties, and limitations remain insufficiently understood. Current evaluations primarily fo- cus on established mathematical and coding benchmarks, emphasizing final answer accuracy. How- ever, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces’ structure and quality. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of composi- tional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs “think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter- intuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) low- complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models’ computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities. *Equal contribution. †Work done during an internship at Apple.Related readings and updates.Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). We propose a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. We observe that models inherently possess the ability to perform interleaved…Read moreRecent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising…Read more",
  "image": "https://mlr.cdn-apple.com/media/Home_1200x630_48225d82e9.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv role=\"main\"\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eAuthors\u003c/span\u003eParshin Shojaee*†, Iman Mirzadeh*, Keivan Alizadeh, Maxwell Horton, Samy Bengio, Mehrdad Farajtabar\u003c/p\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp\u003eRecent generations of frontier language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers. While these models\ndemonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scal-\ning properties, and limitations remain insufficiently understood. Current evaluations primarily fo-\ncus on established mathematical and coding benchmarks, emphasizing final answer accuracy. How-\never, this evaluation paradigm often suffers from data contamination and does not provide insights\ninto the reasoning traces’ structure and quality. In this work, we systematically investigate these\ngaps with the help of controllable puzzle environments that allow precise manipulation of composi-\ntional complexity while maintaining consistent logical structures. This setup enables the analysis\nof not only final answers but also the internal reasoning traces, offering insights into how LRMs\n“think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs\nface a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter-\nintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then\ndeclines despite having an adequate token budget. By comparing LRMs with their standard LLM\ncounterparts under equivalent inference compute, we identify three performance regimes: (1) low-\ncomplexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity\ntasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks\nwhere both models experience complete collapse. We found that LRMs have limitations in exact\ncomputation: they fail to use explicit algorithms and reason inconsistently across puzzles. We\nalso investigate the reasoning traces in more depth, studying the patterns of explored solutions\nand analyzing the models’ computational behavior, shedding light on their strengths, limitations,\nand ultimately raising crucial questions about their true reasoning capabilities.\u003c/p\u003e\n\u003cp\u003e*Equal contribution. \u003cbr/\u003e\n†Work done during an internship at Apple.\u003c/p\u003e\u003c/div\u003e\u003csection\u003e\u003cp\u003e\u003ch2\u003eRelated readings and updates.\u003c/h2\u003e\u003c/p\u003e\u003cdiv\u003e\u003cdiv data-testid=\"card-interleaved-reasoning\"\u003e\u003cp\u003eLong chain-of-thought (CoT) significantly enhances large language models\u0026#39; (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). We propose a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. We observe that models inherently possess the ability to perform interleaved…\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://machinelearning.apple.com/research/interleaved-reasoning\" aria-label=\"Read more about Interleaved Reasoning for Large Language Models via Reinforcement Learning\"\u003eRead more\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv data-testid=\"card-gsm-symbolic\"\u003e\u003cp\u003eRecent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising…\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://machinelearning.apple.com/research/gsm-symbolic\" aria-label=\"Read more about GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\"\u003eRead more\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "4 min read",
  "publishedTime": null,
  "modifiedTime": null
}
