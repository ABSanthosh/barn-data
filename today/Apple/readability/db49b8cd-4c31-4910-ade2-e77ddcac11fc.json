{
  "id": "db49b8cd-4c31-4910-ade2-e77ddcac11fc",
  "title": "How Could Apple Use Open-Source AI Models?",
  "link": "https://www.macstories.net/linked/how-could-apple-use-open-source-ai-models/",
  "description": "Yesterday, Wayne Ma, reporting for The Information, published an outstanding story detailing the internal turmoil at Apple that led to the delay of the highly anticipated Siri AI features last month. From the article: In November 2022, OpenAI released ChatGPT to a thunderous response from the tech industry and public. Within Giannandrea’s AI team, however, […]",
  "author": "Federico Viticci",
  "published": "Fri, 11 Apr 2025 17:36:21 +0000",
  "source": "https://www.macstories.net/feed",
  "categories": [
    "Linked",
    "AI",
    "artificial intelligence",
    "siri"
  ],
  "byline": "",
  "length": 4917,
  "excerpt": "Yesterday, Wayne Ma, reporting for The Information, published an outstanding story detailing the internal turmoil at Apple that led to the delay of the highly anticipated Siri AI features last month. From the article: In November 2022, OpenAI released ChatGPT to a thunderous response from the tech industry and public. Within Giannandrea’s AI team, however,",
  "siteName": "",
  "favicon": "https://www.macstories.net/app/themes/macstories4/images/apple-touch-icon-152x152-precomposed.png",
  "text": "Yesterday, Wayne Ma, reporting for The Information, published an outstanding story detailing the internal turmoil at Apple that led to the delay of the highly anticipated Siri AI features last month. From the article: In November 2022, OpenAI released ChatGPT to a thunderous response from the tech industry and public. Within Giannandrea’s AI team, however, senior leaders didn’t respond with a sense of urgency, according to former engineers who were on the team at the time. The reaction was different inside Federighi’s software engineering group. Senior leaders of the Intelligent Systems team immediately began sharing papers about LLMs and openly talking about how they could be used to improve the iPhone, said multiple former Apple employees. Excitement began to build within the software engineering group after members of the Intelligent Systems team presented demos to Federighi showcasing what could be achieved on iPhones with AI. Using OpenAI’s models, the demos showed how AI could understand content on a user’s phone screen and enable more conversational speech for navigating apps and performing other tasks. Assuming the details in this report are correct, I truly can’t imagine how one could possibly see the debut of ChatGPT two years ago and not feel a sense of urgency. Fortunately, other teams at Apple did, and it sounds like they’re the folks who have now been put in charge of the next generation of Siri and AI. There are plenty of other details worth reading in the full story (especially the parts about what Rockwell’s team wanted to accomplish with Siri and AI on the Vision Pro), but one tidbit in particular stood out to me: Federighi has now given the green light to rely on third-party, open-source LLMs to build the next wave of AI features. Federighi has already shaken things up. In a departure from previous policy, he has instructed Siri’s machine-learning engineers to do whatever it takes to build the best AI features, even if it means using open-source models from other companies in its software products as opposed to Apple’s own models, according to a person familiar with the matter. “Using” open-source models from other companies doesn’t necessarily mean shipping consumer features in iOS powered by external LLMs. I’ve seen some people interpret this paragraph as Apple preparing to release a local Siri powered by Llama 4 or DeepSeek, and I think we should pay more attention to that “build the best AI features” (emphasis mine) line. My read of this part is that Federighi might have instructed his team to use distillation to better train Apple’s in-house models as a way to accelerate the development of the delayed Siri features and put them back on the company’s roadmap. Given Tim Cook’s public appreciation for DeepSeek and this morning’s New York Times report that the delayed features may come this fall, I wouldn’t be shocked to learn that Federighi told Siri’s ML team to distill DeepSeek R1’s reasoning knowledge into a new variant of their ∼3 billion parameter foundation model that runs on-device. Doing that wouldn’t mean that iOS 19’s Apple Intelligence would be “powered by DeepSeek”; it would just be a faster way for Apple to catch up without throwing away the foundational model they unveiled last year (which, supposedly, had a ~30% error rate). In thinking about this possibility, I got curious and decided to check out the original paper that Apple published last year with details on how they trained the two versions of AFM (Apple Foundation Model): AFM-server and AFM-on-device. The latter would be the smaller, ~3 billion model that gets downloaded on-device with Apple Intelligence. I’ll let you guess what Apple did to improve the performance of the smaller model: For the on-device model, we found that knowledge distillation (Hinton et al., 2015) and structural pruning are effective ways to improve model performance and training efficiency. These two methods are complementary to each other and work in different ways. More specifically, before training AFM-on-device, we initialize it from a pruned 6.4B model (trained from scratch using the same recipe as AFM-server), using pruning masks that are learned through a method similar to what is described in (Wang et al., 2020; Xia et al., 2023). Or, more simply: AFM-server core training is conducted from scratch, while AFM-on-device is distilled and pruned from a larger model. If the distilled version of AFM-on-device that was tested until a few weeks ago produced a wrong output one third of the time, perhaps it would be a good idea to perform distillation again based on knowledge from other smarter and larger models? Say, using 250 Nvidia GB300 NVL72 servers? (One last fun fact: per their paper, Apple trained AFM-server on 8192 TPUv4 chips for 6.3 trillion tokens; that setup still wouldn’t be as powerful as “only” 250 modern Nvidia servers today.)",
  "image": "https://56243e3f6f46fe44a301-deabeb5f3878e3553d0b065ea974f9bf.ssl.cf1.rackcdn.com/256px.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle id=\"content\"\u003e\n\n    \n\n    \u003cdiv\u003e\n                        \u003cp id=\"p1\"\u003eYesterday, Wayne Ma, reporting for The Information, \u003ca href=\"https://www.theinformation.com/articles/apple-fumbled-siris-ai-makeover\" rel=\"noopener noreferrer\"\u003epublished an outstanding story\u003c/a\u003e detailing the internal turmoil at Apple that led to the delay of the highly anticipated Siri AI features \u003ca href=\"https://www.macstories.net/linked/apple-delays-siri-personalization/\" rel=\"noopener noreferrer\"\u003elast month\u003c/a\u003e. From the article:\u003c/p\u003e\n\u003cblockquote id=\"blockquote2\"\u003e\u003cp\u003e\n  In November 2022, OpenAI released ChatGPT to a thunderous response from the tech industry and public. Within Giannandrea’s AI team, however, senior leaders didn’t respond with a sense of urgency, according to former engineers who were on the team at the time.\u003c/p\u003e\n\u003cp\u003e  The reaction was different inside Federighi’s software engineering group. Senior leaders of the Intelligent Systems team immediately began sharing papers about LLMs and openly talking about how they could be used to improve the iPhone, said multiple former Apple employees.\u003c/p\u003e\n\u003cp\u003e  Excitement began to build within the software engineering group after members of the Intelligent Systems team presented demos to Federighi showcasing what could be achieved on iPhones with AI. Using OpenAI’s models, the demos showed how AI could understand content on a user’s phone screen and enable more conversational speech for navigating apps and performing other tasks.\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p3\"\u003eAssuming the details in this report are correct, I truly can’t imagine how one could possibly see the debut of ChatGPT two years ago and \u003cem\u003enot\u003c/em\u003e feel a sense of urgency. Fortunately, other teams at Apple did, and it sounds like they’re the folks who have now been put in charge of the next generation of Siri and AI.\u003c/p\u003e\n\u003cp id=\"p4\"\u003eThere are plenty of other details worth reading in the full story (especially the parts about what Rockwell’s team wanted to accomplish with Siri and AI on the Vision Pro), but one tidbit in particular stood out to me: Federighi has now given the green light to rely on third-party, open-source LLMs to build the next wave of AI features.\u003c/p\u003e\n\u003cblockquote id=\"blockquote5\"\u003e\u003cp\u003e\n  Federighi has already shaken things up. In a departure from previous policy, he has instructed Siri’s machine-learning engineers to do whatever it takes to build the best AI features, even if it means using open-source models from other companies in its software products as opposed to Apple’s own models, according to a person familiar with the matter.\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p6\"\u003e“Using” open-source models from other companies doesn’t necessarily mean \u003cem\u003eshipping\u003c/em\u003e consumer features in iOS powered by external LLMs. I’ve seen some people interpret this paragraph as Apple preparing to release a local Siri powered by \u003ca href=\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\" rel=\"noopener noreferrer\"\u003eLlama 4\u003c/a\u003e or DeepSeek, and I think we should pay more attention to that “\u003cem\u003ebuild\u003c/em\u003e the best AI features” (emphasis mine) line.\u003c/p\u003e\n\u003cp id=\"p7\"\u003eMy read of this part is that Federighi might have instructed his team to use \u003ca href=\"https://www.ibm.com/think/topics/knowledge-distillation\" rel=\"noopener noreferrer\"\u003edistillation\u003c/a\u003e to better train Apple’s in-house models as a way to accelerate the development of the delayed Siri features and put them back on the company’s roadmap. Given \u003ca href=\"https://9to5mac.com/2025/03/24/tim-cook-says-chinas-deepseek-ai-is-excellent-during-visit-to-country/\" rel=\"noopener noreferrer\"\u003eTim Cook’s public appreciation\u003c/a\u003e for DeepSeek and this morning’s \u003ca href=\"https://www.nytimes.com/2025/04/11/technology/apple-issues-trump-tariffs.html\" rel=\"noopener noreferrer\"\u003eNew York Times report\u003c/a\u003e that the delayed features may come this fall, I wouldn’t be shocked to learn that Federighi told Siri’s ML team to \u003ca href=\"https://medium.com/data-science-in-your-pocket/what-are-deepseek-r1-distilled-models-329629968d5d\" rel=\"noopener noreferrer\"\u003edistill DeepSeek R1’s reasoning knowledge\u003c/a\u003e into a new variant of their \u003ca href=\"https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models\" rel=\"noopener noreferrer\"\u003e∼3 billion parameter foundation model\u003c/a\u003e that runs on-device. Doing that wouldn’t mean that iOS 19’s Apple Intelligence would be “powered by DeepSeek”; it would just be a faster way for Apple to catch up without throwing away the foundational model they unveiled last year (which, \u003ca href=\"https://x.com/markgurman/status/1900616400823030024\" rel=\"noopener noreferrer\"\u003esupposedly\u003c/a\u003e, had a ~30% error rate).\u003c/p\u003e\n\u003cp id=\"p8\"\u003eIn thinking about this possibility, I got curious and decided to check out the \u003ca href=\"https://arxiv.org/html/2407.21075v1\" rel=\"noopener noreferrer\"\u003eoriginal paper that Apple published\u003c/a\u003e last year with details on how they trained the two versions of AFM (Apple Foundation Model): \u003cem\u003eAFM-server\u003c/em\u003e and \u003cem\u003eAFM-on-device\u003c/em\u003e. The latter would be the smaller, ~3 billion model that gets downloaded on-device with Apple Intelligence. I’ll let you guess what Apple did to improve the performance of the smaller model:\u003c/p\u003e\n\u003cblockquote id=\"blockquote9\"\u003e\u003cp\u003e\n  For the on-device model, we found that knowledge distillation (\u003ca href=\"https://arxiv.org/html/2407.21075v1#bib.bib20\" rel=\"noopener noreferrer\"\u003eHinton et al., 2015\u003c/a\u003e) and structural pruning are effective ways to improve model performance and training efficiency. These two methods are complementary to each other and work in different ways. More specifically, before training AFM-on-device, we initialize it from a pruned 6.4B model (trained from scratch using the same recipe as AFM-server), using pruning masks that are learned through a method similar to what is described in (\u003ca href=\"https://arxiv.org/html/2407.21075v1#bib.bib53\" rel=\"noopener noreferrer\"\u003eWang et al., 2020\u003c/a\u003e; \u003ca href=\"https://arxiv.org/html/2407.21075v1#bib.bib57\" rel=\"noopener noreferrer\"\u003eXia et al., 2023\u003c/a\u003e).\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p10\"\u003eOr, more simply:\u003c/p\u003e\n\u003cblockquote id=\"blockquote11\"\u003e\u003cp\u003e\n  AFM-server core training is conducted from scratch, while AFM-on-device is distilled and pruned from a larger model.\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p12\"\u003eIf the distilled version of AFM-on-device that was tested until a few weeks ago produced a wrong output one third of the time, perhaps it would be a good idea to perform distillation again based on knowledge from other smarter and larger models? Say, using \u003ca href=\"https://9to5mac.com/2025/03/25/apple-is-about-to-spend-1-billion-on-nvidia-servers-for-ai-analyst/\" rel=\"noopener noreferrer\"\u003e250 Nvidia GB300 NVL72 servers\u003c/a\u003e?\u003c/p\u003e\n\u003cp id=\"p13\"\u003e(One last fun fact: per their paper, Apple trained AFM-server on 8192 \u003ca href=\"https://cloud.google.com/tpu/docs/v4\" rel=\"noopener noreferrer\"\u003eTPUv4\u003c/a\u003e chips for 6.3 trillion tokens; that setup still wouldn’t be as powerful as “only” 250 modern \u003ca href=\"https://www.nvidia.com/en-us/data-center/gb300-nvl72/\" rel=\"noopener noreferrer\"\u003eNvidia servers\u003c/a\u003e today.)\u003c/p\u003e\n            \u003c/div\u003e\n\n    \n        \n\n    \n    \n    \n\u003c/article\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-04-11T17:36:21-04:00",
  "modifiedTime": null
}
