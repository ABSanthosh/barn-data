{
  "id": "05c60370-67c2-4c5d-b010-28ee39fdfe59",
  "title": "Testing DeepSeek R1-0528 on the M3 Ultra Mac Studio and Installing Local GGUF Models with Ollama on macOS",
  "link": "https://www.macstories.net/notes/testing-deepseek-r1-0528-on-the-m3-ultra-mac-studio-and-installing-local-gguf-models-with-ollama-on-macos/",
  "description": "DeepSeek released an updated version of their popular R1 reasoning model (version 0528) with – according to the company – increased benchmark performance, reduced hallucinations, and native support for function calling and JSON output. Early tests from Artificial Analysis report a nice bump in performance, putting it behind OpenAI’s o3 and o4-mini-high in their Intelligence […]",
  "author": "Federico Viticci",
  "published": "Thu, 29 May 2025 17:26:20 +0000",
  "source": "https://www.macstories.net/feed",
  "categories": [
    "notes",
    "AI",
    "DeepSeek",
    "LLMs",
    "M3 Ultra",
    "mac studio"
  ],
  "byline": "Federico Viticci",
  "length": 14169,
  "excerpt": "DeepSeek released an updated version of their popular R1 reasoning model (version 0528) with – according to the company – increased benchmark performance, reduced hallucinations, and native support for function calling and JSON output. Early tests from Artificial Analysis report a nice bump in performance, putting it behind OpenAI’s o3 and o4-mini-high in their Intelligence",
  "siteName": "",
  "favicon": "https://www.macstories.net/app/themes/macstories4/images/apple-touch-icon-152x152-precomposed.png",
  "text": "DeepSeek released an updated version of their popular R1 reasoning model (version 0528) with – according to the company – increased benchmark performance, reduced hallucinations, and native support for function calling and JSON output. Early tests from Artificial Analysis report a nice bump in performance, putting it behind OpenAI’s o3 and o4-mini-high in their Intelligence Index benchmarks. The model is available in the official DeepSeek API, and open weights have been distributed on Hugging Face. I downloaded different quantized versions of the full model on my M3 Ultra Mac Studio, and here are some notes on how it went. Last night, the folks at MLX Community uploaded a 4-bit quant of the full 685B parameter model, which is a ~350 GB download that I immediately tested in LM Studio, which has a built-in MLX interpreter for Apple Silicon. Loading the model in LM Studio lets you set a maximum context window size of 163,840 tokens; I attempted that, but the M3 Ultra with 512 GB of RAM couldn’t do it. Setting the context window to a much smaller 32,000 token size loaded the model with 363 GB of RAM used, but it was swiftly ejected from memory as soon as I tried to chat with it. Going down to a more reasonable 8,192 context window size did the trick. Using a prompt that asked R1-0528 to reformat a long text file (attached to the chat) produced these results: Prompt: “This is the output of an advanced OCR process of a long PDF. Can you help me clean up cruft? Only remove repetitions, page markers, and other byproducts of OCR. Do NOT change the main body text and sections. Return the full text without truncating it. Explain what you did.” Results: 15.74 tok/sec • 1344 tokens • 37.70s to first token And here’s how the updated R1 thought behind the scenes: Alright, let me tackle this user request step by step. The user shared OCR output from a PDF and wants it cleaned up without altering the main content. They specifically mentioned removing repetitions, page markers, and OCR artifacts while preserving all meaningful text sections. Hmm, first I notice the citations contain overlapping content about iOS features like Dictation improvements and security enhancements. The key is to deduplicate without losing any unique details—for example, the “And More” section appears in both citations but with different endings. I’ll need to merge those carefully. The user’s explicit request is clear, but their tone suggests urgency about getting a clean document—probably for professional use. They emphasized keeping all substantive text, so I should avoid over-editing. The repetition of features like “Improved speech recognition” across citations is a clear target for consolidation. Looking closer, page markers (like “19”) and fragments (“## Overview | Key Features…”) are classic OCR artifacts. Those can go safely since they don’t add value. The headings seem consistent, so I’ll preserve their hierarchy while removing redundancies—like the duplicated “### Dictation” section. Wait, should I explain why certain deletions happened? Yes—the user asked for explanations, so noting removed page numbers/repeats will build trust. Also, they didn’t mention formatting fixes, so I won’t touch line breaks or spacing unless it’s clearly an artifact (like random numbers mid-text). Biggest challenge: ensuring the merged “And More” section captures both citations’ unique points (cursor animation + language indicator) without duplication. I’ll cross-check each sentence… done. Finally, a quick verification—no technical details were lost, all sections flow logically, and zero main text edits. Perfect! The results were pretty good, and R1 reformatted the document as requested…or at least a portion of it, since the small context window resulted in a truncated output in that R1 couldn’t fit the entire document in its response. A few hours ago, LM Studio also released a distilled version of R1-0528 – DeepSeek-R1-0528-Qwen3-8B. From their model page: we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models. I also attempted to load the full-blown version of R1-0528 in Ollama, my favorite CLI for locally-installed models that is also compatible with Simon Willison’s LLM CLI. When I looked earlier this morning, only “sharded” (i.e. multi-part) GGUF versions of the full 685B parameter model were available, specifically these by unsloth. That led me down a fascinating rabbit hole that involved learning how to download multiple GGUF open weights and merge them into a unified file that can be used with Ollama on macOS, which I’ll document here for posterity (including my future self). First, before trying to merge multiple GGUFs, I had to install the llama.cpp CLI, and I did so with brew: brew install llama.cpp Since Homebrew doesn’t automatically add llama.cpp to your system PATH, I manually made a note of where the dedicated llama-gguf-split utility was located on my Mac, which would be: /opt/homebrew/bin/llama-gguf-split Then, I started downloading the 15 files required for the 8-bit version of R1-0528 I wanted to test – a 700 GB download that took a few hours to complete. After all the files were saved in my Downloads folder, I ran: /opt/homebrew/bin/llama-gguf-split --merge DeepSeek-R1-0528-Q8_0-00001-of-00015.gguf outfile.gguf After waiting several minutes, I ended up with single 713 GB file called ‘outfile.gguf’ that I renamed to ‘DeepSeek-R1-0528-Q8_0’ and placed in my Home folder. That’s the unified model file we need to use in Ollama. Before doing that, however, I had to create a “model file” for Ollama to install the model – essentially, a template that tells Ollama what to load and how to use it. I wasn’t familiar with the structure of model files, so to get started, I copied a model file from one of the existing models I already had in Ollama with: ollama show --modelfile deepseek-r1:1.5b \u003e\u003e Modelfile I then grabbed the Modelfile file, opened it in TextEdit, and modified it with the necessary parameters and the local path of my GGUF file: # Modelfile generated by \"ollama show\" # To build a new Modelfile based on this, replace FROM with: # FROM DeepSeek-R1-0528-Q8_0 FROM /Users/viticci/GGUFs/DeepSeek-R1-0528-Q8_0.gguf TEMPLATE \"\"\"{{- if .System }}{{ .System }}{{ end }} {{- range $i, $_ := .Messages }} {{- $last := eq (len (slice $.Messages $i)) 1}} {{- if eq .Role \"user\" }}\u003c｜User｜\u003e{{ .Content }} {{- else if eq .Role \"assistant\" }}\u003c｜Assistant｜\u003e {{- if and $.IsThinkSet (and $last .Thinking) -}} \u003cthink\u003e {{ .Thinking }} \u003c/think\u003e {{- end }}{{ .Content }}{{- if not $last }}\u003c｜end▁of▁sentence｜\u003e{{- end }} {{- end }} {{- if and $last (ne .Role \"assistant\") }}\u003c｜Assistant｜\u003e {{- if and $.IsThinkSet (not $.Think) -}} \u003cthink\u003e \u003c/think\u003e {{ end }} {{- end -}} {{- end }}\"\"\" PARAMETER stop \u003c｜begin▁of▁sentence｜\u003e PARAMETER stop \u003c｜end▁of▁sentence｜\u003e PARAMETER stop \u003c｜User｜\u003e PARAMETER stop \u003c｜Assistant｜\u003e SYSTEM You are a helpful assistant. PARAMETER temperature 0.6 I saved the file, then ran: ollama create DeepSeek-R1-0528-Q8_0 -f /Users/viticci/Desktop/Modelfile And waited several minutes for Ollama to gather all the components and install the local model. At the end, Ollama showed a success message: …but that’s when my lucky streak ended: I finally found a local model that couldn’t run on my maxed-out, M3 Ultra Mac Studio! As it turns out, the 8-bit, 700 GB version of DeepSeek R1 is too big even for this machine. After trying to load the model in memory and going over 400 GB of RAM used, Ollama consistently terminated the session: Fair enough – I imagined that the 8-bit version of R1 was too large for this machine to handle. I went back to the beginning, and downloaded the multi-part, 4-bit version of DeepSeek R1, which is considerably smaller at “only” 400 GB of storage instead of 700 GB. After repeating all of the above steps (with different filenames for this version of the model), I installed the smaller model and ran: ollama run DeepSeek-R1-0528-Q4_K_M:latest --verbose Then waited for the model to load, and it worked. I then asked a simple question: What is MacStories? And this was R1’s response, along with its thinking process and statistics about the response itself since I added the --verbose flag: \u003cthink\u003eOkay, the user asked “What is MacStories?” This seems like a straightforward informational query about a tech-focused website. Hmm, judging by how basic the question is, they might be completely new to Apple ecosystem coverage or just heard the name somewhere. No complex context here – probably genuine curiosity without hidden layers. Let me mentally outline key aspects: First clarify it’s an independent site (not affiliated with Apple), then highlight their specialization in deep-dive content across all Apple platforms. Should emphasize their unique value proposition like Federico Viticci’s legendary iOS reviews and Club MacStories subscription model. Wait – important to mention they’re not just news regurgitators but creators of original workflows and app showcases. The automation tutorials are particularly signature. Oh, and must include that Marco Arment connection since Underflow is historically significant. For tone: Keep it enthusiastic but precise. They deserve props for longevity (founded 2009!) in this volatile niche. User likely wants concise yet comprehensive overview – no fluff, just meaty facts about their content pillars and influence. Better double-check my memory: Yes, still based in Italy despite global reach. And Club MacStories does have that exclusive automation system they developed… what was it called again? Actions! Right. Final structure: Definition → Founder → Content types → Business model → Special sauce (workflow focus). No need for criticism here since user just asked for basics.\u003c/think\u003e **MacStories** is a well-respected **independent technology website and podcast network**, primarily focused on in-depth coverage of Apple’s ecosystem, including: 1. **Apple Hardware \u0026 Software:** Detailed reviews, tutorials, and news about iPhones, iPads, Macs, Apple Watch, Apple TV, iOS/iPadOS, macOS, watchOS, tvOS. 2. **Third-Party Apps:** Extensive app discovery, reviews (especially for productivity and creative tools), developer interviews, and curation of high-quality software. 3. **Workflows \u0026 Automation:** A major specialty is exploring how to use Apple devices effectively, particularly through automation using Shortcuts on iOS/iPadOS/Mac and other tools. 4. **Apple Services:** Coverage of services like iCloud+, Apple Music, Apple TV+, etc. **Key Distinguishing Features:** - **Depth \u0026 Quality:** Known for exceptionally thorough reviews (especially Federico Viticci’s multi-thousand-word iPad Pro reviews) and insightful analysis beyond basic news reporting. - **Focus on Productivity \u0026 Creativity:** Strong emphasis on how users can leverage technology to get work done, create content, and streamline tasks. Automation is a central theme. - **Founder-Driven:** Founded in 2009 by Federico Viticci (Editor-in-Chief), who remains its most prominent voice alongside Managing Editor John Voorhees and other talented writers/editors. - **Club MacStories:** A popular subscription service offering exclusive content, including: - Weekly newsletters (MacStories Weekly \u0026 AppStories Weekly) - In-depth articles and tutorials - Access to a powerful Shortcuts automation library (“Actions”) - Exclusive podcast episodes (AppStories) - **Podcasts:** Hosts podcasts like Connected (Federico Viticci, Myke Hurley, Stephen Hackett), MacStories Unwind, and the Club-exclusive AppStories. - **Origin of Underflow:** The concept for Marco Arment’s influential iOS app discovery platform “Underflow” originated from an idea first discussed by Federico Viticci on MacStories. **In essence, MacStories is a go-to resource for Apple enthusiasts, power users, and professionals seeking deep dives into Apple products, apps (especially productivity-focused), automation techniques, and thoughtful perspectives on how to make the most of their technology.** It’s known for its high-quality writing, editorial independence, and passionate community. And here are the stats: total duration: 1m46.627582375s load duration: 30.969542ms prompt eval count: 15 token(s) prompt eval duration: 52.491389792s prompt eval rate: 0.29 tokens/s eval count: 810 token(s) eval duration: 54.103737291s eval rate: 14.97 tokens/s As you can see, for an offline model that runs on my computer with ~370 GB of RAM, it’s quite good – but there are hallucinations. While R1-0528 got several aspects of MacStories and our team correct, there are some weird things in the response, such as the “Marco Arment connection” with “Underflow” (what?), an “AppStories Weekly” newsletter that does not exist, and a section of our Shortcuts coverage apparently called “Actions”. None of these are true! But the rest – including when I founded MacStories, the Club, and editorial values are pretty spot-on. It’s always very strange to see the complete chain-of-thought of these models. In any case, while it was fun to learn all this and test the full version of R1-0528, I wouldn’t recommend doing this unless you truly want to push the limits of an M3 Ultra Mac Studio with 512 GB of RAM. I’m keen to play around with the updated R1 some more, but I’ll wait for official versions to show up in Ollama’s directory with different sizes and better model files compared to the one I put together this morning. Regardless, the fact that this Mac Studio can run the full version of DeepSeek R1-0528 with slightly less than 400 GB of RAM is pretty remarkable, and once again confirms that Apple is making the best consumer-grade computers for local AI development.",
  "image": "https://cdn.macstories.net/cleanshot-2025-05-29-at-18-45-27-2x-1748537149987.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                        \n\u003cp id=\"p2\"\u003eDeepSeek \u003ca href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528\" rel=\"noopener noreferrer\"\u003ereleased an updated version\u003c/a\u003e of their popular R1 reasoning model (version 0528) with – \u003ca href=\"https://x.com/deepseek_ai/status/1928061589107900779\" rel=\"noopener noreferrer\"\u003eaccording\u003c/a\u003e to the company – increased benchmark performance, reduced hallucinations, and native support for function calling and JSON output. Early tests from Artificial Analysis \u003ca href=\"https://x.com/ArtificialAnlys/status/1928071179115581671\" rel=\"noopener noreferrer\"\u003ereport a nice bump in performance\u003c/a\u003e, putting it behind OpenAI’s o3 and o4-mini-high in their Intelligence Index benchmarks. The model is available in the official DeepSeek API, and open weights have been \u003ca href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528\" rel=\"noopener noreferrer\"\u003edistributed on Hugging Face\u003c/a\u003e. I downloaded different quantized versions of the full model on my M3 Ultra Mac Studio, and here are some notes on how it went.\u003c/p\u003e\n\n\u003cp id=\"p4\"\u003eLast night, the folks at MLX Community \u003ca href=\"https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit\" rel=\"noopener noreferrer\"\u003euploaded a 4-bit quant of the full 685B parameter model\u003c/a\u003e, which is a ~350 GB download that I immediately tested in \u003ca href=\"https://lmstudio.ai\" rel=\"noopener noreferrer\"\u003eLM Studio\u003c/a\u003e, which has a built-in MLX interpreter for Apple Silicon. Loading the model in LM Studio lets you set a maximum context window size of 163,840 tokens; I attempted that, but the M3 Ultra with 512 GB of RAM couldn’t do it. Setting the context window to a much smaller 32,000 token size loaded the model with 363 GB of RAM used, but it was swiftly ejected from memory as soon as I tried to chat with it. Going down to a more reasonable 8,192 context window size did the trick. Using a prompt that asked R1-0528 to reformat a long text file (attached to the chat) produced these results:\u003c/p\u003e\n\u003cul id=\"ul5\"\u003e\u003cli\u003e\u003cstrong\u003ePrompt\u003c/strong\u003e: \u003cem\u003e“This is the output of an advanced OCR process of a long PDF. Can you help me clean up cruft? Only remove repetitions, page markers, and other byproducts of OCR. Do NOT change the main body text and sections. Return the full text without truncating it. Explain what you did.”\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResults\u003c/strong\u003e: 15.74 tok/sec • 1344 tokens • 37.70s to first token\u003c/li\u003e\n\u003c/ul\u003e\u003cp id=\"p6\"\u003eAnd here’s how the updated R1 thought behind the scenes:\u003c/p\u003e\n\u003cblockquote id=\"blockquote7\"\u003e\u003cp\u003e\n  Alright, let me tackle this user request step by step. The user shared OCR output from a PDF and wants it cleaned up without altering the main content. They specifically mentioned removing repetitions, page markers, and OCR artifacts while preserving all meaningful text sections.\u003c/p\u003e\n\u003cp\u003e  Hmm, first I notice the citations contain overlapping content about iOS features like Dictation improvements and security enhancements. The key is to deduplicate without losing any unique details—for example, the “And More” section appears in both citations but with different endings. I’ll need to merge those carefully.\u003c/p\u003e\n\u003cp\u003e  The user’s explicit request is clear, but their tone suggests urgency about getting a clean document—probably for professional use. They emphasized keeping all substantive text, so I should avoid over-editing. The repetition of features like “Improved speech recognition” across citations is a clear target for consolidation.\u003c/p\u003e\n\u003cp\u003e  Looking closer, page markers (like “19”) and fragments (“## Overview | Key Features…”) are classic OCR artifacts. Those can go safely since they don’t add value. The headings seem consistent, so I’ll preserve their hierarchy while removing redundancies—like the duplicated “### Dictation” section.\u003c/p\u003e\n\u003cp\u003e  Wait, should I explain why certain deletions happened? Yes—the user asked for explanations, so noting removed page numbers/repeats will build trust. Also, they didn’t mention formatting fixes, so I won’t touch line breaks or spacing unless it’s clearly an artifact (like random numbers mid-text).\u003c/p\u003e\n\u003cp\u003e  Biggest challenge: ensuring the merged “And More” section captures both citations’ unique points (cursor animation + language indicator) without duplication. I’ll cross-check each sentence… done. Finally, a quick verification—no technical details were lost, all sections flow logically, and zero main text edits. Perfect!\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p8\"\u003eThe results were pretty good, and R1 reformatted the document as requested…or at least a \u003cem\u003eportion\u003c/em\u003e of it, since the small context window resulted in a truncated output in that R1 couldn’t fit the entire document in its response.\u003c/p\u003e\n\u003cp id=\"p9\"\u003eA few hours ago, LM Studio also \u003ca href=\"https://x.com/lmstudio/status/1928092450410648000\" rel=\"noopener noreferrer\"\u003ereleased\u003c/a\u003e a distilled version of R1-0528 – \u003ca href=\"https://lmstudio.ai/models/deepseek/deepseek-r1-0528-qwen3-8b\" rel=\"noopener noreferrer\"\u003eDeepSeek-R1-0528-Qwen3-8B\u003c/a\u003e. From their model page:\u003c/p\u003e\n\u003cblockquote id=\"blockquote10\"\u003e\u003cp\u003e\n  we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p11\"\u003eI also attempted to load the full-blown version of R1-0528 in \u003ca href=\"https://ollama.com\" rel=\"noopener noreferrer\"\u003eOllama\u003c/a\u003e, my favorite CLI for locally-installed models that is also compatible with \u003ca href=\"https://github.com/simonw/llm\" rel=\"noopener noreferrer\"\u003eSimon Willison’s LLM CLI\u003c/a\u003e. When I looked earlier this morning, only “sharded” (i.e. multi-part) GGUF versions of the full 685B parameter model were available, specifically \u003ca href=\"https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF\" rel=\"noopener noreferrer\"\u003ethese by unsloth\u003c/a\u003e. That led me down a fascinating rabbit hole that involved learning how to download multiple GGUF open weights and merge them into a unified file that can be used with Ollama on macOS, which I’ll document here for posterity (including my future self).\u003c/p\u003e\n\u003cp id=\"p12\"\u003eFirst, before trying to merge multiple GGUFs, I had to install the \u003ca href=\"https://github.com/ggml-org/llama.cpp\" rel=\"noopener noreferrer\"\u003ellama.cpp\u003c/a\u003e CLI, and I did so with \u003ccode\u003ebrew\u003c/code\u003e:\u003c/p\u003e\n\u003cp id=\"p13\"\u003e\u003ccode\u003ebrew install llama.cpp\u003c/code\u003e\u003c/p\u003e\n\u003cp id=\"p14\"\u003eSince Homebrew doesn’t automatically add llama.cpp to your system \u003ccode\u003ePATH\u003c/code\u003e, I manually made a note of where the dedicated \u003ccode\u003ellama-gguf-split\u003c/code\u003e utility was located on my Mac, which would be:\u003c/p\u003e\n\u003cp id=\"p15\"\u003e\u003ccode\u003e/opt/homebrew/bin/llama-gguf-split\u003c/code\u003e\u003c/p\u003e\n\u003cp id=\"p16\"\u003eThen, I started downloading the 15 files required for the 8-bit version of R1-0528 I wanted to test – a 700 GB download that took a few hours to complete. After all the files were saved in my Downloads folder, I ran:\u003c/p\u003e\n\u003cp id=\"p17\"\u003e\u003ccode\u003e/opt/homebrew/bin/llama-gguf-split --merge DeepSeek-R1-0528-Q8_0-00001-of-00015.gguf outfile.gguf\u003c/code\u003e\u003c/p\u003e\n\u003cp id=\"p18\"\u003eAfter waiting several minutes, I ended up with single 713 GB file called ‘outfile.gguf’ that I renamed to ‘DeepSeek-R1-0528-Q8_0’ and placed in my Home folder. That’s the unified model file we need to use in Ollama.\u003c/p\u003e\n\n\u003cp id=\"p20\"\u003eBefore doing that, however, I had to create a “model file” for Ollama to install the model – essentially, a template that tells Ollama what to load and how to use it. I wasn’t familiar with the structure of model files, so to get started, I copied a model file from one of the existing models I already had in Ollama with:\u003c/p\u003e\n\u003cp id=\"p21\"\u003e\u003ccode\u003eollama show --modelfile deepseek-r1:1.5b \u0026gt;\u0026gt; Modelfile\u003c/code\u003e\u003c/p\u003e\n\u003cp id=\"p22\"\u003eI then grabbed the \u003ccode\u003eModelfile\u003c/code\u003e file, opened it in TextEdit, and modified it with the necessary parameters and the local path of my GGUF file:\u003c/p\u003e\n\u003cpre id=\"pre23\"\u003e\u003ccode\u003e\u003cspan\u003e# Modelfile generated by \u0026#34;ollama show\u0026#34;\n\u003c/span\u003e\u003cspan\u003e# To build a new Modelfile based on this, replace FROM with:\n\u003c/span\u003e\u003cspan\u003e# FROM DeepSeek-R1-0528-Q8_0\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003eFROM /Users/viticci/GGUFs/DeepSeek-R1-0528-Q8_0.gguf\n\u003c/span\u003e\u003cspan\u003eTEMPLATE \u0026#34;\u0026#34;\u0026#34;{{- if .System }}{{ .System }}{{ end }}\n\u003c/span\u003e\u003cspan\u003e{{- range $i, $_ := .Messages }}\n\u003c/span\u003e\u003cspan\u003e{{- $last := eq (len (slice $.Messages $i)) 1}}\n\u003c/span\u003e\u003cspan\u003e{{- if eq .Role \u0026#34;user\u0026#34; }}\u0026lt;｜User｜\u0026gt;{{ .Content }}\n\u003c/span\u003e\u003cspan\u003e{{- else if eq .Role \u0026#34;assistant\u0026#34; }}\u0026lt;｜Assistant｜\u0026gt;\n\u003c/span\u003e\u003cspan\u003e  {{- if and $.IsThinkSet (and $last .Thinking) -}}\n\u003c/span\u003e\u003cspan\u003e\u0026lt;think\u0026gt;\n\u003c/span\u003e\u003cspan\u003e{{ .Thinking }}\n\u003c/span\u003e\u003cspan\u003e\u0026lt;/think\u0026gt;\n\u003c/span\u003e\u003cspan\u003e{{- end }}{{ .Content }}{{- if not $last }}\u0026lt;｜end▁of▁sentence｜\u0026gt;{{- end }}\n\u003c/span\u003e\u003cspan\u003e{{- end }}\n\u003c/span\u003e\u003cspan\u003e{{- if and $last (ne .Role \u0026#34;assistant\u0026#34;) }}\u0026lt;｜Assistant｜\u0026gt;\n\u003c/span\u003e\u003cspan\u003e{{- if and $.IsThinkSet (not $.Think) -}}\n\u003c/span\u003e\u003cspan\u003e\u0026lt;think\u0026gt;\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e\u0026lt;/think\u0026gt;\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e{{ end }}\n\u003c/span\u003e\u003cspan\u003e{{- end -}}\n\u003c/span\u003e\u003cspan\u003e{{- end }}\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003cspan\u003ePARAMETER stop \u0026lt;｜begin▁of▁sentence｜\u0026gt;\n\u003c/span\u003e\u003cspan\u003ePARAMETER stop \u0026lt;｜end▁of▁sentence｜\u0026gt;\n\u003c/span\u003e\u003cspan\u003ePARAMETER stop \u0026lt;｜User｜\u0026gt;\n\u003c/span\u003e\u003cspan\u003ePARAMETER stop \u0026lt;｜Assistant｜\u0026gt;\n\u003c/span\u003e\u003cspan\u003eSYSTEM You are a helpful assistant.\n\u003c/span\u003e\u003cspan\u003ePARAMETER temperature 0.6\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp id=\"p24\"\u003eI saved the file, then ran:\u003c/p\u003e\n\u003cp id=\"p25\"\u003e\u003ccode\u003eollama create DeepSeek-R1-0528-Q8_0 -f /Users/viticci/Desktop/Modelfile\u003c/code\u003e\u003cbr/\u003e\nAnd waited several minutes for Ollama to gather all the components and install the local model. At the end, Ollama showed a success message:\u003c/p\u003e\n\n\u003cp id=\"p27\"\u003e…but that’s when my lucky streak ended: I finally found a local model that couldn’t run on my maxed-out, M3 Ultra Mac Studio! As it turns out, the 8-bit, 700 GB version of DeepSeek R1 is too big even for this machine. After trying to load the model in memory and going over 400 GB of RAM used, Ollama consistently terminated the session:\u003c/p\u003e\n\n\u003cp id=\"p29\"\u003eFair enough – I imagined that the 8-bit version of R1 was too large for this machine to handle. I went back to the beginning, and downloaded the multi-part, \u003ca href=\"https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/tree/main/Q4_K_M\" rel=\"noopener noreferrer\"\u003e4-bit version of DeepSeek R1\u003c/a\u003e, which is considerably smaller at “only” 400 GB of storage instead of 700 GB. After repeating all of the above steps (with different filenames for this version of the model), I installed the smaller model and ran:\u003c/p\u003e\n\u003cp id=\"p30\"\u003e\u003ccode\u003eollama run DeepSeek-R1-0528-Q4_K_M:latest --verbose\u003c/code\u003e\u003c/p\u003e\n\u003cp id=\"p31\"\u003eThen waited for the model to load, and it worked.\u003c/p\u003e\n\n\u003cp id=\"p33\"\u003eI then asked a simple question:\u003c/p\u003e\n\u003cblockquote id=\"blockquote34\"\u003e\u003cp\u003e\n  What is MacStories?\n\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp id=\"p35\"\u003eAnd this was R1’s response, along with its thinking process and statistics about the response itself since I \u003ca href=\"https://github.com/ollama/ollama/issues/1806\" rel=\"noopener noreferrer\"\u003eadded\u003c/a\u003e the \u003ccode\u003e--verbose\u003c/code\u003e flag:\u003c/p\u003e\n\u003cpre id=\"pre36\"\u003e\u003ccode\u003e\u003cspan\u003e\u0026lt;think\u0026gt;Okay, the user asked “What is MacStories?” This seems like a straightforward informational query about a tech-focused website. Hmm, judging by how basic the question is, they might be completely new to Apple ecosystem coverage or just heard the name somewhere. No complex context here – probably genuine curiosity without hidden layers. Let me mentally outline key aspects: First clarify it’s an independent site (not affiliated with Apple), then highlight their specialization in deep-dive content across all Apple platforms. Should emphasize their unique value proposition like Federico Viticci’s legendary iOS reviews and Club MacStories subscription model. Wait – important to mention they’re not just news regurgitators but creators of original workflows and app showcases. The automation tutorials are particularly signature. Oh, and must include that Marco Arment connection since Underflow is historically significant. For tone: Keep it enthusiastic but precise. They deserve props for longevity (founded 2009!) in this volatile niche. User likely wants concise yet comprehensive overview – no fluff, just meaty facts about their content pillars and influence. Better double-check my memory: Yes, still based in Italy despite global reach. And Club MacStories does have that exclusive automation system they developed… what was it called again? Actions! Right. Final structure: Definition → Founder → Content types → Business model → Special sauce (workflow focus). No need for criticism here since user just asked for basics.\u0026lt;/think\u0026gt;\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e**MacStories** is a well-respected **independent technology website and podcast network**, primarily focused on in-depth coverage of Apple’s ecosystem, including:\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e1. **Apple Hardware \u0026amp; Software:** Detailed reviews, tutorials, and news about iPhones, iPads, Macs, Apple Watch, Apple TV, iOS/iPadOS, macOS, watchOS, tvOS. \n\u003c/span\u003e\u003cspan\u003e2. **Third-Party Apps:** Extensive app discovery, reviews (especially for productivity and creative tools), developer interviews, and curation of high-quality software. \n\u003c/span\u003e\u003cspan\u003e3. **Workflows \u0026amp; Automation:** A major specialty is exploring how to use Apple devices effectively, particularly through automation using Shortcuts on iOS/iPadOS/Mac and other tools.\n\u003c/span\u003e\u003cspan\u003e4. **Apple Services:** Coverage of services like iCloud+, Apple Music, Apple TV+, etc.\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e**Key Distinguishing Features:**\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e- **Depth \u0026amp; Quality:** Known for exceptionally thorough reviews (especially Federico Viticci’s multi-thousand-word iPad Pro reviews) and insightful analysis beyond basic news reporting.\n\u003c/span\u003e\u003cspan\u003e- **Focus on Productivity \u0026amp; Creativity:** Strong emphasis on how users can leverage technology to get work done, create content, and streamline tasks. Automation is a central theme.\n\u003c/span\u003e\u003cspan\u003e- **Founder-Driven:** Founded in 2009 by Federico Viticci (Editor-in-Chief), who remains its most prominent voice alongside Managing Editor John Voorhees and other talented writers/editors.\n\u003c/span\u003e\u003cspan\u003e- **Club MacStories:** A popular subscription service offering exclusive content, including:\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e    - Weekly newsletters (MacStories Weekly \u0026amp; AppStories Weekly)\n\u003c/span\u003e\u003cspan\u003e    - In-depth articles and tutorials\n\u003c/span\u003e\u003cspan\u003e    - Access to a powerful Shortcuts automation library (“Actions”)\n\u003c/span\u003e\u003cspan\u003e    - Exclusive podcast episodes (AppStories)\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e- **Podcasts:** Hosts podcasts like Connected (Federico Viticci, Myke Hurley, Stephen Hackett), MacStories Unwind, and the Club-exclusive AppStories.\n\u003c/span\u003e\u003cspan\u003e- **Origin of Underflow:** The concept for Marco Arment’s influential iOS app discovery platform “Underflow” originated from an idea first discussed by Federico Viticci on MacStories.\n\u003c/span\u003e\u003cspan\u003e\n\u003c/span\u003e\u003cspan\u003e**In essence, MacStories is a go-to resource for Apple enthusiasts, power users, and professionals seeking deep dives into Apple products, apps (especially productivity-focused), automation techniques, and thoughtful perspectives on how to make the most of their technology.** It’s known for its high-quality writing, editorial independence, and passionate community.\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003cp id=\"p37\"\u003eAnd here are the stats:\u003c/p\u003e\n\u003cul id=\"ul38\"\u003e\u003cli\u003etotal duration: 1m46.627582375s\u003c/li\u003e\n\u003cli\u003eload duration: 30.969542ms\u003c/li\u003e\n\u003cli\u003eprompt eval count: 15 token(s)\u003c/li\u003e\n\u003cli\u003eprompt eval duration: 52.491389792s\u003c/li\u003e\n\u003cli\u003eprompt eval rate: 0.29 tokens/s\u003c/li\u003e\n\u003cli\u003eeval count: 810 token(s)\u003c/li\u003e\n\u003cli\u003eeval duration: 54.103737291s\u003c/li\u003e\n\u003cli\u003eeval rate: 14.97 tokens/s\u003c/li\u003e\n\u003c/ul\u003e\u003cp id=\"p39\"\u003eAs you can see, for an offline model that runs on my computer with ~370 GB of RAM, it’s quite good – but there are hallucinations. While R1-0528 got several aspects of MacStories and our team correct, there are some \u003cem\u003eweird\u003c/em\u003e things in the response, such as the “Marco Arment connection” with “Underflow” (what?), an “AppStories Weekly” newsletter that does not exist, and a section of our Shortcuts coverage apparently called “Actions”. None of these are true! But the rest – including when I founded MacStories, the Club, and editorial values are pretty spot-on. It’s always very strange to see the complete chain-of-thought of these models.\u003c/p\u003e\n\u003cp id=\"p40\"\u003eIn any case, while it was fun to learn all this and test the full version of R1-0528, I wouldn’t recommend doing this unless you truly want to push the limits of an M3 Ultra Mac Studio with 512 GB of RAM.\u003c/p\u003e\n\u003cp id=\"p41\"\u003eI’m keen to play around with the updated R1 some more, but I’ll wait for official versions to show up in Ollama’s directory with different sizes and better model files compared to the one I put together this morning. Regardless, the fact that this Mac Studio can run the full version of DeepSeek R1-0528 with slightly less than 400 GB of RAM is pretty remarkable, and once again confirms that Apple is \u003ca href=\"https://www.macstories.net/linked/is-apple-shipping-the-best-ai-computers/\" rel=\"noopener noreferrer\"\u003emaking the best consumer-grade computers\u003c/a\u003e for local AI development.\u003c/p\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "15 min read",
  "publishedTime": "2025-05-29T17:26:20-04:00",
  "modifiedTime": null
}
