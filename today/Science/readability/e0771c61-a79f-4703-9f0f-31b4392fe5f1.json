{
  "id": "e0771c61-a79f-4703-9f0f-31b4392fe5f1",
  "title": "Google’s DeepMind AI Can Solve Math Problems on Par with Top Human Solvers",
  "link": "https://www.scientificamerican.com/article/googles-deepmind-ai-can-solve-math-problems-on-par-with-top-human-solvers/",
  "description": "Google’s AlphaGeometry2 AI reaches the level of gold-medal students in the International Mathematical Olympiad",
  "author": "",
  "published": "Mon, 10 Feb 2025 19:00:00 +0000",
  "source": "http://rss.sciam.com/ScientificAmerican-Global",
  "categories": null,
  "byline": "Davide Castelvecchi",
  "length": 4344,
  "excerpt": "Google’s AlphaGeometry2 AI reaches the level of gold-medal students in the International Mathematical Olympiad",
  "siteName": "Scientific American",
  "favicon": "",
  "text": "February 10, 20253 min readGoogle’s AI Can Beat the Smartest High Schoolers in MathGoogle’s AlphaGeometry2 AI reaches the level of gold-medal students in the International Mathematical OlympiadGoogle DeepMind’s AI AlphaGeometry2 aced problems set at the International Mathematical Olympiad. Wirestock, Inc./Alamy Stock PhotoA year ago AlphaGeometry, an artificial-intelligence (AI) problem solver created by Google DeepMind, surprised the world by performing at the level of silver medallists in the International Mathematical Olympiad (IMO), a prestigious competition that sets tough maths problems for gifted high-school students.The DeepMind team now says the performance of its upgraded system, AlphaGeometry2, has surpassed the level of the average gold medallist. The results are described in a preprint on the arXiv.“I imagine it won’t be long before computers are getting full marks on the IMO,” says Kevin Buzzard, a mathematician at Imperial College London.On supporting science journalismIf you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.Solving problems in Euclidean geometry is one of the four topics covered in IMO problems — the others cover the branches of number theory, algebra and combinatorics. Geometry demands specific skills of an AI, because competitors must provide a rigorous proof for a statement about geometric objects on the plane. In July, AlphaGeometry2 made its public debut alongside a newly unveiled system, AlphaProof, which DeepMind developed for solving the non-geometry questions in the IMO problem sets.Mathematical languageAlphaGeometry is a combination of components that include a specialized language model and a ‘neuro-symbolic’ system — one that does not train by learning from data like a neural network but has abstract reasoning coded in by humans. The team trained the language model to speak a formal mathematical language, which makes it possible to automatically check its output for logical rigour — and to weed out the ‘hallucinations’, the incoherent or false statements that AI chatbots are prone to making.For AlphaGeometry2, the team made several improvements, including the integration of Google’s state-of-the-art large language model, Gemini. The team also introduced the ability to reason by moving geometric objects around the plane — such as moving a point along a line to change the height of a triangle — and solving linear equations.The system was able to solve 84% of all geometry problems given in IMOs in the past 25 years, compared with 54% for the first AlphaGeometry. (Teams in India and China used different approaches last year to achieve gold-medal-level performance in geometry, but on a smaller subset of IMO geometry problems.)The authors of the DeepMind paper write that future improvements of AlphaGeometry will include dealing with maths problems that involve inequalities and non-linear equations, which will be required to to “fully solve geometry.”Rapid progressThe first AI system to achieve a gold-medal score for the overall test could win a US$5-million award called the AI Mathematical Olympiad Prize — although that competition requires systems to be open-source, which is not the case for DeepMind.Buzzard says he is not surprised by the rapid progress made both by DeepMind and by the Indian and Chinese teams. But, he adds, although the problems are hard, the subject is still conceptually simple, and there are many more challenges to overcome before AI is able to solve problems at the level of research mathematics.AI researchers will be eagerly awaiting the next iteration of the IMO in Sunshine Coast, Australia, in July. Once its problems are made public for human participants to solve, AI-based systems get to solve them, too. (AI agents are not allowed to take part in the competition, and are therefore not eligible to win medals.) Fresh problems are seen as the most reliable test for machine-learning-based systems, because there is no risk that the problems or their solution existed online and may have ‘leaked’ into training data sets, skewing the results.This article is reproduced with permission and was first published on February 7, 2025.",
  "image": "https://static.scientificamerican.com/dam/m/78346ac550b8c77b/original/Geometrics_pyramid_winner.jpg?m=1739212721.141\u0026w=1200",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003eFebruary 10, 2025\u003c/p\u003e\u003cp\u003e3 min read\u003c/p\u003e\u003c/div\u003e\u003ch2\u003e\u003cp\u003eGoogle’s AI Can Beat the Smartest High Schoolers in Math\u003c/p\u003e\u003c/h2\u003e\u003cp\u003eGoogle’s AlphaGeometry2 AI reaches the level of gold-medal students in the International Mathematical Olympiad\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://static.scientificamerican.com/dam/m/78346ac550b8c77b/original/Geometrics_pyramid_winner.jpg?m=1739212721.141\u0026amp;w=600\" alt=\"Blue cube pyramid with blue sky background.\" srcset=\"https://static.scientificamerican.com/dam/m/78346ac550b8c77b/original/Geometrics_pyramid_winner.jpg?m=1739212721.141\u0026amp;w=600 600w, https://static.scientificamerican.com/dam/m/78346ac550b8c77b/original/Geometrics_pyramid_winner.jpg?m=1739212721.141\u0026amp;w=900 900w, https://static.scientificamerican.com/dam/m/78346ac550b8c77b/original/Geometrics_pyramid_winner.jpg?m=1739212721.141\u0026amp;w=1000 1000w, https://static.scientificamerican.com/dam/m/78346ac550b8c77b/original/Geometrics_pyramid_winner.jpg?m=1739212721.141\u0026amp;w=1200 1200w, https://static.scientificamerican.com/dam/m/78346ac550b8c77b/original/Geometrics_pyramid_winner.jpg?m=1739212721.141\u0026amp;w=1350 1350w\" sizes=\"(min-width: 900px) 900px, (min-resolution: 2dppx) 75vw, (min-resolution: 2.1dppx) 50vw, 100vw\" fetchpriority=\"high\"/\u003e\u003cfigcaption\u003e\u003cp\u003eGoogle DeepMind’s AI AlphaGeometry2 aced problems set at the International Mathematical Olympiad.\u003c/p\u003e \u003cp\u003eWirestock, Inc./Alamy Stock Photo\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp data-block=\"sciam/paragraph\"\u003eA year ago AlphaGeometry, an artificial-intelligence (AI) problem solver created by Google DeepMind, surprised the world by \u003ca href=\"https://www.nature.com/articles/d41586-024-02441-2\"\u003eperforming at the level of silver medallists in the International Mathematical Olympiad\u003c/a\u003e (IMO), a prestigious competition that sets tough maths problems for gifted high-school students.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe DeepMind team now says the performance of its upgraded system, AlphaGeometry2, has surpassed the level of the average gold medallist. The results are described in a preprint on the arXiv.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003e“I imagine it won’t be long before computers are getting full marks on the IMO,” says Kevin Buzzard, a mathematician at Imperial College London.\u003c/p\u003e\u003chr/\u003e\u003ch2\u003eOn supporting science journalism\u003c/h2\u003e\u003cp\u003eIf you\u0026#39;re enjoying this article, consider supporting our award-winning journalism by \u003ca href=\"https://www.scientificamerican.com/getsciam/\"\u003esubscribing\u003c/a\u003e. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.\u003c/p\u003e\u003chr/\u003e\u003cp data-block=\"sciam/paragraph\"\u003eSolving problems in Euclidean geometry is one of the four topics covered in IMO problems — the others cover the branches of number theory, algebra and combinatorics. Geometry demands specific skills of an AI, because competitors must provide a rigorous proof for a statement about geometric objects on the plane. In July, AlphaGeometry2 made its public debut alongside a newly unveiled system, AlphaProof, which DeepMind developed for solving the non-geometry questions in the IMO problem sets.\u003c/p\u003e\u003ch2 id=\"mathematical-language\" data-block=\"sciam/heading\"\u003eMathematical language\u003c/h2\u003e\u003cp data-block=\"sciam/paragraph\"\u003eAlphaGeometry is a combination of components that include a specialized language model and a ‘neuro-symbolic’ system — one that does not train by learning from data like a neural network but has abstract reasoning coded in by humans. The team trained the language model to speak a formal mathematical language, which makes it possible to automatically check its output for logical rigour — and to weed out the ‘hallucinations’, the incoherent or false statements that AI chatbots are prone to making.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eFor AlphaGeometry2, the team made several improvements, including the integration of Google’s state-of-the-art large language model, Gemini. The team also introduced the ability to reason by moving geometric objects around the plane — such as moving a point along a line to change the height of a triangle — and solving linear equations.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe system was able to solve 84% of all geometry problems given in IMOs in the past 25 years, compared with 54% for the first AlphaGeometry. (Teams in India and China used different approaches last year to achieve gold-medal-level performance in geometry, but on a smaller subset of IMO geometry problems.)\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe authors of the DeepMind paper write that future improvements of AlphaGeometry will include dealing with maths problems that involve inequalities and non-linear equations, which will be required to to “fully solve geometry.”\u003c/p\u003e\u003ch2 id=\"rapid-progress\" data-block=\"sciam/heading\"\u003eRapid progress\u003c/h2\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe first AI system to achieve a gold-medal score for the overall test could win a US$5-million award called the AI Mathematical Olympiad Prize — although that competition requires systems to be open-source, which is not the case for DeepMind.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eBuzzard says he is not surprised by the rapid progress made both by DeepMind and by the Indian and Chinese teams. But, he adds, although the problems are hard, the subject is still conceptually simple, and there are many more challenges to overcome before AI is able to solve problems at the level of research mathematics.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eAI researchers will be eagerly awaiting the next iteration of the IMO in Sunshine Coast, Australia, in July. Once its problems are made public for human participants to solve, AI-based systems get to solve them, too. (AI agents are not allowed to take part in the competition, and are therefore not eligible to win medals.) Fresh problems are seen as the most reliable test for machine-learning-based systems, because there is no risk that the problems or their solution existed online and may have ‘leaked’ into training data sets, skewing the results.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003e\u003ci\u003eThis article is reproduced with permission and was \u003c/i\u003e\u003ca href=\"https://www.nature.com/articles/d41586-025-00406-7\"\u003e\u003ci\u003efirst published\u003c/i\u003e\u003c/a\u003e\u003ci\u003e on February 7, 2025\u003c/i\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "5 min read",
  "publishedTime": "2025-02-10T14:00:00-05:00",
  "modifiedTime": null
}
