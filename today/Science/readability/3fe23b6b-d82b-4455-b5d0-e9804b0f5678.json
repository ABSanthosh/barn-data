{
  "id": "3fe23b6b-d82b-4455-b5d0-e9804b0f5678",
  "title": "An instantaneous voice-synthesis neuroprosthesis",
  "link": "https://www.nature.com/articles/s41586-025-09127-3",
  "description": "",
  "author": "Maitreyee Wairagkar",
  "published": "2025-06-12",
  "source": "https://www.nature.com/nature.rss",
  "categories": null,
  "byline": "Stavisky, Sergey D.",
  "length": 40671,
  "excerpt": "Brain–computer interfaces (BCIs) have the potential to restore communication for people who have lost the ability to speak owing to a neurological disease or injury. BCIs have been used to translate the neural correlates of attempted speech into text1–3. However, text communication fails to capture the nuances of human speech, such as prosody and immediately hearing one’s own voice. Here we demonstrate a brain-to-voice neuroprosthesis that instantaneously synthesizes voice with closed-loop audio feedback by decoding neural activity from 256 microelectrodes implanted into the ventral precentral gyrus of a man with amyotrophic lateral sclerosis and severe dysarthria. We overcame the challenge of lacking ground-truth speech for training the neural decoder and were able to accurately synthesize his voice. Along with phonemic content, we were also able to decode paralinguistic features from intracortical activity, enabling the participant to modulate his BCI-synthesized voice in real time to change intonation and sing short melodies. These results demonstrate the feasibility of enabling people with paralysis to speak intelligibly and expressively through a BCI. A brain-to-voice neuroprosthesis enables a man with amyotrophic lateral sclerosis to synthesize his voice in real time by decoding neural activity, demonstrating the potential of brain–computer interfaces to enable people with paralysis to speak intelligibly and expressively.",
  "siteName": "Nature",
  "favicon": "https://www.nature.com/static/images/favicons/nature/apple-touch-icon-f39cb19454.png",
  "text": "Data availability Neural data and brain-to-voice models related to this study are publicly available on Dryad (https://doi.org/10.5061/dryad.2280gb64f)45. Code availability Code to implement brain-to-voice synthesis described in this study is publicly available on GitHub (https://github.com/Neuroprosthetics-Lab/brain-to-voice-2025). ReferencesCard, N. S. et al. An accurate and rapidly calibrating speech neuroprosthesis. N. Engl. J. Med. 391, 609–618 (2024).Article  PubMed  PubMed Central  Google Scholar  Willett, F. R. et al. A high-performance speech neuroprosthesis. Nature 620, 1031–1036 (2023).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Metzger, S. L. et al. A high-performance neuroprosthesis for speech decoding and avatar control. Nature 620, 1037–1046 (2023).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Silva, A. B., Littlejohn, K. T., Liu, J. R., Moses, D. A. \u0026 Chang, E. F. The speech neuroprosthesis. Nat. Rev. Neurosci. 25, 473–492 (2024).Article  CAS  PubMed  PubMed Central  Google Scholar  Herff, C. et al. Generating natural, intelligible speech from brain activity in motor, premotor, and inferior frontal cortices. Front. Neurosci. 13, 1267 (2019).Article  PubMed  PubMed Central  Google Scholar  Angrick, M. et al. Speech synthesis from ECoG using densely connected 3D convolutional neural networks. J. Neural Eng. 16, 036019 (2019).Article  ADS  PubMed  PubMed Central  Google Scholar  Anumanchipalli, G. K., Chartier, J. \u0026 Chang, E. F. Speech synthesis from neural decoding of spoken sentences. Nature 568, 493–498 (2019).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Meng, K. et al. Continuous synthesis of artificial speech sounds from human cortical surface recordings during silent speech production. J. Neural Eng. 20, 046019 (2023).Article  ADS  Google Scholar  Le Godais, G. et al. Overt speech decoding from cortical activity: a comparison of different linear methods. Front. Hum. Neurosci. 17, 1124065 (2023).Article  PubMed  PubMed Central  Google Scholar  Liu, Y. et al. Decoding and synthesizing tonal language speech from brain activity. Sci. Adv. 9, eadh0478 (2023).Article  ADS  PubMed  PubMed Central  Google Scholar  Berezutskaya, J. et al. Direct speech reconstruction from sensorimotor brain activity with optimized deep learning models. J. Neural Eng. 20, 056010 (2023).Article  ADS  PubMed Central  Google Scholar  Shigemi, K. et al. Synthesizing speech from ECoG with a combination of transformer-based encoder and neural vocoder. In ICASSP 2023 – 2023 IEEE Int. Conf. Acoust. Speech Signal Process. 1–5 (IEEE, 2023).Chen, X. et al. A neural speech decoding framework leveraging deep learning and speech synthesis. Nat. Mach. Intell. 6, 467–480 (2024).Article  Google Scholar  Wilson, G. H. et al. Decoding spoken English from intracortical electrode arrays in dorsal precentral gyrus. J. Neural Eng. 17, 066007 (2020).Article  ADS  PubMed  PubMed Central  Google Scholar  Wairagkar, M., Hochberg, L. R., Brandman, D. M. \u0026 Stavisky, S. D. Synthesizing speech by decoding intracortical neural activity from dorsal motor cortex. In 2023 11th Int. IEEE/EMBS Conf. on Neural Eng. (NER) 1–4 (IEEE, 2023).Angrick, M. et al. Real-time synthesis of imagined speech processes from minimally invasive recordings of neural activity. Commun. Biol. 4, 1055 (2021).Article  PubMed  PubMed Central  Google Scholar  Wu, X., Wellington, S., Fu, Z. \u0026 Zhang, D. Speech decoding from stereo-electroencephalography (sEEG) signals using advanced deep learning methods. J. Neural Eng. 21, 036055 (2024).Article  Google Scholar  Angrick, M. et al. Online speech synthesis using a chronically implanted brain–computer interface in an individual with ALS. Sci. Rep. 14, 9617 (2024).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Glasser, M. F. et al. A multi-modal parcellation of human cerebral cortex. Nature 536, 171–178 (2016).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing Systems 30 (NIPS, 2017).Downey, J. E., Schwed, N., Chase, S. M., Schwartz, A. B. \u0026 Collinger, J. L. Intracortical recording stability in human brain–computer interface users. J. Neural Eng. 15, 046016 (2018).Article  ADS  PubMed  Google Scholar  Valin, J.-M. \u0026 Skoglund, J. LPCNET: improving neural speech synthesis through linear prediction. In ICASSP 2019 – 2019 IEEE Int. Conf. on Acoust. Speech Signal Process. 5891–5895 (IEEE, 2019).Li, Y. A., Han, C., Raghavan, V. S., Mischler, G. \u0026 Mesgarani, N. StyleTTS 2: towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. Adv. Neural Inf. Process. Syst. 36, 19594–19621 (2023).Dichter, B. K., Breshears, J. D., Leonard, M. K. \u0026 Chang, E. F. The control of vocal pitch in human laryngeal motor cortex. Cell 174, 21–31 (2018).Article  CAS  PubMed  PubMed Central  Google Scholar  Kaufman, M. T., Churchland, M. M., Ryu, S. I. \u0026 Shenoy, K. V. Cortical activity in the null space: permitting preparation without movement. Nat. Neurosci. 17, 440–448 (2014).Article  CAS  PubMed  PubMed Central  Google Scholar  Stavisky, S. D., Kao, J. C., Ryu, S. I. \u0026 Shenoy, K. V. Motor cortical visuomotor feedback activity is initially isolated from downstream targets in output-null neural state space dimensions. Neuron 95, 195–208 (2017).Article  CAS  PubMed  PubMed Central  Google Scholar  Churchland, M. M. \u0026 Shenoy, K. V. Preparatory activity and the expansive null-space. Nat. Rev. Neurosci. 25, 213–236 (2024).Article  CAS  PubMed  Google Scholar  Moses, D. A. et al. Neuroprosthesis for decoding speech in a paralyzed person with anarthria. N. Engl. J. Med. 385, 217–227 (2021).Article  PubMed  PubMed Central  Google Scholar  Kunz, E. M. et al. Representation of verbal thought in motor cortex and implications for speech neuroprostheses. Preprint at bioRxiv https://doi.org/10.1101/2024.10.04.616375 (2024).Bouchard, K. E., Mesgarani, N., Johnson, K. \u0026 Chang, E. F. Functional organization of human sensorimotor cortex for speech articulation. Nature 495, 327–332 (2013).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Chartier, J., Anumanchipalli, G. K., Johnson, K. \u0026 Chang, E. F. Encoding of articulatory kinematic trajectories in human speech sensorimotor cortex. Neuron 98, 1042–1054 (2018).Article  CAS  PubMed  PubMed Central  Google Scholar  Lu, J. et al. Neural control of lexical tone production in human laryngeal motor cortex. Nat. Commun. 14, 6917 (2023).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Breshears, J. D., Molinaro, A. M. \u0026 Chang, E. F. A probabilistic map of the human ventral sensorimotor cortex using electrical stimulation. J. Neurosurg. 123, 340–349 (2015).Article  PubMed  Google Scholar  Ammanuel, S. G. et al. Intraoperative cortical stimulation mapping with laryngeal electromyography for the localization of human laryngeal motor cortex. J. Neurosurg. 141, 268–277 (2024).Article  PubMed  Google Scholar  Pandarinath, C. et al. Neural population dynamics in human motor cortex during movements in people with ALS. eLife 4, e07436 (2015).Article  PubMed  PubMed Central  Google Scholar  Stavisky, S. D. et al. Neural ensemble dynamics in dorsal motor cortex during speech in people with paralysis. eLife 8, e46015 (2019).Article  PubMed  PubMed Central  Google Scholar  Willett, F. R. et al. Hand knob area of premotor cortex represents the whole body in a compositional way. Cell 181, 396–409 (2020).Article  CAS  PubMed  PubMed Central  Google Scholar  Ali, Y. H. et al. BRAND: a platform for closed-loop experiments with deep network models. J. Neural Eng. 21, 026046 (2024).Article  ADS  PubMed Central  Google Scholar  Young, D. et al. Signal processing methods for reducing artifacts in microelectrode brain recordings caused by functional electrical stimulation. J. Neural Eng. 15, 026014 (2018).Article  ADS  CAS  PubMed  PubMed Central  Google Scholar  Levelt, W. J., Roelofs, A. \u0026 Meyer, A. S. A theory of lexical access in speech production. Behav. Brain Sci. 22, 1–38 (1999).Article  CAS  PubMed  Google Scholar  Räsänen, O., Doyle, G. \u0026 Frank, M. C. Unsupervised word discovery from speech using automatic segmentation into syllable-like units. Proc. Interspeech 2015, 3204–3208 (2015). Google Scholar  Williams, A. H. et al. Discovering precise temporal patterns in large-scale neural recordings through robust and interpretable time warping. Neuron 105, 246–259 (2020).Article  CAS  PubMed  Google Scholar  Roussel, P. et al. Observation and assessment of acoustic contamination of electrophysiological brain signals during speech production and sound perception. J. Neural Eng. 17, 056028 (2020).Article  ADS  PubMed  Google Scholar  Shah, N., Sahipjohn, N., Tambrahalli, V., Subramanian, R. \u0026 Gandhi, V. StethoSpeech: speech generation through a clinical stethoscope attached to the skin. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 123 (2024).Article  Google Scholar  Wairagkar, M. et al. Data for an instantaneous voice synthesis neuroprosthesis. Dryad https://doi.org/10.5061/dryad.2280gb64f (2025).Download referencesAcknowledgementsWe thank participant T15 and his family and care partners for their contributions to this research. Support was provided by the Office of the Assistant Secretary of Defense for Health Affairs through the Amyotrophic Lateral Sclerosis Research Program under award number AL220043; a New Innovator Award (DP2) from the NIH Office of the Director and managed by NIDCD (1DP2DC021055); a Seed Grant from the ALS Association (23-SGP-652); A. P. Giannini Postdoctoral Fellowship (N.S.C.); Searle Scholars Program; a Pilot Award from the Simons Collaboration for the Global Brain (AN-NC-GB-Pilot Extension-00002343-01); NIH-NIDCD (U01DC017844) and VA RR\u0026D (A2295-R). S.D.S. holds a Career Award at the Scientific Interface from the Burroughs Wellcome Fund, and a Cultivating Team Science Award from the University of California Davis School of Medicine.Author informationAuthor notesThese authors jointly supervised this work: David M. Brandman, Sergey D. StaviskyAuthors and AffiliationsDepartment of Neurological Surgery, University of California, Davis, Davis, CA, USAMaitreyee Wairagkar, Nicholas S. Card, Tyler Singer-Clark, Xianda Hou, Carrina Iacobacci, David M. Brandman \u0026 Sergey D. StaviskyDepartment of Biomedical Engineering, University of California, Davis, Davis, CA, USATyler Singer-ClarkDepartment of Computer Science, University of California, Davis, Davis, CA, USAXianda HouCenter for Mind and Brain, University of California, Davis, Davis, CA, USALee M. MillerDepartment of Neurobiology, Physiology and Behavior, University of California, Davis, Davis, CA, USALee M. MillerDepartment of Otolaryngology, Head and Neck Surgery, University of California, Davis, Davis, CA, USALee M. MillerSchool of Engineering and Carney Institute for Brain Sciences, Brown University, Providence, RI, USALeigh R. HochbergVA Center for Neurorestoration and Neurotechnology, VA Providence Healthcare, Providence, RI, USALeigh R. HochbergCenter for Neurotechnology and Neurorecovery, Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USALeigh R. HochbergAuthorsMaitreyee WairagkarYou can also search for this author inPubMed Google ScholarNicholas S. CardYou can also search for this author inPubMed Google ScholarTyler Singer-ClarkYou can also search for this author inPubMed Google ScholarXianda HouYou can also search for this author inPubMed Google ScholarCarrina IacobacciYou can also search for this author inPubMed Google ScholarLee M. MillerYou can also search for this author inPubMed Google ScholarLeigh R. HochbergYou can also search for this author inPubMed Google ScholarDavid M. BrandmanYou can also search for this author inPubMed Google ScholarSergey D. StaviskyYou can also search for this author inPubMed Google ScholarContributionsM.W., S.D.S. and D.M.B. conceived the study and experiment design. M.W. led the experiments and developed and implemented the target speech generation, decoder training algorithms and end-to-end pipeline for instantaneous voice synthesis: feature extraction, noise removal, preprocessing, real-time brain-to-voice decoders, pitch decoders, vocoder and output audio playback, experimental tasks, post-processing. M.W. also performed human listener evaluations, analysed all of the data and created figures. M.W. and N.S.C. developed and implemented the real-time neural signal processing, noise removal and feature-extraction pipelines. M.W., N.S.C., T.S.-C. and X.H. coded the real-time data-collection system and built the neuroprosthetic cart system. N.S.C. generated cloned voice samples for T15. M.W., N.S.C. and C.I. collected the primary data for this study. N.S.C. and C.I. interfaced with the participant and scheduled research sessions. L.M.M. contributed to the human listener evaluations. D.M.B. led planning and performed the surgical-implant-placement procedure. L.R.H. was the sponsor–investigator of the multisite clinical trial. D.M.B. was responsible for all clinical-trial-related activities at University of California Davis. S.D.S. and D.M.B. supervised all aspects of the project. M.W and S.D.S. wrote the paper. All authors reviewed and edited the paper.Corresponding authorsCorrespondence to Maitreyee Wairagkar, David M. Brandman or Sergey D. Stavisky.Ethics declarations Competing interests S.D.S. is an inventor on intellectual property related to speech decoding submitted and owned by Stanford University (US patent no. 12008987) that has been licensed to Blackrock Neurotech and Neuralink. M.W., S.D.S. and D.M.B. have patent applications related to speech BCI submitted and owned by the Regents of the University of California (US patent application no. 63/461,507 and 63/450,317), including intellectual property licensed by Paradromics. D.M.B. was a surgical consultant with Paradromics, completing his consultation during the revision period of the paper. He is a consultant for Globus Medical. S.D.S. is a scientific adviser to Sonera. The MGH Translational Research Center has a clinical research support agreement with Ability Neuro, Axoft, Neuralink, Neurobionics, Paradromics, Precision Neuro, Synchron and Reach Neuro, for which L.R.H. provides consultative input. Mass General Brigham is convening the Implantable Brain-Computer Interface Collaborative Community (iBCI-CC); charitable gift agreements to Mass General Brigham, including those received to date from Paradromics, Synchron, Precision Neuro, Neuralink and Blackrock Neurotech, support the iBCI-CC, for which L.R.H. provides effort. The other authors declare no competing interests. Peer review Peer review information Nature thanks Nai Ding, Nick Ramsey and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available. Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Extended data figures and tablesExtended Data Fig. 1 Microelectrode array placement and brain-to-voice synthesis latencies.a. The estimated resting state language network from Human Connectome Project data overlaid on T15’s brain anatomy. b. Intraoperative photograph showing the four microelectrode arrays placed on T15’s precentral gyrus. Images in a and b are adapted from ref. 1 (Copyright © 2024 Massachusetts Medical Society, reprinted with permission from Massachusetts Medical Society). c. Closed-loop cumulative latencies across different stages in the voice synthesis and audio playback pipeline are shown. Voice samples were synthesized from raw neural activity measurements within 10 ms and the resulting audio was played out loud continuously to provide closed-loop feedback. Note the linear horizontal axis is split to expand the visual dynamic range. We focused our engineering primarily on reducing the brain-to-voice inference latency, which fundamentally bounds the speech synthesis latency. As a result, the largest remaining contribution to the latency occurred after voice synthesis decoding during the (comparably more mundane) step of audio playback through a sound driver. The cumulative latencies with the audio driver settings used for T15 closed-loop synthesis in earlier experiments are shown in dark grey. Audio playback latencies were subsequently substantially lowered through software optimizations (light grey) in latter sessions and we predict that further reductions will be possible with additional computer engineering.Extended Data Fig. 2 Additional BCI speech synthesis performance metrics.a. Mel-cepstral distortion (MCD) is computed across 25 Mel-frequency bands between the closed-loop synthesized speech and the target speech after removing silences between words. The four subpanels show MCDs (mean ± s.d) between the synthesized and target speech for different speech tasks in evaluation research sessions. b. Performance of brain-to-voice decoder measured over time by evaluating neural trials from different sessions offline with a fixed decoder. Decoder trained on post-implant day 165 was fixed and used to synthesize voice offline from neural trials collected in sessions over the next month. Performance was measured by computing Pearson correlation coefficient between the target speech and the synthesized speech across 40 Mel-frequencies after removing silences between words (mean ± s.d., n = 956 sentences). A noticeable decline in brain-to-voice performance was observed after approximately 15 days.Extended Data Fig. 3 Electrodes show variability in speech tuning.a. Example closed-loop speech synthesis trial. Spike-band power and threshold crossing spikes from each electrode are shown for one example sentence. These neural features were binned and causally normalized and smoothed on a rolling basis before being decoded to synthesize speech. The mean spike-band power and threshold crossing activity for each individual array are also shown. Speech-related modulation was observed on all arrays, with the highest modulation recorded in v6v and 55b. The synthesized speech is shown in the bottom-most row. The grey trace above it shows the participant’s attempted (unintelligible) speech as recorded with a microphone. b, d. Pearson correlation coefficients of spike-band power and threshold crossings, respectively, between each electrode and the speech envelope (first LPCNet feature predicted by the brain-to-voice decoder). Electrodes are grouped by array and sorted in ascending order to show that different electrodes have different tuning (both positive and negative) with speech. Arrays v6v and 55b have higher correlations with speech and the majority of electrodes show positive tuning. Arrays M1 and d6v have lower correlations with more electrodes tuned negatively. Electrodes with non-significant correlation (p \u003e 0.05) are shown in grey. Insets show the same correlations for each electrode arranged spatially in the array. c, e. The time course of average spike-band power and threshold crossings across trials of two example electrodes with positive (solid line) and negative (dashed line) correlations from each array (example electrodes are marked by the ‘+’ symbol in (b, d)). Different electrodes show complex neural dynamics with respect to speech onset and a rich variety in speech tuning. For example, some electrodes have higher activity before speech onset (possibly contributing to speech preparation) while others have higher activity during or after speech onset.Extended Data Fig. 4 Speech is not synthesized during non-speech vocalizations or orofacial movements.a. Three example trials show microphone recording (grey) of T15 during attempted speech trials with coughing, throat clearing, non-speech vocalizations or people speaking in the background and the corresponding brain-to-voice synthesis output (blue). The brain-to-voice decoder did not synthesize audible speech and instead output silence during these non-speech vocalizations or when other people were speaking simultaneously (note that T15 starts speaking via the neuroprosthesis midway through the background conversation in example 3). In contrast, it did synthesize voice when T15 voluntarily attempted to speak. Speech was synthesized instantaneously at the exact pace with which T15 attempted to speak and with very low latency. b. Samples of non-speech vocalization events (grey) and the corresponding speech synthesis output, which was zero (silence) throughout all these events (blue). c. Examples of speech synthesis during attempted speech of each word. Here, the speech is synthesized (blue) appropriately as expected during attempted speech (grey).Extended Data Fig. 5 Neural activity is not contaminated by acoustic artifacts and residual vocalization and movement cannot synthesize intelligible speech.a. Three example trials’ audio recording, audio spectrogram, and the spectrograms of the two most acoustic-correlated neural electrodes. Examples are shown for the three types of speech tasks. The prominent spectral structures in the audio spectrogram cannot be observed even in the top two most correlated neural electrodes. An increase in neural activity can be observed before speech onset for each word, reflecting speech preparatory activity and further arguing against acoustic contamination. Note that in the word emphasis example, the last word ‘going’ is not vocalized fully (there is minimal activity in its audio spectrum), yet an increase in neural activity can be observed that is similar to other words. Contamination matrices and statistical criteria are shown in the bottom row, where P-value indicates whether the trial is significantly acoustically contaminated or not. b. An example trial of attempted speech with simultaneous recording of intracortical neural signals and various biosignals measured using a microphone, stethoscopic microphone and IMU sensors (accelerometer and gyroscope). Separate independent decoders were trained to synthesize speech using each of the biosignals (or all three together). c. Intelligible speech could not be synthesized from biosignals measuring sound, movement, and vibrations during attempted speech. (Left) Cross-validated Pearson correlation coefficients (mean ± s.d.) (compared to target speech) of speech synthesized using neural signals, each of the biosignals, and all biosignals together. Reconstruction accuracy is significantly lower for decoding speech from biosignals as compared to neural activity (two-sided Wilcoxon rank-sum, P = 10−59, n = 240 sentences). (Right) Distribution of Pearson correlation coefficients of speech decoding from biosignals and neural signals are mostly non-overlapping, indicating that synthesis quality from biosignals is much lower than that of neural signals. d. To assess the intelligibility of voice synthesis from neural activity and biosignals (stethoscopic mic decoder), naive human listeners performed open transcription of (the same) 30 synthesized trials using both the decoders. Median phoneme error rates and word error rates for neural decoding were significantly lower (43.60%) than decoding stethoscope recordings, which had word error rate of 100%. This indicates that intelligible speech cannot be decoded from these non-neural biosignals.Extended Data Fig. 6 Encoding of paralinguistic features in neural activity.a. Neural modulation during question intonation. Trial-averaged normalized spike-band power (each row in a group is one electrode) during trials where the participant modulated his intonation to say the cued sentence as a question. Trials with the same cue sentence (n = 16) were aligned using dynamic time warping and the mean activity across trials spoken as statements was subtracted to better show the increased neural activity around the intonation-modulation at the end of the sentence. The onset of the word that was pitch-modulated in closed-loop is indicated by the arrowhead at the bottom of each example. b. Paralinguistic features encoding recorded from individual arrays. Trial-averaged spike-band power (mean ± s.e.m.), averaged across all electrodes within each array, for words spoken as statements and as questions. At every time point, the spike-band power for statement words and question words were compared using the Wilcoxon rank-sum test. The blue line at the bottom indicates the time points where the spike-band power in statement words and question words were significantly different (P \u003c 0.001, n1 = 970 words, n2 = 184 words). c. Trial averaged spike-band power across each array for non-emphasized and emphasized words. The spike-band power was significantly different between non-emphasized words and emphasized words at time points shown in blue (P \u003c 0.001, n1 = 1269 words, n2 = 333 words). d. Trial-averaged spike-band power across each array for words without pitch modulation and words with pitch modulation (from the three-pitch melodies singing task). Words with low and high pitch targets are grouped together as the ‘pitch modulation’ category (we excluded medium pitch target words where the participant used his normal pitch). The spike-band power was significantly different between no pitch modulation and pitch modulation at time points shown in blue (P \u003c 0.001, n1 = 486 words, n2 = 916 words). e. Confusion matrix showing offline accuracies for decoding question intonation and word emphasis paralinguistic features together using a single combined 3-class classifier.Extended Data Fig. 7 Closed-loop paralinguistic features modulation.a. An overview of the paralinguistic feature decoder and pitch modulation pipeline. An independent paralinguistic feature decoder ran in parallel to the regular brain-to-voice decoder. Its output causally modulated the pitch feature predicted by brain-to-voice, resulting in a pitch-modulated voice. b. An example trial of closed-loop intonation modulation for speaking a sentence as a question. A separate binary decoder identified the change in intonation and sent a trigger (downward arrow) to modulate the pitch feature output of the regular brain-to-voice decoder according to a predefined pitch profile for asking a question (low pitch to high pitch). Neural activity of an example trial with its synthesized voice output is shown along with the intonation decoder output, time of modulation trigger (downward arrow), originally predicted pitch feature and the modulated pitch feature used for voice synthesis. c. An example trial of closed-loop word emphasis where the word “YOU” from “What are YOU doing” was emphasized. To emphasize a word, we applied a predefined pitch profile (high pitch to low pitch) along with a 20% increase in the loudness of the predicted speech samples. d. An example trial of closed-loop pitch modulation for singing a melody with three pitch levels. The three-pitch classifier output was used to continuously modulate the predicted pitch feature output from the brain-to-voice decoder.Extended Data Fig. 8 Pearson correlation coefficients over the course of a sentence.Pearson correlation coefficient (r) of individual words in sentences of different lengths (mean ± s.d.). The correlation between target and synthesized speech remained consistent throughout the length of sentence, indicating that the quality of synthesized voice was consistent throughout the sentence. Note that there were fewer longer evaluation sentences.Extended Data Fig. 9 Output-null and output-potent neural dynamics during speech production in individual arrays.a-d. Average approximated output-null (orange) and output-potent (blue) components of neural activity during attempted speech of cued sentences of different lengths. Here the neural components are computed for each array independently by training separate linear decoders (i.e., repeating the analyses of Fig. 4 for individual arrays independently). A subset of sentence lengths are shown in the interest of space. Note that the d6v array had much less speech-related modulation. Bar plots within each panel show a summary of all the data (including the not-shown sentence lengths) by taking the average null/potent activity ratios for words in the first-quarter, second-quarter, third-quarter, and fourth-quarter of each sentence (mean ± s.e.m., nQ1 = 3,600, nQ2 = 4,181, nQ3 = 3,456, nQ4 = 3,134 words). e-h. Average output-null and output-potent activity during intonation modulation (question-asking or word emphasis) computed separately for each array. Output-null activity shows an increase during intonation modulated word in all arrays. Null/potent activity ratios are summarized in bar plots of intonation-modulated word (red) and the words preceding or following it (grey) (mean ± s.e.m.). The null/potent ratios of modulated words were significantly different from that of non-modulated words for the v6v, M1 and d6v arrays (two-sided Wilcoxon rank-sum, v6v: p = 10−11, M1: p = 10−16, 55b: p = 0.3, d6v: p = 10−26, n1 = 460 modulated words, n2 = 922 non-modulated words).Extended Data Fig. 10 Head motion during speech and its relationship with the neural dynamics.a. Head motion was tracked from videos by mapping the x and y positions of the NeuroPort pedestal in each frame (yellow points) using OpenCV. Overall head motion was summarized by the first principal component (pink axis) of x-y motion. The inset shows a single frame of head motion tracking. b. Small head motion was observed during uttering each word. Head motion remained consistent throughout the attempted speech sentence as measured by the motion from baseline during utterance of words in each of the four word-quartiles, regardless of the length of the sentence. c. The ratio of output-null and output-potent components of simultaneously recorded neural activity decayed over the course of the sentence, in contrast to the head motion in (b). d. Time course and amplitude of the output-null and output-potent components of the neural activity and simultaneous head motion in different quartiles of the sentence. The head motion (purple) follows the output-null activity (orange) but precedes the output-potent activity (blue). The output-null activity decayed over the course of the sentence, whereas the head motion during each word in a sentence remained constant. Taken together, this shows that the neural dynamics do not closely match the head motion time course.Supplementary informationSupplementary Table 1Data used for training the brain-to-voice decoders in evaluation sessions.Reporting SummaryPeer Review FileSupplementary Video 1Dysarthric speech of the participant. This video shows the participant, who has severe dysarthria due to ALS, attempting to speak the sentences cued on the screen. The speech of the participant is unintelligible to naive listeners. Taken on day 25 after implant.Supplementary Video 2Closed-loop voice synthesis during attempted vocalized speech. This video shows 13 consecutive closed-loop trials of instantaneous voice synthesis as the participant attempts to speak cued sentences. The synthesized voice was played back continuously in real time through a speaker. Taken on day 179 after implant.Supplementary Video 3Closed-loop voice synthesis with simultaneous brain-to-text decoding. This video shows 15 consecutive closed-loop trials of instantaneous voice synthesis with simultaneous brain-to-text decoding that acted as closed captioning when the participant attempted to speak cued sentences. Taken on day 110 after implant.Supplementary Video 4Closed-loop voice synthesis during attempted mimed speech. This video shows ten consecutive closed-loop trials of instantaneous voice synthesis with audio feedback as the participant mimed the cued sentences without vocalizing. The decoder was not trained on any mimed-speech neural data. Taken on day 195 after implant.Supplementary Video 5Closed-loop voice synthesis during self-initiated free responses. This video shows nine closed-loop trials of instantaneous voice synthesis with audio feedback as the participant responds to open-ended questions or is asked to say whatever he wanted. We used this opportunity to ask the participant for his feedback on this brain-to-voice neuroprosthesis. A brain-to-text decoder was used simultaneously to help with understanding what the participant was saying. Taken on days 172, 179, 186, 188, 193 and 195 after implant.Supplementary Video 6Closed-loop own-voice synthesis during attempted speech. This video shows nine consecutive closed-loop trials of instantaneous speech synthesis in a voice that sounds like the voice of the participant before ALS as the participant attempts to speak cued sentences. Taken on day 286 after implant.Supplementary Video 7Closed-loop voice synthesis of pseudo-words. This video shows five consecutive trials of closed-loop synthesis of made-up pseudo-words using the brain-to-voice decoder. The decoder was not trained on any pseudo-words. Taken on day 179 after implant.Supplementary Video 8Closed-loop voice synthesis of interjections. This video shows five trials of closed-loop synthesis of interjections using the brain-to-voice decoder. The decoder was not trained on these words. Taken on day 186 after implant.Supplementary Video 9Closed-loop voice synthesis for spelling words. This video shows seven trials of closed-loop synthesis in which the participant was spelling cued words one letter at a time using the brain-to-voice decoder. The decoder was not trained on this task. Taken on day 186 after implant.Supplementary Video 10Closed-loop question intonation. This video shows ten selected trials in which the participant modulated his intonation to say a sentence as a question (indicated by ‘?’ in the cue) or as a statement by using an intonation decoder that modulated the brain-to-voice synthesis in a closed loop. Taken on day 286 after implant.Supplementary Video 11Closed-loop word emphasis. This video shows eight selected trials in which certain (capitalized) words in the cued sentences were emphasized by the participant by using an emphasis decoder that modulated the brain-to-voice synthesis in a closed loop. Taken on day 286 after implant.Supplementary Video 12Singing three-pitch melodies in a closed loop. This video shows three consecutive trials in which the participant sung short melodies with three pitch targets by using a pitch decoder that modulated the brain-to-voice synthesis. At the start of each trial, an audio cue plays the target melody. The on-screen targets then turn from red to green to indicate that the participant should begin. The vertical bar on the left shows the instantaneous decoded pitch (low, mid and high). Interactive visual cues for each pitch target are shown on the screen. Visual feedback cues show the note in the melody that the participant is singing. Taken on day 342 after implant.Supplementary Video 13Singing three-pitch melodies using a unified brain-to-voice decoder. This video shows three trials in which the participant sung short melodies with three pitch targets by using a single unified brain-to-voice decoder that inherently synthesizes intended pitch in a closed loop. At the start of each trial, an audio cue plays the target melody. The vertical bar on the left shows the instantaneous decoded pitch (low, mid and high) for visual feedback only (that is, this separately decoded pitch, which is the same as in Supplementary Video 12, is not used in the unified brain-to-voice model). Interactive cues show the note in the melody that the participant is singing, providing visual feedback. Taken on day 342 after implant.Supplementary Video 14Closed-loop voice synthesis in session 1. This video shows three closed-loop trials of instantaneous voice synthesis from the first day of neural recording (day 25 after implant). The brain-to-voice decoder was trained during this session using 190 sentence trials with a limited 50-word vocabulary recorded earlier on the same day. The second part of the video shows the same three trials reconstructed offline using an optimized brain-to-voice decoder (that is, the algorithm used throughout the rest of this paper), which improved intelligibility.Supplementary Video 15Intelligible voice cannot be decoded from biosignal recordings of residual speech of T15. This video shows examples of speech synthesized from simultaneous recordings from microphone, stethoscopic microphone, IMU sensor and intracortical neural activity as T15 attempts to speak. Speech synthesized from microphone, stethoscope and IMU biosignals was not intelligible (word error rate for stethoscope decoding: 100%), whereas the voice synthesized from neural activity was more intelligible (word error rate: 43.60%). From day 482 after implant.Supplementary Video 16Comparison of the voice of T15 before ALS with the own-voice synthesis by the brain-to-voice BCI. This video shows examples of (1) the voice of T15 before ALS; (2) the voice cloned by the StyleTTS 2 model, which was trained using T15’s voice before ALS; (3) target audio generated using this cloned-voice, time-aligned with neural signals during attempted speech used as training data for the personalized own-voice BCI speech-synthesis decoder; and (4) the own voice synthesized by the personalized brain-to-voice decoder in a closed loop from neural activity.Supplementary Audio 1Acausal speech synthesis by predicting discrete speech units. Audio recording of three example trials of speech reconstructed offline using the approach of predicting discrete speech units acausally at the end of the sentence using connectionist temporal classification (CTC) loss. From day 25 after implant.Rights and permissionsSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.Reprints and permissionsAbout this articleCite this articleWairagkar, M., Card, N.S., Singer-Clark, T. et al. An instantaneous voice-synthesis neuroprosthesis. Nature (2025). https://doi.org/10.1038/s41586-025-09127-3Download citationReceived: 19 September 2024Accepted: 08 May 2025Published: 12 June 2025DOI: https://doi.org/10.1038/s41586-025-09127-3",
  "image": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig1_HTML.png",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n                \u003cdiv id=\"data-availability-section\" data-title=\"Data availability\"\u003e\u003ch2 id=\"data-availability\"\u003eData availability\u003c/h2\u003e\u003cdiv id=\"data-availability-content\"\u003e\n            \n            \u003cp\u003eNeural data and brain-to-voice models related to this study are publicly available on Dryad (\u003ca href=\"https://doi.org/10.5061/dryad.2280gb64f\"\u003ehttps://doi.org/10.5061/dryad.2280gb64f\u003c/a\u003e)\u003csup\u003e\u003ca data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 45\" title=\"Wairagkar, M. et al. Data for an instantaneous voice synthesis neuroprosthesis. Dryad \n                https://doi.org/10.5061/dryad.2280gb64f\n                \n               (2025).\" href=\"https://www.nature.com/articles/s41586-025-09127-3#ref-CR45\" id=\"ref-link-section-d54370174e2057\"\u003e45\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n          \u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"code-availability-section\" data-title=\"Code availability\"\u003e\u003ch2 id=\"code-availability\"\u003eCode availability\u003c/h2\u003e\u003cdiv id=\"code-availability-content\"\u003e\n            \n            \u003cp\u003eCode to implement brain-to-voice synthesis described in this study is publicly available on GitHub (\u003ca href=\"https://github.com/Neuroprosthetics-Lab/brain-to-voice-2025\"\u003ehttps://github.com/Neuroprosthetics-Lab/brain-to-voice-2025\u003c/a\u003e).\u003c/p\u003e\n          \u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"MagazineFulltextArticleBodySuffix\" aria-labelledby=\"Bib1\" data-title=\"References\"\u003e\u003ch2 id=\"Bib1\"\u003eReferences\u003c/h2\u003e\u003cdiv data-container-section=\"references\" id=\"Bib1-content\"\u003e\u003col data-track-component=\"outbound reference\" data-track-context=\"references section\"\u003e\u003cli data-counter=\"1.\"\u003e\u003cp id=\"ref-CR1\"\u003eCard, N. S. et al. An accurate and rapidly calibrating speech neuroprosthesis. \u003ci\u003eN. Engl. J. Med.\u003c/i\u003e \u003cb\u003e391\u003c/b\u003e, 609–618 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1056/NEJMoa2314132\" data-track-item_id=\"10.1056/NEJMoa2314132\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1056%2FNEJMoa2314132\" aria-label=\"Article reference 1\" data-doi=\"10.1056/NEJMoa2314132\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=39141853\" aria-label=\"PubMed reference 1\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11328962\" aria-label=\"PubMed Central reference 1\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 1\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=An%20accurate%20and%20rapidly%20calibrating%20speech%20neuroprosthesis\u0026amp;journal=N.%20Engl.%20J.%20Med.\u0026amp;doi=10.1056%2FNEJMoa2314132\u0026amp;volume=391\u0026amp;pages=609-618\u0026amp;publication_year=2024\u0026amp;author=Card%2CNS\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"2.\"\u003e\u003cp id=\"ref-CR2\"\u003eWillett, F. R. et al. A high-performance speech neuroprosthesis. \u003ci\u003eNature\u003c/i\u003e \u003cb\u003e620\u003c/b\u003e, 1031–1036 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41586-023-06377-x\" data-track-item_id=\"10.1038/s41586-023-06377-x\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41586-023-06377-x\" aria-label=\"Article reference 2\" data-doi=\"10.1038/s41586-023-06377-x\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2023Natur.620.1031W\" aria-label=\"ADS reference 2\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXhslGrs7zK\" aria-label=\"CAS reference 2\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=37612500\" aria-label=\"PubMed reference 2\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10468393\" aria-label=\"PubMed Central reference 2\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 2\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=A%20high-performance%20speech%20neuroprosthesis\u0026amp;journal=Nature\u0026amp;doi=10.1038%2Fs41586-023-06377-x\u0026amp;volume=620\u0026amp;pages=1031-1036\u0026amp;publication_year=2023\u0026amp;author=Willett%2CFR\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"3.\"\u003e\u003cp id=\"ref-CR3\"\u003eMetzger, S. L. et al. A high-performance neuroprosthesis for speech decoding and avatar control. \u003ci\u003eNature\u003c/i\u003e \u003cb\u003e620\u003c/b\u003e, 1037–1046 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41586-023-06443-4\" data-track-item_id=\"10.1038/s41586-023-06443-4\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41586-023-06443-4\" aria-label=\"Article reference 3\" data-doi=\"10.1038/s41586-023-06443-4\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2023Natur.620.1037M\" aria-label=\"ADS reference 3\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXhslGrs77J\" aria-label=\"CAS reference 3\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=37612505\" aria-label=\"PubMed reference 3\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10826467\" aria-label=\"PubMed Central reference 3\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 3\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=A%20high-performance%20neuroprosthesis%20for%20speech%20decoding%20and%20avatar%20control\u0026amp;journal=Nature\u0026amp;doi=10.1038%2Fs41586-023-06443-4\u0026amp;volume=620\u0026amp;pages=1037-1046\u0026amp;publication_year=2023\u0026amp;author=Metzger%2CSL\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"4.\"\u003e\u003cp id=\"ref-CR4\"\u003eSilva, A. B., Littlejohn, K. T., Liu, J. R., Moses, D. A. \u0026amp; Chang, E. F. The speech neuroprosthesis. \u003ci\u003eNat. Rev. Neurosci.\u003c/i\u003e \u003cb\u003e25\u003c/b\u003e, 473–492 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41583-024-00819-9\" data-track-item_id=\"10.1038/s41583-024-00819-9\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41583-024-00819-9\" aria-label=\"Article reference 4\" data-doi=\"10.1038/s41583-024-00819-9\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2cXhtVyht7zE\" aria-label=\"CAS reference 4\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=38745103\" aria-label=\"PubMed reference 4\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11540306\" aria-label=\"PubMed Central reference 4\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 4\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=The%20speech%20neuroprosthesis\u0026amp;journal=Nat.%20Rev.%20Neurosci.\u0026amp;doi=10.1038%2Fs41583-024-00819-9\u0026amp;volume=25\u0026amp;pages=473-492\u0026amp;publication_year=2024\u0026amp;author=Silva%2CAB\u0026amp;author=Littlejohn%2CKT\u0026amp;author=Liu%2CJR\u0026amp;author=Moses%2CDA\u0026amp;author=Chang%2CEF\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"5.\"\u003e\u003cp id=\"ref-CR5\"\u003eHerff, C. et al. Generating natural, intelligible speech from brain activity in motor, premotor, and inferior frontal cortices. \u003ci\u003eFront. Neurosci.\u003c/i\u003e \u003cb\u003e13\u003c/b\u003e, 1267 (2019).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.3389/fnins.2019.01267\" data-track-item_id=\"10.3389/fnins.2019.01267\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.3389%2Ffnins.2019.01267\" aria-label=\"Article reference 5\" data-doi=\"10.3389/fnins.2019.01267\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=31824257\" aria-label=\"PubMed reference 5\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6882773\" aria-label=\"PubMed Central reference 5\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 5\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Generating%20natural%2C%20intelligible%20speech%20from%20brain%20activity%20in%20motor%2C%20premotor%2C%20and%20inferior%20frontal%20cortices\u0026amp;journal=Front.%20Neurosci.\u0026amp;doi=10.3389%2Ffnins.2019.01267\u0026amp;volume=13\u0026amp;publication_year=2019\u0026amp;author=Herff%2CC\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"6.\"\u003e\u003cp id=\"ref-CR6\"\u003eAngrick, M. et al. Speech synthesis from ECoG using densely connected 3D convolutional neural networks. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e16\u003c/b\u003e, 036019 (2019).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/ab0c59\" data-track-item_id=\"10.1088/1741-2552/ab0c59\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Fab0c59\" aria-label=\"Article reference 6\" data-doi=\"10.1088/1741-2552/ab0c59\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2019JNEng..16c6019A\" aria-label=\"ADS reference 6\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=30831567\" aria-label=\"PubMed reference 6\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822609\" aria-label=\"PubMed Central reference 6\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 6\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Speech%20synthesis%20from%20ECoG%20using%20densely%20connected%203D%20convolutional%20neural%20networks\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Fab0c59\u0026amp;volume=16\u0026amp;publication_year=2019\u0026amp;author=Angrick%2CM\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"7.\"\u003e\u003cp id=\"ref-CR7\"\u003eAnumanchipalli, G. K., Chartier, J. \u0026amp; Chang, E. F. Speech synthesis from neural decoding of spoken sentences. \u003ci\u003eNature\u003c/i\u003e \u003cb\u003e568\u003c/b\u003e, 493–498 (2019).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41586-019-1119-1\" data-track-item_id=\"10.1038/s41586-019-1119-1\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41586-019-1119-1\" aria-label=\"Article reference 7\" data-doi=\"10.1038/s41586-019-1119-1\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2019Natur.568..493A\" aria-label=\"ADS reference 7\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXovFejsbk%3D\" aria-label=\"CAS reference 7\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=31019317\" aria-label=\"PubMed reference 7\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9714519\" aria-label=\"PubMed Central reference 7\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 7\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Speech%20synthesis%20from%20neural%20decoding%20of%20spoken%20sentences\u0026amp;journal=Nature\u0026amp;doi=10.1038%2Fs41586-019-1119-1\u0026amp;volume=568\u0026amp;pages=493-498\u0026amp;publication_year=2019\u0026amp;author=Anumanchipalli%2CGK\u0026amp;author=Chartier%2CJ\u0026amp;author=Chang%2CEF\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"8.\"\u003e\u003cp id=\"ref-CR8\"\u003eMeng, K. et al. Continuous synthesis of artificial speech sounds from human cortical surface recordings during silent speech production. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e20\u003c/b\u003e, 046019 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/ace7f6\" data-track-item_id=\"10.1088/1741-2552/ace7f6\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Face7f6\" aria-label=\"Article reference 8\" data-doi=\"10.1088/1741-2552/ace7f6\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2023JNEng..20d6019M\" aria-label=\"ADS reference 8\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 8\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Continuous%20synthesis%20of%20artificial%20speech%20sounds%20from%20human%20cortical%20surface%20recordings%20during%20silent%20speech%20production\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Face7f6\u0026amp;volume=20\u0026amp;publication_year=2023\u0026amp;author=Meng%2CK\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"9.\"\u003e\u003cp id=\"ref-CR9\"\u003eLe Godais, G. et al. Overt speech decoding from cortical activity: a comparison of different linear methods. \u003ci\u003eFront. Hum. Neurosci.\u003c/i\u003e \u003cb\u003e17\u003c/b\u003e, 1124065 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.3389/fnhum.2023.1124065\" data-track-item_id=\"10.3389/fnhum.2023.1124065\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.3389%2Ffnhum.2023.1124065\" aria-label=\"Article reference 9\" data-doi=\"10.3389/fnhum.2023.1124065\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=37425292\" aria-label=\"PubMed reference 9\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10326283\" aria-label=\"PubMed Central reference 9\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 9\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Overt%20speech%20decoding%20from%20cortical%20activity%3A%20a%20comparison%20of%20different%20linear%20methods\u0026amp;journal=Front.%20Hum.%20Neurosci.\u0026amp;doi=10.3389%2Ffnhum.2023.1124065\u0026amp;volume=17\u0026amp;publication_year=2023\u0026amp;author=Le%20Godais%2CG\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"10.\"\u003e\u003cp id=\"ref-CR10\"\u003eLiu, Y. et al. Decoding and synthesizing tonal language speech from brain activity. \u003ci\u003eSci. Adv.\u003c/i\u003e \u003cb\u003e9\u003c/b\u003e, eadh0478 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1126/sciadv.adh0478\" data-track-item_id=\"10.1126/sciadv.adh0478\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1126%2Fsciadv.adh0478\" aria-label=\"Article reference 10\" data-doi=\"10.1126/sciadv.adh0478\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2023usnb.book.....L\" aria-label=\"ADS reference 10\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=37294753\" aria-label=\"PubMed reference 10\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10256166\" aria-label=\"PubMed Central reference 10\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 10\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Decoding%20and%20synthesizing%20tonal%20language%20speech%20from%20brain%20activity\u0026amp;journal=Sci.%20Adv.\u0026amp;doi=10.1126%2Fsciadv.adh0478\u0026amp;volume=9\u0026amp;publication_year=2023\u0026amp;author=Liu%2CY\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"11.\"\u003e\u003cp id=\"ref-CR11\"\u003eBerezutskaya, J. et al. Direct speech reconstruction from sensorimotor brain activity with optimized deep learning models. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e20\u003c/b\u003e, 056010 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/ace8be\" data-track-item_id=\"10.1088/1741-2552/ace8be\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Face8be\" aria-label=\"Article reference 11\" data-doi=\"10.1088/1741-2552/ace8be\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2023JNEng..20e6010B\" aria-label=\"ADS reference 11\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10510111\" aria-label=\"PubMed Central reference 11\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 11\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Direct%20speech%20reconstruction%20from%20sensorimotor%20brain%20activity%20with%20optimized%20deep%20learning%20models\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Face8be\u0026amp;volume=20\u0026amp;publication_year=2023\u0026amp;author=Berezutskaya%2CJ\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"12.\"\u003e\u003cp id=\"ref-CR12\"\u003eShigemi, K. et al. Synthesizing speech from ECoG with a combination of transformer-based encoder and neural vocoder. In \u003ci\u003eICASSP 2023 – 2023 IEEE Int. Conf. Acoust. Speech Signal Process.\u003c/i\u003e 1–5 (IEEE, 2023).\u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"13.\"\u003e\u003cp id=\"ref-CR13\"\u003eChen, X. et al. A neural speech decoding framework leveraging deep learning and speech synthesis. \u003ci\u003eNat. Mach. Intell.\u003c/i\u003e \u003cb\u003e6\u003c/b\u003e, 467–480 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s42256-024-00824-8\" data-track-item_id=\"10.1038/s42256-024-00824-8\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs42256-024-00824-8\" aria-label=\"Article reference 13\" data-doi=\"10.1038/s42256-024-00824-8\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 13\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=A%20neural%20speech%20decoding%20framework%20leveraging%20deep%20learning%20and%20speech%20synthesis\u0026amp;journal=Nat.%20Mach.%20Intell.\u0026amp;doi=10.1038%2Fs42256-024-00824-8\u0026amp;volume=6\u0026amp;pages=467-480\u0026amp;publication_year=2024\u0026amp;author=Chen%2CX\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"14.\"\u003e\u003cp id=\"ref-CR14\"\u003eWilson, G. H. et al. Decoding spoken English from intracortical electrode arrays in dorsal precentral gyrus. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e17\u003c/b\u003e, 066007 (2020).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/abbfef\" data-track-item_id=\"10.1088/1741-2552/abbfef\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Fabbfef\" aria-label=\"Article reference 14\" data-doi=\"10.1088/1741-2552/abbfef\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2020JNEng..17f6007W\" aria-label=\"ADS reference 14\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=33236720\" aria-label=\"PubMed reference 14\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8293867\" aria-label=\"PubMed Central reference 14\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 14\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Decoding%20spoken%20English%20from%20intracortical%20electrode%20arrays%20in%20dorsal%20precentral%20gyrus\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Fabbfef\u0026amp;volume=17\u0026amp;publication_year=2020\u0026amp;author=Wilson%2CGH\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"15.\"\u003e\u003cp id=\"ref-CR15\"\u003eWairagkar, M., Hochberg, L. R., Brandman, D. M. \u0026amp; Stavisky, S. D. Synthesizing speech by decoding intracortical neural activity from dorsal motor cortex. In \u003ci\u003e2023 11th Int. IEEE/EMBS Conf. on Neural Eng. (NER)\u003c/i\u003e 1–4 (IEEE, 2023).\u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"16.\"\u003e\u003cp id=\"ref-CR16\"\u003eAngrick, M. et al. Real-time synthesis of imagined speech processes from minimally invasive recordings of neural activity. \u003ci\u003eCommun. Biol.\u003c/i\u003e \u003cb\u003e4\u003c/b\u003e, 1055 (2021).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s42003-021-02578-0\" data-track-item_id=\"10.1038/s42003-021-02578-0\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs42003-021-02578-0\" aria-label=\"Article reference 16\" data-doi=\"10.1038/s42003-021-02578-0\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=34556793\" aria-label=\"PubMed reference 16\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8460739\" aria-label=\"PubMed Central reference 16\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 16\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Real-time%20synthesis%20of%20imagined%20speech%20processes%20from%20minimally%20invasive%20recordings%20of%20neural%20activity\u0026amp;journal=Commun.%20Biol.\u0026amp;doi=10.1038%2Fs42003-021-02578-0\u0026amp;volume=4\u0026amp;publication_year=2021\u0026amp;author=Angrick%2CM\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"17.\"\u003e\u003cp id=\"ref-CR17\"\u003eWu, X., Wellington, S., Fu, Z. \u0026amp; Zhang, D. Speech decoding from stereo-electroencephalography (sEEG) signals using advanced deep learning methods. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e21\u003c/b\u003e, 036055 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/ad593a\" data-track-item_id=\"10.1088/1741-2552/ad593a\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Fad593a\" aria-label=\"Article reference 17\" data-doi=\"10.1088/1741-2552/ad593a\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 17\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Speech%20decoding%20from%20stereo-electroencephalography%20%28sEEG%29%20signals%20using%20advanced%20deep%20learning%20methods\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Fad593a\u0026amp;volume=21\u0026amp;publication_year=2024\u0026amp;author=Wu%2CX\u0026amp;author=Wellington%2CS\u0026amp;author=Fu%2CZ\u0026amp;author=Zhang%2CD\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"18.\"\u003e\u003cp id=\"ref-CR18\"\u003eAngrick, M. et al. Online speech synthesis using a chronically implanted brain–computer interface in an individual with ALS. \u003ci\u003eSci. Rep.\u003c/i\u003e \u003cb\u003e14\u003c/b\u003e, 9617 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41598-024-60277-2\" data-track-item_id=\"10.1038/s41598-024-60277-2\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41598-024-60277-2\" aria-label=\"Article reference 18\" data-doi=\"10.1038/s41598-024-60277-2\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2024NatSR..14.9617A\" aria-label=\"ADS reference 18\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2cXpvFKhs70%3D\" aria-label=\"CAS reference 18\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=38671062\" aria-label=\"PubMed reference 18\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11053081\" aria-label=\"PubMed Central reference 18\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 18\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Online%20speech%20synthesis%20using%20a%20chronically%20implanted%20brain%E2%80%93computer%20interface%20in%20an%20individual%20with%20ALS\u0026amp;journal=Sci.%20Rep.\u0026amp;doi=10.1038%2Fs41598-024-60277-2\u0026amp;volume=14\u0026amp;publication_year=2024\u0026amp;author=Angrick%2CM\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"19.\"\u003e\u003cp id=\"ref-CR19\"\u003eGlasser, M. F. et al. A multi-modal parcellation of human cerebral cortex. \u003ci\u003eNature\u003c/i\u003e \u003cb\u003e536\u003c/b\u003e, 171–178 (2016).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/nature18933\" data-track-item_id=\"10.1038/nature18933\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fnature18933\" aria-label=\"Article reference 19\" data-doi=\"10.1038/nature18933\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2016Natur.536..171G\" aria-label=\"ADS reference 19\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28Xht1GrurnK\" aria-label=\"CAS reference 19\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=27437579\" aria-label=\"PubMed reference 19\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4990127\" aria-label=\"PubMed Central reference 19\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 19\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=A%20multi-modal%20parcellation%20of%20human%20cerebral%20cortex\u0026amp;journal=Nature\u0026amp;doi=10.1038%2Fnature18933\u0026amp;volume=536\u0026amp;pages=171-178\u0026amp;publication_year=2016\u0026amp;author=Glasser%2CMF\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"20.\"\u003e\u003cp id=\"ref-CR20\"\u003eVaswani, A. et al. Attention is all you need. In \u003ci\u003eAdvances in Neural Information Processing Systems\u003c/i\u003e \u003cb\u003e30\u003c/b\u003e (NIPS, 2017).\u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"21.\"\u003e\u003cp id=\"ref-CR21\"\u003eDowney, J. E., Schwed, N., Chase, S. M., Schwartz, A. B. \u0026amp; Collinger, J. L. Intracortical recording stability in human brain–computer interface users. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e15\u003c/b\u003e, 046016 (2018).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/aab7a0\" data-track-item_id=\"10.1088/1741-2552/aab7a0\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Faab7a0\" aria-label=\"Article reference 21\" data-doi=\"10.1088/1741-2552/aab7a0\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2018JNEng..15d6016D\" aria-label=\"ADS reference 21\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=29553484\" aria-label=\"PubMed reference 21\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 21\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Intracortical%20recording%20stability%20in%20human%20brain%E2%80%93computer%20interface%20users\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Faab7a0\u0026amp;volume=15\u0026amp;publication_year=2018\u0026amp;author=Downey%2CJE\u0026amp;author=Schwed%2CN\u0026amp;author=Chase%2CSM\u0026amp;author=Schwartz%2CAB\u0026amp;author=Collinger%2CJL\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"22.\"\u003e\u003cp id=\"ref-CR22\"\u003eValin, J.-M. \u0026amp; Skoglund, J. LPCNET: improving neural speech synthesis through linear prediction. In \u003ci\u003eICASSP 2019 – 2019 IEEE Int. Conf. on Acoust. Speech Signal Process.\u003c/i\u003e 5891–5895 (IEEE, 2019).\u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"23.\"\u003e\u003cp id=\"ref-CR23\"\u003eLi, Y. A., Han, C., Raghavan, V. S., Mischler, G. \u0026amp; Mesgarani, N. StyleTTS 2: towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. \u003ci\u003eAdv. Neural Inf. Process. Syst.\u003c/i\u003e \u003cb\u003e36\u003c/b\u003e, 19594–19621 (2023).\u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"24.\"\u003e\u003cp id=\"ref-CR24\"\u003eDichter, B. K., Breshears, J. D., Leonard, M. K. \u0026amp; Chang, E. F. The control of vocal pitch in human laryngeal motor cortex. \u003ci\u003eCell\u003c/i\u003e \u003cb\u003e174\u003c/b\u003e, 21–31 (2018).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.cell.2018.05.016\" data-track-item_id=\"10.1016/j.cell.2018.05.016\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.cell.2018.05.016\" aria-label=\"Article reference 24\" data-doi=\"10.1016/j.cell.2018.05.016\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXht1eitr3J\" aria-label=\"CAS reference 24\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=29958109\" aria-label=\"PubMed reference 24\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6084806\" aria-label=\"PubMed Central reference 24\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 24\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=The%20control%20of%20vocal%20pitch%20in%20human%20laryngeal%20motor%20cortex\u0026amp;journal=Cell\u0026amp;doi=10.1016%2Fj.cell.2018.05.016\u0026amp;volume=174\u0026amp;pages=21-31\u0026amp;publication_year=2018\u0026amp;author=Dichter%2CBK\u0026amp;author=Breshears%2CJD\u0026amp;author=Leonard%2CMK\u0026amp;author=Chang%2CEF\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"25.\"\u003e\u003cp id=\"ref-CR25\"\u003eKaufman, M. T., Churchland, M. M., Ryu, S. I. \u0026amp; Shenoy, K. V. Cortical activity in the null space: permitting preparation without movement. \u003ci\u003eNat. Neurosci.\u003c/i\u003e \u003cb\u003e17\u003c/b\u003e, 440–448 (2014).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/nn.3643\" data-track-item_id=\"10.1038/nn.3643\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fnn.3643\" aria-label=\"Article reference 25\" data-doi=\"10.1038/nn.3643\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXhs1Sgtrk%3D\" aria-label=\"CAS reference 25\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=24487233\" aria-label=\"PubMed reference 25\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3955357\" aria-label=\"PubMed Central reference 25\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 25\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Cortical%20activity%20in%20the%20null%20space%3A%20permitting%20preparation%20without%20movement\u0026amp;journal=Nat.%20Neurosci.\u0026amp;doi=10.1038%2Fnn.3643\u0026amp;volume=17\u0026amp;pages=440-448\u0026amp;publication_year=2014\u0026amp;author=Kaufman%2CMT\u0026amp;author=Churchland%2CMM\u0026amp;author=Ryu%2CSI\u0026amp;author=Shenoy%2CKV\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"26.\"\u003e\u003cp id=\"ref-CR26\"\u003eStavisky, S. D., Kao, J. C., Ryu, S. I. \u0026amp; Shenoy, K. V. Motor cortical visuomotor feedback activity is initially isolated from downstream targets in output-null neural state space dimensions. \u003ci\u003eNeuron\u003c/i\u003e \u003cb\u003e95\u003c/b\u003e, 195–208 (2017).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.neuron.2017.05.023\" data-track-item_id=\"10.1016/j.neuron.2017.05.023\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.neuron.2017.05.023\" aria-label=\"Article reference 26\" data-doi=\"10.1016/j.neuron.2017.05.023\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhtVantLfI\" aria-label=\"CAS reference 26\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=28625485\" aria-label=\"PubMed reference 26\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5547570\" aria-label=\"PubMed Central reference 26\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 26\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Motor%20cortical%20visuomotor%20feedback%20activity%20is%20initially%20isolated%20from%20downstream%20targets%20in%20output-null%20neural%20state%20space%20dimensions\u0026amp;journal=Neuron\u0026amp;doi=10.1016%2Fj.neuron.2017.05.023\u0026amp;volume=95\u0026amp;pages=195-208\u0026amp;publication_year=2017\u0026amp;author=Stavisky%2CSD\u0026amp;author=Kao%2CJC\u0026amp;author=Ryu%2CSI\u0026amp;author=Shenoy%2CKV\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"27.\"\u003e\u003cp id=\"ref-CR27\"\u003eChurchland, M. M. \u0026amp; Shenoy, K. V. Preparatory activity and the expansive null-space. \u003ci\u003eNat. Rev. Neurosci.\u003c/i\u003e \u003cb\u003e25\u003c/b\u003e, 213–236 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41583-024-00796-z\" data-track-item_id=\"10.1038/s41583-024-00796-z\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41583-024-00796-z\" aria-label=\"Article reference 27\" data-doi=\"10.1038/s41583-024-00796-z\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB2cXlt1OntL4%3D\" aria-label=\"CAS reference 27\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=38443626\" aria-label=\"PubMed reference 27\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 27\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Preparatory%20activity%20and%20the%20expansive%20null-space\u0026amp;journal=Nat.%20Rev.%20Neurosci.\u0026amp;doi=10.1038%2Fs41583-024-00796-z\u0026amp;volume=25\u0026amp;pages=213-236\u0026amp;publication_year=2024\u0026amp;author=Churchland%2CMM\u0026amp;author=Shenoy%2CKV\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"28.\"\u003e\u003cp id=\"ref-CR28\"\u003eMoses, D. A. et al. Neuroprosthesis for decoding speech in a paralyzed person with anarthria. \u003ci\u003eN. Engl. J. Med.\u003c/i\u003e \u003cb\u003e385\u003c/b\u003e, 217–227 (2021).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1056/NEJMoa2027540\" data-track-item_id=\"10.1056/NEJMoa2027540\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1056%2FNEJMoa2027540\" aria-label=\"Article reference 28\" data-doi=\"10.1056/NEJMoa2027540\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=34260835\" aria-label=\"PubMed reference 28\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8972947\" aria-label=\"PubMed Central reference 28\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 28\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Neuroprosthesis%20for%20decoding%20speech%20in%20a%20paralyzed%20person%20with%20anarthria\u0026amp;journal=N.%20Engl.%20J.%20Med.\u0026amp;doi=10.1056%2FNEJMoa2027540\u0026amp;volume=385\u0026amp;pages=217-227\u0026amp;publication_year=2021\u0026amp;author=Moses%2CDA\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"29.\"\u003e\u003cp id=\"ref-CR29\"\u003eKunz, E. M. et al. Representation of verbal thought in motor cortex and implications for speech neuroprostheses. Preprint at \u003ci\u003ebioRxiv\u003c/i\u003e \u003ca href=\"https://doi.org/10.1101/2024.10.04.616375\" data-track=\"click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"10.1101/2024.10.04.616375\"\u003ehttps://doi.org/10.1101/2024.10.04.616375\u003c/a\u003e (2024).\u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"30.\"\u003e\u003cp id=\"ref-CR30\"\u003eBouchard, K. E., Mesgarani, N., Johnson, K. \u0026amp; Chang, E. F. Functional organization of human sensorimotor cortex for speech articulation. \u003ci\u003eNature\u003c/i\u003e \u003cb\u003e495\u003c/b\u003e, 327–332 (2013).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/nature11911\" data-track-item_id=\"10.1038/nature11911\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fnature11911\" aria-label=\"Article reference 30\" data-doi=\"10.1038/nature11911\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2013Natur.495..327B\" aria-label=\"ADS reference 30\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3sXktlCmsLw%3D\" aria-label=\"CAS reference 30\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=23426266\" aria-label=\"PubMed reference 30\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3606666\" aria-label=\"PubMed Central reference 30\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 30\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Functional%20organization%20of%20human%20sensorimotor%20cortex%20for%20speech%20articulation\u0026amp;journal=Nature\u0026amp;doi=10.1038%2Fnature11911\u0026amp;volume=495\u0026amp;pages=327-332\u0026amp;publication_year=2013\u0026amp;author=Bouchard%2CKE\u0026amp;author=Mesgarani%2CN\u0026amp;author=Johnson%2CK\u0026amp;author=Chang%2CEF\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"31.\"\u003e\u003cp id=\"ref-CR31\"\u003eChartier, J., Anumanchipalli, G. K., Johnson, K. \u0026amp; Chang, E. F. Encoding of articulatory kinematic trajectories in human speech sensorimotor cortex. \u003ci\u003eNeuron\u003c/i\u003e \u003cb\u003e98\u003c/b\u003e, 1042–1054 (2018).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.neuron.2018.04.031\" data-track-item_id=\"10.1016/j.neuron.2018.04.031\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.neuron.2018.04.031\" aria-label=\"Article reference 31\" data-doi=\"10.1016/j.neuron.2018.04.031\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXpslaqsLw%3D\" aria-label=\"CAS reference 31\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=29779940\" aria-label=\"PubMed reference 31\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5992088\" aria-label=\"PubMed Central reference 31\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 31\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Encoding%20of%20articulatory%20kinematic%20trajectories%20in%20human%20speech%20sensorimotor%20cortex\u0026amp;journal=Neuron\u0026amp;doi=10.1016%2Fj.neuron.2018.04.031\u0026amp;volume=98\u0026amp;pages=1042-1054\u0026amp;publication_year=2018\u0026amp;author=Chartier%2CJ\u0026amp;author=Anumanchipalli%2CGK\u0026amp;author=Johnson%2CK\u0026amp;author=Chang%2CEF\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"32.\"\u003e\u003cp id=\"ref-CR32\"\u003eLu, J. et al. Neural control of lexical tone production in human laryngeal motor cortex. \u003ci\u003eNat. Commun.\u003c/i\u003e \u003cb\u003e14\u003c/b\u003e, 6917 (2023).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41467-023-42175-9\" data-track-item_id=\"10.1038/s41467-023-42175-9\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41467-023-42175-9\" aria-label=\"Article reference 32\" data-doi=\"10.1038/s41467-023-42175-9\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2023NatCo..14.6917L\" aria-label=\"ADS reference 32\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXit1CntLfE\" aria-label=\"CAS reference 32\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=37903780\" aria-label=\"PubMed reference 32\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10616086\" aria-label=\"PubMed Central reference 32\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 32\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Neural%20control%20of%20lexical%20tone%20production%20in%20human%20laryngeal%20motor%20cortex\u0026amp;journal=Nat.%20Commun.\u0026amp;doi=10.1038%2Fs41467-023-42175-9\u0026amp;volume=14\u0026amp;publication_year=2023\u0026amp;author=Lu%2CJ\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"33.\"\u003e\u003cp id=\"ref-CR33\"\u003eBreshears, J. D., Molinaro, A. M. \u0026amp; Chang, E. F. A probabilistic map of the human ventral sensorimotor cortex using electrical stimulation. \u003ci\u003eJ. Neurosurg.\u003c/i\u003e \u003cb\u003e123\u003c/b\u003e, 340–349 (2015).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.3171/2014.11.JNS14889\" data-track-item_id=\"10.3171/2014.11.JNS14889\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.3171%2F2014.11.JNS14889\" aria-label=\"Article reference 33\" data-doi=\"10.3171/2014.11.JNS14889\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=25978714\" aria-label=\"PubMed reference 33\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 33\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=A%20probabilistic%20map%20of%20the%20human%20ventral%20sensorimotor%20cortex%20using%20electrical%20stimulation\u0026amp;journal=J.%20Neurosurg.\u0026amp;doi=10.3171%2F2014.11.JNS14889\u0026amp;volume=123\u0026amp;pages=340-349\u0026amp;publication_year=2015\u0026amp;author=Breshears%2CJD\u0026amp;author=Molinaro%2CAM\u0026amp;author=Chang%2CEF\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"34.\"\u003e\u003cp id=\"ref-CR34\"\u003eAmmanuel, S. G. et al. Intraoperative cortical stimulation mapping with laryngeal electromyography for the localization of human laryngeal motor cortex. \u003ci\u003eJ. Neurosurg.\u003c/i\u003e \u003cb\u003e141\u003c/b\u003e, 268–277 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.3171/2023.10.JNS231023\" data-track-item_id=\"10.3171/2023.10.JNS231023\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.3171%2F2023.10.JNS231023\" aria-label=\"Article reference 34\" data-doi=\"10.3171/2023.10.JNS231023\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=38181494\" aria-label=\"PubMed reference 34\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 34\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Intraoperative%20cortical%20stimulation%20mapping%20with%20laryngeal%20electromyography%20for%20the%20localization%20of%20human%20laryngeal%20motor%20cortex\u0026amp;journal=J.%20Neurosurg.\u0026amp;doi=10.3171%2F2023.10.JNS231023\u0026amp;volume=141\u0026amp;pages=268-277\u0026amp;publication_year=2024\u0026amp;author=Ammanuel%2CSG\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"35.\"\u003e\u003cp id=\"ref-CR35\"\u003ePandarinath, C. et al. Neural population dynamics in human motor cortex during movements in people with ALS. \u003ci\u003eeLife\u003c/i\u003e \u003cb\u003e4\u003c/b\u003e, e07436 (2015).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.7554/eLife.07436\" data-track-item_id=\"10.7554/eLife.07436\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.7554%2FeLife.07436\" aria-label=\"Article reference 35\" data-doi=\"10.7554/eLife.07436\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=26099302\" aria-label=\"PubMed reference 35\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4475900\" aria-label=\"PubMed Central reference 35\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 35\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Neural%20population%20dynamics%20in%20human%20motor%20cortex%20during%20movements%20in%20people%20with%20ALS\u0026amp;journal=eLife\u0026amp;doi=10.7554%2FeLife.07436\u0026amp;volume=4\u0026amp;publication_year=2015\u0026amp;author=Pandarinath%2CC\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"36.\"\u003e\u003cp id=\"ref-CR36\"\u003eStavisky, S. D. et al. Neural ensemble dynamics in dorsal motor cortex during speech in people with paralysis. \u003ci\u003eeLife\u003c/i\u003e \u003cb\u003e8\u003c/b\u003e, e46015 (2019).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.7554/eLife.46015\" data-track-item_id=\"10.7554/eLife.46015\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.7554%2FeLife.46015\" aria-label=\"Article reference 36\" data-doi=\"10.7554/eLife.46015\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=31820736\" aria-label=\"PubMed reference 36\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6954053\" aria-label=\"PubMed Central reference 36\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 36\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Neural%20ensemble%20dynamics%20in%20dorsal%20motor%20cortex%20during%20speech%20in%20people%20with%20paralysis\u0026amp;journal=eLife\u0026amp;doi=10.7554%2FeLife.46015\u0026amp;volume=8\u0026amp;publication_year=2019\u0026amp;author=Stavisky%2CSD\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"37.\"\u003e\u003cp id=\"ref-CR37\"\u003eWillett, F. R. et al. Hand knob area of premotor cortex represents the whole body in a compositional way. \u003ci\u003eCell\u003c/i\u003e \u003cb\u003e181\u003c/b\u003e, 396–409 (2020).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.cell.2020.02.043\" data-track-item_id=\"10.1016/j.cell.2020.02.043\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.cell.2020.02.043\" aria-label=\"Article reference 37\" data-doi=\"10.1016/j.cell.2020.02.043\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXlslKksb4%3D\" aria-label=\"CAS reference 37\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=32220308\" aria-label=\"PubMed reference 37\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7166199\" aria-label=\"PubMed Central reference 37\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 37\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Hand%20knob%20area%20of%20premotor%20cortex%20represents%20the%20whole%20body%20in%20a%20compositional%20way\u0026amp;journal=Cell\u0026amp;doi=10.1016%2Fj.cell.2020.02.043\u0026amp;volume=181\u0026amp;pages=396-409\u0026amp;publication_year=2020\u0026amp;author=Willett%2CFR\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"38.\"\u003e\u003cp id=\"ref-CR38\"\u003eAli, Y. H. et al. BRAND: a platform for closed-loop experiments with deep network models. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e21\u003c/b\u003e, 026046 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/ad3b3a\" data-track-item_id=\"10.1088/1741-2552/ad3b3a\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Fad3b3a\" aria-label=\"Article reference 38\" data-doi=\"10.1088/1741-2552/ad3b3a\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2024JNEng..21b6046A\" aria-label=\"ADS reference 38\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11021878\" aria-label=\"PubMed Central reference 38\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 38\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=BRAND%3A%20a%20platform%20for%20closed-loop%20experiments%20with%20deep%20network%20models\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Fad3b3a\u0026amp;volume=21\u0026amp;publication_year=2024\u0026amp;author=Ali%2CYH\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"39.\"\u003e\u003cp id=\"ref-CR39\"\u003eYoung, D. et al. Signal processing methods for reducing artifacts in microelectrode brain recordings caused by functional electrical stimulation. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e15\u003c/b\u003e, 026014 (2018).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/aa9ee8\" data-track-item_id=\"10.1088/1741-2552/aa9ee8\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Faa9ee8\" aria-label=\"Article reference 39\" data-doi=\"10.1088/1741-2552/aa9ee8\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2018JNEng..15b6014Y\" aria-label=\"ADS reference 39\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BC1M3ntl2gsw%3D%3D\" aria-label=\"CAS reference 39\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=29199642\" aria-label=\"PubMed reference 39\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5818316\" aria-label=\"PubMed Central reference 39\"\u003ePubMed Central\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 39\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Signal%20processing%20methods%20for%20reducing%20artifacts%20in%20microelectrode%20brain%20recordings%20caused%20by%20functional%20electrical%20stimulation\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Faa9ee8\u0026amp;volume=15\u0026amp;publication_year=2018\u0026amp;author=Young%2CD\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"40.\"\u003e\u003cp id=\"ref-CR40\"\u003eLevelt, W. J., Roelofs, A. \u0026amp; Meyer, A. S. A theory of lexical access in speech production. \u003ci\u003eBehav. Brain Sci.\u003c/i\u003e \u003cb\u003e22\u003c/b\u003e, 1–38 (1999).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1017/S0140525X99001776\" data-track-item_id=\"10.1017/S0140525X99001776\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1017%2FS0140525X99001776\" aria-label=\"Article reference 40\" data-doi=\"10.1017/S0140525X99001776\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3M3ivFGgsw%3D%3D\" aria-label=\"CAS reference 40\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=11301520\" aria-label=\"PubMed reference 40\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 40\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=A%20theory%20of%20lexical%20access%20in%20speech%20production\u0026amp;journal=Behav.%20Brain%20Sci.\u0026amp;doi=10.1017%2FS0140525X99001776\u0026amp;volume=22\u0026amp;pages=1-38\u0026amp;publication_year=1999\u0026amp;author=Levelt%2CWJ\u0026amp;author=Roelofs%2CA\u0026amp;author=Meyer%2CAS\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"41.\"\u003e\u003cp id=\"ref-CR41\"\u003eRäsänen, O., Doyle, G. \u0026amp; Frank, M. C. Unsupervised word discovery from speech using automatic segmentation into syllable-like units. \u003ci\u003eProc. Interspeech\u003c/i\u003e \u003cb\u003e2015\u003c/b\u003e, 3204–3208 (2015).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 41\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Unsupervised%20word%20discovery%20from%20speech%20using%20automatic%20segmentation%20into%20syllable-like%20units\u0026amp;journal=Proc.%20Interspeech\u0026amp;volume=2015\u0026amp;pages=3204-3208\u0026amp;publication_year=2015\u0026amp;author=R%C3%A4s%C3%A4nen%2CO\u0026amp;author=Doyle%2CG\u0026amp;author=Frank%2CMC\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"42.\"\u003e\u003cp id=\"ref-CR42\"\u003eWilliams, A. H. et al. Discovering precise temporal patterns in large-scale neural recordings through robust and interpretable time warping. \u003ci\u003eNeuron\u003c/i\u003e \u003cb\u003e105\u003c/b\u003e, 246–259 (2020).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.neuron.2019.10.020\" data-track-item_id=\"10.1016/j.neuron.2019.10.020\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.neuron.2019.10.020\" aria-label=\"Article reference 42\" data-doi=\"10.1016/j.neuron.2019.10.020\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXit12ksLnN\" aria-label=\"CAS reference 42\"\u003eCAS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=31786013\" aria-label=\"PubMed reference 42\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 42\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Discovering%20precise%20temporal%20patterns%20in%20large-scale%20neural%20recordings%20through%20robust%20and%20interpretable%20time%20warping\u0026amp;journal=Neuron\u0026amp;doi=10.1016%2Fj.neuron.2019.10.020\u0026amp;volume=105\u0026amp;pages=246-259\u0026amp;publication_year=2020\u0026amp;author=Williams%2CAH\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"43.\"\u003e\u003cp id=\"ref-CR43\"\u003eRoussel, P. et al. Observation and assessment of acoustic contamination of electrophysiological brain signals during speech production and sound perception. \u003ci\u003eJ. Neural Eng.\u003c/i\u003e \u003cb\u003e17\u003c/b\u003e, 056028 (2020).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1088/1741-2552/abb25e\" data-track-item_id=\"10.1088/1741-2552/abb25e\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1088%2F1741-2552%2Fabb25e\" aria-label=\"Article reference 43\" data-doi=\"10.1088/1741-2552/abb25e\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"ads reference\" data-track-action=\"ads reference\" href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT\u0026amp;bibcode=2020JNEng..17e6028R\" aria-label=\"ADS reference 43\"\u003eADS\u003c/a\u003e \n    \u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\u0026amp;db=PubMed\u0026amp;dopt=Abstract\u0026amp;list_uids=33055383\" aria-label=\"PubMed reference 43\"\u003ePubMed\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 43\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=Observation%20and%20assessment%20of%20acoustic%20contamination%20of%20electrophysiological%20brain%20signals%20during%20speech%20production%20and%20sound%20perception\u0026amp;journal=J.%20Neural%20Eng.\u0026amp;doi=10.1088%2F1741-2552%2Fabb25e\u0026amp;volume=17\u0026amp;publication_year=2020\u0026amp;author=Roussel%2CP\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"44.\"\u003e\u003cp id=\"ref-CR44\"\u003eShah, N., Sahipjohn, N., Tambrahalli, V., Subramanian, R. \u0026amp; Gandhi, V. StethoSpeech: speech generation through a clinical stethoscope attached to the skin. \u003ci\u003eProc. ACM Interact. Mob. Wearable Ubiquitous Technol.\u003c/i\u003e \u003cb\u003e8\u003c/b\u003e, 123 (2024).\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1145/3678515\" data-track-item_id=\"10.1145/3678515\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1145%2F3678515\" aria-label=\"Article reference 44\" data-doi=\"10.1145/3678515\"\u003eArticle\u003c/a\u003e \n    \u003ca data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 44\" href=\"http://scholar.google.com/scholar_lookup?\u0026amp;title=StethoSpeech%3A%20speech%20generation%20through%20a%20clinical%20stethoscope%20attached%20to%20the%20skin\u0026amp;journal=Proc.%20ACM%20Interact.%20Mob.%20Wearable%20Ubiquitous%20Technol.\u0026amp;doi=10.1145%2F3678515\u0026amp;volume=8\u0026amp;publication_year=2024\u0026amp;author=Shah%2CN\u0026amp;author=Sahipjohn%2CN\u0026amp;author=Tambrahalli%2CV\u0026amp;author=Subramanian%2CR\u0026amp;author=Gandhi%2CV\"\u003e\n                    Google Scholar\u003c/a\u003e \n                \u003c/p\u003e\u003c/li\u003e\u003cli data-counter=\"45.\"\u003e\u003cp id=\"ref-CR45\"\u003eWairagkar, M. et al. Data for an instantaneous voice synthesis neuroprosthesis. \u003ci\u003eDryad\u003c/i\u003e \u003ca href=\"https://doi.org/10.5061/dryad.2280gb64f\" data-track=\"click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"10.5061/dryad.2280gb64f\"\u003ehttps://doi.org/10.5061/dryad.2280gb64f\u003c/a\u003e (2025).\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003ca data-track=\"click\" data-track-action=\"download citation references\" data-track-label=\"link\" rel=\"nofollow\" href=\"https://citation-needed.springer.com/v2/references/10.1038/s41586-025-09127-3?format=refman\u0026amp;flavour=references\"\u003eDownload references\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"Ack1-section\" data-title=\"Acknowledgements\"\u003e\u003ch2 id=\"Ack1\"\u003eAcknowledgements\u003c/h2\u003e\u003cp\u003eWe thank participant T15 and his family and care partners for their contributions to this research. Support was provided by the Office of the Assistant Secretary of Defense for Health Affairs through the Amyotrophic Lateral Sclerosis Research Program under award number AL220043; a New Innovator Award (DP2) from the NIH Office of the Director and managed by NIDCD (1DP2DC021055); a Seed Grant from the ALS Association (23-SGP-652); A. P. Giannini Postdoctoral Fellowship (N.S.C.); Searle Scholars Program; a Pilot Award from the Simons Collaboration for the Global Brain (AN-NC-GB-Pilot Extension-00002343-01); NIH-NIDCD (U01DC017844) and VA RR\u0026amp;D (A2295-R). S.D.S. holds a Career Award at the Scientific Interface from the Burroughs Wellcome Fund, and a Cultivating Team Science Award from the University of California Davis School of Medicine.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"author-information-section\" aria-labelledby=\"author-information\" data-title=\"Author information\"\u003e\u003ch2 id=\"author-information\"\u003eAuthor information\u003c/h2\u003e\u003cdiv id=\"author-information-content\"\u003e\u003cp\u003e\u003cspan id=\"author-notes\"\u003eAuthor notes\u003c/span\u003e\u003c/p\u003e\u003col\u003e\u003cli id=\"na1\"\u003e\u003cp\u003eThese authors jointly supervised this work: David M. Brandman, Sergey D. Stavisky\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3 id=\"affiliations\"\u003eAuthors and Affiliations\u003c/h3\u003e\u003col\u003e\u003cli id=\"Aff1\"\u003e\u003cp\u003eDepartment of Neurological Surgery, University of California, Davis, Davis, CA, USA\u003c/p\u003e\u003cp\u003eMaitreyee Wairagkar, Nicholas S. Card, Tyler Singer-Clark, Xianda Hou, Carrina Iacobacci, David M. Brandman \u0026amp; Sergey D. Stavisky\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff2\"\u003e\u003cp\u003eDepartment of Biomedical Engineering, University of California, Davis, Davis, CA, USA\u003c/p\u003e\u003cp\u003eTyler Singer-Clark\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff3\"\u003e\u003cp\u003eDepartment of Computer Science, University of California, Davis, Davis, CA, USA\u003c/p\u003e\u003cp\u003eXianda Hou\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff4\"\u003e\u003cp\u003eCenter for Mind and Brain, University of California, Davis, Davis, CA, USA\u003c/p\u003e\u003cp\u003eLee M. Miller\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff5\"\u003e\u003cp\u003eDepartment of Neurobiology, Physiology and Behavior, University of California, Davis, Davis, CA, USA\u003c/p\u003e\u003cp\u003eLee M. Miller\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff6\"\u003e\u003cp\u003eDepartment of Otolaryngology, Head and Neck Surgery, University of California, Davis, Davis, CA, USA\u003c/p\u003e\u003cp\u003eLee M. Miller\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff7\"\u003e\u003cp\u003eSchool of Engineering and Carney Institute for Brain Sciences, Brown University, Providence, RI, USA\u003c/p\u003e\u003cp\u003eLeigh R. Hochberg\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff8\"\u003e\u003cp\u003eVA Center for Neurorestoration and Neurotechnology, VA Providence Healthcare, Providence, RI, USA\u003c/p\u003e\u003cp\u003eLeigh R. Hochberg\u003c/p\u003e\u003c/li\u003e\u003cli id=\"Aff9\"\u003e\u003cp\u003eCenter for Neurotechnology and Neurorecovery, Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA\u003c/p\u003e\u003cp\u003eLeigh R. Hochberg\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv data-test=\"author-info\"\u003e\u003cp\u003e\u003cspan\u003eAuthors\u003c/span\u003e\u003c/p\u003e\u003col\u003e\u003cli id=\"auth-Maitreyee-Wairagkar-Aff1\"\u003e\u003cspan\u003eMaitreyee Wairagkar\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Maitreyee%20Wairagkar\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Maitreyee%20Wairagkar%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-Nicholas_S_-Card-Aff1\"\u003e\u003cspan\u003eNicholas S. Card\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Nicholas%20S.%20Card\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Nicholas%20S.%20Card%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-Tyler-Singer_Clark-Aff1-Aff2\"\u003e\u003cspan\u003eTyler Singer-Clark\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Tyler%20Singer-Clark\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Tyler%20Singer-Clark%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-Xianda-Hou-Aff1-Aff3\"\u003e\u003cspan\u003eXianda Hou\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Xianda%20Hou\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Xianda%20Hou%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-Carrina-Iacobacci-Aff1\"\u003e\u003cspan\u003eCarrina Iacobacci\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Carrina%20Iacobacci\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Carrina%20Iacobacci%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-Lee_M_-Miller-Aff4-Aff5-Aff6\"\u003e\u003cspan\u003eLee M. Miller\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Lee%20M.%20Miller\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Lee%20M.%20Miller%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-Leigh_R_-Hochberg-Aff7-Aff8-Aff9\"\u003e\u003cspan\u003eLeigh R. Hochberg\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Leigh%20R.%20Hochberg\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Leigh%20R.%20Hochberg%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-David_M_-Brandman-Aff1\"\u003e\u003cspan\u003eDavid M. Brandman\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=David%20M.%20Brandman\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22David%20M.%20Brandman%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003cli id=\"auth-Sergey_D_-Stavisky-Aff1\"\u003e\u003cspan\u003eSergey D. Stavisky\u003c/span\u003e\u003cdiv\u003e\u003cp\u003e\u003cspan\u003eYou can also search for this author in\u003c/span\u003e\u003cspan\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search\u0026amp;term=Sergey%20D.%20Stavisky\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\"\u003ePubMed\u003c/a\u003e\u003cspan\u003e \u003c/span\u003e\u003ca href=\"https://scholar.google.co.uk/scholar?as_q=\u0026amp;num=10\u0026amp;btnG=Search+Scholar\u0026amp;as_epq=\u0026amp;as_oq=\u0026amp;as_eq=\u0026amp;as_occt=any\u0026amp;as_sauthors=%22Sergey%20D.%20Stavisky%22\u0026amp;as_publication=\u0026amp;as_ylo=\u0026amp;as_yhi=\u0026amp;as_allsubj=all\u0026amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\"\u003eGoogle Scholar\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003ch3 id=\"contributions\"\u003eContributions\u003c/h3\u003e\u003cp\u003eM.W., S.D.S. and D.M.B. conceived the study and experiment design. M.W. led the experiments and developed and implemented the target speech generation, decoder training algorithms and end-to-end pipeline for instantaneous voice synthesis: feature extraction, noise removal, preprocessing, real-time brain-to-voice decoders, pitch decoders, vocoder and output audio playback, experimental tasks, post-processing. M.W. also performed human listener evaluations, analysed all of the data and created figures. M.W. and N.S.C. developed and implemented the real-time neural signal processing, noise removal and feature-extraction pipelines. M.W., N.S.C., T.S.-C. and X.H. coded the real-time data-collection system and built the neuroprosthetic cart system. N.S.C. generated cloned voice samples for T15. M.W., N.S.C. and C.I. collected the primary data for this study. N.S.C. and C.I. interfaced with the participant and scheduled research sessions. L.M.M. contributed to the human listener evaluations. D.M.B. led planning and performed the surgical-implant-placement procedure. L.R.H. was the sponsor–investigator of the multisite clinical trial. D.M.B. was responsible for all clinical-trial-related activities at University of California Davis. S.D.S. and D.M.B. supervised all aspects of the project. M.W and S.D.S. wrote the paper. All authors reviewed and edited the paper.\u003c/p\u003e\u003ch3 id=\"corresponding-author\"\u003eCorresponding authors\u003c/h3\u003e\u003cp id=\"corresponding-author-list\"\u003eCorrespondence to\n                \u003ca id=\"corresp-c1\" href=\"mailto:mwairagkar@ucdavis.edu\"\u003eMaitreyee Wairagkar\u003c/a\u003e, \u003ca id=\"corresp-c2\" href=\"mailto:dmbrandman@ucdavis.edu\"\u003eDavid M. Brandman\u003c/a\u003e or \u003ca id=\"corresp-c3\" href=\"mailto:sstavisky@ucdavis.edu\"\u003eSergey D. Stavisky\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"ethics-section\" data-title=\"Ethics declarations\"\u003e\u003ch2 id=\"ethics\"\u003eEthics declarations\u003c/h2\u003e\u003cdiv id=\"ethics-content\"\u003e\n            \n              \u003ch3 id=\"FPar7\"\u003eCompeting interests\u003c/h3\u003e\n              \u003cp\u003eS.D.S. is an inventor on intellectual property related to speech decoding submitted and owned by Stanford University (US patent no. 12008987) that has been licensed to Blackrock Neurotech and Neuralink. M.W., S.D.S. and D.M.B. have patent applications related to speech BCI submitted and owned by the Regents of the University of California (US patent application no. 63/461,507 and 63/450,317), including intellectual property licensed by Paradromics. D.M.B. was a surgical consultant with Paradromics, completing his consultation during the revision period of the paper. He is a consultant for Globus Medical. S.D.S. is a scientific adviser to Sonera. The MGH Translational Research Center has a clinical research support agreement with Ability Neuro, Axoft, Neuralink, Neurobionics, Paradromics, Precision Neuro, Synchron and Reach Neuro, for which L.R.H. provides consultative input. Mass General Brigham is convening the Implantable Brain-Computer Interface Collaborative Community (iBCI-CC); charitable gift agreements to Mass General Brigham, including those received to date from Paradromics, Synchron, Precision Neuro, Neuralink and Blackrock Neurotech, support the iBCI-CC, for which L.R.H. provides effort. The other authors declare no competing interests.\u003c/p\u003e\n            \n          \u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"peer-review-section\" data-title=\"Peer review\"\u003e\u003ch2 id=\"peer-review\"\u003ePeer review\u003c/h2\u003e\u003cdiv id=\"peer-review-content\"\u003e\n            \n            \n              \u003ch3 id=\"FPar6\"\u003ePeer review information\u003c/h3\u003e\n              \u003cp\u003e\u003ci\u003eNature\u003c/i\u003e thanks Nai Ding, Nick Ramsey and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. \u003ca data-track=\"click\" data-track-label=\"link\" data-track-action=\"supplementary material anchor\" href=\"https://www.nature.com/articles/s41586-025-09127-3#MOESM3\"\u003ePeer reviewer reports\u003c/a\u003e are available.\u003c/p\u003e\n            \n          \u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"additional-information-section\" data-title=\"Additional information\"\u003e\u003ch2 id=\"additional-information\"\u003eAdditional information\u003c/h2\u003e\u003cp\u003e\u003cb\u003ePublisher’s note\u003c/b\u003e Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"Sec34-section\" data-title=\"Extended data figures and tables\"\u003e\u003ch2 id=\"Sec34\"\u003eExtended data figures and tables\u003c/h2\u003e\u003cdiv data-test=\"supplementary-info\" id=\"Sec34-content\"\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig5\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 1 microelectrode array placemen\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/5\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig5_ESM.jpg\"\u003eExtended Data Fig. 1 Microelectrode array placement and brain-to-voice synthesis latencies.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. The estimated resting state language network from Human Connectome Project data overlaid on T15’s brain anatomy. \u003cb\u003eb\u003c/b\u003e. Intraoperative photograph showing the four microelectrode arrays placed on T15’s precentral gyrus. Images in \u003cb\u003ea\u003c/b\u003e and \u003cb\u003eb\u003c/b\u003e are adapted from ref. \u003csup\u003e\u003ca data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 1\" title=\"Card, N. S. et al. An accurate and rapidly calibrating speech neuroprosthesis. N. Engl. J. Med. 391, 609–618 (2024).\" href=\"https://www.nature.com/articles/s41586-025-09127-3#ref-CR1\" id=\"ref-link-section-d54370174e2119\"\u003e1\u003c/a\u003e\u003c/sup\u003e (Copyright © 2024 Massachusetts Medical Society, reprinted with permission from Massachusetts Medical Society). \u003cb\u003ec\u003c/b\u003e. Closed-loop cumulative latencies across different stages in the voice synthesis and audio playback pipeline are shown. Voice samples were synthesized from raw neural activity measurements within 10 ms and the resulting audio was played out loud continuously to provide closed-loop feedback. Note the linear horizontal axis is split to expand the visual dynamic range. We focused our engineering primarily on reducing the brain-to-voice inference latency, which fundamentally bounds the speech synthesis latency. As a result, the largest remaining contribution to the latency occurred after voice synthesis decoding during the (comparably more mundane) step of audio playback through a sound driver. The cumulative latencies with the audio driver settings used for T15 closed-loop synthesis in earlier experiments are shown in dark grey. Audio playback latencies were subsequently substantially lowered through software optimizations (light grey) in latter sessions and we predict that further reductions will be possible with additional computer engineering.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig6\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 2 additional bci speech synthes\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/6\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig6_ESM.jpg\"\u003eExtended Data Fig. 2 Additional BCI speech synthesis performance metrics.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. Mel-cepstral distortion (MCD) is computed across 25 Mel-frequency bands between the closed-loop synthesized speech and the target speech after removing silences between words. The four subpanels show MCDs (mean ± s.d) between the synthesized and target speech for different speech tasks in evaluation research sessions. \u003cb\u003eb\u003c/b\u003e. Performance of brain-to-voice decoder measured over time by evaluating neural trials from different sessions offline with a fixed decoder. Decoder trained on post-implant day 165 was fixed and used to synthesize voice offline from neural trials collected in sessions over the next month. Performance was measured by computing Pearson correlation coefficient between the target speech and the synthesized speech across 40 Mel-frequencies after removing silences between words (mean ± s.d., n = 956 sentences). A noticeable decline in brain-to-voice performance was observed after approximately 15 days.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig7\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 3 electrodes show variability i\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/7\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig7_ESM.jpg\"\u003eExtended Data Fig. 3 Electrodes show variability in speech tuning.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. Example closed-loop speech synthesis trial. Spike-band power and threshold crossing spikes from each electrode are shown for one example sentence. These neural features were binned and causally normalized and smoothed on a rolling basis before being decoded to synthesize speech. The mean spike-band power and threshold crossing activity for each individual array are also shown. Speech-related modulation was observed on all arrays, with the highest modulation recorded in v6v and 55b. The synthesized speech is shown in the bottom-most row. The grey trace above it shows the participant’s attempted (unintelligible) speech as recorded with a microphone. \u003cb\u003eb, d\u003c/b\u003e. Pearson correlation coefficients of spike-band power and threshold crossings, respectively, between each electrode and the speech envelope (first LPCNet feature predicted by the brain-to-voice decoder). Electrodes are grouped by array and sorted in ascending order to show that different electrodes have different tuning (both positive and negative) with speech. Arrays v6v and 55b have higher correlations with speech and the majority of electrodes show positive tuning. Arrays M1 and d6v have lower correlations with more electrodes tuned negatively. Electrodes with non-significant correlation (p \u0026gt; 0.05) are shown in grey. Insets show the same correlations for each electrode arranged spatially in the array. \u003cb\u003ec, e\u003c/b\u003e. The time course of average spike-band power and threshold crossings across trials of two example electrodes with positive (solid line) and negative (dashed line) correlations from each array (example electrodes are marked by the ‘+’ symbol in (\u003cb\u003eb\u003c/b\u003e, \u003cb\u003ed\u003c/b\u003e)). Different electrodes show complex neural dynamics with respect to speech onset and a rich variety in speech tuning. For example, some electrodes have higher activity before speech onset (possibly contributing to speech preparation) while others have higher activity during or after speech onset.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig8\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 4 speech is not synthesized dur\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/8\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig8_ESM.jpg\"\u003eExtended Data Fig. 4 Speech is not synthesized during non-speech vocalizations or orofacial movements.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. Three example trials show microphone recording (grey) of T15 during attempted speech trials with coughing, throat clearing, non-speech vocalizations or people speaking in the background and the corresponding brain-to-voice synthesis output (blue). The brain-to-voice decoder did not synthesize audible speech and instead output silence during these non-speech vocalizations or when other people were speaking simultaneously (note that T15 starts speaking via the neuroprosthesis midway through the background conversation in example 3). In contrast, it did synthesize voice when T15 voluntarily attempted to speak. Speech was synthesized instantaneously at the exact pace with which T15 attempted to speak and with very low latency. \u003cb\u003eb\u003c/b\u003e. Samples of non-speech vocalization events (grey) and the corresponding speech synthesis output, which was zero (silence) throughout all these events (blue). \u003cb\u003ec\u003c/b\u003e. Examples of speech synthesis during attempted speech of each word. Here, the speech is synthesized (blue) appropriately as expected during attempted speech (grey).\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig9\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 5 neural activity is not contam\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/9\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig9_ESM.jpg\"\u003eExtended Data Fig. 5 Neural activity is not contaminated by acoustic artifacts and residual vocalization and movement cannot synthesize intelligible speech.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. Three example trials’ audio recording, audio spectrogram, and the spectrograms of the two most acoustic-correlated neural electrodes. Examples are shown for the three types of speech tasks. The prominent spectral structures in the audio spectrogram cannot be observed even in the top two most correlated neural electrodes. An increase in neural activity can be observed before speech onset for each word, reflecting speech preparatory activity and further arguing against acoustic contamination. Note that in the word emphasis example, the last word ‘going’ is not vocalized fully (there is minimal activity in its audio spectrum), yet an increase in neural activity can be observed that is similar to other words. Contamination matrices and statistical criteria are shown in the bottom row, where \u003ci\u003eP\u003c/i\u003e-value indicates whether the trial is significantly acoustically contaminated or not. \u003cb\u003eb\u003c/b\u003e. An example trial of attempted speech with simultaneous recording of intracortical neural signals and various biosignals measured using a microphone, stethoscopic microphone and IMU sensors (accelerometer and gyroscope). Separate independent decoders were trained to synthesize speech using each of the biosignals (or all three together). \u003cb\u003ec\u003c/b\u003e. Intelligible speech could not be synthesized from biosignals measuring sound, movement, and vibrations during attempted speech. (Left) Cross-validated Pearson correlation coefficients (mean ± s.d.) (compared to target speech) of speech synthesized using neural signals, each of the biosignals, and all biosignals together. Reconstruction accuracy is significantly lower for decoding speech from biosignals as compared to neural activity (two-sided Wilcoxon rank-sum, \u003ci\u003eP\u003c/i\u003e = 10\u003csup\u003e−59\u003c/sup\u003e, n = 240 sentences). (Right) Distribution of Pearson correlation coefficients of speech decoding from biosignals and neural signals are mostly non-overlapping, indicating that synthesis quality from biosignals is much lower than that of neural signals. \u003cb\u003ed\u003c/b\u003e. To assess the intelligibility of voice synthesis from neural activity and biosignals (stethoscopic mic decoder), naive human listeners performed open transcription of (the same) 30 synthesized trials using both the decoders. Median phoneme error rates and word error rates for neural decoding were significantly lower (43.60%) than decoding stethoscope recordings, which had word error rate of 100%. This indicates that intelligible speech cannot be decoded from these non-neural biosignals.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig10\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 6 encoding of paralinguistic fe\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/10\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig10_ESM.jpg\"\u003eExtended Data Fig. 6 Encoding of paralinguistic features in neural activity.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. Neural modulation during question intonation. Trial-averaged normalized spike-band power (each row in a group is one electrode) during trials where the participant modulated his intonation to say the cued sentence as a question. Trials with the same cue sentence (n = 16) were aligned using dynamic time warping and the mean activity across trials spoken as statements was subtracted to better show the increased neural activity around the intonation-modulation at the end of the sentence. The onset of the word that was pitch-modulated in closed-loop is indicated by the arrowhead at the bottom of each example. \u003cb\u003eb\u003c/b\u003e. Paralinguistic features encoding recorded from individual arrays. Trial-averaged spike-band power (mean ± s.e.m.), averaged across all electrodes within each array, for words spoken as statements and as questions. At every time point, the spike-band power for statement words and question words were compared using the Wilcoxon rank-sum test. The blue line at the bottom indicates the time points where the spike-band power in statement words and question words were significantly different (\u003ci\u003eP\u003c/i\u003e \u0026lt; 0.001, n\u003csub\u003e1\u003c/sub\u003e = 970 words, n\u003csub\u003e2\u003c/sub\u003e = 184 words). \u003cb\u003ec\u003c/b\u003e. Trial averaged spike-band power across each array for non-emphasized and emphasized words. The spike-band power was significantly different between non-emphasized words and emphasized words at time points shown in blue (\u003ci\u003eP\u003c/i\u003e \u0026lt; 0.001, n\u003csub\u003e1\u003c/sub\u003e = 1269 words, n\u003csub\u003e2\u003c/sub\u003e = 333 words). \u003cb\u003ed\u003c/b\u003e. Trial-averaged spike-band power across each array for words without pitch modulation and words with pitch modulation (from the three-pitch melodies singing task). Words with low and high pitch targets are grouped together as the ‘pitch modulation’ category (we excluded medium pitch target words where the participant used his normal pitch). The spike-band power was significantly different between no pitch modulation and pitch modulation at time points shown in blue (\u003ci\u003eP\u003c/i\u003e \u0026lt; 0.001, n\u003csub\u003e1\u003c/sub\u003e = 486 words, n\u003csub\u003e2\u003c/sub\u003e = 916 words). \u003cb\u003ee\u003c/b\u003e. Confusion matrix showing offline accuracies for decoding question intonation and word emphasis paralinguistic features together using a single combined 3-class classifier.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig11\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 7 closed-loop paralinguistic fe\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/11\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig11_ESM.jpg\"\u003eExtended Data Fig. 7 Closed-loop paralinguistic features modulation.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. An overview of the paralinguistic feature decoder and pitch modulation pipeline. An independent paralinguistic feature decoder ran in parallel to the regular brain-to-voice decoder. Its output causally modulated the pitch feature predicted by brain-to-voice, resulting in a pitch-modulated voice. \u003cb\u003eb\u003c/b\u003e. An example trial of closed-loop intonation modulation for speaking a sentence as a question. A separate binary decoder identified the change in intonation and sent a trigger (downward arrow) to modulate the pitch feature output of the regular brain-to-voice decoder according to a predefined pitch profile for asking a question (low pitch to high pitch). Neural activity of an example trial with its synthesized voice output is shown along with the intonation decoder output, time of modulation trigger (downward arrow), originally predicted pitch feature and the modulated pitch feature used for voice synthesis. \u003cb\u003ec\u003c/b\u003e. An example trial of closed-loop word emphasis where the word “\u003ci\u003eYOU\u003c/i\u003e” from “\u003ci\u003eWhat are YOU doing\u003c/i\u003e” was emphasized. To emphasize a word, we applied a predefined pitch profile (high pitch to low pitch) along with a 20% increase in the loudness of the predicted speech samples. \u003cb\u003ed\u003c/b\u003e. An example trial of closed-loop pitch modulation for singing a melody with three pitch levels. The three-pitch classifier output was used to continuously modulate the predicted pitch feature output from the brain-to-voice decoder.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig12\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 8 pearson correlation coefficie\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/12\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig12_ESM.jpg\"\u003eExtended Data Fig. 8 Pearson correlation coefficients over the course of a sentence.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003ePearson correlation coefficient (\u003ci\u003er\u003c/i\u003e) of individual words in sentences of different lengths (mean ± s.d.). The correlation between target and synthesized speech remained consistent throughout the length of sentence, indicating that the quality of synthesized voice was consistent throughout the sentence. Note that there were fewer longer evaluation sentences.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig13\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 9 output-null and output-potent\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/13\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig13_ESM.jpg\"\u003eExtended Data Fig. 9 Output-null and output-potent neural dynamics during speech production in individual arrays.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea-d\u003c/b\u003e. Average approximated output-null (orange) and output-potent (blue) components of neural activity during attempted speech of cued sentences of different lengths. Here the neural components are computed for each array independently by training separate linear decoders (i.e., repeating the analyses of Fig. \u003ca data-track=\"click\" data-track-label=\"link\" data-track-action=\"figure anchor\" href=\"https://www.nature.com/articles/s41586-025-09127-3#Fig4\"\u003e4\u003c/a\u003e for individual arrays independently). A subset of sentence lengths are shown in the interest of space. Note that the d6v array had much less speech-related modulation. Bar plots within each panel show a summary of all the data (including the not-shown sentence lengths) by taking the average null/potent activity ratios for words in the first-quarter, second-quarter, third-quarter, and fourth-quarter of each sentence (mean ± s.e.m., n\u003csub\u003eQ1\u003c/sub\u003e = 3,600, n\u003csub\u003eQ2\u003c/sub\u003e = 4,181, n\u003csub\u003eQ3\u003c/sub\u003e = 3,456, n\u003csub\u003eQ4\u003c/sub\u003e = 3,134 words). \u003cb\u003ee-h\u003c/b\u003e. Average output-null and output-potent activity during intonation modulation (question-asking or word emphasis) computed separately for each array. Output-null activity shows an increase during intonation modulated word in all arrays. Null/potent activity ratios are summarized in bar plots of intonation-modulated word (red) and the words preceding or following it (grey) (mean ± s.e.m.). The null/potent ratios of modulated words were significantly different from that of non-modulated words for the v6v, M1 and d6v arrays (two-sided Wilcoxon rank-sum, v6v: \u003ci\u003ep\u003c/i\u003e = 10\u003csup\u003e−11\u003c/sup\u003e, M1: \u003ci\u003ep\u003c/i\u003e = 10\u003csup\u003e−16\u003c/sup\u003e, 55b: \u003ci\u003ep\u003c/i\u003e = 0.3, d6v: \u003ci\u003ep\u003c/i\u003e = 10\u003csup\u003e−26\u003c/sup\u003e, n\u003csub\u003e1\u003c/sub\u003e = 460 modulated words, n\u003csub\u003e2\u003c/sub\u003e = 922 non-modulated words).\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"Fig14\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"extended data fig. 10 head motion during speech an\" href=\"https://www.nature.com/articles/s41586-025-09127-3/figures/14\" data-supp-info-image=\"//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_Fig14_ESM.jpg\"\u003eExtended Data Fig. 10 Head motion during speech and its relationship with the neural dynamics.\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003ea\u003c/b\u003e. Head motion was tracked from videos by mapping the \u003ci\u003ex\u003c/i\u003e and \u003ci\u003ey\u003c/i\u003e positions of the NeuroPort pedestal in each frame (yellow points) using OpenCV. Overall head motion was summarized by the first principal component (pink axis) of \u003ci\u003ex-y\u003c/i\u003e motion. The inset shows a single frame of head motion tracking. \u003cb\u003eb\u003c/b\u003e. Small head motion was observed during uttering each word. Head motion remained consistent throughout the attempted speech sentence as measured by the motion from baseline during utterance of words in each of the four word-quartiles, regardless of the length of the sentence. \u003cb\u003ec\u003c/b\u003e. The ratio of output-null and output-potent components of simultaneously recorded neural activity decayed over the course of the sentence, in contrast to the head motion in (\u003cb\u003eb\u003c/b\u003e). \u003cb\u003ed\u003c/b\u003e. Time course and amplitude of the output-null and output-potent components of the neural activity and simultaneous head motion in different quartiles of the sentence. The head motion (purple) follows the output-null activity (orange) but precedes the output-potent activity (blue). The output-null activity decayed over the course of the sentence, whereas the head motion during each word in a sentence remained constant. Taken together, this shows that the neural dynamics do not closely match the head motion time course.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"Sec35-section\" data-title=\"Supplementary information\"\u003e\u003ch2 id=\"Sec35\"\u003eSupplementary information\u003c/h2\u003e\u003cdiv data-test=\"supplementary-info\" id=\"Sec35-content\"\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM1\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary table 1\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM1_ESM.pdf\" data-supp-info-image=\"\"\u003eSupplementary Table 1\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eData used for training the brain-to-voice decoders in evaluation sessions.\u003c/p\u003e\u003c/div\u003e\u003cp data-test=\"supp-item\" id=\"MOESM2\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"reporting summary\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM2_ESM.pdf\" data-supp-info-image=\"\"\u003eReporting Summary\u003c/a\u003e\u003c/h3\u003e\u003c/p\u003e\u003cp data-test=\"supp-item\" id=\"MOESM3\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"peer review file\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM3_ESM.pdf\" data-supp-info-image=\"\"\u003ePeer Review File\u003c/a\u003e\u003c/h3\u003e\u003c/p\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM4\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 1\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM4_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 1\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eDysarthric speech of the participant\u003c/b\u003e. This video shows the participant, who has severe dysarthria due to ALS, attempting to speak the sentences cued on the screen. The speech of the participant is unintelligible to naive listeners. Taken on day 25 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM5\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 2\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM5_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 2\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis during attempted vocalized speech\u003c/b\u003e. This video shows 13 consecutive closed-loop trials of instantaneous voice synthesis as the participant attempts to speak cued sentences. The synthesized voice was played back continuously in real time through a speaker. Taken on day 179 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM6\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 3\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM6_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 3\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis with simultaneous brain-to-text decoding\u003c/b\u003e. This video shows 15 consecutive closed-loop trials of instantaneous voice synthesis with simultaneous brain-to-text decoding that acted as closed captioning when the participant attempted to speak cued sentences. Taken on day 110 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM7\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 4\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM7_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 4\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis during attempted mimed speech\u003c/b\u003e. This video shows ten consecutive closed-loop trials of instantaneous voice synthesis with audio feedback as the participant mimed the cued sentences without vocalizing. The decoder was not trained on any mimed-speech neural data. Taken on day 195 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM8\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 5\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM8_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 5\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis during self-initiated free responses\u003c/b\u003e. This video shows nine closed-loop trials of instantaneous voice synthesis with audio feedback as the participant responds to open-ended questions or is asked to say whatever he wanted. We used this opportunity to ask the participant for his feedback on this brain-to-voice neuroprosthesis. A brain-to-text decoder was used simultaneously to help with understanding what the participant was saying. Taken on days 172, 179, 186, 188, 193 and 195 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM9\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 6\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM9_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 6\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop own-voice synthesis during attempted speech\u003c/b\u003e. This video shows nine consecutive closed-loop trials of instantaneous speech synthesis in a voice that sounds like the voice of the participant before ALS as the participant attempts to speak cued sentences. Taken on day 286 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM10\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 7\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM10_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 7\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis of pseudo-words\u003c/b\u003e. This video shows five consecutive trials of closed-loop synthesis of made-up pseudo-words using the brain-to-voice decoder. The decoder was not trained on any pseudo-words. Taken on day 179 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM11\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 8\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM11_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 8\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis of interjections\u003c/b\u003e. This video shows five trials of closed-loop synthesis of interjections using the brain-to-voice decoder. The decoder was not trained on these words. Taken on day 186 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM12\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 9\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM12_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 9\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis for spelling words\u003c/b\u003e. This video shows seven trials of closed-loop synthesis in which the participant was spelling cued words one letter at a time using the brain-to-voice decoder. The decoder was not trained on this task. Taken on day 186 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM13\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 10\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM13_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 10\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop question intonation\u003c/b\u003e. This video shows ten selected trials in which the participant modulated his intonation to say a sentence as a question (indicated by ‘?’ in the cue) or as a statement by using an intonation decoder that modulated the brain-to-voice synthesis in a closed loop. Taken on day 286 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM14\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 11\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM14_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 11\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop word emphasis\u003c/b\u003e. This video shows eight selected trials in which certain (capitalized) words in the cued sentences were emphasized by the participant by using an emphasis decoder that modulated the brain-to-voice synthesis in a closed loop. Taken on day 286 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM15\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 12\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM15_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 12\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eSinging three-pitch melodies in a closed loop\u003c/b\u003e. This video shows three consecutive trials in which the participant sung short melodies with three pitch targets by using a pitch decoder that modulated the brain-to-voice synthesis. At the start of each trial, an audio cue plays the target melody. The on-screen targets then turn from red to green to indicate that the participant should begin. The vertical bar on the left shows the instantaneous decoded pitch (low, mid and high). Interactive visual cues for each pitch target are shown on the screen. Visual feedback cues show the note in the melody that the participant is singing. Taken on day 342 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM16\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 13\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM16_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 13\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eSinging three-pitch melodies using a unified brain-to-voice decoder\u003c/b\u003e. This video shows three trials in which the participant sung short melodies with three pitch targets by using a single unified brain-to-voice decoder that inherently synthesizes intended pitch in a closed loop. At the start of each trial, an audio cue plays the target melody. The vertical bar on the left shows the instantaneous decoded pitch (low, mid and high) for visual feedback only (that is, this separately decoded pitch, which is the same as in Supplementary Video 12, is not used in the unified brain-to-voice model). Interactive cues show the note in the melody that the participant is singing, providing visual feedback. Taken on day 342 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM17\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 14\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM17_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 14\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eClosed-loop voice synthesis in session 1\u003c/b\u003e. This video shows three closed-loop trials of instantaneous voice synthesis from the first day of neural recording (day 25 after implant). The brain-to-voice decoder was trained during this session using 190 sentence trials with a limited 50-word vocabulary recorded earlier on the same day. The second part of the video shows the same three trials reconstructed offline using an optimized brain-to-voice decoder (that is, the algorithm used throughout the rest of this paper), which improved intelligibility.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM18\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 15\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM18_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 15\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eIntelligible voice cannot be decoded from biosignal recordings of residual speech of T15\u003c/b\u003e. This video shows examples of speech synthesized from simultaneous recordings from microphone, stethoscopic microphone, IMU sensor and intracortical neural activity as T15 attempts to speak. Speech synthesized from microphone, stethoscope and IMU biosignals was not intelligible (word error rate for stethoscope decoding: 100%), whereas the voice synthesized from neural activity was more intelligible (word error rate: 43.60%). From day 482 after implant.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM19\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary video 16\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM19_ESM.mp4\" data-supp-info-image=\"\"\u003eSupplementary Video 16\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eComparison of the voice of T15 before ALS with the own-voice synthesis by the brain-to-voice BCI\u003c/b\u003e. This video shows examples of (1) the voice of T15 before ALS; (2) the voice cloned by the StyleTTS 2 model, which was trained using T15’s voice before ALS; (3) target audio generated using this cloned-voice, time-aligned with neural signals during attempted speech used as training data for the personalized own-voice BCI speech-synthesis decoder; and (4) the own voice synthesized by the personalized brain-to-voice decoder in a closed loop from neural activity.\u003c/p\u003e\u003c/div\u003e\u003cdiv data-test=\"supp-item\" id=\"MOESM20\"\u003e\u003ch3\u003e\u003ca data-track=\"click\" data-track-action=\"view supplementary info\" data-test=\"supp-info-link\" data-track-label=\"supplementary audio 1\" href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09127-3/MediaObjects/41586_2025_9127_MOESM20_ESM.wav\" data-supp-info-image=\"\"\u003eSupplementary Audio 1\u003c/a\u003e\u003c/h3\u003e\u003cp\u003e\u003cb\u003eAcausal speech synthesis by predicting discrete speech units\u003c/b\u003e. Audio recording of three example trials of speech reconstructed offline using the approach of predicting discrete speech units acausally at the end of the sentence using connectionist temporal classification (CTC) loss. From day 25 after implant.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"rightslink-section\" data-title=\"Rights and permissions\"\u003e\u003ch2 id=\"rightslink\"\u003eRights and permissions\u003c/h2\u003e\u003cdiv id=\"rightslink-content\"\u003e\u003cp\u003eSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.\u003c/p\u003e\u003cp\u003e\u003ca data-track=\"click\" data-track-action=\"view rights and permissions\" data-track-label=\"link\" href=\"https://s100.copyright.com/AppDispatchServlet?title=An%20instantaneous%20voice-synthesis%20neuroprosthesis\u0026amp;author=Maitreyee%20Wairagkar%20et%20al\u0026amp;contentID=10.1038%2Fs41586-025-09127-3\u0026amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer%20Nature%20Limited\u0026amp;publication=0028-0836\u0026amp;publicationDate=2025-06-12\u0026amp;publisherName=SpringerNature\u0026amp;orderBeanReset=true\"\u003eReprints and permissions\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv id=\"article-info-section\" aria-labelledby=\"article-info\" data-title=\"About this article\"\u003e\u003ch2 id=\"article-info\"\u003eAbout this article\u003c/h2\u003e\u003cdiv id=\"article-info-content\"\u003e\u003cp\u003e\u003ca data-crossmark=\"10.1038/s41586-025-09127-3\" target=\"_blank\" rel=\"noopener\" href=\"https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-025-09127-3\" data-track=\"click\" data-track-action=\"Click Crossmark\" data-track-label=\"link\" data-test=\"crossmark\"\u003e\u003cimg loading=\"lazy\" width=\"57\" height=\"81\" alt=\"Check for updates. Verify currency and authenticity via CrossMark\" src=\"data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>\"/\u003e\u003c/a\u003e\u003c/p\u003e\u003cdiv\u003e\u003ch3 id=\"citeas\"\u003eCite this article\u003c/h3\u003e\u003cp\u003eWairagkar, M., Card, N.S., Singer-Clark, T. \u003ci\u003eet al.\u003c/i\u003e An instantaneous voice-synthesis neuroprosthesis.\n                    \u003ci\u003eNature\u003c/i\u003e  (2025). https://doi.org/10.1038/s41586-025-09127-3\u003c/p\u003e\u003cp\u003e\u003ca data-test=\"citation-link\" data-track=\"click\" data-track-action=\"download article citation\" data-track-label=\"link\" data-track-external=\"\" rel=\"nofollow\" href=\"https://citation-needed.springer.com/v2/references/10.1038/s41586-025-09127-3?format=refman\u0026amp;flavour=citation\"\u003eDownload citation\u003c/a\u003e\u003c/p\u003e\u003cul data-test=\"publication-history\"\u003e\u003cli\u003e\u003cp\u003eReceived\u003cspan\u003e: \u003c/span\u003e\u003cspan\u003e\u003ctime datetime=\"2024-09-19\"\u003e19 September 2024\u003c/time\u003e\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAccepted\u003cspan\u003e: \u003c/span\u003e\u003cspan\u003e\u003ctime datetime=\"2025-05-08\"\u003e08 May 2025\u003c/time\u003e\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003ePublished\u003cspan\u003e: \u003c/span\u003e\u003cspan\u003e\u003ctime datetime=\"2025-06-12\"\u003e12 June 2025\u003c/time\u003e\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cabbr title=\"Digital Object Identifier\"\u003eDOI\u003c/abbr\u003e\u003cspan\u003e: \u003c/span\u003e\u003cspan\u003ehttps://doi.org/10.1038/s41586-025-09127-3\u003c/span\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\u003c/div\u003e",
  "readingTime": "42 min read",
  "publishedTime": null,
  "modifiedTime": null
}
