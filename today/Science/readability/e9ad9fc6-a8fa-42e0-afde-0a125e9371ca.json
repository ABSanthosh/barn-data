{
  "id": "e9ad9fc6-a8fa-42e0-afde-0a125e9371ca",
  "title": "Brain Implant Lets Man with ALS Speak and Sing with His ‘Real Voice’",
  "link": "https://www.scientificamerican.com/article/brain-implant-lets-man-with-als-speak-and-sing-with-his-real-voice/",
  "description": "A new brain-computer interface turns thoughts into singing and expressive speech in real time",
  "author": "",
  "published": "Thu, 12 Jun 2025 12:00:00 +0000",
  "source": "http://rss.sciam.com/ScientificAmerican-Global",
  "categories": null,
  "byline": "Miryam Naddaf",
  "length": 4564,
  "excerpt": "A new brain-computer interface turns thoughts into singing and expressive speech in real time",
  "siteName": "Scientific American",
  "favicon": "",
  "text": "A new brain-computer interface turns thoughts into singing and expressive speech in real timeThe motor cortex (orange, illustration). Electrodes implanted in this region helped to record the speech-related brain activity of a man who could not speak intelligibly. Kateryna Kon/Science Photo Library/Alamy Stock PhotoA man with a severe speech disability is able to speak expressively and sing using a brain implant that translates his neural activity into words almost instantly. The device conveys changes of tone when he asks questions, emphasizes the words of his choice and allows him to hum a string of notes in three pitches.The system — known as a brain–computer interface (BCI) — used artificial intelligence (AI) to decode the participant’s electrical brain activity as he attempted to speak. The device is the first to reproduce not only a person’s intended words but also features of natural speech such as tone, pitch and emphasis, which help to express meaning and emotion.In a study, a synthetic voice that mimicked the participant’s own spoke his words within 10 milliseconds of the neural activity that signalled his intention to speak. The system, described today in Nature, marks a significant improvement over earlier BCI models, which streamed speech within three seconds or produced it only after users finished miming an entire sentence.On supporting science journalismIf you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.“This is the holy grail in speech BCIs,” says Christian Herff, a computational neuroscientist at Maastricht University, the Netherlands, who was not involved in the study. “This is now real, spontaneous, continuous speech.”Real-time decoderThe study participant, a 45-year-old man, lost his ability to speak clearly after developing amyotrophic lateral sclerosis, a form of motor neuron disease, which damages the nerves that control muscle movements, including those needed for speech. Although he could still make sounds and mouth words, his speech was slow and unclear.Five years after his symptoms began, the participant underwent surgery to insert 256 silicon electrodes, each 1.5-mm long, in a brain region that controls movement. Study co-author Maitreyee Wairagkar, a neuroscientist at the University of California, Davis, and her colleagues trained deep-learning algorithms to capture the signals in his brain every 10 milliseconds. Their system decodes, in real time, the sounds the man attempts to produce rather than his intended words or the constituent phonemes — the subunits of speech that form spoken words.“We don’t always use words to communicate what we want. We have interjections. We have other expressive vocalizations that are not in the vocabulary,” explains Wairagkar. “In order to do that, we have adopted this approach, which is completely unrestricted.”The team also personalized the synthetic voice to sound like the man’s own, by training AI algorithms on recordings of interviews he had done before the onset of his disease.The team asked the participant to attempt to make interjections such as ‘aah’, ‘ooh’ and ‘hmm’ and say made-up words. The BCI successfully produced these sounds, showing that it could generate speech without needing a fixed vocabulary.Freedom of speechUsing the device, the participant spelt out words, responded to open-ended questions and said whatever he wanted, using some words that were not part of the decoder’s training data. He told the researchers that listening to the synthetic voice produce his speech made him “feel happy” and that it felt like his “real voice”.In other experiments, the BCI identified whether the participant was attempting to say a sentence as a question or as a statement. The system could also determine when he stressed different words in the same sentence and adjust the tone of his synthetic voice accordingly. “We are bringing in all these different elements of human speech which are really important,” says Wairagkar. Previous BCIs could produce only flat, monotone speech.“This is a bit of a paradigm shift in the sense that it can really lead to a real-life tool,” says Silvia Marchesotti, a neuroengineer at the University of Geneva in Switzerland. The system’s features “would be crucial for adoption for daily use for the patients in the future.”This article is reproduced with permission and was first published on June 11, 2025.",
  "image": "https://static.scientificamerican.com/dam/m/274f15c3752ae49c/original/brain-precentral-gyrus.jpg?m=1749674621.371\u0026w=1200",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cp\u003eA new brain-computer interface turns thoughts into singing and expressive speech in real time\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://static.scientificamerican.com/dam/m/274f15c3752ae49c/original/brain-precentral-gyrus.jpg?m=1749674621.371\u0026amp;w=600\" alt=\"Human brain with highlighted precentral gyrus, illustration\" srcset=\"https://static.scientificamerican.com/dam/m/274f15c3752ae49c/original/brain-precentral-gyrus.jpg?m=1749674621.371\u0026amp;w=600 600w, https://static.scientificamerican.com/dam/m/274f15c3752ae49c/original/brain-precentral-gyrus.jpg?m=1749674621.371\u0026amp;w=900 900w, https://static.scientificamerican.com/dam/m/274f15c3752ae49c/original/brain-precentral-gyrus.jpg?m=1749674621.371\u0026amp;w=1000 1000w, https://static.scientificamerican.com/dam/m/274f15c3752ae49c/original/brain-precentral-gyrus.jpg?m=1749674621.371\u0026amp;w=1200 1200w, https://static.scientificamerican.com/dam/m/274f15c3752ae49c/original/brain-precentral-gyrus.jpg?m=1749674621.371\u0026amp;w=1350 1350w\" sizes=\"(min-width: 900px) 900px, (min-resolution: 2dppx) 75vw, (min-resolution: 2.1dppx) 50vw, 100vw\" fetchpriority=\"high\"/\u003e\u003cfigcaption\u003e\u003cp\u003eThe motor cortex (\u003ci\u003eorange, illustration\u003c/i\u003e). Electrodes implanted in this region helped to record the speech-related brain activity of a man who could not speak intelligibly.\u003c/p\u003e \u003cp\u003eKateryna Kon/Science Photo Library/Alamy Stock Photo\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp data-block=\"sciam/paragraph\"\u003eA man with a severe speech disability is able to speak expressively and sing using a brain implant that translates his neural activity into words almost instantly. The device conveys changes of tone when he asks questions, emphasizes the words of his choice and allows him to hum a string of notes in three pitches.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe system — known as \u003ca href=\"https://www.nature.com/articles/d41586-022-01047-w\"\u003ea brain–computer interface (BCI)\u003c/a\u003e — used artificial intelligence (AI) to decode the participant’s electrical brain activity as he attempted to speak. The device is the first to reproduce not only a person’s intended words but also features of natural speech such as tone, pitch and emphasis, which help to express meaning and emotion.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eIn a study, a synthetic voice that mimicked the participant’s own spoke his words within 10 milliseconds of the neural activity that signalled his intention to speak. The system, described today in \u003ci\u003eNature\u003c/i\u003e, marks a significant improvement over earlier BCI models, \u003ca href=\"https://www.nature.com/articles/d41586-023-02682-7\"\u003ewhich streamed speech within three seconds\u003c/a\u003e or produced it only after users finished miming an entire sentence.\u003c/p\u003e\u003chr/\u003e\u003ch2\u003eOn supporting science journalism\u003c/h2\u003e\u003cp\u003eIf you\u0026#39;re enjoying this article, consider supporting our award-winning journalism by \u003ca href=\"https://www.scientificamerican.com/getsciam/\"\u003esubscribing\u003c/a\u003e. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.\u003c/p\u003e\u003chr/\u003e\u003cp data-block=\"sciam/paragraph\"\u003e“This is the holy grail in speech BCIs,” says Christian Herff, a computational neuroscientist at Maastricht University, the Netherlands, who was not involved in the study. “This is now real, spontaneous, continuous speech.”\u003c/p\u003e\u003ch2 id=\"real-time-decoder\" data-block=\"sciam/heading\"\u003eReal-time decoder\u003c/h2\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe study participant, a 45-year-old man, lost his ability to speak clearly after developing \u003ca href=\"https://www.nature.com/articles/d41586-023-01039-4\"\u003eamyotrophic lateral sclerosis, a form of motor neuron disease\u003c/a\u003e, which damages the nerves that control muscle movements, including those needed for speech. Although he could still make sounds and mouth words, his speech was slow and unclear.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eFive years after his symptoms began, the participant underwent surgery to insert 256 silicon electrodes, each 1.5-mm long, in a brain region that controls movement. Study co-author Maitreyee Wairagkar, a neuroscientist at the University of California, Davis, and her colleagues trained deep-learning algorithms to capture the signals in his brain every 10 milliseconds. Their system decodes, in real time, the sounds the man attempts to produce rather than his intended words or the constituent phonemes — the subunits of speech that form spoken words.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003e“We don’t always use words to communicate what we want. We have interjections. We have other expressive vocalizations that are not in the vocabulary,” explains Wairagkar. “In order to do that, we have adopted this approach, which is completely unrestricted.”\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe team also personalized the synthetic voice to sound like the man’s own, by training AI algorithms on recordings of interviews he had done before the onset of his disease.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe team asked the participant to attempt to make interjections such as ‘aah’, ‘ooh’ and ‘hmm’ and say made-up words. The BCI successfully produced these sounds, showing that it could generate speech without needing a fixed vocabulary.\u003c/p\u003e\u003ch2 id=\"freedom-of-speech\" data-block=\"sciam/heading\"\u003eFreedom of speech\u003c/h2\u003e\u003cp data-block=\"sciam/paragraph\"\u003eUsing the device, the participant spelt out words, responded to open-ended questions and said whatever he wanted, using some words that were not part of the decoder’s training data. He told the researchers that listening to the synthetic voice produce his speech made him “feel happy” and that it felt like his “real voice”.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eIn other experiments, the BCI identified whether the participant was attempting to say a sentence as a question or as a statement. The system could also determine when he stressed different words in the same sentence and adjust the tone of his synthetic voice accordingly. “We are bringing in all these different elements of human speech which are really important,” says Wairagkar. Previous BCIs could produce only flat, monotone speech.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003e“This is a bit of a paradigm shift in the sense that it can really lead to a real-life tool,” says Silvia Marchesotti, a neuroengineer at the University of Geneva in Switzerland. The system’s features “would be crucial for adoption for daily use for the patients in the future.”\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003e\u003ci\u003eThis article is reproduced with permission and was \u003c/i\u003e\u003ca href=\"https://www.nature.com/articles/d41586-025-01818-1\"\u003e\u003ci\u003efirst published\u003c/i\u003e\u003c/a\u003e\u003ci\u003e on June 11, 2025\u003c/i\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "6 min read",
  "publishedTime": "2025-06-12T08:00:00-04:00",
  "modifiedTime": null
}
