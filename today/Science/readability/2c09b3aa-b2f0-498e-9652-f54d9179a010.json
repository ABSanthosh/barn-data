{
  "id": "2c09b3aa-b2f0-498e-9652-f54d9179a010",
  "title": "Nobel Prize in Physics Awarded for Breakthroughs in Machine Learning",
  "link": "https://www.scientificamerican.com/article/nobel-prize-in-physics-awarded-for-breakthroughs-in-machine-learning/",
  "description": "The 2024 Nobel Prize in Physics was given to John Hopfield and Geoffrey Hinton for development of techniques that laid the foundation for revolutionary advances in artificial intelligence",
  "author": "",
  "published": "Tue, 08 Oct 2024 10:30:00 +0000",
  "source": "http://rss.sciam.com/ScientificAmerican-Global",
  "categories": null,
  "byline": "Lee Billings",
  "length": 6744,
  "excerpt": "The 2024 Nobel Prize in Physics was given to John Hopfield and Geoffrey Hinton for development of techniques that laid the foundation for revolutionary advances in artificial intelligence",
  "siteName": "Scientific American",
  "favicon": "",
  "text": "October 8, 20244 min readThe 2024 Nobel Prize in Physics was given to John Hopfield and Geoffrey Hinton for development of techniques that laid the foundation for revolutionary advances in artificial intelligenceThe Nobel Committee for Physics has announced that John Hopfield and Geoffrey Hinton won this year’s Nobel Prize in Physics “for foundational discoveries and inventions that enable machine learning with artificial neural networks.” vanbeets/Getty Images (medal)The human brain, with its billions of interconnected neurons giving rise to consciousness, is generally considered the most powerful and flexible computer in the known universe. Yet for decades scientists have been seeking to change that via machine-learning approaches that emulate the brain’s adaptive computational prowess. The 2024 Nobel Prize in Physics was awarded on Tuesday to U.S. scientist John Hopfield and British-Canadian scientist Geoffrey Hinton, each of whom used the tools of physics to develop artificial neural networks that laid the foundations for many of today’s most advanced artificial intelligence applications.Reached via telephone while in California, Hinton told the Royal Swedish Academy of Sciences that he was “flabbergasted” to learn he’d received the award. After decades of effort to advance AI, he is now one of the most prominent advocates for better safeguards. Last year he stepped down from an influential position at Google to speak more freely about the technology’s risks. “[AI] will be comparable with the industrial revolution,” he said during his telephone interview with the academy. “But instead of exceeding people in physical strength, it’s going to exceed people in intellectual ability. We have no experience of what it’s like to have things smarter than us, and it’s going to be wonderful in many respects.... But we also have to worry about a number of possible bad consequences, particularly the threat of these things getting out of control.”Artificial neural networks seek to emulate the brain’s cognitive function by using nodes with different values as stand-ins for neurons. These nodes form networks of connections, akin to the brain’s natural neural synapses, which can be made stronger or weaker through training on any arbitrary dataset. This adaptive response allows the artificial neural network to better recognize patterns within data and make subsequent predictions for the future—that is, to learn without being explicitly programmed.On supporting science journalismIf you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.“This Nobel recognizes physics inspired by biology and the broader field of biological physics,” says Ajay Gopinathan, a professor and biophysicist at the University of California, Merced. “And here this interface has led to some truly transformative advances in our understanding of these fields, as well as applications in computer science and AI.”In the early 1980s Hopfield, now a professor emeritus at Princeton University, and his colleagues devised and refined an artificial neural network—the so-called Hopfield network—inspired by the physics of atomic spin. The method proved to be transformative for storing, retrieving and reconstructing patterns in a manner thought to mimic that of the human brain.A Hopfield network’s operations can be imagined as balls rolling across a landscape of hills and valleys, where connections between nodes form topographic contours; the network is trained by finding values for those connections that minimize their energy differences. Describing the process in a 1987 edition of Scientific American, Hopfield and his co-author explained that the network “computes by following a path that decreases the computational energy until the path reaches the bottom of a valley, just as a raindrop moves downhill to minimize its gravitational potential energy.” The technique proved broadly applicable to a host of optimization problems—mathematical quandaries in which one ideal solution is selected from a very large number of possibilities.Hinton, now a professor emeritus at the University of Toronto, worked with his colleagues to advance Hopfield’s approach, making it the basis for a more sophisticated artificial neural network called the Boltzmann machine, which leveraged feedbacks between multiple node layers to infer statistical distributions of patterns from training data. Crucially, this more advanced artificial neural network could use “hidden” layers of nodes to catch and correct computational errors without prohibitive computational costs. Hinton’s method excels at pattern recognition and can be used, for example, to classify images or create novel elaborations of an observed pattern.Hinton summarized many of the approach’s core ideas and possible applications in a 1992 article for Scientific American, in which he predicted that biologically inspired machine learning would eventually lead to “many new applications of artificial neural networks.” Today the technique has helped fuel the ongoing explosion of progress in AI that is transforming myriad sectors of our society.“Artificial neural networks mimic biological neurons in the sense that they take in pieces of information (analogs to chemical signals for a biological neuron), compute a weighted sum of these pieces of information (factoring in the significance of the inputs in the ‘decision-making’ process) and produce an output (an analog to a neuron firing or at rest),” says Jerome Delhommelle, an associate professor and machine-learning expert at the University of Massachusetts Lowell. “Machine-learning models can learn intricate interdependencies from data, make predictions on the ideal composition of materials for a given functionality and even discover as-yet-unknown governing equations in complex systems. Machine learning is poised to make great contributions to physics.”Ellen Moons, a professor at Karlstad University in Sweden and chair of the Nobel Committee for Physics, described the promise and peril of these developments in remarks at a press conference in Stockholm on Tuesday. “The laureates’ discoveries and inventions form the building blocks of machine learning that can aid humans in making faster and more reliable decisions—for instance, when diagnosing medical conditions. However, while machine learning has enormous benefits, its rapid development has also raised concerns about our future. Collectively, humans carry the responsibility for using this new technology in a safe and ethical way for the greatest benefits of humankind.”",
  "image": "https://static.scientificamerican.com/dam/m/320ffa27091771a9/original/nobels-2024-physics.jpg?w=1200",
  "html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003eOctober 8, 2024\u003c/p\u003e\u003cp\u003e4 min read\u003c/p\u003e\u003c/div\u003e\u003cp\u003eThe 2024 Nobel Prize in Physics was given to John Hopfield and Geoffrey Hinton for development of techniques that laid the foundation for revolutionary advances in artificial intelligence\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"https://static.scientificamerican.com/dam/m/320ffa27091771a9/original/nobels-2024-physics.jpg?w=600\" alt=\"Nobel Prize in Physics medal\" srcset=\"https://static.scientificamerican.com/dam/m/320ffa27091771a9/original/nobels-2024-physics.jpg?w=600 600w, https://static.scientificamerican.com/dam/m/320ffa27091771a9/original/nobels-2024-physics.jpg?w=900 900w, https://static.scientificamerican.com/dam/m/320ffa27091771a9/original/nobels-2024-physics.jpg?w=1000 1000w, https://static.scientificamerican.com/dam/m/320ffa27091771a9/original/nobels-2024-physics.jpg?w=1200 1200w, https://static.scientificamerican.com/dam/m/320ffa27091771a9/original/nobels-2024-physics.jpg?w=1350 1350w\" sizes=\"(min-width: 900px) 900px, (min-resolution: 2dppx) 75vw, (min-resolution: 2.1dppx) 50vw, 100vw\" fetchpriority=\"high\"/\u003e\u003cfigcaption\u003e\u003cp\u003eThe Nobel Committee for Physics has announced that John Hopfield and Geoffrey Hinton won this year’s Nobel Prize in Physics “for foundational discoveries and inventions that enable machine learning with artificial neural networks.”\u003c/p\u003e \u003cp\u003evanbeets/Getty Images (\u003ci\u003emedal\u003c/i\u003e)\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv\u003e\u003cp data-block=\"sciam/paragraph\"\u003eThe human brain, with its billions of interconnected neurons giving rise to consciousness, is generally considered the most powerful and flexible computer in the known universe. Yet for decades scientists have been seeking to change that via \u003ca href=\"https://www.scientificamerican.com/video/what-is-machine-learning-and-how-does-it-work-heres-a-short-video-primer/\"\u003emachine-learning\u003c/a\u003e approaches that emulate the brain’s adaptive computational prowess. The 2024 Nobel Prize in Physics was awarded on Tuesday to U.S. scientist John Hopfield and British-Canadian scientist Geoffrey Hinton, each of whom used the tools of physics to develop \u003ca href=\"https://www.scientificamerican.com/article/springtime-for-ai-the-rise-of-deep-learning/\"\u003eartificial neural networks\u003c/a\u003e that laid the foundations for many of today’s most advanced artificial intelligence applications.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eReached via telephone while in California, Hinton told the Royal Swedish Academy of Sciences that he was “flabbergasted” to learn he’d received the award. After decades of effort to advance AI, he is now one of the most prominent advocates for better safeguards. Last year he stepped down from an influential position at Google to speak more freely about the technology’s risks. “[AI] will be comparable with the industrial revolution,” he said during his telephone interview with the academy. “But instead of exceeding people in physical strength, it’s going to exceed people in intellectual ability. We have no experience of what it’s like to have things smarter than us, and it’s going to be wonderful in many respects.... But we also have to worry about a number of possible bad consequences, particularly the threat of these things getting out of control.”\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eArtificial neural networks seek to emulate the brain’s cognitive function by using nodes with different values as stand-ins for neurons. These nodes form networks of connections, akin to the brain’s natural neural synapses, which can be made stronger or weaker through training on any arbitrary dataset. This adaptive response allows the artificial neural network to better recognize patterns within data and make subsequent predictions for the future—that is, to learn without being explicitly programmed.\u003c/p\u003e\u003chr/\u003e\u003ch2\u003eOn supporting science journalism\u003c/h2\u003e\u003cp\u003eIf you\u0026#39;re enjoying this article, consider supporting our award-winning journalism by \u003ca href=\"https://www.scientificamerican.com/getsciam/\"\u003esubscribing\u003c/a\u003e. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.\u003c/p\u003e\u003chr/\u003e\u003cp data-block=\"sciam/paragraph\"\u003e“This Nobel recognizes physics inspired by biology and the broader field of biological physics,” says Ajay Gopinathan, a professor and biophysicist at the University of California, Merced. “And here this interface has led to some truly transformative advances in our understanding of these fields, as well as applications in computer science and AI.”\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eIn the early 1980s Hopfield, now a professor emeritus at Princeton University, and his colleagues devised and refined an artificial neural network—the so-called Hopfield network—inspired by the physics of atomic spin. The method proved to be transformative for storing, retrieving and reconstructing patterns in a manner thought to mimic that of the human brain.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eA Hopfield network’s operations can be imagined as balls rolling across a landscape of hills and valleys, where connections between nodes form topographic contours; the network is trained by finding values for those connections that minimize their energy differences. \u003ca href=\"https://www.scientificamerican.com/issue/sa/1987/12-01/\"\u003eDescribing the process in a 1987 edition of \u003ci\u003eScientific American\u003c/i\u003e\u003c/a\u003e\u003ci\u003e,\u003c/i\u003e Hopfield and his co-author explained that the network “computes by following a path that decreases the computational energy until the path reaches the bottom of a valley, just as a raindrop moves downhill to minimize its gravitational potential energy.” The technique proved broadly applicable to a host of optimization problems—mathematical quandaries in which one ideal solution is selected from a very large number of possibilities.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eHinton, now a professor emeritus at the University of Toronto, worked with his colleagues to advance Hopfield’s approach, making it the basis for a more sophisticated artificial neural network called the Boltzmann machine, which leveraged feedbacks between multiple node layers to infer statistical distributions of patterns from training data. Crucially, this more advanced artificial neural network could use “hidden” layers of nodes to catch and correct computational errors without prohibitive computational costs. Hinton’s method excels at pattern recognition and can be used, for example, to classify images or create novel elaborations of an observed pattern.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eHinton summarized many of the approach’s core ideas and possible applications in \u003ca href=\"https://www.scientificamerican.com/issue/sa/1992/09-01/\"\u003ea 1992 article\u003c/a\u003e for \u003ci\u003eScientific American,\u003c/i\u003e in which he predicted that biologically inspired machine learning would eventually lead to “many new applications of artificial neural networks.” Today the technique has helped fuel the ongoing explosion of progress in AI that is transforming myriad sectors of our society.\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003e“Artificial neural networks mimic biological neurons in the sense that they take in pieces of information (analogs to chemical signals for a biological neuron), compute a weighted sum of these pieces of information (factoring in the significance of the inputs in the ‘decision-making’ process) and produce an output (an analog to a neuron firing or at rest),” says Jerome Delhommelle, an associate professor and machine-learning expert at the University of Massachusetts Lowell. “Machine-learning models can learn intricate interdependencies from data, make predictions on the ideal composition of materials for a given functionality and even discover as-yet-unknown governing equations in complex systems. Machine learning is poised to make great contributions to physics.”\u003c/p\u003e\u003cp data-block=\"sciam/paragraph\"\u003eEllen Moons, a professor at Karlstad University in Sweden and chair of the Nobel Committee for Physics, described the promise and peril of these developments in remarks at a press conference in Stockholm on Tuesday. “The laureates’ discoveries and inventions form the building blocks of machine learning that can aid humans in making faster and more reliable decisions—for instance, when diagnosing medical conditions. However, while machine learning has enormous benefits, its rapid development has also raised concerns about our future. Collectively, humans carry the responsibility for using this new technology in a safe and ethical way for the greatest benefits of humankind.”\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
  "readingTime": "8 min read",
  "publishedTime": "2024-10-08T10:30:00Z",
  "modifiedTime": null
}
